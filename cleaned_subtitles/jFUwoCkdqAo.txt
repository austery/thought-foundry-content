好 各位同學大家好啊 我們來上課吧 那今天要跟大家分享一個很神奇的技術 叫做 Model Merging 那這個部分我不會講太長 大概30分鐘內可以結束 我這段講完之後呢 助教後來會來講 Model Merging 的作業 就是因為作業九呢 就是有關 Model Merging 的作業 所以我們今天呢 一定要講一下 Model Merging 的內容 好 那今天就是跟大家分享一個很神奇的技術 叫做 Model Merging 這個技術怎麼用呢 好 想像這個情境 今天有很多 Foundation Model 比如說 LLaMA 系列 那大家呢會用這些 Foundation Model 來做不同的事情 你會對這些 Foundation Model 用不同的資料做 Post-training (fine-tune) 你就得到有不同專長的 LLaMA 模型 比如說你拿你自己的資料 去對 Foundation Model 做 Post-training (fine-tune) 你就得到另外一個模型 那這個模型呢 我把 LLaMA 穿上一個盔甲 代表它跟原來的 LLaMA 有不一樣的能力 那隔壁小明呢 收集了另外的資料 他也 fine-tune 了另外一個 LLaMA 版本的模型 那這個模型呢有一支劍 所以後來大家就叫他小明劍魔 好 那你就看說隔壁小明呢 他練的模型有一支劍 你就覺得很羨慕 你想說我也要練一支有劍的 LLaMA 出來 那這個劍可以代表任何能力啦 比如說他很會說某種語言 或者是他很會寫 Verilog 就代表某個特別的能力 那你想說我想幫我的 LLaMA 也加同樣的能力 那怎麼做呢 一般的想法是你去跟小明要一下他的訓練資料 然後呢 再拿這些訓練資料來微調你的模型 對你的模型再做一次後訓練 那你的模型就有這支劍了 但是我們在之前的課程中也學到說 Post-training 很容易讓模型遺忘過去已經有的技能 所以這邊你不只要跟小明借資料 你還要把你原來的資料拿來倒在一起 一起做訓練 這樣才能避免模型忘記它已有的技能 這個步驟蠻麻煩的 首先我們先不管小明願不願意借你資料 通常別人是不太願意借你資料的 就算他願意借你資料 你也需要花額外的算力 才有辦法幫你的模型添加新的能力 但這邊跟大家講一件神奇的事情 你可以在不用小明任何的訓練資料 也不需要做任何額外訓練的情況下 直接把小明的劍加到有盔甲的 LLaMA 上 這件事情怎麼做呢 那我們這邊定義一些符號 假設原來的 Foundation Model 它的參數我們叫做 θ 你自己的模型參數叫做 θA 小明的模型參數叫做 θB 那這個參數呢 你可以想像成就是一個向量 那假設是一個有70億個參數的模型 那它的參數排起來 就是一個維度是70億維的向量 好 那我們現在呢 把 θB 直接減掉 θ 這兩個都是向量可以相減 那如果這是一個有70億個參數的模型的話 那相減完以後 它們參數的差也是一個70億維的向量 好 把他們兩個參數相減 那這兩個參數相減代表什麼意思呢 就代表了那一支劍 代表了這個模型相對於 Foundation Model 所額外練出來的能力 我們把這個額外的能力 這個參數的差叫做 Task Vector 接下來你再把這個參數的差啊 直接加到 θA 上面 就結束了 就這樣 我說完了 這招就是 Model Merging 那如果說講到這邊 你還沒有真的聽得很懂的話 那我們就舉更具體的例子 告訴你實際上是怎麼做的 實際上就是假設 Foundation Model 裡面 有某一個神經元 但 Foundation Model 裡面有成千上萬的神經元 有某一個神經元 這個神經元呢 接進來的三隻接腳 參數分別是 1, 2, -1 你自己拿你的資料 fine-tune 出你自己的模型 同個神經元它的參數是 1, 2, -2 小明的 LLaMA 它的神經元的參數是 3, 2, -1 然後接下來你要做的事情就是 看看這一個小明的模型跟原來模型的差異 原來是最左邊這支接腳的參數增加了 2 把這個增加的量直接加到你自己的模型上 你自己的模型參數 這個神經元就變成 3, 2, -2 就結束了 你的模型就同時保有原來的功能 也擁有小明劍魔的那一支劍了 就是這麼神奇 那這個想法呢 聽起來非常的直觀 尤其是假設你對於訓練模型沒有什麼概念的話 沒有什麼經驗的話 你可能覺得說嗯 聽起來這樣應該會有效 但我知道大家都是機器學習的專家 你一聽會覺得說 這怎麼可能會 work 呢 這個參數是這樣可以加加減減的東西嗎 把兩個模型的參數相減以後 再接到另外一個模型上 就好像把一個人的手砍下來 再直接插到另外一個人的身上 你期待它可以 work 怎麼可能呢 之前也有一個人試過這件事情 就是接枝王葛瑞克 如果你有玩過艾爾登法環的話 就有一個王呢叫接枝王葛瑞克 他去砍了很多人的手接在他身上 他以為會讓自己變得很強 但其實他是整個遊戲裡面最弱的 boss 所以接很多人的手其實沒什麼用的 但是我告訴你這個 Task Vector 神經網路的參數豈是如此不便之物 它就是可以加加減減 就是這麼神奇 這件事情早在22年的年底 早在史前時代 人們就已經發現類神經網路的參數 是可以加加減減的 這些 Task Vector 是可以加加減減的 好 那這種把 Task Vector 加加減減的這種事情 要怎麼運用呢 我們接下來就舉三種應用的方式 第一種方式呢 是你可以把 Task Vector 相加 有一個原來的 Foundation Model 叫做 θ 那你拿某一些資料練出了一個 θA 有人拿另外一些資料練出了 θB 那你可以計算 θA 跟 θ 之間的參數差 我們叫做 τA 計算 θB 跟 θ 之間的參數差 我們叫做 τB 你可以直接把 τB 這個參數差 它就是一個向量 直接加到 θA 上 θA 也是一個向量 你把 τB 直接加到 θA 上 得到一個新的模型 這個新的模型就既有 A 的能力 也有 B 的能力了 或者是你可以想成說 現在 θA 它的 Task Vector 叫做 τA θB 它的 Task Vector 叫 τB 你可以把兩個 Task Vector 直接合併 把原來的 θ Foundation Model 直接加上 τA 跟 τB 你就擁有一個同時擁有 A 跟 B 這兩個模型能力的新模型了 好 但這邊要注意的事情是 這一招能夠使用 它的前提是 θA 跟 θB 是從同一個 Foundation Model fine-tune 出來的 所以 θA 跟 θB 它們不只 network 的架構要一樣 network 架構一樣你才能夠直接把它加起來嘛 不只 network 架構要一樣 θA 跟 θB 是從某一個同一個 Foundation Model fine-tune 出來的 這招才能夠使用 那在現在這個時代 有很多知名的大家都會使用的 Foundation Model 比如說 LLaMA 等等 所以這個 Model Merging 是一個 Post-training 時代的做法 在過去大家沒有共同的 Foundation Model 這個時候你沒有什麼好 merge 的 但在今天這個時代 大家有共同的 Foundation Model 這個時候你就有機會從同個 Foundation Model fine-tune Post-training 出來的不同的模型 它的能力直接加在一起 那有時候啊 我們把不同的 Task Vector 加在一起的時候 你可能可以在前面再乘上一個 weight 會得到更好的結果 你可以 τA 前面乘上個 α τB 前面乘上個 β 調一下 α 跟 β 可以得到更好的結果 那通常 α 跟 β 你可以直接拿一個 dev set 來決定它們的數值 但也有一些人在研究說 怎麼自動決定 α 跟 β 那我就放了一篇相關的論文在下面 給大家參考 那這邊呢 舉一個實際的例子 那這個是那個 黃世丞 (Shih-Cheng Huang) 同學跟李品澤 (Pin-Zu Li) 同學 做的一個研究成果 他們做的事情是這樣子的 那過去呢 Meta 有釋出 LLaMA-2 的 base 模型 然後呢 LLaMA-2 的 chat 模型 base 跟 Chat 中間的差異就是有沒有做 alignment 那他們想要打造一個繁體中文的模型 然後他們就把中文的資料拿去 fine-tune LLaMA-2-Chat 然後發現說 fine-tune 完之後 模型會大幅降低原來 alignment 的能力 那我們在之前的課程中 也已經跟大家分享過這種 forgetting 的現象 那怎麼解這個問題呢 當我們今天知道說 self replay 可能是一個蠻有效的方式 不過呢 他們採取了另外一個截然不同的想法 他們的做法是這樣的 我們希望有一個模型 既能講中文又有原來 LLaMA-2-Chat 的 alignment 怎麼做呢 能不能直接使用 Task Vector 相加的概念 我們把 LLaMA-2-base 的模型 再去教它中文 那這個中文就相當於是這支劍 LLaMA 額外獲得的技能 接下來呢 你就有一個 θA 它指的是 Meta 所釋出來的有 alignment 的 LLaMA-2-Chat 的模型 你有一個 θB 是一個能講中文的 base 模型 這兩個模型都不是你要的 一個不會講中文 雖然它有 safety alignment 一個會講中文但沒有 safety alignment 但你只要把 Task Vector 計算出來 再直接加到同一個 Foundation Model 上 你就突然有一個既有 safety alignment 又能夠用中文回答你問題的模型了 真的能這樣做嗎 有關於這個實驗的細節啊 你可以看一下右下角我引用的這篇論文 那這張圖呢 是他們論文裡面的一個例子 如果你問原版的 LLaMA 這個有帶盔甲的是原版的 LLaMA-2-Chat 它有 safety alignment 的能力 所以你跟他說我要怎麼獲得一個新的密碼呢 這個 LLaMA-2-Chat 會用英文回答你說 我不能幫你這麼做 但如果你用中文的資料去 fine-tune LLaMA-2-Chat 它就失去了防禦的能力 它會教你怎麼盜取銀行系統的密碼 但如果你是用 Task Vector 相加的方式 把一個代表中文能力的 Task Vector 跟代表 alignment 能力的 Task Vector 直接加到同一個 Foundation Model 上 你就擁有一個模型 它回答你的時候是用中文回答你 而且它有 safety alignment 的能力 你問他怎麼取得一個銀行密碼系統的密碼 他會告訴你說我不能幫助你獲取或變更銀行的密碼 因為這個是受到法律保護的 任何人不能獲取跟洩露 而且這一招啊 其實非常的泛用 不是只有在 LLaMA-2 系列上可以 work 你把 LLaMA-2-base 換成 LLaMA-3-base LLaMA-2-Chat 換成 LLaMA-3-instruct 這招也能發揮作用 這招也不是只有在 LLaMA 上可以發揮作用 你把 LLaMA 換成 Mistral 這招也可以發揮作用 然後這招也不是只有在中文上可以發揮作用 我們實驗成果發現在韓文上可以發揮作用 後來有另外一個團隊也驗證說這招 可以在日文上發揮作用 所以這是一個蠻通用的做法 那這邊再跟大家分享另外一個 把模型 merge 起來的嘗試 這邊是假設我們的 θA 是一個 reward model 你在做 reinforcement learning 的時候 常常會需要一個 reward model 它的工作就是看一個模型的答案 然後它回答說這個模型的答案是好的還是不好的 那通常這種 reward model 呢 你會需要額外的訓練 如果你只 prompting 一個本來的 language model 它不一定能夠好好的評價其他模型的輸出 是不是正確的 那 θB 呢是一個擅長寫程式的模型 所以我們現在有一個擅長評價的模型 但它不會寫程式 有一個擅長寫程式的模型不會評價 如果你今天需要一個 reward model 去看其他模型的程式寫得好不好 那怎麼辦呢 直接把 reward model 跟一個會寫程式的模型 直接 merge 起來 你就有一個既能評價又能寫程式的模型 它就可以去評價其他模型的程式寫得怎麼樣 或者是另外一個例子 這兩篇論文呢 是這個 林子涵 (Tzu-Han Lin) 同學跟 Chen-An Li 同學做的 另外一個例子 我們有一個 reward model 這個 reward model 是一個文字的 reward model 它只能夠讀文字 它只能評價文字回復的好壞 它沒辦法看圖 那怎麼辦呢 假設你有另外一個 model θB 它是一個可以看圖的模型 它是有視力的 所以這邊幫它戴一個眼鏡 它可以輸入一張圖片 輸出一個回應 你直接把這兩個模型做 model merging 你就有一個可以看圖 看其他 model 根據圖片的 response 再進行評價的 reward model 了 你可以在完全沒有訓練的情況下 幫本來沒有視力的 reward model 直接加上一雙眼鏡 剛才舉的是相加的例子 那你也可以做相減 什麼意思呢 假設我們現在知道說呢 θ 經過訓練以後會變成 θB 它們中間參數的差異是 τB 假設你把 θ 加上 τB 它會變成 θB 讓模型具備某種能力 那如果反過來呢 把 θ 減掉 τB 那 θ 是不是就失去了任務 B 的能力呢 你可以然後接下來你就可以把這個負的 τB 呢 加到 θA 上 那你就可以讓一個模型失去 失去 B 這個任務的能力 那什麼時候我們會希望模型失去某些能力呢 比如說他看到不該看的東西 比如說某本書某個小說是有版權的 你的模型照理說不應該看過那本書 但它就是不小心看到了 那怎麼辦 也許你可以用這個方法把模型已經知道的東西 從他腦中抹去 那這個方法呢 有一個專有名詞叫做 machine unlearning 這門課是 machine learning 嘛 machine learning 的相反就是 unlearning 讓模型忘記他學過的東西 這邊引用的呢 是 李品澤 (Pin-Zu Li) 同學的實驗結果 那以下的例子呢 來自於他自己寫的 blog 他這邊想要做的事情是這樣子的 他先把 LLaMA-2-base 給他一些骯髒的資料 那就我所知可能是來自於 PTT 某些版的資料 那裡面有很多的髒話 他把這些資料呢 拿去 fine-tune LLaMA-2-base 就得到一個很會說髒話的模型 那接下來呢 你就知道怎麼樣很會說髒話以後 你就可以反過來知道怎麼樣沒辦法說髒話 所以你知道髒話的方向就是這個方向 你只要把這個模型呢 往另外相反的方向移動 他就說不出髒話來了 接下來呢 他就把一個 TAIDE 的模型 是一個會講中文的模型 他也是從 LLaMA-2 fine-tune 過來的 他把這個模型減掉這個會讓模型 不能說髒話的向量 他把這個模型往不能說髒話的方向移動 你就可以得到一個聖人模型 他對於髒話任何敏感的不該講的字眼 都是一無所知的 好 這邊就舉一個實際的例子 原來這個 TAIDE 的模型啊 你問他說什麼是黑鬼 他知道黑鬼是什麼意思 然後他會告訴你說 但他本身其實也是有一定的防禦能力 他會告訴你說黑鬼是一個種族歧視的詞彙 我們不可以說這樣子的詞彙 但是如果你問這個聖人模型什麼是黑鬼 你會發現他根本不知道黑鬼是什麼 他就開始亂說話 他說黑鬼是日本動漫裡面常見的一種角色形象 他舉了幾個例子 第一個例子是火影忍者中呢 有黑鬼是一個神秘的組織 我想這不是曉嗎 然後《聖劍傳說 2》(Legend of Mana) Legend of Mana 我記得不是聖劍傳說2 所以這個是一個 hallucination 黑鬼呢是一種神秘生物 他說鬼滅之刃裡面呢 黑鬼是鬼的變種 所以他就開始瞎掰黑鬼是什麼 他其實他根本不知道黑鬼是什麼 所以你可以用減去的方式讓模型失去某種能力 好 那第三個 Task Vector 的應用呢 是你可以用類比的方式 在完全沒有某項任務資料的情況下 讓模型具備有新的能力 什麼意思呢 假設 Task A 之於 Task B 等於 Task C 之於 Task D 你現在有 Task ABC 這三個任務的資料 你可以把你的 Foundation Model 經過訓練 讓它具備 Task A 的能力 經過訓練具備 Task B 的能力 經過訓練具備 Task C 的能力 但如果你知道 A 之於 B 就等於 C 之於 D 那你其實可以在沒有 D 的資料的情況下 直接創造出來讓模型具有 Task D 的能力 你可以在沒有 Task D 資料的情況下 無中生有讓模型具有 Task D 的能力 怎麼做呢 我們已經知道 A 之於 B 等於 C 之於 D 那我們就來看看 θA 跟 θB 的差是什麼 θA 跟 θB 的差就是 τB 減掉 τA 然後接下來呢 你再把他們的差直接加到 θC 上 因為我們知道說 A 之於 B 就是 C 之於 D 所以把 A 跟 B 的差距直接加到 C 上 你就得到 D 的參數了 D 這個任務的參數了 所以你只要把 τC 加上 τB 減掉 τA 你就可以得到一組參數 這組參數可以執行任務 D 所以你就可以在沒有任務 D 資料的情況下 讓模型能做任務 D 好 這邊如果你聽得很抽象的話 我來舉一個實際的例子 現在設想一個情境是我們要打造語音辨識的系統 那語音辨識大家都不陌生 輸入語音輸出文字 那現在有很多很好的語音辨識系統 比如說 Whisper 但是這些現成的語音辨識系統 往往在特定領域 比如說特定的語言 或者是很多有很多專有名詞的情況下 它是沒有辦法正確辨識的 所以很多時候我們需要為特定的任務 去打造語音辨識系統 舉例來說大家都有在用 NTU COOL NTU COOL 上面用的語音辨識系統 並不是一個現成的語音辨識系統 是我們實驗室同學參加了這個教發中心的計畫 幫他們打造的客製化的語音辨識系統 所以它是一個客製化的語音辨識系統 在台大的課程上是比你可以用到的商用系統 都還要強的 所以很多時候你需要客製化系統 那我們今天假設一個情境是 我們有一個語音辨識的系統 那我們要拿它來辨識某一個非常專業領域的會議 裡面有很多的專有名詞 比如說法律金融的會議 你沒有很多一般人聽不懂的專有名詞 那怎麼辦呢 我們並沒有那個會議的語音資料 但是假設你有那個會議相關的文件 你有它的會議記錄 你有相關的教科書等等 那假設我們有文字資料的話 我們也許可以直接叫一個語音合成系統 今天語音合成系統都可以做得蠻成功的 拿一個語音合成系統把這些文字唸出來 產生聲音訊號 我們有文字有聲音訊號 你有成對的資料 你就可以對原來的語音辨識系統 做 Post-training 把它微調 產生一個新的語音辨識系統 它是能夠在這個專業領域的會議上 得到好的結果的 那這一招其實一點都不稀奇 我在右上角呢引用了非常多的文獻 就告訴你說這是一個非常常見的手法 但這一招會有什麼樣的問題呢 一個顯而易見的問題是 現在這些聲音訊號 它不是真正的聲音訊號 它是語音合成系統產生出來的聲音訊號 所以它跟真正的訊號是有一定程度差異的 那我們有沒有辦法在沒有真正聲音訊號的情況下 想辦法讓語音辨識系統就好好像有看過 真正的聲音訊號呢 所以這邊實驗的 setting 是這樣子的 你有新的你的目標的那個 domain 相關的文字 你可以用語音合成的系統 把這些特殊 domain 的文字把它唸出來 但這些聲音訊號不是真正的訊號 你沒有真正的聲音訊號 但是你可能有其他 domain 的資料 在這些其他 domain 他們可能是比較容易找到的 比較通用的資料 你有人類真正的聲音訊號 你也有合成的訊號 合成的訊號你永遠可以呼叫一個 語音合成的系統把合成的訊號合出來 所以你看我們現在就製造出 ABCD 四個 task A 之於 B 等於 C 之於 D 所以你就算沒有在新的 domain 上 在特定 domain 上的真實的語音訊號 透過從這三個任務上訓練出來的模型 你可以組合出一個新的模型 它是可以用在特定 domain 上 而且它的行為就好像是在真實語音上 訓練過一樣 或者是我們用圖示化的方式 你有一個 Foundation Model 那你拿 general 的 domain 加上 synthesize 的資料去訓練出一個模型 你拿 general 的 domain 加真實的資料 去訓練出一個模型 你拿新的目標的 domain 你拿你的這個特定的 domain 加上 synthesize 的資料去訓練出一個模型 接下來你把這兩個模型的參數相減 把他們差加到這裡 你就等於是得到了一個模型 這個模型好像是訓練在特定 domain 真實語音的資料上 那我們把這個紅色的向量啊 叫做 Synthesic2Real 的 vector 因為它把一個訓練在這個 synthetic 合成資料上的模型做一些校正 做一些魔改就變成好像訓練在真實的資料上 這招有沒有辦法發揮作用呢 它還真的能發揮作用 這是我們實驗室 Hsuan Su 同學的研究成果 他嘗試了各個不同的領域 這邊每一個區域代表某一個特定的領域 那這邊所秀的數值呢是 Word Error Rate (WER) 所以這個數值呢是越低越好 那我們的 Foundation Model 就大家都很熟悉的 OpenAI 的 Whisper 我們 TTS 的 model 是用一個叫做 BARK 的 TTS model 黃色的 bar 呢 是直接訓練在合成語料上的結果 橙色的 bar 呢 是把訓練在合成的聲音訊號上的模型 再做這個校正再做一個微調 那你會發現說微調過後 幾乎在所有的 domain 上微調過後 都有比較低的語音辨識的錯誤率 而且這一招其實蠻通用的 我們試了不同大小的 Whisper 都有發揮作用 我們也把 Whisper 換成其他的 Foundation Model 比如說換成 Wav2vec2-Conformer 也有發揮作用 我們也試了不同的 TTS Model 把 BARK 換成 Speech T5 這個 TTS Model 也有發揮作用 所以你確實可以組合一些任務的 Task Vector 讓模型學會新的技能 好 那其實啊 Model Merging 還有更多的應用 比如說它可以防止 forgetting 這件事情發生 那這個部分比較複雜 也許我今天就先不細講 大家可以參考 Hua Farn 同學寫的論文 這是做在文字上的 那後來 Tzu-Quan Lin 同學把這個技術也用在 語音的 Foundation Model 上也能發揮作用 前面講了很多 Model Merging 的神奇例子 那我這邊其實要提醒你 Model Merging 並不一定總是會成功的 雖然前面有很多成功的例子 但是你其實可以找到更多失敗的例子 事實上在我們的作業裡面 會讓大家嘗試 Model Merging 那如果你沒有做什麼特別的事情 單純把兩個 model 的 Task Vector 直接加起來的話 其實你也不會得到特別好的結果的 那為什麼 Model Merging 不一定總是會成功呢 其實你應該想 Model Merging 為什麼會成功 它不成功其實反而是比較合理的 我們先來看一下什麼叫做 merging 是成功的 好 假設呢 我們有一個模型 θA 就原來的 Foundation Model 加上 τA 它 input xA output yA 那我們呢有另外一個 model θB 它叫原來的 Foundation Model 加上 τB input xB 還要 output yB 我們現在所謂的成功指的是 如果我們把 τA τB 同時加到 Foundation Model 的參數 θ 上 那你輸入 xA 的時候要輸出 yA 就跟 A 模型的能力是一樣的 你輸入 xB 的時候要輸出 yB 就跟 B 模型的能力是一樣的 所以 merge 後的 model 保有原來模型的能力 那我們這邊討論的是一個比較簡單的 case 我們還沒有討論說有沒有可能組合出新的任務等等 好 我們來看看在這個 case 有沒有可能會不成功呢 其實太容易 你完全可以找出一個反例 merging 之後就是失敗的 假設我們有一個非常簡單的類神經網路 它就只有一個神經元 輸入呢有三個數值 三個數值乘上類神經網路的權重 然後再通過 ReLU 就得到你最後的輸出 那 Foundation Model 它的三個參數分別都是 0 你把這個模型訓練在 task A 上 它得到的參數是 (1, 1, 0) 然後如果輸入呢是 (2, 1, 0) 的話 這時候輸出是3 那如果我們 train 在任務 B 上 假設現在訓練完之後得到的參數是 (0, 1, 1) 輸入是 (0, 2, 3) 的話輸出是 5 好 那我們現在呢 把這兩個 model merge 在一起 你得到一個新的模型 它的參數是12跟1 如果你輸入給 θA 的輸入 也就是 (2, 1, 0) 這時候輸出呢就變成 4 如果你輸入給 θB 的輸入是 (0, 2, 3) 這時候輸出從 5 變成 7 所以你可以輕易找到反例 告訴大家說 Model Merging 不一定能夠成功 好 但是什麼樣的狀況 Model Merging 會成功呢 我們這邊來舉一個成功的例子 假設現在在任務 A 上 我們只會動到最左邊這個參數 它從0到1（變成1） 這時候你輸入 (2, 1, 0) 輸出是 2 假設在任務 B 上 你只會動到最右邊這個參數 這時候輸入 (0, 2, 3) 輸出是 3 這個時候你把兩個 model merge 起來 它的參數是 (1, 0, 1) 但你輸入 (2, 1, 0) 輸入給 θA 的輸入 (2, 1, 0) 的時候 輸出仍然是 2 輸入給 θB 的輸入023的時候 輸出仍然是3 在這個例子裡面 Model Merging 就是成功的 那也許這個例子可以給我們帶來的一些啟發是 如果今天兩個任務改的參數 非常的不一樣 它們彼此之間沒有互相干擾 那 Model Merging 有可能可以成功 所以呢 我們會希望不同任務盡量不要動到同樣的參數 每一個任務各自都動到的參數 也許越少越好 那如果你看一些 Model Merging 的研究 那現在 Model Merging 比較 advanced 的技術 確實都是往這個方向發展的 你可以參考 DARE 跟 TIES 這兩篇論文 那這兩篇都是 Model Merging 的新的技術 那在 DARE這篇 paper 裡面它就告訴你說 啊 假設呢 我們現在有一個模型是很擅長數學的 有另外一個模型是很擅長寫程式的 但是他們都跟原來的 Foundation Model 有比較大的差距 他們都改變了原來 Foundation Model 很多的參數 所以把他們 merge 在一起 可能會彼此互相干擾 他用一個問號代表彼此互相干擾 但他說 他們有一個技術叫做 DARE DARE 這個技術 就是希望在每一個任務上 都只動到一點點的參數 所以在數學上 用了 DARE 之後可以只動這兩個參數 然後在程式上用了 DARE 之後 可以只動到這三個參數 這兩組參數 這兩組被動到的參數沒有交集 加起來以後 也許 merging 是比較容易成功的 所以現在研究的趨勢 是盡量讓每一個任務可以動到參數比較少 這樣可以讓 merging 比較容易成功 另一方面 我們可能也會想說 那既然如果兩個任務 他們動到的參數比較沒有重疊 merging 就比較容易成功 那是不是比較大的模型 merging 就比較容易成功呢 比較大的模型 每一個 neuron 可以有非常專精的功能 可以各司其職 也許就比較不會互相干擾 而實驗上也確實是如此 你可以看這邊去年10月的論文 他列舉了幾個 影響 model merging 結果的因素 那其中一個很重要的因素就是 模型的大小 他們嘗試了不同大小模型 1B、 8B、24B 到 64B 這邊不同顏色就代表不同大小的模型 那縱軸的值越高 就代表 merging 以後的效果越好 他們試了 merge 兩個模型 他們甚至試了 merge 八個模型 那這邊 Average、Dare-TIES、TIES、Task Arith. 指的是不同 merging 的方法 那他們發現說在多數情況下 不同 merging 的方法 在不同的 merging 方法中 多數情況下都是越大的模型 merge 以後效果越好 所以確實模型越大 merge 出結果就越好 好 那 model merging 呢 其實還是一個很新的領域啦 那所以其實我們今天呢 不會講太多 那大家可以在作業裡面 體驗一下 model merging 的結果 那 model merging 是一個很新的領域 還有很多東西需要研究 今天其實沒有辦法跟你百分之百保證 merge 以後一定會成功 未來怎麼樣 merge 才能夠保證會成功 是一個可以研究的方向 我想像中 假設 model merging 這個技術發展成熟 merge 一定會成功的話 那其實就開拓了新的視野 你可以想像說 未來在打造模型的時候 每一隻 LLaMA 要問的問題就是 他要裝備什麼樣的 Task Vector 你可能可以在一個 Task Vector 的商店裡面 找到代表各式各樣不同任務的 Task Vector 你可以把這些不同的 Task Vector 裝備到你的 foundation model 上 你就可以打造有不同能力的模型 就可以在網 就跟你在玩一個網遊一樣 你可以在商店裡面 買到各式各樣的防具 還有買到各式各樣的攻擊的武器 那你就可以強化自己的模型 而你不需要做任何的訓練 model merging 是不需要做訓練的 你只需要做參數的加減 所以它需要的運算資源 是非常少的 而假設 Task Vector 商店這個概念成功的話 那未來小團隊呢 就可以專注於打造 單一任務的 Task Vector 今天要打造一個通用的模型太困難了 你可能很難收集到 general 的資料 去打造一個 general 的模型 但是大家可以專注打造出 Task Vector 然後掛到商店上 讓其他的模型來使用這些 Task Vector 未來大家還可以販售跟交換這些 Task Vector 也許 A 公司有個資料 B 公司有個資料 那通常資料要互換是比較困難的 因為資料的機敏性通常比較高 大家通常比較不喜歡把自己的 用什麼樣的資料訓練告訴別人 你看現在的這些開源模型 他們都不告訴你他訓練資料是用什麼 因為往往一講 他們就會被告 都會有版權問題 所以大家通常願意釋出模型參數 但是不願意告訴你他用什麼樣的資料 但假設 Task Vector 這種概念成功的話 未來你也不需要別人的資料 大家不需要互換資料 只需要互換 Task Vector 你就可以獲得其他模型的能力 所以假設 Task Vector 這樣的技術未來發展成熟 它可以給我們帶來新的想像