I would actually go as far as saying that intelligence is going to become a commodity. If you're a foundation model company and you're not building physical infrastructure, you're cosplaying your business. Project Horizon is us building out one of the largest data center complexes in the United States. We're talking about a scale of compute. It gets counted in the hundreds of megawatts and soon gigawatts. So we call it reinforcement learning to learn RL to L. This is the first time I'm probably publicly saying this.
>> Hi, I'm Matt Turk from First Mark. Welcome to the Mad Podcast. Today my guest is Iso Clant, a co-founder of Pullside. Poolside is a foundation model lab company focused on software engineering that's currently reported to be raising a $2 billion round at a $14 billion valuation, including a reported $1 billion check from Nvidia. We dig into Poolside's ambitious project horizon, an AI factory data center at multi- gigawatt scale, and why AI labs must own energy, compute, and intelligence. ISO also unveils reinforcement learning to learn, a new path that goes beyond pre-training and classic RL. Please enjoy this deeply insightful conversation with ISO.
>> ISO, welcome.
>> Good to be here, Matt.
>> I should say welcome back. You and I chatted almost 2 years ago now when you guys were just getting started.
>> I was actually listening back to our podcast and I hadn't realized that it was the very first podcast we had done as poolside and yeah, two years have flown by. Fast forward to today at the end of 2025 the race between Frontier AGI Labs has only gotten crazier frothier. So where do you guys stand? What is the reason for Poolside to exist in a world of hyper competition with other labs? So it comes back to why we started the company. Right? So if if you go back to early 2023 when we started Poolside, we started it because we had our own point of view on the research. So if you remember early in earlier in that year, you know, GPT4 had just come out and the narrative in the world was all we have to do to reach AGI is scale up language models, do more next token prediction, a larger number of parameters and more data. And we agree with the importance of scale and till the date still do. But our point of view was that reinforcement learning was going to become the most important scaling access for model capabilities. Extremely contrarian opinion at that point still when we were doing our podcast as well and probably for the following, you know, 12 months as well. Today that's of course very different. uh today I think that's become consensus and I think the world has has understood that we now have an ability to continue to scale models towards more and more capable like direction and really close the gap between human intelligence and AI and and that mission was the original mission of poolside is still very much the mission of poolside but along the way I think what we had predicted would happen played out we were going to continue to scale up more compute was going to be required but at the same time the first paradigm of kind of pre-training of predicting the next token on the web was becoming sigmoidal and was slowing down in terms of the gains that it had. So over the last 2 and 1/2 years we caught up on the pre-training side and really on like the what we refer to as the table stakes of foundation model building and language modeling while building some pretty serious advantages on the reinforcement learning side and and now we're at a moment where those two things have come together. All of us like as Frontier companies are you know developing our big model runs are still at the same scale. So there's series of advantages that we've built in our research. There's advantages that we've built on our engineering and those are probably some of the most important sustained ones. Uh but now we're getting to a point where scaling up our models to frontier sizes uh is also necessary for us. And hence we've recently had some announcements about the sheer scale of compute that's coming online. And so we're competing like we're we're in this race. Uh I think I I listened back to to our podcast and I said, you know, we're going after open eye and entropic. That's still very much true. But our starting point was a little bit different, right? We said, you know, to get to highly capable intelligence that can reason, that can do planning, that can understand the world, you're almost better off putting a set of blinders and focusing on on one proxy for that. And for us, that proxy was software development. And as we've gotten more and more capable at doing longer horizon software development tasks, we've also gotten more and more capable at frankly all type of knowledge work tasks. the world from a model perspective. I think we are all over time converging to a similar point. I would actually go as far as saying that intelligence is going to become a commodity. In that world, who we want to be is we want to be trusted by by enterprises. We want to be trusted by businesses to to power the knowledge workforce that started with coding agents and is now going beyond that as well. Uh and I think in the space, you know, so much is going to transition, right? We're like a pre and post electricity moment. So I don't think it's a winner takes all market but it does seem to be that it's a small number of companies who are who are able to get there.
>> As part of that race towards AGI you've had uh some very big news in the last couple of weeks. First of all there is a rumored large fund raise up to $2 billion where Nvidia reportedly would be investing up to a billion dollars at 12 billion pre 14 billion post. It's a very large round that seems to be very tied to another big news that you guys did formally announce which is Project Horizon. What is Project Horizon?
>> Yeah. So, so Project Horizon is us building out one of the largest data center complexes in the United States. And this comes back to something I said earlier. We we talked about intelligence becoming a commodity, right? Our our view for pretty much since starting the company over 2 and a half years has been that there's three layers of the stack that fundamentally are going to matter. It's energy, it's comput, and it's the intelligence built on top. And within this world, if you think that intelligence is going to become less distinguishable between the companies building it and becomes a commodity, a commodity probably more like oil or cloud comput like bread at the bakery, is there's two things that matter. Your ability to scale it and the cost at which you deliver it to your end user. Now the ability to scale it was frankly the primary motivating driver for project horizon but cost as well and it's important to think about what you could do 12 months ago versus what you could do today at the scale of compute that we were talking about you know 2 years ago in our industry you could call up a data center colo and say hey I want this much compute in 6 months and there would be space like physical space where you could deploy it or there'd be someone who has the capacity available to you today we're talking about a scale of compute that gets counted in the hundreds of megawatts and soon gigawatt but there's no one you can call
>> right these are built to suit data centers that are so large that no one is building them before having a tenant and so as you're approaching the frontier and your models are getting more capable and you want to serve that intelligence to the world and you want to scale up your training as well over time now your lead time from deciding to do that to being able to do that is no longer calling up a hyperscaler or calling up another partner and having it, you know, in months. Now, we're talking 12 months, 14 months, 18 months with huge capital numbers attached to it.
>> And so, my co-founders say this thing to each other and probably not really for for podcast material, but it's it's a if if you're a foundation model company and you're not building physical infrastructure, you're cosplaying your business.
>> Mhm.
>> And and this is kind of the the really fundamental nature. It's not that we went out and said it'll be cool to build infrastructure. It was a necessity for the mission to be able to achieve it. So you think you just get boxed out, right? So obviously OpenAI is a whole target project and they just announced like I think a fourth data center that they're about to build. Enthropic has a special relationship with AWS. I think AWS is building one for them. That's I guess that's what you were saying. You need a tenant in that case. The scal is willing to do it because they have anthropic as a tenant. So as an indie lab for lack of a better term like you have to own your own destiny and that it involves building your own data center right just to play it back.
>> Yeah. I think as a foundation model company you can go two paths right you can choose to to deeply partner with a hyperscaler and kind of you know become uh you know have them become an owner in you and and really go all the way uh and I think that's one direction but at the same time the world is getting to a point where I don't think anyone has any doubts anymore that we're now on track to to reach human level capabilities and and intelligence and in that world $29 trillion of knowledge work rewrites itself right scientific progress starts pushing beyond levels that we've ever seen And all of it is intelligence on compute. And so we are far more in a in a foundation model company is far more in a physical infrastructure business than most people realize because the ability to scale that compute, right, and and bring it to end users and frankly do so cost effectively, right? The cost of your tokens are going to matter more and more. as our as our intelligences all become closer to each other, the ability to scale this up and and do so to end users where you know the cents per token are kind of a determinant if someone's going to buy it is is critical. So we already started several years ago uh asking ourselves the question like what would it take to move towards this and then over time you know we learned more we observed more and and then we started acting towards it and and the acting towards it is project horizon. So project horizon in a nutshell is today announced a 2 gawatt campus we can actually go far beyond 2 gawatts on the site that we're in. It's a partnership with an incredible family out of Texas called the Mitchells. The Mitchells have one of the single largest parcels of land in the United States. is half a million acres.
>> And it always blows my mind because for context, LA is 300,000 acres. And on that land is renewables being built. There's grid connection. There's water, but there's also a 20-in main gas line. And that offers us a possibility to start scaling up energy that powers data centers incredibly fast and incredibly large scale.
>> Mhm. And so what you're seeing from us is that before the end of this year, we're starting construction on a 250 megawatt data center. It's natural gas powered uh with grid connection as a backup and it will house an incredible amount of compute, but it will be the first phase of many that we're building out.
>> Great. So just to unpack some of the things you said, you mentioned the impact of owning your physical infrastructure on the economics of Frontier AI labs. to help us understand understand that obviously one key question in the world of AI has been gross margin where do you think owning your own uh physical infrastructure lead you towards in terms of like overall structure
>> I'm not yet convinced that gross margins in our industry will look like a SAS company like 80% plus I think when we're talking about um a commodity as intelligence that we build value added services on top right as a foundation model company on one hand you you sell your tokens right your barrels of oil, your raw material. And on the other hand, you're building up value added services, your products that you bring to end users and customers that you know unlock value for their businesses or for consumers in the case of others. And in that world, I think it looks a lot more like a cloud company. So I think when we're going to look at the scale of this, uh, it's going to look like cloud companies with a zero behind that. from a margin perspective probably pretty similar because if you think about what is an AWS or GCP or an Azure it's effectively virtualization of hardware with services on top. I don't think foundation models are are that different. So I think from a margin profile will probably sit far closer towards that like 40% mark that you see in in cloud companies than you do like the 80% on the software side. And now when you start thinking about what sits in the stack of cost of intelligence, you effectively you've got the land, um you've got the energy, you have the data center, you have the chips inside the data center and and just for context, if we for instance break this down with illustrative numbers that are roughly in the right ballpark, uh to energize 250 megawatt is about half a billion dollars worth of physical infrastructure. Mhm.
>> So in the case of gas turbines, it's gas turbines, but there's different paths to it. Building a data center like the power shell that brings together all of the equipment where the compute can run inside, you're talking kind of north of $2 billion, $2 to $2.5 billion. Now the compute that goes inside today would be about $5.5 billion. So kind of all in all, when we're like adding this together, you're talking
>> being the chips,
>> being the chips and the networking and and and everything that kind of is what we'd refer to as the fit out of of a data center. And so you got about an $8 billion like cost of 250 megawatt. So when you hear in the in the industry you might say a gigawatt is 40 or $50 billion. That's what people are referring to, right? It's it's the breakdown of those costs. Now the life of a chip has a certain amount of years to it. Uh and in our industry there's there's lots of you know argumentation about how long they last and it's a demand and supply question. A data center has kind of traditionally, you know, a 15-year life, but it needs retrofitting as as new generations of chips come online. And your power infrastructure can last a very long time, but requires maintenance.
>> Mhm.
>> Now, in that $8 billion project of 250 megawatt, you're annually spending somewhere north of 300 to $350 million of opex. And so, you can split that about 50/50 between the cost of financing and the cost of energy and operations. energy like being the most. And this is where there's already an interesting insight. All of a sudden, you realize that the energy part today is not the biggest part of the stack, right? You're talking $160 million a year in energy. And of course, we're I'm taking numbers that are West Texas numbers. So energy can be, you know, double that in other places in the country, but still compared to the total, you know, cost TCO of of of compute, uh, it's relatively, you know, minor. Uh, numbers are still very big. Now the reason I I mentioned is that what is going to happen over time right like what's always happened in capitalism is that margins compress everywhere now margins have already compressed for energy we already you know try to make energy as cheaply as possible in the world data centers effectively have already been increasingly more commoditized over the last 20 30 years GPU compute today is still very high margin business and over time we'll find a similar level of compression and as the chips become more compressed the other parts of the stack are more important as well. And when you end to end own all of this and build all of this, everywhere where previously margin sat in between, you know, falls away.
>> If that's the person who you were paying for, you know, the cost of land, if you're talking about the cost of like the energy, the cost of the building, the data center, kind of all the layers in the stack. This adds up to a very large amount because the foundation model itself is, you know, it has capex for building it. that's your R&D cost and you you know you advertise it over time. Uh but the physical infrastructure is really frankly where the majority of your cost sits and so I know I went super detailed straight away but I think it's useful for people to kind of get this sense of where it is. So when you take all those margins out all of a sudden you can start seeing that you can serve your tokens 20 30 40% cheaper than someone else. And this is going to matter because it's we're shifting from, you know, human labor that leads to economic output to intelligence on compute combined with human labor. And we've barely started this in the world. And so we're going to be in a world where this is a commodity that will look far larger than we've seen cloud compute be. Frankly, is often the largest bills of companies, right? What they're what they're paying in that world. Yeah. Controlling that cost is a big impact. But cost is really only part of the answer here. You have to be able to scale it. You have to be able to say if I next quarter need more compute, can I bring it online? Because there's no shortage of chips. There's just a clip yesterday from Satcha uh on a podcast that I saw where he said, "Look, it's it's energy and and data centers that are my bottleneck. It's not chips." And it's true, right? Nvidia does an incredible job at supplying the market with chips. and and TSMC and everything in the stack and scale up to to huge levels but physical space bringing power online and building data centers well that's when we kind of in tech all of us start realizing the real world hits us
>> and you mentioned cost of energy and cost of financing uh just quickly this is finance how this is project finance kind of a setup
>> exactly so so if you think about traditional data center business and this is not unlike it you you have an amount of equity that goes into that business you have a loan to cost ratio and the loan own to cost ratio really depends on the lease, the tenant that you have on that data center. You got a 15-year lease contract traditionally and and then it's traditional project financing uh that is highly effective. The world has been doing this for a long time. It is relatively, you know, low rate financing. Uh and uh this is not new. This is not you know new to tech. It's been around for a long time.
>> Part of the idea is that Pside will be the anchor tenant but you'll be renting the facility out to others as well. what is going to be the revenue stream.
>> We are not yet at a place where we can be signing 15-year leases on massive multi-billion dollar, you know, like buildouts. Uh and so, but we are ramping up rapidly where we see a path to being able to scale up compute uh at those levels. And so, we found a really great partnership here with Cororeweave that was announced. Corewave frankly is second to none in terms of operating uh Nvidia's compute, right? first have GB200s online at scale uh now coming with the GB300s and and have been this great partner like for us in in the space and so we found a really great hybrid solution with them. So they are the anchor tenant on our data center. Jointly we can scale up our compute inside that data center and any capacity that we choose not to take or not able to take in our space. Uh they can bring to other customers
>> and and this was really an important thing because we're we're not a hyperscaler who can make you know multi-billion dollar bets for two or three years out. So, we needed to find our way of of having great partnerships that were kind of win-win situations where we could scale up our compute but didn't need to make a lead time decision years ahead of it. And and this has been a fantastic setup because uh it kind of brings the best of both worlds to to to the site. as you explain all of this fascinating in terms of ambition. It's also fascinating in terms of like what that means for ultimately a young software AI company to become you know a major physical instructor company. So the core partnership helps but presumably you have to hire all sorts of people that's a whole different skill set. How did you think about that and um also the the financial aspect of it? Yeah, we followed an algorithm. I would probably say most of my career by now. Uh at least I remember going this back as far as 9 years is you have to by all accounts avoid the Dunning Krueger effect as a founder, right? Uh because when you get into something new like it has been on the physical infrastructure side, you start with learning, right? You start with reading, you start with like meeting the experts, learning and learning and as you learn more about a topic, you fall in this point where you start thinking you really know it. But you don't really know it because you haven't hit the real world or you haven't done it before. And what I found to be one of the best advice I got a very long time ago whenever you find yourself in that situation, well, this is when you want to start finding the experts to join you. But how do you find the experts? Because you yourself aren't. And here's when you just start interviewing 100 plus people for every role and you build a distribution. You start understanding who sits on the right tail end outlier of that distribution. And it turns out, you know, even just with relatively I would I wouldn't say our knowledge at this point was shallow because we'd spent years like, you know, understanding the space and uh getting close to it, but with definitely not the level of like an expert, we got to the point where we started being able to identify who were the outliers and those are the people you hire and you bring on, right? And you empower, you give the right autonomy to and that in combination with the values that we care about. So at Poolside since day zero, we've always cared about low ego, kind-hearted, extremely hardworking, like in our space, you work way too many hours, a lot of intellectual curiosity and horsepower, and deeply care about the work that you deliver.
>> Uh, and we've recently added a six because we found that's really been true is like people are very explicit and direct in their communications, right? Or it can be very clear
>> and and so we started with that and you start from the top, right? uh we found an incredible hire in Lance who's our VP of data centers uh and and from there we've been building out the team. What was particularly interesting to learn about the infrastructure space over time is that it's an incredibly small world. Everybody knows each other. It makes tech feel like a big world. Uh right, the the construction of data centers at scale has really only been done by quite small number of players. And so everybody knows each other across the entire like endto-end supply chain. So you also have an ability to very quickly understand who's considered a good actor and who's considered a bad actor, right? Who has built up a reputation. Small industries are very reputation driven. And so it all starts with people um empowering them, learning from them like continuously and and knowing where you know you you trust and knowing where you go deep and you ask questions and and try to challenge people. And that has actually not just led us to building a an infrastructure company with an incredible team, but also to find the people in the industry who were able to start rethinking some of the ways data centers have been built. So we haven't just gone and said, "Oh, we're doing the exact traditional thing everyone else is doing." We've actually taken a quite new approach uh to data center construction as well.
>> That sounds great. Unpack that. So I think the industry has seen the XAI data center being built in record speed which I think has reset the bar for for everyone. How do you guys go about building that data center? So if you think about data centers, there's kind of been a traditional way of building them and I wouldn't say new way because it's been around for for over a decade. But on one end you have kind of stickuilt buildings. Everything is done on site, right? So all the materials are brought there. Thousands of tradesmen who who are assembling the data center and on the other hand of kind of like Verdive here in uh in the west or players like Day One and their parent company out of China who've gone full modular. to think of like your your data center units are rolling out of a factory. And what was interesting in in that both of those approaches uh have their limitations, right? The moment you're bringing thousands of people to site uh and you're assembling everything and and especially in what is effectively the Peran basin, the desert, right, in in West Texas comes with its challenges both on mobilizing workforces, but also the weather, the dust, uh the logistics, the supply chain. But then the factory side has not yet scaled up at a level in the west. I would say this is different in China uh where you can actually you know bring out gigawatts like over the course of you know like couple of years uh and so we we kind of decided to meet in the middle uh and took an approach where we said we're building the actual building stick built so that's being built on site uh think of it as an extremely long corridor uh with leaves attached to it but let's take a modular approach but one that fits on the back of a flatbed truck. So how do we kind of do 2 and a half megawatt computeworthy skids for think of this for electrical for mechanical and cooling and for compute. Those are kind of the three elements that sit inside a data center and do it with manufacturing partners here in the United States. And what you start finding here is and once you start breaking the problem down you realize that the electrical modular components can be done by lots of great manufacturers. This is not unique. This is so you actually have great players including very large ones uh that can help you there. when you can kind of get to the cooling and mechanical part. uh it's a smaller number of companies but again have been doing this for a long time highly reputable uh and you can work with and then when it gets to compute side there's recently been an unlock uh because on the compute mixed with cooling side what you're finding is that now we have you know direct liquid to chip cooling the designs are changing there's a lot more that you can do in a far more contained manner and so we decided we said we'd rather have a little bit more redundancy on every individual kind of room that we build so that we can bring them incrementally completely online. So where most of the data center is being built for AI factories today, you'll see them bringing them online at kind of 40 megawatt chunks. We said what if you designed it in a way where you could bring 2.5 megawatts of compute online, you know, as you're doing construction. Think of it again on this hallway as you're bringing on like units of kind of a thousand GPUs or depending on the GPU type can be less in the future. uh you're bringing this online and the combination of those things that we refer to as incrementally delivered hybrid modular data centers is kind of bringing the best of both worlds. It usually derisks your ability to deliver. Uh it drisks your ability to bring compute online. It also means that if all of a sudden next month you need more compute, you can make decisions on 2 and a half megawatts like I just want a little bit more. On the other hand, you're taking advantage of the fact that for the kind of bigger construction, the building and stuff, you take the traditional approach and on the manufacturing side, you can partner with multiple manufacturers and you can do so actually very geographically advantageous as well. Most of the things that we are bringing to our site are actually manufactured in Texas or very nearby.
>> And so now you're actually reducing as well like your supply chain risk and your time like to sight risk. The biggest advantage though of all of this is not just speed, but it's also your workforce that you can mobilize because we're in a quite remote area and that remote area offers all of these advantages, right? Like pretty much infinite land so we can, you know, build a single level horizontally uh an incredible amount of source of energy, but also being remote meaning that mobilizing workforces there is more challenging. So all of a sudden, you know, when we're building 250 megawws, we're doing it with a less than 450 person on-site workforce with very skilled tradesmen and and and and construction workers that were bringing over there and we're housing over there versus a world where you might need traditionally 2,000 plus people to build the exact same thing. So it's all about de-risking and speed. And you kind of see a common theme. We think about lead time and we do the same by the way on our model building site. We think you know how can quickly can you go from decision to having something online uh and how can you make sure the capital that you have to deploy does not have to sit for 12 to 18 months because that's when you as a smaller company can all of a sudden scale up right because the big numbers really have more to do with the fact that these decisions need to be made years ahead of time we can make them months ahead of time
>> what's the target delivery date for like the first part to come online
>> so um at the beginning of Q4 end of Q3 so next year we've got the first compute coming online uh and then we're finalizing in Q1 2027 the first uh you know 250 megawatt but already next summer the next 250 megawatt starts construction it's a staggered build uh and this means that we will always have a horizon on you know essentially three times a year an additional 250 megawatts coming online
>> last question on this and then we'll transition over to the more familiar software and AI side of the conversation the uh environmental impact that's obvious viously a question that the whole data center industry grapples with. How do you guys think about it?
>> Let's first be honest about this, right? We're using natural gas for the generation of power and in the world that we are right now at the scale that data centers are being built out. Uh the sun, you know, doesn't shine all day, the wind doesn't blow all the time. And so renewables only in combination with the battery capacity that you would need to power a data center if you weren't renewable only is just not economically viable yet. But the combination of like natural gas uh with renewables with batteries. We've got a big best being built out uh on the site. So a big battery system. Uh allows for kind of an an optimal point in the middle. Uh now when you talk about natural gas generation, you want to make sure that you have your emissions in check. So what you're adding is SCTRs. Uh and so you're an SCR is essentially cataly reduction. So what you're doing is you are filtering out some of the more harmful particles that come out of natural gas generation and you do this all within federal standards. You you file for air permits that you have that are approved by like at the federal level. And so this is a critical thing to do as a as a business. And so you're you're combining those things. So that's on the energy side. The second side of environmental concerns around data centers are usually water, right? Water is a depending on where you are scarce resource. in other places. It's an abundant resource. It can impact like a local community. And so we're very privileged because of where we sit and the sheer scale of land that our partners have. Water is a both a resource that is there at the levels that's required for us without actually taking away from like the local community. And this is not the case everywhere else in the United States or in the world. So picking this site was not just about like how large it can scale because it can scale into incredibly large size but also can you do so in a way that's you know responsible and community what goes beyond the environmental topic is incredibly important right you're building something out even though we are building out in a relatively remote area at the end of the day this would not be possible without the support that we get from the local community so we're in a place called POS county which is just some of the best people that I've ever met like it's I honestly like if you want to go and and just walk into a bar somewhere and spend time with some of like the greatest people in the United States, go to Pekos County because we've been amazed with like how welcoming like the community has been. And so this goes hand inhand with how do we invest in you know job programs to bring people there. I don't mean to start sounding like a politician Matt because I think this is almost when they talk about community it feels like you are but these are you know these are projects that are not one-off right when we talk about this big multi-gawatt project. We've got about 8 years worth of construction that we are planning out there. Uh and potentially as nuclear, you know, becomes a viable option in the coming years and we can build out more power potentially in nuclear over there. This can be construction that happens out there for as long as we can imagine because the space is close as infinite as you're going to get. And so bringing kind of everyone along uh has been kind of a critical like point for us. We just try to be good actors and try to be transparent about you know the positives and also the negative externalities and how to mitigate them.
>> Fascinating and very different from the world of RL. But let's precisely um switch to RL now since we chatted a couple of years ago. You mentioned that the world has changed and um everybody was very pre-training focused uh for the last few years and sort of feels like in 2025 the combination of pre-training and RL now has become sort of like the the both the state-of-the-art in what what a lot of people do. Walk us through your approach. So when we did this podcast two years ago, you know, we spoke about our approach of of reinforcement learning from code execution feedback. Yes. I think at the time I called out we had tens of thousands of these environments where the models would go in they would be given synthetic tasks explore solutions and then we rewarded on the solutions that
>> and maybe remind people what what that is. So you have a whole layer where you have those uh execution environment that you built.
>> Exactly. Yeah. So, so what we have done is uh at the time it was about 10,000 or uh I believe 20,000 code bases like real world repositories that you know have complex software in them where we define tasks and we would look for often singleshot solutions. So here's a task model can write some code it gets tested it might have additional signal from reward models and others in there and it will be able to provide you know feedback was the code correct was it wrong was it rightly styled etc. uh over time that has grown a lot across two axis. One is right now we have over a million of those environments uh and tens of millions of revisions putting us in like the 30 million range. So just yeah two orders of magnitude larger over the last 2 years and that's because as you increase the diversity of the type of problems that the models need to learn in you can increase the capabilities of the models. The second axis is that the models got so capable in their reasoning and longer time horizon capabilities multi-step like complex planning and execution that RL moved from kind of singleshot reinforcement learning to agentic RL. So meaning that now we give a much higher level task. We send in an agent that agent goes and you know tries to solve for the problem by using its tools. So editing code, executing commands, updating packages, like doing everything that a software developer would do. And when you see it, you know, you see it today in coding agents, it looks even quite natural to how we do things, right? Increasingly more like how we do it. And then it gets rewarded along the way as well. So what we're seeing is that we're entering a world where the reasoning capabilities of models and their longerterm horizon planning and execution is now getting so capable that the tasks we provide them are no longer simple tasks like the questions at the end of a textbook but now it's like the projects that you you know you're made to do in school uh where you have to do a lot more steps and so we've continued to scale on those axis today I believe everyone is scaling amongst those axes um and it's true where 2 years ago this was a contrarian opinion that no one was doing to date is absolutely consensus where we have again found ourselves in a point where we now have again a contrarian opinion on the future. Uh and it's really not by by trying to be contrarian. It's just that we I think have had a pretty had a couple of you know a long time thinking about this. Right. I started thinking about this first in 2016 when I was building source and did the first RL with with LSTMs with language models and now the last two and a half years with with these larger LLMs and our view now has become that the world is going in a direction again that we actually might not fully agree with on so what you're hearing a lot about at the moment is the scaling up of environments with human expert rubrics. So it's not just going into coding where we've got verified rewards. Now we're entering into areas if that's you know how to operate on a spreadsheet or if that's how to do a complicated task as a as a chemist or you know marketing uh professional where you use kind of a rubric created by an expert uh to make it part of what a model can use to judge the answers and it's highly effective uh and by the way we do it as well. It is a highly effective way to to get models singular capabilities on singular tasks uh to push it up. And I think what we'll see in the next couple of years is a lot of model companies ourselves as well. We go into these enterprises. We find high value use cases. We use our agents to get to 60 or 70%, you know, of of quality and then use additional reinforcement learning with, you know, domain experts knowledge to get it to 80 or 90 or potentially even 100%. But the narrative that that's going to scale us to AGI, yeah, we don't agree with. We think this is the right thing to do in the awkward teenage years ahead of AGI. It's because it allows you to create really economically valuable outcomes for your customers. At the end of the day, you know, we're we're we're businesses, right? We we need to be able to fund our economic engine towards our mission to Mars, towards reaching, you know, human level intelligence and beyond. But the singularly collecting of skills this way by creating environments and experts, we don't think will scale to artificial general intelligence. Our view is that reinforcement learning will have a moment similar like we've seen in language modeling. So, what makes language modeling so beautiful? It's the fact that by predicting the next token on the web, you have this general self-supervised objective, right? You just can throw the entire internet at it and of course continue to filter it into more quality and improve it with synthetic, all the things we spoke about last time, but it forces the model to learn. But the reason that never got us to AGI is because well, the internet never actually included the data set of the thoughts and actions that created it.
>> It's the final piece of code. It's the final article you write uh but not the thoughts that you had and actions that led to it.
>> Not the trial and error.
>> Exactly. Uh and so our view is that there is an generalized objective for reinforcement learning that doesn't require you know human judges or environments or reward models or anything external but can generalize over language. Uh the same way that we have seen you know next token prediction generalize. A way of thinking about it is if you have traditionally we've taken the web and we've used synthetic techniques to reformulate it to improve it. We've moved forward like we taken the web and generated forward. What if you could generate backwards? So what I mean by that is what if you could reverse engineer the web or decompress the web into the thoughts and actions that created it. Is there such a general objective for RL that can be found? This is where we've done a lot of our research in the last year and a half and we have now started seeing a lot of promise towards that being possible. Not yet too ready yet to talk more openly about it because we're still at a stage of the research where we've got a lot of things to prove out.
>> Does it have a name as an approach or
>> Yeah. So we call it reinforcement learning to learn RL.
>> Okay.
>> This is the first time I'm probably publicly saying this. Uh and so
>> and just to play it back and make sure that it's understandable by everyone. So there's one approach which is reinforcement learning where you create environments and reinforcement learning does trial and error uh gets rewarded not rewarded and the path to scaling reinforcement learning is to just uh expand reinforcement learning to all sort of different tasks. So that's option A. What you're saying is that you you're working on an option B or maybe that's option A and the first one was option uh B but you're working on a on a different approach where you're going back to internet data reverse engineer the the thoughts that went in reaching that conclusion in the first place is that
>> it's a perfect way of framing it and and we don't think they're mutually exclusive right we think that you want to have a model learn how to think and reason as early as possible in its training and then you want to actually have it learn in the environments to sharpen its skill sets, right? And it's not unlike us, right? Like it's we'll have learned a lot coming out of university, but then we're put in the job and we're actually doing it and we're learning from experiences. So the learning from experiences, uh the reinforcement learning from environments, right, and from experience is effectively like a renewable energy. The density of information in those tokens is not as high as like in a physics book is, right, where huge amount of density of like within a small amount of tokens. In experience, it's less, but it's highly valuable. So our view is is that you know we pre-training and predicting the next token on the web is an incredible bootstrap of of understanding language and and helping us get you know to a level of intelligence. Reinforcement learning to learn RL internally also refer to as the Bondi techniques. It's kind of our our our code name for it. We think will push models to a level of reasoning and thought that will happen far earlier in their training than it does today. And then you have reinforcement learning from code execution feedback and and from other verified environments that help learn really what is you know to sharpen those skill sets in simulated environments. And then the fourth stage of training over time increasingly will become learning continuous learning from real world experiences of these agents. And so those are kind of the four stages that we think training will go to. If you will talk about uh that fourth stage a little bit like continuous learning is something that people may have heard about that keeps coming back as one of the next avenues for the whole AI systems to progress. What is it and how does that work currently and how does that work in the context of poolside
>> today? There there's one thing about foundation models that we have yet to really optimize almost at all like over all of this time which is the ability to learn from a single you know sample of data. Foundation models today can do it and we don't yet have a path in in them successfully doing so in a way that like really improves it. Internally we call it the hot stove problem. If you're a kid and you touch a hot stove once, you're never touching it again. Single sample and you're good, right? Foundation models because of the the underlying technology gradient descent, right, is it's such a it's such a data hungry algorithm. It requires so many samples to be able to navigate that higher dimensionality space to a place where it does something more optimal. And so we've got this big hole towards AGI of like how can we get a model to learn continuously from a smaller number of experiences like you and I can. Now there's a question if that's required to reach AGI. I would actually tend to say that if we take the definition of AGI to be that you know foundation models are as capable as you and I to do the vast majority of economically valuable tasks maybe first behind a laptop and over time embodied in robotics. I would tend to say that it might not be necessary. Uh but it of course is an incredible thing to add to intelligence because it will massively make it more compute efficient and will also make it more valuable over time. And what we do have already a path towards is when we've got a large number of users or the real world giving feedback on what an agent is doing, what a model is doing. Models and agents to me are effectively the same thing. we can incorporate that feedback in improving the models and and this is really the learning from real world experiences. This is when you've got you know maybe hundreds of thousands or millions of agent trajectories right tasks that were done where some feedback was provided and that can be by the final by the person who instigated it or can also be by some form of environment or system. And so it's kind of taking it from in our clusters as foundation model companies to where it actually touches the users uh and that feedback will come back. And so is that necessary to reach AGI? Not so sure yet. It's a very honest answer. But will it be valuable uh in the journey towards it to make our models more capable and and more directed to what they want them to be? Like absolutely. In all of this, you know, I have a bit of an issue started to use the term more because it's just more commonly used, you know, AGI, but it treats intelligence. We often treat it as this like singular spot that once we're there, it encompasses everything. But intelligence is so multi-dimensional,
>> right? You've got people are incredible creative writers who can create, you know, incredible works of art at DStoski. On the other hand, you've got incredible mathematicians who can prove themselves. on our hands. You have incredible people who are, you know, amazing at designing a factory. And it is not yet obvious that that is all a singular thing, right? There's uh and and we see this very clearly with modalities, right? Someone who is not able to see can still be an incredible software developer, but it's unlikely to be able to be an incredible race car driver,
>> right? Or unlikely, it's called impossible to be an incredible race car driver. And so intelligence is so multi-dimensional that we can get to a point where it's going to be as capable of doing all the knowledge work that we do behind a laptop and we're still not going to agree that it's AGI and we still won't have solved every technical challenge. And that's also okay. Like I think we've got a long road ahead of us to truly get to the incredible like versatile like features that we have as humans. But will we get to a point in the next couple of years where economically valuable knowledge work can be done by I? I think absolutely.
>> You just mentioned something interesting about the model and the agent being more or less the same thing. So to to weave that question into the you know current sort of debate topics a few days ago said that agents were 10 years away. So what is your take on where we are with agents as it relates to AGI and um is there an opportunity to build agents without being a foundation model company? So I haven't yet listened to the kopathy interview and I really want to because I don't think you noticed but I got into the space because of karapathy. So in 2015, Andreopathy wrote an article called the unreasonable effectiveness of recurrent neural networks, a blog post about language models. And that singular blog post let me start my company sourced and got me down this obsession for now a decade of like what you know language models and and can do. And so I have an immense amount of respect for for his opinion. And so while I haven't listened yet to the interview, I can't comment specifically on on what he said. But what I can say is this, right? What is today the definition that we use of an agent? It's a model running in a loop inside an environment with access to a set of tools, right? And it's and it's doing longer trajectory work. And how are we training agents, right? Agent capabilities as foundation model companies is that we take that agent, right, the binary, the piece of code that that runs this in a loop and has access to could be a container or could be access to a bunch of tools and we train it together with the model. And so this is why you see the most capable coding agents, right, really coming out of people who are training with the models because and this by the way will likely hold true for lots of domains. But when you already have a model that has all of the intelligence and all of the capabilities to do an agentic task, right? So it doesn't require more intelligence. It doesn't need to be pushed further in like reinforcement learning. Well, then the question, what's the differentiation in the agent? Well, differentiation in the agent is whoever is building it needs to have access to some form of proprietary data, some form of proprietary environment, something that allows that, you know, that loop that the model is running in to to be, you know, more advantageous than another competitor. And there's lots of opportunity there. But what I am careful about and we see this in coding, right? Maybe we see this ourselves is if you decide to start a company to build a coding agent where you are not able to improve the model, right? The agent is not able to train together with the model and foundation model companies like like ourselves are deeply focused on doing so. You you don't have that edge and so the cat-and- mouse game that you're playing becomes a lot more difficult. It's not impossible. I've seen incredible agents being built for specific things because no one can singly focus on everything. Not any foundation model company and including ourselves. But we have a phrase at poolside which is over time everything collapses into the models. And I think increasingly that's what we're seeing. Frameworks or agents or products that were 2 years ago you know around today are have you know either gone or they've sometimes their UIs have collapsed right? So you used to see these products with lots of bells and whistles and lots of things behind the scenes and you would see lots of UI options and today it's just like a screen that says agent and you talk to it and it goes off and does the work
>> and so I think you have to ask yourself the question like where do you sit like what's your unfair advantage that you have and if a foundation model company decides to focus on it will that advantage you know still sustain or will it fall away because they're able to train the models to be further and and more capable in combination with their agent and yours.
>> Yeah, it's it's funny how we went from talking about thin rappers 2 years ago, 3 years ago, to thick rappers last year, and it feels like in 2025 going into 2026, we're starting to think about agents that are really thin rappers. Again, it's it's a constant.
>> We're all ignoring something, right? And it's maybe it's the wrong way of putting it. Uh, I think we have a hard time holding the point of view that models will reach the same level of intelligence and capabilities that the world's most capable people in every field have. And when you take a step back and don't take the next 12 month view, but just take the next I think it's in the next 36 months for knowledge work. Maybe it's the next someone else might think it's the next 5 years. I think few people today would argue that it's not going to happen in the next decade. And when you're building a business, you're not building it for the next 5 years. you're building it for the next 20 or 30, right? You're building a company that can sustain and and be at the level of success, at least particularly when it's venturebacked businesses. In that world, what does your business look like if AI is as capable as the most capable person in the field? And that means certain things still exist like my Uber Eats app still exists because I want to look at my food and etc. But a whole bunch of vertical software or vertical agents are probably gone. And finding out where you sit in that world, I think, is is critical. I think we have a tendency all of us and and and myself included to get so caught up in what's right in front of us that we don't take a step back and and come back to the big things and and I think the big things really here are is that we are going to reach those levels of capabilities and it rewrites the world really like that pre and post electricity moment far more than we I think have even factored in today. I think not even in the financial markets or not even in the way that we operate our businesses. It's uh it's truly living on an exponential to a point that I don't think we have ever seen before.
>> Everything we've created till now was because of intelligence. But now intelligence itself is something we will be able to create and scale and I think it becomes a very different world afterwards.
>> So what do you tell people that argue that AI progress is actually plateauing?
>> The frankly the same thing. I've I've had this question for two and a half years now since starting poolside. This is not the first podcast where you know there was a are we hitting a wall? We are I think at a point where we are continuing to see with every new generation of chips an ability to make the model larger. Uh and it's important that this links to every new generation of chips because it is not a world where you can throw infinite dollars at scaling. It's that's a false narrative and you can think of it very easily yourself because if I take the size of a model uh which is still effectively the biggest determinant of how much compute it takes to train it because of the limitations on the networking and because of the limitations on the flops on those chips. It is not that I can linearly add more GPUs and train increasingly a more larger model. If I do so the time it takes becomes exponentially longer. So if tomorrow someone said I wanted to train a 300 trillion parameter model completely out of the realm of anything that anyone's doing today, it just wouldn't be possible. No matter how much money you have, no matter how many chips that you put up, it would become exponentially more expensive and and longer to train that. So and this is because of the current hardware limitations. But every 2 years and now the chip cycle is even getting less. We have an incredible new chip and an incredible new networking stack that is improving that all of a sudden makes the next generation of size model possible. Now I don't think this will scale infinitely. Uh I look at intelligence as compression and you know at some point you're you're you can compress further uh or at least not to a point where it's worth it. It's diminishing returns but we still have an ability to do that. So on one hand we have a a free lunch like as as new generations of chips come out we can build and train larger models uh and it as continues to show that it improves the model capabilities on the other hand and I think far more interesting as we're now getting into the world of scaling reinforcement learning we're able to train models for longer duration with more data right because I want to have this the size axis and the other thing we have to duration axis how much data we're showing the model and every single year. We've also become multiples more efficient with the data we had to make models become more capable in a lesser compute budget. It's kind of mind-blowing to see where even if I think about our pre-training at the beginning of the year or we have a new run that's that's at going on at the moment versus now like you can really see incredible improvements because you're refining the data better. You're creating it cleaner. You're creating a better curriculum. I know there's a long-winded answer to your question, but I think it's important to understand that it is not an infinite dollar you can just keep throwing intelligence. It's kind of bounded by physics of every chip generation. As reinforcement learning is becoming increasingly more scaling axis, we are able to improve models within those generations far more. But we have not yet found a generalized infinitely scalable, you know, dollars we can throw at reinforcement learning either. And so both on increasing model size and on increasing data from RL, there's still limitations. And if those limitations didn't exist, Pulside wouldn't have been able to catch up, right? I think this is a really important part because we we're now at a point where we're starting like we should definitely spend some time with the models after this. They've gotten really darn good. We're now scaling up compute to make it to the frontier. That wouldn't have been possible if it was just a world of dollars. But it is definitely a place where yeah I think right now I don't see any limitations. I see more than limitations. I see opportunities in some of the research we're doing in reinforcement learning to learn and others that might completely open open up new scaling access that can go even further and then every generation of chip that comes out we'll see a stepwise function that goes up in our industry.
>> You're very focused on software development. Uh how do you think about the bitter lesson? Is that something that you guys worry about?
>> If you go back to our first podcast or we put a post on our website on day zero of the company, we always said the path for AGR runs through software. It is not software. And we laid out this three-step master plan which was step one assist people in coding. It's kind of very early days. Step two, allow people, anyone to build software. I think the world is clearly there right now. And then step three, generalize to all domains. And we're now really in that step three moment. And so already our models today have become generally capable across the board. But because we had kept our blinders on for those first couple of years on really software development capabilities as a proxy for intelligence, it allowed us to go really far allowed us to push the things that mattered and our view. What was really missing was pushing reasoning like improving knowledge. You can improve knowledge outside the model, right? You can give it access to the right sources of information, but improving this kind of complex reasoning is what was missing. So we're kind of converged on the same point. So today I use our models to the other day I had to write a sci-fi book while I was reading it which was a lot of fun right uh and uh my brother uses it in his in his growth marketing job and and prefers it over you know using uh using other models and so we're already in that domain now but software development is still with our deep focus on enterprise and and government often the most highest value like value or highest driver of cost and also impact of knowledge work in a lot of businesses. It's not the only one, but it's a big one. So, it's been an amazing place to enter into organizations with and and now we're starting to increasingly go beyond that. So, we had a big announcement this week with company called Red Panda. We're big fans of and we just integrated their 300 plus enterprise data connectors inside it. And the demo that we were showing at the New York Stock Exchange was one of an AML process in finance that our agents are endtoend together with these data connectors like doing. And so we're we're starting to open up much more broader outside of software development and come back to what we've always said like we want to be able to to power all of knowledge work in the world.
>> For the last part of this conversation, let's uh go back down in the weeds and talk about some of the stuff that you just uh mentioned. I think I and and and people may be curious about the current reality of of poolside both from a model standpoint, product standpoint and commercial. So starting with the model. So you have you have three products on your website. You have a Malibu, you have a point, and then you have assistant.
>> What's the current status of those products and what do what do they do product models?
>> Yeah. So if you go back 2 and a half years ago, right? One of the first things was point which was like the code completion models.
>> Today they're table stakes. We have them, but it's not where the intelligence sits, right? So our Malibu was our first big family of models. And so within the Malibu models, they were really oriented towards originally to be very capable coding assistants. Now they've become incredibly capable coding agents. Uh and now they've also become incredible knowledge work agents. And so within that uh those models are right now for their their size and weight class, best-in-class. They have become incredibly capable coding, but they're not yet at what I would define as the frontier. So frontier today is OpenAI, Anthropic, Google, and XAI. And that's why we have all of this compute coming online to scale up those model sizes. That's our upcoming family of models, Laguna, where we have a small, a medium, and a large. So, small is finishing training in a couple of weeks. Medium actually starts training this week. Uh, and and large starts training as our 41,000 GB300s come online. And
>> you have benchmarks for those. Do you have benchmark?
>> We do have benchmarks. Yeah. So, so if you think about our benchmarks today, um, and we primarily share them privately with the organizations, the commercial part that we work with, but if you think about the Malibu agent as a coding agent for instance, right now, it sits at a level of like Sweetbench verified for instance, where Gemini 2 and a half pro was when it came out. Uh, and so it's a much smaller model. Uh, but it's really pushed those capabilities because of our work in reinforcement learning.
>> And so we should do a a demo after. I think we're actually doing a demo in New York uh publicly because we've only done it privately with enterprises in a couple of weeks at the AI engineer summit.
>> Uh and so if anybody's interested, they can come and see it there and I think that will be recorded as well.
>> It's an interesting uh thought from a go to market standpoint because you you guys have been both public but kind of stealth in a way for the last couple of years and we're in a world of just like massive noise and like everybody's like struggling for attention. How do you think about about that tension working with enterprises on the one hand but like on the other hand like everybody's uh you know all developers want those products to be in their hands. So our view has been quite simple actually is that we will make models publicly available when we are clearly best on on a very valuable axis right and I think that's that's important it's not just about having a hype on Twitter for a couple of weeks and and having something out you need to bring something in that can scale and that is valuable to the world so before and this gets to your commercial part of the question we weren't at the frontier and now we've are getting increasingly closer and closer and uh and that's al also opened up our market. So before we were at the frontier, we picked a market where no one else could go, which was the defense industrial base and government because one of the things that we're willing to do uh kind of from the enterprise DNA we have is take our entire model weights and the full stack all the way with the agents anywhere the customer needs it to be. So we today operate on workstations that go into skiffs. We operate on servers that are you know onrem or even as far as airgapped. We operate in commercial cloud like AWS in private VPC environments. Uh we also oper in more commercial just cloud regular environments and then on GVC cloud top secret cloud kind of the places where the the weights have to travel to the customer. And we did this because it wasn't just about the model is because we knew that as the world was moving towards agents doing increasingly more work in the enterprise it was going to become incredibly important to build out everything around the model. So today we can rattle off you know a very long list of enterprise features that we've built over time so that these agents and models can actually operate in complex regulated environments. And so that goes everything from a data access layer towards the managing of assigning of agents across role-based access control deep integrations with all of the kind of active directory-l like systems that people have all of the monitoring audit logging like that associates with observability you know go on and on and we battle tested that in the defense industry because they're extremely large organizations they're highly complex they're highly regulated and they often need segregated deployments for different missions that that they're on and so we've been scaling that up in that industry. Now, as the models have gotten to a point where we say, "Okay, now we feel that we can compete, we're going to wider enterprise." And so, you'll see us increasingly more showing up in kind of the large enterprise names amongst financial services, industrials, like technology companies. And we treat our business as kind of two business models. On one hand, we want to make our models publicly available and allow anyone to use them. But we want to do so when we say, "Hey, we have something here that sets us apart."
>> Mhm. And that's the scaling up of comput is necessary for that.
>> Laguna the Laguna family will be public. And then we have the part of business where we go very value ad. And here we started with product.
>> So we started with a coding assistant that sits in VS code and IntelliJ now also sits outside of it. Then we started with kind of the web interfaces that you expect uh to be able to chat and use the models you know connected with data sources. But we found something over and over again, which was that a lot of these enterprises and organizations, they have incredibly high valuable problems that can be solved with AI today. Hell, in many cases with AI capabilities of 2 years ago or a year ago, but they're not successfully doing so. Uh and a lot of that has to do with the gap between what the intent of the project is versus being able to bring it all together, the right data sources, the right context engineering, often additional reinforcement learning that's needed. And here we started building up a very strong forward deployed motion. At Poolside, we have former Palunteerians and who came over and really kind of instilled some of that DNA, the DNA of what it means to find a high value problem and come in with high agency and help a customer solve that. Yeah.
>> You even have like FD R you call them FDR. FD is for chumps like you forward deployed research
>> engineers. Yeah. And and look what we found is that there's a there is a gap between the skill set of doing traditional forward deployed engineering right where you're focused on uh a high value problem and using kind of piping data sources together and and building you know interfaces on top which is by the way incredible thing. I have a lot of respect for everyone who has done this. The research engineering side is interesting because what you're looking for is the people in that who also have the experience and the natural tendency to work very well with models and with agents and can who work on additional reinforcement learning if it's necessary. And so we're taking these highly capable research engineers and we're putting them inside our customers. And that's for us something you're going to see us scale up increasingly more. It's why we've uh going to be opening an office in the UK actually next week. Uh we're going to be opening an office here in New York and really kind of scaling up that motion of forward deployed research engineering.
>> On that topic of geography uh you guys started mostly in Europe I believe but it feels like over the last couple of years you've recentered the company in large part towards the US. Is that a fair sentiment? And if so what drove it?
>> Yeah so we've always been an American company. We've been incorporated headquartered US since day zero and and I think at any given moment the balance of people will have always sat somewhere between 4060 or 6040 one way or another like throughout the life of the company. So building it as a global company was critical for us. But the decision we made early on and I think we spoke about it on the podcast 2 years ago is to hire researchers outside of the Bay Area and particularly originally very centered on Europe. And still today if we look at where like researchers for pool sites sit they sit predominantly you know 95% outside of Silicon Valley. And this offered an incredible opportunity for us. It offered the opportunity to find highly capable like highly motivated people who were not in the echo chamber that is the Bay Area. It's one of the most valuable echo chambers in the world. I I personally love it. I I spent a fair amount of time there. But you've seen and probably realize that there's a bunch of things we've done that were quite contrarian to the belief of of that echo chamber. And I think that was partially unlocked by the fact that we sat outside of it and continue to do so. So we hire all across the United States. We hire all across Europe. Uh we're hiring increasingly more in Asia as well. Uh as we're scaling up and so but we have and continue to to try to avoid as much of an echo chamber as possible because I think the world we're very early in all of this still and the path towards AGI is is one that deserves deserves to be built by opinions and people that aren't all the same. And we have just found that there is no shortage of incredible talent uh in the world. it just might not be as obvious on a on a CV or a LinkedIn as it is in the Bay Area.
>> So, zooming out the next 12 to 18 months, what happens at Poolside? What can we expect?
>> Models reach the frontier that that's uh that's right now I think we see a straight line towards this. Uh you'll see the the scaling up of physical infrastructure to both empower training even more larger and capable models but also serving them to the world. uh you'll see us uh have forward deployed research engineers and increasingly larger number of enterprises uh globally you'll see us just continue to work towards the mission right I mean for for us we never want to lose sight of the fact that we are building this company and and and the economic engine associated with it because we think that the world that lives after this is one where a lot of abundance can be created abundance through scientific progress that's going to happen but also abundance through the fact that costing of goods and services ultimately are dependent on the cost of what it takes to create them. And as we shift intelligence to compute, we can find ourselves in a world where we can drive that cost down. And if you look back at the last 100 years, there's no moment that I think any of us rather live than today, right? And so that's it core mission. It always has been. And you'll you'll see more of that. Uh and uh yeah, looking forward to probably being here in 12 months to to tell you about what happened.
>> Looking forward to it. Uh ISO great to catch up. Thank you so much. Congrats on everything uh that you guys have accomplished in the last couple of years. Excited to see the data center and like the new models. And thank you for spending time with us.
>> Appreciate it.
>> Thank you, Matt.
>> Hi, it's Matt Turk again. Thanks for listening to this episode of the MAD podcast. If you enjoyed it, we'd be very grateful if you would consider subscribing if you haven't already, or leaving a positive review or comment on whichever platform you're watching this or listening to this episode from. This really helps us build a podcast and get great guests. Thanks, and see you at the next episode.