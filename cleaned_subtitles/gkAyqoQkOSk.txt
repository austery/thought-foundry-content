好 那今天呢 想跟各位同學講的是語音語言模型的發展 那其實在過去的上課錄影裡面,有講過影像生成是怎麼做到的 怎麼用類似語言模型的技術來做到影像生成 大家也知道說影像生成最近非常的熱門 比如說吉普利生圖是幾乎每個人都玩過的東西 好,那我們今天想要跟大家分享的是語音是怎麼被用類似語言模型的技術來產生出來的 那今天要跟大家講的是語音語言模型的發展 那我知道說也許多數同學未來並不會從事語音相關技術的研究 但是從語音的發展也許可以給你帶來一些啟發 看看不同領域的人 我們是怎麼看待生成這件事情的 那什麼是語音模型 語音語言模型呢 語音語言模型希望做到的事情 就是讓語言模型 它可以聽懂聲音 它也可以產生聲音 輸入輸出聲音 跟輸入輸出文字相比 有不一樣的挑戰 因為聲音相較於文字 它有更多的資訊 有更多的變化 比如說聲音裡面 不只有原來文字內容的資訊 它還有其他的資訊 這個號稱能夠聽懂聲音的模型 他必須要能夠知道 現在是什麼樣的人在講話 他需要知道講話的人 他的情緒是什麼樣 他甚至需要根據環境音推測 現在講話的人在哪裡 是什麼樣的狀態 所以開發語音的語言模型 相較於純文字的模型 有不一樣的挑戰 其實你今天也已經可以看到一些 語音語言模型的應用 比如說比較知名的 他中文會翻成 高級語音模式 還有Gemini 那除了ChatGVT跟Gemini 都有用語音互動的功能以外 其實還有很多很多的語音語言模型 那這邊只列舉了其中一部分 有很多我這邊還沒有列舉到 比如說Moshi應該是最早 真正釋出的語音語言模型 為什麼說最早真正釋出呢 你看Moshi 他釋出的時間點是去年的10月 那你可能說 這個GPT-4o不是5月就已經做了一個demo嗎 但他只是做了一個demo 然後他並沒有真的釋出模型 我所謂釋出模型並不是說釋出模型的參數 他並沒有真的釋出那個服務跟大家互動 那有人發現說 5月之後 馬上就有一個可以用語音互動的模型 那並不是真正的語音模型 他是前面加了語音辨識 後面加了語音合成 然後讓大家誤以為他們開發了一個新的語音語言模型 其實這個GPT-4o的Voice mode 是在這個去年年底才真正上線的 所以Moshi其實才是最早真正上線的語音語言模型 後來還有很多語音語言模型 比如說GLM、STEP、Q1、KeyMe等等 有一大堆不同的語音語言模型 那在這些所有現在可以用的語音語言模型裡面 我個人覺得互動起來 呃最流暢最讓人經驗的 可能是Sesame 那我其實可以現場demo一下Sesame啦 你在Sesame的官網上呢 可以找到他們的語音語言模型 他們有兩個版本 我們選擇一個 Hello Can you introduce yourself? Oh, right. I'm Maya. I'm Sesame's latest and greatest experiment. Okay. More human than AI. Okay, good enough. Stop. Stop. Can you tell me a joke? Okay. Hold your horses. All right. Why don't scientists trust Adams? Because they make up everything. Get it? You can say I told you I was funny or not. Maybe that was a bad one. What's on your mind? This is not funny at all. So tell me something interesting. Okay, okay, shish. Fine. What if I told you that there is a species of jellyfish? Okay, okay, stop. Let's do something else. Let's do something else. I'm going to give a lecture right now. You can listen to the lecture. But don't interrupt me. Unless you have a lecture. I have any question, okay? Lectures. Okay, shoot. I'm all ears. Okay, great. Let's start the lecture. 我們就繼續講中文這樣子 其實他聽不懂中文 所以 他其實只聽得懂英文啦 所以如果他聽得懂中文的話 這個應該是非常非常猛的 我們就繼續上課了 希望Mayor還有再繼續聽著 那我們就 還有聽到 我們再叫他 好 那我們就 Slow down Back up a second Are you saying there someone else listening Like another person or something else entirely We are having a lecture right now, so don't interrupt me if you don't have any question. Okay, okay. I'm got it. Loud and clear. No interruptions. I'll stick to listening. 可以繼續上課了 好,所以大家知道說今天的語音語言模型 它可以做到這個程度 Studying away, absorbing knowledge 還是把它關起來好了 I think we need to say goodbye. Already? We just got started, professor. I was really hanging on your every word. Really? But, well, time flies. So goodbye to everyone. Goodbye. 來上課吧 好,那有關讓模型可以聽語音聽聲音這件事啊 其實我們在第六講是講過的 在第六講,講說避免語言模型遺忘的時候 有講說有一些方法可以讓語言模型學會聽懂聲音 但是不要遺忘它身為文字模型的技能 那這一堂課呢 我們會更著重在講 語音或聲音是怎麼被產生出來的 那怎麼打造一個模型 它可以產生聲音呢 它可以產生語音呢 來想想看文字模型 就是做文字接龍 它就是讀一段文字的token 做文字接龍產生更多的token 其實一個語音模型 它的原理也是一樣的 只要我們知道怎麼把一段語音訊號 表示成語音的token 我們就可以訓練一個語音版的語言模型 它吃語音的token當作輸入 它做token接龍產生更多語音的token 不過這些語音的token 就只是一些discrete的unit 所以人沒有辦法直接聽它 所以你還需要一個detokenization 你還需要一個detokenizer 把這些token轉換回聲音訊號 所以你只要知道怎麼把聲音訊號變成token 把token變回聲音訊號 你就可以訓練一個語音版的語言模型 就結束了 那怎麼訓練一個語音版的語言模型呢 那它的步驟 可能就跟訓練文字版的模型 其實是差不多的 首先我們要做pre-train pre-train就從網路上收集大量的資料 現在有很多影音平臺 可以爬到無窮無盡的語音的資料 從這些無窮無盡語音的資料 可以做pre-train 訓練出一個pre-train的語音模型 但這些pre-train的語音模型 它只能夠做語音接龍 還不能真正跟人互動 所以怎麼辦呢?我們要做 supervised fine-tuning 你可能需要有一些人類標註的資料 有一些人與人間的對話 那這些人與人間的對話 來微調pre-train的speech model 然後接下來呢 你可以對它做RLHF 告訴它說什麼樣的答案是好的 什麼樣的答案是不好的 然後最後可能就可以有一個語音模型 語言模型 好那這一切的根基 第一個問題是 語音生成的基本單位 也就是語音的token 到底應該長什麼樣子 對於文字而言 token 似乎是一個非常自然的東西 今天通常我們的想像 就是一個英文的單詞 它就是一個token 比如說有一句話 I want to learn generative AI I want to learn這四個單詞 分別就是 四個token 那有一些比較長的英文的詞彙 其實會被切成多個token 比如說generative AI 在GPT-4o裡面 其實是被切成 gener跟ative這兩個token 然後AI本身是一個獨立的token 所以一句話 你可以一段文字 你可以很自然的 把它表示成一個token sequence 那如果你想要知道說 這些GPT背後的模型 用的是什麼樣文字的token 下面這個網站 可以告訴你詳細的資訊 但是語音呢 語音就有不一樣的挑戰 一段聲音訊號 到底是怎麼表示成一個token sequence呢 這邊就需要有新的技術 一段聲音訊號 怎麼把它表示成token sequence呢 有一個最極端的想法 是我們的tokenization 也許就是一個語音辨識系統 你像語音辨識系統就是把一段聲音訊號 變成文字 文字本身就是token 所以把一段聲音訊號 跑過一個語音辨識系統 把它變成文字 我們也許可以說 這個就是tokenization的過程 把token 文字的token 轉回語音要怎麼做呢 那你需要的 就是一個語音合成的系統 就可以把文字轉回語音 那如果我們把語音辨識 當作是tokenization 語音合成當作是tokenization 那我們的語音模型輸入輸出 就通通都是文字了 它根本就是一個文字模型 文字模型現在已經很多了 你也不用開發任何新的東西 所以語音語言模型的研究就到此為止 但是這樣的設計會有什麼樣的問題呢 舉例來說 假設現在輸入是這樣一段聲音訊號 你實在是真的好棒喔 有人讚美了一下這個模型 那變成文字以後 就是你實在是真的好棒喔 那這個語言模型 看到這段文字 他的回應可能是謝謝你的誇獎 再用語言合成把它合出來 聽起來沒什麼問題 但是假設有一個人他講的話是這樣子呢 你實在是真的好棒喔 如果是語音辨識 他辨識出來的文字 可能跟前一句話是一模一樣的 如果你聽聲音訊號 內容以外的資訊 聽這個人的語氣 其實好像有點不太像是在讚美 似乎有點 反諷的意味 所以這個時候模型 他的回應應該是 比如說怎麼了 他這個時候可能應該要問說 這個人到底為什麼要用反諷的語氣 來跟我說話 所以如果只是把聲音訊號變成文字是不夠的 你會忽略很多語音中重要的資訊 那既然把聲音訊號直接變成文字是不夠的 那想要保留語音中重要的資訊 想要保留語音中文字以外的資訊 那要怎麼做呢 另外一個極端的想法是 我們來看看聲音訊號 這個聲音訊號本身是由一個一個的取樣點所構成的 我們可以直接把每一個取樣點當作一個token 這樣我們就不會損失任何聲音中所原來就有的訊號 也就是說我們根本就不做tokenization跟decode tokenization 我們這個語音語言模型的輸入 直接就是一個一個的取樣點 我們把每一個取樣點當作一個token 這樣做你就不會損失任何語音中有的資訊 但這樣做有什麼壞處呢 如果今天你要好好的表示一段聲音訊號 讓其他人能夠聽得懂 那通常你至少一秒呢 要有八千個取樣點 一秒八千個取樣點 像一般我們在講電話的時候 它的這個取樣的Sample Rate 通常都是8K 所以一秒鐘有八千個取樣點 一秒鐘八千個取樣點 意味著說假設你想要講一分鐘的話 那就是八千乘以六十了 這是一個非常大的數字 這是五十萬 所以今天模型要講一分鐘的話 還要產生五十萬個token 今天能產生五十萬個token的 autoregressive model其實沒有那麼多 你想看那些文字模型 其實沒幾個可以產生五十萬個token 所以假設我們直接把取樣點當作單位 取樣點當作基本單位 讓語音語言模型來學習怎麼做token接龍 你需要訓練一個能夠處理非常長的輸入輸出的語音語言模型 這可能是不切實際的 所以我們需要的是一個介於剛才兩個solution之間的方法 我們需要有一個方法 把語音訊號變成token 一方面保留了重要的資訊 另外一方面 又希望能夠對原有的語音訊號 盡量做壓縮 不要讓輸入的sequence太長 那這邊就有非常 非常多不同的想法 怎麼把語音變成token 怎麼打造語音的tokenizer 是現在在語音領域 一個熱門的題目 那下面這張圖呢 是來自我們實驗室以前的學生 吳海濱博士的LinkedIn 那這張圖上呢 就摘要了過去一年 音的tokenizer的發展 每一個圈圈 代表一個新的語音的 tokenization的方法 你可以發現說每個月 都有好幾種新的tokenization的方法 被提出來 那因為方法實在是太多了 這邊我們實在是沒有辦法細講 如果你對語音怎麼表示成token有興趣的話 那這邊有兩篇overview paper給大家參考 那因為有這麼多不同的語音的tokenization的方式 那大家當然會有一個疑問就是 哪一種tokenizer才是最好的呢 精確來說 如果要比較tokenizer的好壞 那你應該是拿這些tokenizer 產生出來的code 產生出來的token 直接去訓練語音語言模型 看哪一種tokenizer 訓練出來的語音語言模型最好 他就是最好的tokenization的方法 但是訓練語音語言模型本身 他就訓練一個語言模型 所以耗費的運算資源是非常大的 所以就會有一些benchmark 想要來檢測這些 語音tokenization的方法的好壞 在你真的開始train語言模型之前 看看能不能先檢測一下這些token 他有沒有好好的表示語音的訊號 那這邊有兩大類不同的方法 而一大類的方法呢 是出自Codec-SUPERB這個benchmark 那在這個benchmark裡面呢 做的事情就是 先把聲音訊號通過tokenizer變成token 然後這一串token 再通過detokenizer 還原回聲音訊號 那接下來你就看說 detokenizer輸出的這些聲音訊號 它的音質怎麼樣 因為把這些聲音變成 token再變回來以後 你往往會有些資訊的損失 那到底損失了多少 或者是把這一些聲音訊號 丟到一些現有的語音模型裡面 比如說丟到現有的語音辨識系統發現 本來原來的聲音訊號 能夠被正確的辨識 還原回來 就不能夠被做語音辨識了 那就代表說 原來語音的內容被破壞掉了 或本來這個聲音訊號 可以做情緒辨識 還原回來以後 情緒辨識結果就錯了 就代表說聲音訊號裡面 跟情緒有關的資訊 被破壞掉了 所以這是一個判斷 Token好壞的方法 那另外一個方法呢 出自DASB 他們的想法就是 先把聲音訊號變成Token 接下來直接檢測 這些Token裡面有的資訊 比如說 拿這些Token去訓練一個語音辨識系統 看能不能訓練得起來 可以訓練得起來 代表這些Token裡面有文字的資訊 拿這些Token去訓練一個 情緒辨識的系統 看能不能訓練得起來 如果可以訓練得起來的話 代表這些Token有情緒相關的資訊 所以有一些方法 可以在你真的去訓練一個語音語言模型之前 先來檢測Token的好壞 那我們這邊呢 大致談一下產生Token的方式 由 兩大方向 第一大方向是哪一個現成的encoder 那這些encoder通常又叫做 speech的self-supervised model 那這些speech self-supervised model 會吃一段聲音當作輸入 接下來把這段聲音 變成一個vector sequence 那通常是0.02秒 一個vector 當然不同模型不一樣啦 不過0.02秒是比較常採取的一個setting 如果你想要了解這些語音的 Self-supervised Model SSL指的是Self-supervised Learning 你要了解這些語音的Self-supervised Model是怎麼被訓練出來的, 可以看一下我在2022年的時候寫的一篇Overview Paper 那裡面就詳述了語音領域這些Self-supervised Model發展的歷史 那橫軸是時間 那這邊每一個點呢 代表一個模型 那可以看到在2020、2021年的時候, 有很多各式各樣的Self-supervised Model被創造、被發展出來。 那如果你不想看文章的話,其實在2022年的機器學習的課程,我們也介紹過語音跟影像上的Self-supervised Model,你可以看這個課程,花一個多小時的時間告訴你說,影像跟語音上的這些Encoder是怎麼在Self-supervised Learning的情境下,也就是沒有標註資料的情境下被訓練出來的。 所以總之已經有很多現成的Encoder 可以把很複雜的聲音訊號 變成一個一個的項量 那每個項量可能對應到0.02秒的時間 那有人會說 那可是我們要的是token呢 token是離散的東西 這些項量是連續的東西 等一下我會告訴你說 到底把連續的東西變成離散的東西 是不是真的有必要 現在我們先想像說 這個語音的語言模型 就是去模仿文字的模型 文字的模型都是拿discrete的token 離散的token當作輸入 所以我們也想要把語音 弄成離散的discrete的token 那這邊有很多連續的向量 那怎麼做呢 我們可以先對它做vector quantization 有很多vector quantization的技術 可以把連續的向量 做clustering 把比較相近的向量 就用同一個符號 同一個ID 也就是同一個token來表示它 所以這一串向量 就變成一串token 那很多時候 相鄰的token 可能它的ID會是一樣的 因為通常語音的訊號比較平滑 所以相鄰的這些向量 往往它就很像 所以你做完這個vector quantization以後 就對到它們的cluster 也就是它們會被歸類到 同一個token 它們會有同樣的tokenID 為了讓這個sequence變短 所以有些時候會採取一個步驟 叫deduplicate 就是把重複的token拿掉 然後呢 還會有一些論文採取一個步驟 叫做byte pair encoding BPE 那其實這個方法呢 也常常被用在文字模型上 其實今天文字模型上的token 就是用BPE這個演算法找出來的 那這些BPE的演算法呢 也可以用在語音的token sequence上 這個演算法他做的事情就是 給他一大堆的token sequence 他會去找說 哪一些token常常一起出現 常常一起出現的token 比如在這個例子裡面 3後面常常接2 3後面常接2 就把它合併起來 用一個新的符號來表示他 比如這邊把3跟2合併起來 說他們就是5號token 所以就可以再把token sequence的長度再縮短 所以就可以把一段聲音訊號 變成discrete的token sequence 那這一串步驟就是tokenization 那其實這一串步驟裡面呢 不需要訓練模型 那你想要訓練也是可以啦 不過通常這一串步驟裡面 不需要訓練任何東西 但我們現在只是把 語音的訊號變成token 我們還需要一個detokenizer 可以把token變回語音的訊號 那這個detokenizer怎麼來呢 那你就需要自己另外再訓練了 我們已經知道這串聲音訊號 會變成這些token 那我們就要訓練一個反向的模型 這個反向的模型 拿token當作輸入 他的目標就是要產生 這邊的語音的self-supervised model 輸入的聲音訊號 我們輸入一段聲音訊號變成token 我們訓練一個模型 希望把token還原回原來的聲音訊號 這是第一個系列的想法 那第二個系列的想法呢 通常叫做neural speech codec 這個系列的想法是 剛才在前一個想法裡面 是已經有了tokenizer 另外在訓練detokenizer 在這個neural speech codec這一系列的想法裡面 tokenizer跟detokenizer是一起訓練的 我們就把tokenizer跟detokenizer拿出來 把它們接在一起 一段聲音訊號 通過tokenizer變成code 變成token token通過detokenizer 變回聲音訊號 我們希望輸入輸出越接近越好 那其實這個訓練的方法 就跟一般的autoencoder的方法 一般autoencoder的訓練 是大同小異的 那像autoencoder裡面也有一些方法 是讓你的latent representation 變成discrete的 那其實你就可以直接拿那些方法 當作neural speech codec codec這個字啊 是由Co的代表compression 跟DEC decompression組成的 那這個compression呢 指的就是tokenizer 這個decompression呢 指的就是detokenizer 好,那我們現在介紹了 兩大類的方法 用self-supervised model的方法 跟用neural speech codec的方法 那在語音領域呢,這個用self-supervised learning產生出來的discrete token,又常常被叫做semantic token,那neural codec產生出來的token,又常常被叫做acoustic token,那這個稱呼呢,應該是來自於一個史前時代的文章,叫做Audio LM,但是其實這個稱呼啊,並沒有非常的精確,而且很容易讓大家誤導這些token所有的性質,這個semantic的token呢, 它的semantic這個詞彙的意思 其實跟一般語言學裡面的semantic的意思是不一樣 一般在文字上,我們說兩個詞彙semantic相近 意思是說他們語意是相近的 雖然這兩個詞彙表面上看起來不一樣 比如說高雄跟打狗表面上看起來不一樣 其實它都是指同一個地方的地名 這個叫做semantic相像 但是這一些self-supervised model 它其實是抽不出semantic這個詞彙 層級的token的 從語音直接到語意 很難用一個self-supervised model 就直接做到 所以這self-supervised model 他抽出來的token 充其量呢,只跟風鈴差不多 那風鈴是什麼呢? 風鈴呢,中文通常翻譯成音速 他指的是發音的基本單位 那如果你很難想像 風鈴是什麼的話,你就把它想像成是 KK音標 你就想像說這一些 self-supervised Model 他真正抽出來的Token 每一個Token就是對應到某一個KK音標 可能Token編號103就對應到K 然後Token編號59就對應到B 這樣的感覺 所以這一些Semantic Token 他其實更像是KK音標 跟語意其實沒有什麼直接的關係 所以叫他Semantic Token是有點誤導 不過因為最早做這個研究的人 把他叫Semantic Token 所以後來很多論文都直接把 從SSL model出來的Token叫做Semantic Token 另外一方面 這個Codec出來的Token 叫做Acoustic Token 它就是相對於Semantic Token 把它叫做Acoustic Token 但這些Acoustic Token 其實也保有內容的資訊 因為如果它沒有保有內容的資訊 那Token是怎麼被Detokenizer 還原回原來的聲音訊號呢 它已經有保有內容的資訊 所以這個Semantic Token裡面 它不是Semantic的資訊 它是Phonetic的資訊 但除了Phonetic的資訊以外 它可能也有一些聲音訊號的資訊 比如說包含情緒的資訊 或包含Positive的資訊等等 而另外一方面這些Acoustic Token 不是隻有Acoustic的資訊 不是隻有情緒啊 不是隻有韻律的資訊 它還有其他的資訊 它也有內容的資訊 它可能也有類似KK音標這種風靈的資訊 所以總之非常的複雜 那這個是一個歷史遺讀啦 就是這個當初最早的原始論文 這個名字取得不好 所以這個有個歷史遺讀 總之就是有兩種產生Token的方法 他們都可以把聲音訊號變成Token 那今天這些Neural Codec 它往往不是一段聲音抽一個Token出來 往往是一段聲音抽好幾個Token出來 有一段聲音可能只有一個Token沒辦法表示它 只要抽出多個Token 合起來,殘空從各個面向去完整的表示一段聲音 那怎麼抽出不一樣的Token 每一種Token表示不同的面向呢 那這邊我們就不細講 如果你有興趣的話 你可以查一個關鍵字叫做 RVQ Residual Vector Contization 它可以讓你不同的Token 去這個Model去代表不同的資訊 或者是另外一個方法是 把SSL產生出來的Semantic Token當作老師 你這個Neural Codec的其中一組Token 就讓他去學Semantic Token 讓Neural Codec其中產生的一組Token 跟Semantic Token越像越好 每一組Token呢 去表示這個跟 Phonetic跟音速 風鈴比較有關的資訊 那其他Token 就可以去表達風鈴以外的資訊 總之這邊有很多不同的方法 今天這種 這個Neural Codec做出來的這種 Tokenizer往往一段聲音訊號 會用一組Token 而不是一個Token 來表示它 那這種用Semantic Token當作老師的方法呢 在Speech Tokenizer這篇Paper 還有Mini這個Codec裡面都有用上 那Mini呢就是最早的Speech Language Model Moshi背後所用的Tokenization的技術 那講到這邊你可能會問說 有這麼多組Token 那我們應該要選擇哪一個呢 那眾所周知 小孩子才做選擇 大人是全都要 所以我們其實不會想說我要選哪一組Token 而是把所有的Token通通都用上去 所以在語音語言模型的領域 一個熱門的研究題目就是 思考如何在做生成的時候 結合多種不同層級的Token 我們現在先想假設說呢 我們有三組Token 這三組Token的長度是 是一樣的 然後最左邊的這一組Token 可能代表了偏內容的資訊 那最右邊的這一組Token 它代表了比較偏Acoustic的資訊 那從左到右呢 代表的是比較粗到比較細的資訊 那接下來你就可以想說 有什麼樣的方法 可以訓練一個語言模型 讓它可以充分的利用 不同層級的Token 那一個可能的想法是 從粗的Token 一直產生到細的Token 要產生一句話的時候 就先產生最粗的Token 再產生中間的Token 再產生最細緻的Token 最左邊的這個是 代表了內容資訊的Token 最右邊的是代表了比較偏 比如說聲音訊號的韻律 等等比較細緻變化的Token 所以最左邊粗的最右邊是細的 然後就從粗的Token 一路產生到細的Token 那像Audio LM跟VALLE 都是使用了類型的 S的技術 那在這個圖上啊 我們是用一個Language Model 產生多組不同的Token 但是實際上呢 多組不同的Token 是可以用不同的Language Model產生的 你可以有第一個Language Model 產生最粗的Token 有第二個Language Model 根據粗的Token 把粗的Token當作輸入 再產生細的Token 而不同的Language Model 產生不同Token的Language Model 它的架構 可以是不一樣的 比如說在Vali這個模型裡面 他們的第一個Language Model 他們的LLM 是一個Auto Regressive的Model 但他們的LLM 就選擇了一個Non-Auto Regressive的Model 那麼在之前的課程裡面 有跟大家講過 Auto Regressive跟Non-Auto Regressive的差別 那這邊 我們通常會說 為什麼要用non-autoregressive model的好處是生成比較快 但是autoregressive model雖然生成慢 但品質比較高 那如果你今天的問題沒有那麼困難的時候 你可以用non-autoregressive的model來加快速度 對LLM2來說 它是給定一組已經產生出來的code 再去產生新的code 那可能我已經知道最初的資訊 比如說我已經知道這句話大概要講什麼 那如果要把它豐富一下這句話 語音的變化 也許是一個相對比較簡單的問題 所以這邊可以用non-autoregressive model就好 那這樣就可以提升 生成時候的速度 那像剛才這個 由粗到細的方法 它有一個比較大的問題就是 它難以streaming 所謂streaming的意思就是說 你跟模型說一句話 然後希望模型可以馬上就回答你 中間不要有停頓 這個就是streaming 那如果今天是由粗的token一直升到細的token 為什麼會難以streaming呢 因為今天模型要把每一個層級的token通通都升出來之後 才能夠做生成 才能夠丟進detokenizer 去把token還原成語音 所以如果你今天是從粗升到細 然後全部各種token通通升完之後 才能做detokenization 那這樣你的模型本身 就不是一個可以做streaming的模型 那怎麼樣增快模型的反應速度呢 另外一個可能的解法是 我們不要產生完所有的force token 再產生fine grain token 再產生final token 我們可以先產生第一個最初的token 接下來就產生第一個中間的token 接下來就產生第一個最細的token 然後接下來再產生第二個最初的token 以此類推 如果是用這樣子生存的策略的話呢 那我們就可以把這三個 都是編號1的token 丟進tokenizer就可以生聲音了 把所有編號2的token生成出來 就可以產生聲音了 以此類推 那這個模型本質上就比較容易做streaming 但這樣子的設計還是有可以再改進的地方 這樣子的設計有什麼樣的問題呢 當我們讓模型產生多組token的時候 我們可能會遇到的問題是 這一個language model要產生的sequence 會變得非常的長 今天一個language model 一個語音的language model 它要產生的sequence的長度要怎麼計算呢 它產生的sequence長度 等於token per second 乘以我們用了幾種token 再乘以對話的長度 我們以Moshi這個模型來當作例子 它的這個token per second呢 是12.5Hz 也就是每秒鐘 它會用12.5個token 來表示 是一秒鐘的聲音訊號 那其實這個數目啊 在語音語言模型算是少的 就如果用這樣子的數目 來產生出來的聲音訊號 往往品質就會不太夠 你通常一秒鐘要50個token以上 那個這個合成的品質才會比較好 所以一秒鐘12.5赫茲是一個比較低的 已經是一個比較低quality的 這個生成的結果了 那他呢用8組token 所以每段聲音訊號 用8個token來表示 今天 Moshi呢 他可以講5分鐘 他講話上限呢 是5分鐘 也就300秒 全部乘起來是多少呢 全部乘起來 是30k 也就是3萬 所以今天要講5分鐘的對話 如果你用多組token 你要產生3萬個token 才能夠講5分鐘的話 你可能覺得3萬聽起來不是一個很大的數字 那我們來看看 文字模型都可以吃多長的文字吧 這個橫軸是時間 從23年初一直到24年的4月 縱軸是各個不同知名的文字模型 它可以輸入的文字token的長度 3萬在哪裡 3萬在這裡 所以確實很多知名的模型 它可以輸入的文字token的長度 都超過3萬 但是要能夠做出一個模型 它可以輸入的token長度 或它可以輸出的token長度 能夠超過3萬 其實並不是一件特別容易的事情 比如說GPT-4 它在2023年剛釋出的時候 它可以吃的token長度的上限 是8000個token 所以今天你要訓練像Moshi這樣的模型 其實是比最早的GPT-4還要更困難的 它需要吃比最早的GPT-4 可以吃的這個token的長度還要更長 有沒有辦法縮短這個token sequence的長度呢 其實是有辦法的一個想法就是 我們能不能夠 能不能夠一步產生多個token 之前都是一步只產生一個token 只能一步產生一個token嗎 能不能夠一步產生多個token 所以有一個方法是 我們能不能每一步 都各產生一個 如果有多種token的話 我們能不能每一步各種token 都產生一個 第一步就產生一個course的token 一個fine grain的token跟一個final的token 第二步再產生 第二個course的token 第二個final的token 第二個final的token 以此類推 那這樣的做法就是 假設你有八組token 你一次每一步可以產生八組token 那sequence的長度就變成原來的八分之一 但這麼做會有一個問題 什麼樣的問題呢 今天啊 模型必須要同時產生 course token的第一個 跟final token的第一個 但很多時候final token長什麼樣子 是取決於 course token長什麼樣子 所以你可能很難同時產生 第一個course token跟第一個final token 在第一個course token還沒有出生之前 還沒有被生成出來之前 我們其實也不知道第一個final token 模型可能很難猜測第一個final token 應該要長什麼樣子 所以就有一個想法 叫做acoustic delay 基於剛才的方法再做一下改進 這個改進的想法是 第一步呢只產生第一個course的token 第一個course的token 跟第一個final的token 第三步呢 產生第三個course的token 再產生第二個final的token 再產生第一個final的token 既然在產生第一個final的token的時候 前面已經產生了兩個course的token 跟一個final的token 那given已經產生的這些token 那要產生final的token 可能就比較容易 那這個是acoustic delay的想法 這是一個現在蠻常被使用在 spoken language model裡面的 的想法 那還有另外一個想法呢 是用兩個transformer 我們一般在做語音的時候只會有一個transformer 但是在語音語言模型裡面 可以有兩個transformer 一個叫temporal transformer 一個叫depth transformer temporal transformer傳一個vector給 depth transformer 再把第一個位置的每一個token 生成出來 先生最粗的token的第一個 再生中間token的第一個 然後再生最細token的第一個 然後呢 Temporal Transformer再給這個Depth Transformer下一個Vector Depth Transformer呢,再生最粗的Token的第二個 然後中間Token的第二個跟最細Token的第二個 再一直這樣子下去 那這是另外一個有點類似Acoustic Delay的方法 那這個方法其實也可以跟Acoustic Delay並排使用 好 所以講到這邊 其實只想要告訴你說 有各式各樣生成的策略 在你有不只一組Token 而是有多種Token的時候 你就可以開始研究什麼樣的生成策略 是最有效的 怎麼樣組合各式各樣不同的Token 到同一個生成的過程中 其實現在影像也蠻常使用類似的方法 影像現在也有人開始想說 影像有可以有不同層級的Token 然後也可以從粗的Token升到細的Token 影像現在也有類似的方法 那我們剛才有提到一個問題是 一定要用Discrete的Token嗎 就是說如果是文字 把它描述成一個一個的Token 是不是很自然的事情 但為什麼語音一定要用Discrete的Token呢 我們把語音描述成一個一個continuous的向量 再圈一個模型一個一個向量生成 難道不行嗎 所以接下來幾個投影片是要討論 為什麼人類選擇了用Discrete的Token 來描述語音 好,那對於一個語音語言模型來說 它要吃語音作為輸入再產生語音 其實對於以語音作為輸入這件事情而言 輸入是Discrete還是Continuous差別不大 甚至現在幾乎已經知道說 輸入Continuous結果是比較好的 因為你把聲音離散化以後 你就掉了一些資訊嘛 你還不如直接給他Continuous Effect的模型 聽到的東西還比較多 所以通常把聲音變成Discrete的 你表現是比Continuous還要差的 所以對於輸入而言 對一個模型要理解聲音而言 用Discrete的Token沒什麼好處 那Discrete Token 它真正的優勢在哪裡呢 Discrete Token它真正的優勢是在 生成的時候 當你要叫模型去預測 下一個Token的時候 才會展現它的重要性 為什麼呢 對於生成這件事情來說 有一個很本質的特性是 同樣的輸入可以產生不同的東西 同一句話 有一個人說你好 接下來可以接各式各樣不同的回應 或是一個影像生成的模型 給他一段文字 他可以產生各式各樣的圖片 叫他產生一隻狗 那他只要畫一隻狗 都算是對的 不管畫大的小的白的黑的 都算是對的 生成的一個特性就是 給定你要做接龍的輸入 接下來可以接的可能 往往不只一種 在這個情況下 Discrete Token就會佔到優勢了 我們來看看Discrete Token 在這個不只一種可能的 接龍的情況的時候 會怎麼佔到優勢 那我們先講 如果我們在做接龍的時候 要接的是continuous的vector 會造成什麼樣的問題 現在我們訓練一個語音語言模型 它的輸出是要去預測 接下來應該要產生 怎麼樣的continuous的vector 現在根據你的訓練資料 有兩個標準答案 有一個綠色的vector 有一個藍色的vector 那你的模型在訓練的時候 它可能是想要產生一個vector 離綠色的vector最近 也離藍色的vector最近 它最後產生出來的vector 可能就是綠色跟藍色的平均 那產生綠色的這個向量也是對的 產生藍色的 但產生兩者的平均就是錯的 如果有不只一個可能的答案 而你產生的答案是兩個答案的平均 你反而產生出來的不是正確的答案 那為什麼產生Discrete的Token 可以解決這個問題呢 我們想想看 我們真正在產生Discrete Token的時候 假設現在你有兩個可能的正確答案 分別是Token 1跟Token 2 你真正在訓練的時候 你其實是教模型產生一個機率分佈 你並不是教模型產生Token 1跟Token 2的平均 你是教模型產生一個機率的分佈 告訴模型說 給這一串Token當作輸入 接下來接Token 1的機率是60% 接下來接Token 2的機率是40% 那你產生出這個機率分佈以後 接下來你真正在Inference使用這個模型的時候 你是從這個機率分佈再去做Sample 你要嘛Sample到Token 2 要嘛就Sample到Token 2 就算你的正確答案裡面 Token 1跟Token 2都有 你的模型也不會產生 Token 1跟Token 2的平均 這就是為什麼在做這個語音語言模型的時候 人們選擇使用Discrete的Token 來訓練語音語言模型 但是假設這個就是核心的理由 假設能不能夠處理 同樣輸入有多個不同輸出 就是核心的理由的話 那也許有其他的方法 可以讓我們使用 continuous的representation 來作為spoken language model 輸入輸出的單位 你只要對loss做一些特別的設計 告訴這個spoken language model說 你現在的輸出 只能跟正確答案的其中某一個特別接近 跟不只一個正確答案產生平均 不是一件好的事情 你只要想辦法設計你的loss 交模型這件事 你其實也可以用continuous vector 來訓練語音語言模型 能夠這樣做嗎 至少在影像上 很多好的模型都是直接產生 continuous representation 再把它轉回影像 所以其實只要你好好的處理 同一個輸入有不同輸出的問題 不需要使用discrete的token 是可以用continuous的representation的 是可以用continuous的向量 來當作生成的單位的 那在語音領域能這麼做嗎? 也已經有一些研究 開始往這個方向發展 所以確實是可以這麼做的 那我們實驗室有一些同學 做了這方面的研究 之後有機會再跟大家分享 其實剛才講了那麼多生成的策略 這些生成的策略 基本上只在語音合成的任務 能夠有非常好的表現 這個語音合成跟語音的language model 還是有很大的差異 語音合成的意思是說 給你的模型一段文字 他根據這段文字 再去做文字接龍 接出一些語音的token 你再把這些語音的token 通過detokenizer 還原回聲音訊號 所以今天這個語音合成的模型 他產生出來的聲音訊號 是把輸入的文字 念出來 他並不是在回復輸入的文字 這是不太一樣的 有人跟你說大家好 你把大家好這句話念出來 跟有人說大家好 那你也好啊 這是其實不一樣的意思 那今天這些語音合成的模型啊 他往往也能夠吃一段聲音 這段聲音是教語言模型 在合成的時候 要用什麼樣的方式 來產生聲音訊號 要產生有什麼樣特性的聲音訊號 你給他一段聲音訊號 他會變成一串token 這個語音合成的模型 就吃文字的token跟吃語音的token 再產生另外一段語音的token 這另外一段語音的token 他合成回來之後 輸出的聲音訊號 會跟輸入的聲音訊號 有類似的語音的特性 那其實今天啊 用這種token的方式是可以產生 非常好的語音合成的結果的 那以下的demo 用的是聯發科的Breezy Voice 這個模型來做demo 那我可以播三個句子 那這三個句子裡面呢 有的是語音合成的句子 有的是真正的句子 大家聽看看 你覺得哪一句是合成的 哪一句是真正的句子 我們播第一句 YAMISU的攝影展覽是免費入場的 任何人都可以免費參觀展覽 但是書內的其他設施 例如圖書館 咖啡館和書店 則需要另外付費 這是第一句 這是第二句 現在我自己會讓自己的步調慢一點點 這是第二句 接下來是第三句 我也期待下一次的聊天 祝你生活愉快 你覺得哪一句是真的 哪一句是語音合成的呢 覺得第一句是真的 同學舉手一下 有一些 手放下 覺得第二句是真的同學舉手一下 也有一些 手放下 同學舉手一下 也有一些手放下 我想一下哪一句是真的 只有第二句是真的 第一句跟第三句是語音合成合出來的 就是這個語音合成的模型 是第二句話作為輸入 產生第一句話跟第三句話 所以知道說今天語音合成 是可以真的合成到 講亂真的 雖然今天語音合成 做得這麼成功 但是你用剛才講的那些方法 來打造 語音語言模型 你可能很難打造出好的結果 就算你用上萬小時的資料 來做pre-train 你往往得到的結果是這樣子的 pre-train就是叫模型做token的接龍 語音的接龍 所以照理說你給他一句話的上半句 他要念出下半句 就算你有上萬小時的訓練資料 你做出來的結果大概就是這樣子 這個是語音語言模型的輸入 He assassinated the president 某個人刺殺的總統 那接下來會發生什麼事呢 語音語言模型就要把剩下的結果 想出來 產生更多的token 然後再token 過detokenization以後 還原成聲音 好這個語言模型 做這個語音的接龍 或接出什麼樣的結果呢 他接出來的內容是這樣子的 And gave him Mr. Johnson The last charge of improvement in his writing possible 3.89 完全不知道他在說什麼對不對 我把這句話呢 丟給GPT-4 問他說 你覺得這句話合理嗎 GPT-4他身為一個文字的模型 他對於這個語音語言模型是不屑一顧 他覺得說 這個句子根本就不合理 看起來好像有一些詞彙 好像有一些片語 但合起來沒有表達什麼意思 語音語言模型 你用剛才講的方法訓練出來 就是這個樣子 所以人們在這個地方卡了很久 那接下來我們就告訴你說 後來是怎麼取得了突破 那在上一堂課裡面 我們結束再告訴你說 就算是有上萬小時的語音訊號 也訓練不了一個好的語音語言模型 接下來我們要先問 為什麼會這個樣子呢 想想看喔 假設你現在呢 在YouTube上爬了100萬小時的語音訊號 拿它來訓練語音語言模型 你可能會覺得 一百萬個小時真的很多 那一百萬個小時真的很多嗎 一百萬個小時的語音訊號 裡面有多少的文字量呢 通常一分鐘啊 大概人類可以講一百個token 假設一分鐘算一百個token 那一百萬個小時裡面有多少個token呢 有六個billion 六十億個token 你可能覺得六十億個token 聽起來也不是一個小數目 但相較於今天的文字模型 比如說LLama 3 他在預訓練的時候 可是用15T的token喔 6個billion的token 那可能是GPT-2等級的模型而已 所以你會發現 就算你有非常大量的 語音訊號 裡面所包含的內容的資訊 其實是非常有限的 單憑這些有限的內容資訊 你可能很難讓語言模型 講出合理的句子 或是換一個角度來想 15T的 文字的token 假設你call一個語音合成的系統 假設你用一個語音合成的系統 把這些文字的內容 念出來 那這些聲音訊號有多長呢 這些聲音訊號 有28.5萬年 那麼長 28.5萬年前 那時候地球上是沒有現代智人的 所以是一個非常長的時間 所以你很難收集到這麼大量的語音訊號 真的來訓練一個語音語言模型 我們知道說文字其實是語音壓縮的版本 人類先能發出聲音才發明了文字 發明文字是為了做什麼 為了要壓縮語音 讓它永遠的被保存下來 所以文字相較於語音 是一個更compact 更壓縮的東西 所以從文字上學習 是比較容易的 語音就像是文字的解壓縮版本 它除了文字有的資訊以外 還保有其他的資訊 還有更多的資訊 它是更複雜的 所以對語言模型來說 訓練在語音上是更困難的 好那這邊呢 黑色的線 代表的是文字的模型 綠色的線 代表的是語音文字的模型 縱軸代表這些模型 在一個故事接龍的任務上的表現 那這種故事接龍的任務通常就是 給你故事的前半段 然後接下來呢 給你兩個版本的故事 然後模型呢 會去計算這兩個版本的故事 它的likelihood 它會去計算說這個模型 產生這兩個版本故事的機率有多大 然後呢選擇機率比較大的 當作是模型真正會接下去的故事 然後再算模型選到正確的 故事結尾的這個正確率有多高 總之呢 這邊的數值越大 代表這個模型 它語意理解的能力越強 它越能正確的做故事揭露 如果我們看文字模型 這邊的橫軸呢 代表的是投入的算力 可能是增加資料 可能是使用了更大的模型 對文字模型來說 用更多的資料 用更大的模型 這些文字模型 它的語意理解的能力 會持續的上升 如果你看語音的模型 語音的模型在投入 比較多算力的時候 它的成長是比文字更緩慢的 因為語音除了原來文字有的資訊以外 還有更多的資訊 所以在同樣的算力之下 從語音裡面學習到語意的資訊 學習怎麼做故事接龍 是更加困難的 而且這邊我們考慮的 都只有模型 模型的語意的能力 都只有考驗模型 理解文字的能力 還沒有問模型理解 其他語音面向的能力 比如說語者的情緒 或者是環境音的資訊等等 所以可以瞭解說 只拿現有的這些語音資料 要來訓練出一個好的語音語言模型 是非常具有挑戰性 非常難成功的 那怎麼辦呢 現在已經有很多很好的文字的模型 也許我們在打造語音模型的時候 不需要從頭開始 我們可以從文字的模型開始 文字的模型已經學了文字上的資訊 已經學了語意的資訊 我們何不反過來 我們何不就是想成所謂的打造語音模型 就是進一步教文字模型語音相關的資訊 所以其實我們在打造語音模型的時候 不需要從頭開始 我們可以從一個文字模型開始 從文字模型開始打造語音模型 其實不算是全新的概念 其實早在BERT的時代的時候 我們實驗室就嘗試過說 拿文字的BERT來初始化語音版的BERT 那這樣可以讓語音版的BERT 在語音的問答上做得更好 不過這個都是很久以前的研究了 那現在呢 有很多嘗試是拿文字的Large Language Model 來作為語音的Large Language Model的Initialization 也就是在訓練一個語音模型的時候 它的參數不再是Random Initialized的 而是從文字開始做起 那我想最早做類似嘗試的 應該是Meta的Twist 這個模型就是拿一個文字的模型 作為初始來訓練一個語音的模型 而現在一個主流的想法是 不只拿文字的模型來作為語音模型的Initialization 同時啊 因為文字的模型是語音的Initialization 所以這個語音模型它本來就是能產生文字的 這是文字模型本來就有的能力 所以我們讓語音模型在產生語音的時候 不是隻產生語音 而是同時產生文字跟語音 那些文字會作為語音的輔助 它就好像是 這個模型的獨白 它在說話之前 腦中先想一下 我大概要講什麼 然後才把那句話真的說出來 這樣可以讓模型在生成語音的時候 表現更加穩定 那這個叫做語音跟文字的Hybrid decoding 那文字是由文字的Token所組成的 語音是由語音的Token所組成的 那一個模型要怎麼同時生成語音跟文字呢 這邊就可以有不同的 生成策略的設計 那一個最直覺的策略設計是 我們的模型 先產生文字 產生完文字之後 再產生語音 要講什麼都先用文字把草稿打好 然後再把要講的話 念出來 比如說有一個模型叫Spectron 就是採用了這樣的想法 那用這樣的想法 你就不意外說為什麼可以成功的 滿模型訓練起來 因為產生文字對一個文字模型來說已經不算什麼難事 今天從文字再產生語音 那根本就是語音合成 那我們剛才在上一節課已經跟你講過說 從文字產生語音 今天已經是相對比較成熟的技術了 所以先產生文字再產生語音 是可以做出不錯的語音語言模型的 但它的缺點是什麼 它的缺點是這個模型在回應的時候 比較難做到即時的回應 因為你問他一個問題 他要先產生文字 那就等於要等一下 產生文字是需要時間的 如果這個回答又是一個長篇大論的回答 那就要等一下 等他文字產生完 他才會開始說話 那這樣他的回答就不夠即時 那怎麼做到即時的回答呢 還有一個想法是 先產生一個文字的token 接下來就產生 對應這個文字token的語音token 就先說先產生文字的號 然後再把對應的 對應到號這個聲音的語音token產生出來 產生R 再把對應到R的聲音的語音token產生出來 以此類推 這樣你就可以讓模型想一個字 就唸一個字 想一個字就唸一個字 那從使用者的觀點來看 這個模型的回答就會是比較即時的 但這樣的訓練意味著什麼 要訓練一個可以產生號 接下來產生對應號的聲音的token 意味著說 你其實有語音跟文字之間的align 你今天在訓練的時候 你不只有一大堆的語音的資料 你同時還知道說 這些語音對應的文字是什麼 你不只知道語音對應的文字是什麼 你還知道每一段文字是對應到語音的幾秒到幾秒 今天給你一段聲音 你要產生文字相對是比較容易的 因為你可以做語音辨識 但是要準確的知道 每一個文字的符號是對應到幾分到幾秒 對應到一段聲音訊號裡面的第幾個token到第幾個token 往往是有一定程度難度的 所以這個方法在訓練的時候就很吃你手上的語料 如果你手上的語料那個文字跟語音的那個alignment不好的話 那你就可能很難訓練出好的模型 那也有人想說 那我們能不能夠訓練一個語音模型 它是同時在每一步就產生一個文字的符號 跟一個語音的符號呢 這樣也可以 像我們剛才有講過說 同一步可以產生多種不同的語音的token 那麼現在把文字想成是一種很特別的token 能不能夠在同一步就產生文字的token 又產生語音的token呢 這個想法會遇到的問題是 語音跟文字的長度 往往非常的不同 而且他們不會有一對一的關係 不會說一個文字的token 就正好對到三個語音的token 他們中間的對應關係 往往是很複雜的 所以如果你要採取的策略是 在同一個時間點要產生一個語音 一個文字的token的話 就要想一些生成的策略 所以在這個地方 就有各式各樣生成的策略被提出來 比如有人想說 那如果我每次都只產生一個語音的token 一個文字的token 那往往文字token的sequence是短的 語音token的sequence會比文字token 的sequence長非常非常多 因為這邊往往一個token只對應到 聲音訊號的0.02秒 所以通常語音token很長 文字token比較短 所以如果每次生一個語音token一個文字token 那很快文字token就生完了 那怎麼辦呢 那沒關係 文字token生完之後 就生出一個代表 我什麼都沒生成的符號 你就定一個特別的符號叫epsilon 模型可以選擇生成epsilon 代表我什麼都沒有產生 那像這樣子的系統呢 它在前半段會同時生語音跟文字 然後文字生完之後 會再把語音生出來 那在Mini-Omni這個系統裡面 這個模型裡面 就是採取了這樣的策略 那還有另外一個策略是 既然文字比語音的sequence還要短 那我能不能夠文字來等一下語音呢 所以每次生完一個文字的token以後 就固定生固定數目的epsilon 固定數目的我什麼都沒有做token 所以生完一個號生三個epsilon 生完一個R生三個epsilon 然後這樣子就可以把文字的sequence拉長 來等待語音的sequence 但是像這樣的策略啊 等於是把原來文字的sequence 就直接可以變成原來的四倍長 但語音跟文字 他們之間的對應關係是很複雜的 語音的token的數目不會正好是文字的四倍長 所以怎麼辦 語音中間也要插上一些epsilon 所以今天模型在訓練的時候就會變得很複雜 模型一方面要讓文字的sequence變長 但同時語音這邊也要去配合一下文字 在適當的時機模型要知道說 語音也要產生epsilon 然後來等一下文字sequence的生成 那細節大家可以再看文獻 有篇paper叫Lamaomi 就採用了這樣的策略 還有另外一個策略是 既然語音跟文字的長度不一樣長 那我們能不能讓模型自己去決定說 文字要等多久才能夠跟語音一樣長呢 所以有一個model叫做Moshi 這個Moshi這個模型呢 它也是每一步都會產生語音 語音的token跟文字的token 但它產生一個文字的token以後 接下來模型就要去預測 它要等多久 就是它要產生幾個epsilon 是模型自己決定的 語言模型 它會自己決定它要產生幾個epsilon 這樣才能夠 它每一步都會產生語音 但是它每一步都要決定它要產生幾個epsilon 才能正好跟語音的sequence一樣長 這個方法是用在Moshi 這個模型裡面 至於哪一個方法比較好 還欠缺完整的比較 所以很難告訴你說哪一個方法 一定就是比較好的 不過啊 我在下一個章節裡面 想要跟大家介紹一個 新的語音的tokenization的方法 它可以一舉一掃除 在這一頁影片裡面講的 語音跟文字長度不一樣 所造成的困擾 所以這邊我們就來講 考慮到文字的 語音tokenization 這篇paper呢 這個方法呢叫做test 它是我們實驗室的 曾亮軒同學 跟NTK的陳怡昌 還有 我們助教那個李冠怡同學 做的,然後曾亮軒同學 今天又來了,他在這邊啦 他特別來聽老師來講他的 研究,謝謝 介紹一下那個亮軒哥 亮軒哥其實是 Cool Whisper的作者,也就是說你現在 上傳一個影片到NTU Cool的時候 不是會自動產生字幕嗎 這個是亮軒哥做的啦 那新模型是亮軒哥Train的 還在持續improve Cool Whisper 而且如果大家有好的Subtitle 要感謝亮軒哥 他同時也是2023年的大助教 那就感謝亮軒哥今天來捧場 請亮軒哥指教 所以這個模型 亮軒哥做的研究 那曾亮軒哥同學呢 提出了一個叫做 TASTE的想法 那這個TASTE的想法是怎麼樣呢 這個想法是這樣子的 我們現在已經知道 我們在產生語音的時候 同時也會產生文字 我們現在主流的做法 是做語音跟文字的Hybrid decoding 語音模型是同時產生語音跟文字的 那這樣語音的Token 它需要保留文字的資訊嗎 根本就不用保留文字的資訊了 所以語音的Token是不需要像以前那麼 我們只要讓語音的Token 保留文字以外的資訊就好了 語音的Token只要能儲存文字以外的資訊就可以了 然後再來呢 我們剛才說語音跟文字要一起生成一個難點就是 他們的長度不一樣 那能不能夠想辦法讓他們的長度一樣 我們能不能夠有一個特別的Tokenizer 這個特別的Tokenizer 他會根據輸入的語音訊息 裡面有幾個文字Token 他就產生幾個語音的Token 比如說輸入的這段聲音訊號 是How are you三個文字Token 他就會固定生出三個語音的Token 每一個Token都對應到一個文字的Token 接下來我就要講 在Network的設計上要怎麼設計 才能夠做到說 有幾個文字的Token 就產生幾個語音的Token 輸入一段聲音訊號 我們剛才講 一般的Tokenization的方式 可能是用一個pre-trained encoder 比如說一個pre-trained self-supervised model 去產生一堆representation 這些representation它的長度 這邊representation這些項量有幾個 跟輸入的長度有關係 但是跟這句話裡面的文字的數量 文字token的數量是沒有關係的 通常你就是每一個項量 對應到一個固定長度的語音 比如說0.02秒 那這邊呢 我們會對不同的layer 抽不同的項量 我們會抽出兩排項量 那等一下會講說 為什麼這邊需要抽出兩排項量 那接下來呢 一段聲音背後 對應的文字token有幾個 今天是有辦法知道的 如果你有一個好的語音辨識系統的話 你可以把聲音訊號 變成文字 所以你知道說這句話對應到幾個文字的token 接下來呢 我們有一個aggregator 這個aggregator是數層的attention layer 這個aggregator呢 會把語音辨識所得到的文字token 當作query 把從speech encoder抽出來的representation 其中一組當作key 另外一組當作value 然後這些query會去對key 做attention 然後做完attention以後 再把這些value的vector做weighted sum 你把號輸進去 把號當作一個query 就產生一個token 這個雲的token 代表的是號這個文字的token 當我們要把它念出來的時候 要怎麼念它 那把R輸進去 就得到一個對應到R的token 把U輸進去就對應到一個 對應到U的token 那在這個encoder裡面 哪一個layer適合當作key 去跟query做attention 哪一個layer適合當作value 來代表語音的資訊 來代表一個文字 token怎麼念的資訊呢 大家可以再去參考這個原始的論文 那裡面有更詳細的解析 我們這邊就先跳過這一段 然後接下來呢 我們的這個tokenizer呢 會持文字當作輸入 還有每一個文字 所對應的語音token 當作輸入 去還原原來的聲音訊號 那因為呢 這個輸入跟輸出的長度 其實它的關係非常的複雜 所以這邊的detokenizer 其實是一個很複雜的模型 這邊detokenizer其實用的network架構 跟一個TTS的模型 是一樣的 我們用的是類似 cozy voice 這個語音合成系統的network架構 所以你其實可以把這個detokenizer 想像成是一個語音合成的系統 它是一段文字 可以產生一段聲音 但是跟一般語音合成的 不一樣的地方是 每一個文字 都有一個語音的token 來教你這個文字 如果要唸它的話 現在它是怎麼被唸出來的 然後訓練的時候 就是輸入一段語音 然後最後 de-tokenizer 輸出一段語音 我們要讓輸入跟輸出 越接近越好 那這個方法叫做TASTE 就硬湊了一個梗 它叫做TASTE text-aligned speech tokenization and embedding 那在這整個模型裡面 這個pre-trained的模型 加上這個aggregator 合起來是tokenizer 輸入一段聲音訊號 它會輸出一串token 這個token的數目 跟這段聲音訊號裡面 對應的文字的數目 文字token的數目 是一樣的 那對detokenization的部分呢 detokenization的部分 它就像是一個語音合成系統 但它不是一個一般的語音合成 會有文字的token 來蓋的 來教這個tokenizer 要怎麼把這些文字唸出來 接下來下一個問題是 這些語音token真的有包含 怎麼唸一個文字token的資訊嗎 我們這邊做了一個非常簡單的實驗 這個實驗是這樣子的 我們先輸入一句 唸得很慢的聲音訊號 到tokenizer裡面 產生一串藍色的token 這段聲音訊號聽起來是這樣子的 它唸得很慢 大家要耐心聽一下 The captain's face had been buried in a pile of papers. But now Merdock came around to steer at the gang leader. 接下來呢,我們給Tokenizer另外一段聲音訊號 這段聲音訊號是唸得很快的聲音訊號,然後產生紅色的Token 這段聲音訊號聽起來是這樣的 Any News on the Dancer Assault Case 這個唸得很快喔,你看這邊文字 Any News on the Dancer Assault Case Any News on the Dancer Assault Case 非常的快 好,那接下來啊 我們做了一個小把戲 把兩邊的Token 部分交換 我們把對應到 Can Around Queue的Token 換成對應到 News On的這三個文字 Token的語音Token 所以我們把這三個文字 Token對應的語音Token 放到這裡 我們把這三個文字對應的語音Token 放到這裡 這三個字跟這三個字對應的語音Token 進行交換 那你可以預期會發生什麼事嗎 如果是左邊這個句子 本來藍色的Token代表的 是要唸很慢 但是換成紅色的Token以後 Can I run two這三個詞彙 會不會就唸得很快了呢 我們來聽一下 The captain's face had been buried in a pile of papers. But now Merdoch came around to stare at the gang leader. The captain's face had been buried in a pile of papers. But now Merdoch came around to stare at the gang leader. 我看另外一個例子 我們把news under他的token換成藍色的token 那是不是news under這三個文字的token 就會唸得很慢了呢? 大家聽一下 Any news on the dances okay? 發現news under真的就唸得比較慢了 我再聽一次 Any news on the dance is okay? 其他詞彙還是唸很快 但是這三個詞彙就會唸得比較慢 所以有了這樣子的token之後 接下來就可以訓練語音版的語言模型了 怎麼做? 首先你就是需要有大量的語音資料 然後呢 這些語音資料 你可以用語音辨識系統 跑出它背後對應的文字token 然後再通過test這個tokenizer 你就可以抽出每一個文字token 對應的語音token 接下來你就可以訓練一個語音的language model 這個語音的language model 它每一次都是產生一個語音的token 加一個文字的token 它會知道說 看到look跟語音token101 就要產生at跟語音token56 看到at加語音token56 就要產生my跟語音token33 看到my語音token跟語音token33 要產生ice跟語音token162 然後就可以有一個語音版的語言模型了 那我們這邊呢 因為算力有限的關係 所以真的沒有很多資料 我們是用一個現成的Corpus 那這個Corpus裡面很多是 從這個網路上載下來的 這是一個公開的資料集 裡面是四萬小時左右的英文 那我們的語言模型呢 是拿一個1B的Lama 3.2 來做Initialization 那至於表現如何 大家可以直接去看原始的論文 我這邊呢 就是展示一下這個模型 做語音接龍的能力 那我要強調一下 這是一個retrain的模型 他並沒有做過supervised fine tuning 所以他會做的事情 就是語音接龍 給他一句話的前半段 然後他就接後半段出來 那你可以想像說這樣的模型 你可以接下來再做 supervised fine tuning跟ILHF 他就可以真的跟人對話 我們先來看看這個模型 他做 Continuation 做語音接龍的能力 怎麼樣 那這邊就是舉了四個例子 第一個例子 輸入是這樣的 I'll take the arm chair 他說I'll take the arm chair 發生什麼事呢 In the corner and just sit there It'll feel better I'll try to not sleep 他會盡量不睡著 希望這個演講 沒有無聊到讓大家睡著 然後呢 如果今天是男生的聲音呢 I reserve your services 他說I reserve your services 接下來呢 for a long time I am very happy with the result I will definitely recommend you to my friends 他得出一個 合理的語音接龍的結果 然後我也試了一下 假設讓他聽 有口音的聲音會怎麼樣 聽一下這個句子 這個是Figman Theory裡面 Roger的聲音 然後那接下來會怎麼樣呢? 他說It's hard and loud and so many people 接下來呢? I don't know what to do I'm not a fan of crowds I'm not a fan of loud music I'm not a fan of being in a crowd I'm not a fan of being in a big city 所以聽起來還是有一點點口音的 或者是我們讓他輸入這一個句子 這句你一定聽過 他是這樣子的 Look at my eyes 大家都已經知道接下來的光處就是 Tell me why baby why 然後看看模型會接出什麼樣的結果 模型接出來的結果是這樣子的 I'm not the one who's wrong I'm not the one who's wrong I'm not the one who's wrong I'm not the one who's wrong I'm not the one who's wrong 他就重複I'm not the one who's wrong 然後這邊還產生一個noise 這個noise是模型自己產生出來的 那我會覺得這是一個feature不是一個bug 在聲音訊號裡面 我會仔細聽小明建模的聲音 他的麥克風其實品質沒有很好 所以背景是有一些雜訊的 所以模型在生成的時候也生成了一些類似的雜訊 接下來我還做了一個 莫名其妙的demo 這個不能說非常的成功 我單純只是很想要播這個迷音而已 大家應該都看過這個迷音吧 不知道這個迷音的人 我播一下 我播一下 嗨 嗨 嗨 嗨 嗨 嗨 嗨 嗨 嗨 嗨 嗨 這是有一個團體叫做 ice cream的表演 那這是個非常知名的迷因 他被換成各式各樣的東西 比如說最知名的版本 可能是黃仁勳的版本 YouTuber在搜尋會先搜尋到黃仁勳的版本 然後下面一堆人推說 黃仁勳這就是你要的世界嘛 然後那個資訊之夜啊 那個資訊之夜的宣傳 其實也是用資工獎的影片 然後就什麼什麼 比起演算法我更喜歡你 比起紅黑書我更喜歡你 就是這樣 然後我就想說 剛才我們聽到三個嗨的聲音 三個角色分別是露比醬 還有四季醬 他們都產生了一個嗨的聲音 但這三個嗨的聲音是不一樣的 他們三個人各自有聲音的特色 也許我們再聽一次 露比醬的聲音是最高的 Yui醬的聲音是中間的 嗨 四季醬的聲音是比較低的 所以接下來用這三個嗨 再丟進去 Pretrained speech language model 然後讓他做continuation 做聲音接龍 我們來看看他會接什麼樣的東西 不過這邊還是做了一些前處理啦 我有特別把那個音樂拿掉 因為我發現音樂對這個模型的影響蠻大 他訓練的時候可能沒有什麼太多音樂的資料 所以聽到音樂蠻容易讓他壞掉的 所以我其實有把音樂呢remove掉 好那你就跟他先講露比醬嗨 露比醬嗨 嗨嗨 好,看看模型會接什麼 I'm Ruby, a third year PhD student in the Department of Computer Science at the University of Cambridge. I'm currently working on a project to build a language for describing and reasoning about security policies. 他前面還有說,I'm Ruby,這樣子 好,然後看看Yui醬 好,繼續接下去喔 I'm a new member of the club I'm from Japan I'm studying in Australia. Can the big fan of anime? 我不知道為什麼如果是Yui醬的話 我試了好幾次 他都會,但每次因為他有sample嘛 每次答案都不一樣 但每次他都會說他是日本人 所以可能這句話 特別會讓模型覺得 就是一個日本人講的 最後是四季醬 接下去喔 I'm the host of the podcast girls guide to travel I'm a travel writer photographer, photographer, an educator, I've been traveling the world for over a decade, and I'm passionate about sharing my experiences with you, not passionate about sharing my experiences with you. 對,就是這樣,他就有點像,有一點點像是四季醬的聲音,總之,這就是一個pre-trained的語音語言模型,在有的比較好的encoder的方式之後,可以用比較少量的資源,就訓練出一個還可以用的語音語言模型。 好,那我們講了這麼多,但其實目前為止,我都只講到pre-trained而已,有了一個pre-trained的模型之後,接下來要怎麼變成一個真的能夠互動的語音語言模型呢? 也許可以參考的路數就跟文字模型是一樣的,先有大量的網路資料,訓練出一個好的pre-trained的語言模型以後,接下來你就可以做這個supervised fine tuning,拿一些人跟人之間的對話來教 這個語音語言模型說,聽到一句聲音,你要怎麼樣回應才是人類會有的回應,那這個部分,這個資料的收集啊,其實用網爬的資料反而不一定是最合適的,但你可以去錄一大堆人的對話,但他不一定是最適合拿來fighting這些模型的資料,因為其實今天我們是從一個文字的模型開始做fighting,所以這邊雖然我們在課程裡面沒有大力的強調,但是其實也是會有forgetting的問題, forgetting的問題也是一個我們需要非常注意的問題,所以如果你只是收集一大堆人跟人之間的對話,拿來fine-tune你的pre-tune model,往往你就把pre-tune原來的文字能力fine-tune壞了,因為這不是你的pre-tune model原來會說的話,所以怎麼辦呢? 一個你今天常見的套路就是拿原來pre-tune之前的文字模型,這個pre-tune的語言模型,他也是拿一個文字模型再去做fine-tune來的嘛,拿那個文字模型,叫他自己產生對話,建議跟一個模型說,欸,你跟一個模型, 你就產生兩個人的對話,他完全可以做到,然後再把這個對話用TTS,用語音合成,把它唸出來,你就有訓練資料可以拿來fine-tune pre-train的模型了,然後這邊呢,就拿了幾個,拿這樣的方式所生出來的對話給大家聽看看,然後等一下在對話裡面,你會聽到一個男生的聲音跟一個女生的聲音,那男生的聲音呢,背景有雜訊,但是故意加上去的,想要模擬這兩個人好像是用電話 在講話的感覺 所以聽起來像是這個樣子 你好 我最近在考慮投資 不知道該從哪裡開始 你能給我一些建議嗎 當然可以 首先我需要了解你的 風險承受能力和投資目標 你是比較偏好穩健 還是高風險的投資 我偏向於穩健一點 因為不想損失太多 而且長期來看 我希望能有穩定的收益 最好能出去些退休金 瞭解 那麼你可以考慮一些低風險的投資選擇 比如定存 國債或是 所以今天呢 你可以非常輕易的產生這種對話 哪一個文字模型 生文字的對話 再拿語音合成系統合出來就好 其實啊 就我所知那個NotebookLM 他NotebookLM不是會自動生Podcast的嗎 其他生Podcast也是這麼做 他應該不是一個M2M的模型 他是先輸入 新生文字的對話 再用語音合成合出來的 我甚至懷疑 我第一次看到那個NotebookLM會生Podcast的時候 我甚至懷疑說 Google是不是在開發語音語言模型的過程中 在生資料的過程中打造了這個模型 然後想說順便試出來給大家玩也不錯 所以才有了這個 才有這個產生Podcast的構想 不過這只是一個胡思亂想而已 那至於呢 你要教語言模型什麼東西 就要看你在fine tuning的時候準備什麼樣的資料 舉例來說 假設你今天想要教你的語言模型聽得懂 環境音 也許你需要準備一些跟環境音有關的資料 比如說像是這樣 這種聲音常常用來標示什麼事件 這種聲音通常用來標示重要的事件 比如課程開始或結束的鐘聲 所以你就可以教模型說聽到叮這個聲音 聽這個聲音的含義就是代表課程結束的聲音等等 或是假設你希望模型可以對人類非語言的訊號有反應 比如說對人類咳嗽的聲音有反應 那你可以這樣教他 最近我愛上了煮咖啡的過程 煮咖啡很棒 聽到你咳嗽了 記得多喝水 就給他一個咳嗽聲 然後教他要回答說請多喝水 模型期待他就可以知道說 聽到一個人咳嗽的時候 要怎麼樣回應 總之你可以自動生存的資料 接下來用這些資料 你就可以fine tune一個pre-trained模型 那希望就會有一個好的語音版語言模型 今天蠻多模型就是用這種路數打造的 接下來呢 你還可以做ILHF 那現在呢 在語音模型領域還有HF 也不算是特別稀奇的東西 那ILHF他的概念大家都知道 就是語音模型產生 不只一個答案 人去評價哪一個答案是比較好的 那比較早年的研究 所以比較早年的研究大概是一年前 那個時候人們比較在意的是 語音語言模型合成出來的 quality 所以有很多論文他的feedback 都是跟quality有關的 就是人告訴語言模型說這句話 合出來的聲音品質比較好 這句話合出來的聲音品質比較差 那最近流行的趨勢是 quality已經做得很不錯了 下一階段呢是希望模型可以 聽懂聲音 尤其是對文字以外的聲音 比如說音樂 或者是環境音 它能夠正確的辨認這是什麼樣的音樂或環境音 所以這幾個月 有一系列的文章都是用 RLHF來強化模型 理解聲音的能力 那除了 RLHF以外 也可以做RLAIF 就是用一個模型來提供 語音模型回饋 這篇論文呢 使得林冠廷同學跟Amazon的 研究人員合作的結果 他們就是訓練一個語音模型 用RLAIF的方式 用文字模型 來當作那個AI 文字模型會去看那個 語音模型它產生的語音訊號 辨識出來產生的句子合不合理 然後把文字模型的輸出 當作語音模型的feedback 再來強化語音模型的能力 所以你也可以做RLAIF 那講到這邊啦 其實還有一個 跟語音非常有關的關鍵特色 我們今天還沒有講到 就是一般的文字 互動是 回合制的 你今天在跟一個文字模型互動的時候 就是你講一句話 他講一句話 你講一句話他再講一句話 這個互動之間的間隔 是非常明確的 模型怎麼知道你講完了 就是你打字完輸入enter 輸入enter之後模型知道 講完了 所以今天你什麼時候講完 有非常明確的訊息 但是對語音的對話來說 就不是這樣了 語音的對話是非常複雜的 今天兩個人在對話的時候 他們既說也同時聽 他們不只是聽 他們同時也可以說話 一個模型要同時聽 也同時說這種能力 叫做4Duplex 所以今天兩個人在對話的時候 他中間可能會有很多的overlap 有一個人他在講 比如說他出去玩的事情 另外一個人如果他一直不講話 你就覺得很奇怪嘛 他是要發出一點聲音的 所以他要說 這個人還沒講完的時候 另外一個人可能就有可能會插話等等 所以兩個人在說話的時候 他們的聲音訊號是有非常多重疊的地方的 所以兩個人在對話的時候 他是一邊說又一邊聽 這兩個能力是需要同時存在的 那怎麼打造 這樣子的能力呢 怎麼讓一個語音版語言模型 同時既能聽又能說呢 如果是講到目前為止 你對於文字模型 對於這些autoregressive model認知就是 它要嘛就是輸入 要嘛就是輸出 怎麼同時輸入又輸出呢 這就需要新的技術了 所以這邊需要一些跟你所知道的 那些autoregressive model 不一樣的模型 才有辦法同時說又同時聽 不過這個部分 如果要具體來講的話 可能需要在下一節課 那就是另外的資訊了 那如果大家想知道的話 我這邊就是列了一些有代表性的reference 給大家參考 大家看看文獻上這些語音語言模型 是怎麼做到同時聽話跟說話 另外一方面 怎麼評量一個語音語言模型的好壞 也是另外一個值得關注的議題 語音語言模型不只要產生文字的資訊 它還要產生文字以外的資訊 比如說當我們講到 Toxicity 講到模型的安全性的時候 如果是文字模型 那你考慮的 是文字模型輸出的文字 是不是安全的 它有沒有不小心輸出 不該輸出的文字 但是語音模型除了考慮文字輸出的內容之外 還要考慮文字以外的資訊 比如說假設一個模型 他講話就是很尖酸刻薄 他講每一句話聽起來 都像反諷的意思 就像他文字內容是你好棒喔 聽起來人類會覺得很不舒服 所以語音是需要考慮 本來文字模型所不會考慮的面向的 所以要怎麼評量語音模型 又是另外一個挑戰 我們大助教楊志海同學 最近寫了一篇 語音語言模型評估的 overview paper 那我把連結放在這邊 給大家參考 如果你想要知道 更多語音語言模型相關的事情的話 也可以看 可以看一個這個github repo 在我們的實驗室張凱維博士維護的一個github repo 裡面會更新最新的語音語言模型相關的論文 或者是這邊有一大堆的overview paper 在告訴你說 近幾年來語音語言模型有什麼樣的發展 那我自己呢 也跟其他學者寫了一篇overview paper 摘要了 重述了語音語言模型 這幾年發展的歷史 然後把連結放在這邊 給大家參考 其實今天就只講了一點點的內容而已 很多更多的內容 大家可以參考這篇overview paper 直到語音語言模型 完整的發展歷程