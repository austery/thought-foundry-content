大家好！ 近期AI數據中心太火了 從OpenAI到特斯拉 到各個雲計算巨頭 它們都在摩拳擦掌 大規模建設全新的AI數據中心 這些AI數據中心的規模越來越大 一次次挑戰人類工程的極限 可以説AI數據中心的基礎設施建設 是美國曆史上繼鐵路 公路，和互聯網之後 又一次基礎設施建設的熱潮 它帶動了相關産業的繁榮 也驅動着美國GDP的持續增長 我今天就給大家講一講爲什麼AI數據中心的規模越來越大？ 什麼是吉瓦特的數據中心？ 爲什麼數據中心的需求 導緻電力成爲了瓶頸？ 電力，製冷和芯片 在數據中心的建設和運營中的成本比例是多少？ 數據中心未來的走勢是什麼 尤其是哪些關鍵技術驅動着未來的數據中心走勢？ 這期我將讓你不但知道是什麼 還要讓你理解爲什麼 話不多説，讓我們進入正題吧 我先説什麼是數據中心 數據中心在颱灣叫做資料中心 在公司裡 你可能去過你們公司的機房 機房裡有可以遠程訪問的服務器 有網絡設備，還需要製冷，需要供電 你可以把數據中心理解成一個更加大號的機房 在數據中心，有大量的服務器 服務器放置在機架上 而機架又和機架通過網線或者光纖密密麻麻的連接起來 最終它們通過電信運營商的光纖連接到外部的世界 有些公司，有自己的數據中心 如銀行或者政府 往往是出於安全的考量 大部分公司是租用數據中心 僅僅是把自己的服務器放置到別人管理的數據中心 於是就有了專門做數據中心的運營商 如Equinix和Digital Realty 在中國 數據中心主要是電信運營商提供的 有些公司甚至連服務器都不是自己的 而是租用的 這就是雲計算的方式了 這種數據中心也可以稱爲雲數據中心 提供服務器出租的就是雲計算運營商 如亞馬遜的AWS 微軟的Azure和Google的Google Cloud 在ChatGPT爆火之前 大部分數據中心裡麵的服務器都是使用CPU CPU是處理通用任務的，如網站訪問 流媒體，電子商務交易等等 ChatGPT的爆火，使GPU的需求暴漲 無論是大模型的訓練，還是推理 都需要大量的GPU 以支持AI任務爲主要目標 以GPU爲主的數據中心 就是AI數據中心 這兩年數據中心建設熱潮 其實絶大部分都是在建AI數據中心 而不是傳統的數據中心 AI數據中心的一個趨勢就是規模越來越大 這導緻了很多工程上的挑戰 爲什麼AI數據中心的規模越來越大呢？ 我給大家解釋一下 大家知道爲什麼半導體芯片的製程要從5納米 到4納米 到3納米，爲什麼製程越小越好呢？ 根本原因是一繫列的好處 例如當晶體管距離越來越近的時候 它們之間的通信性能會更好 延遲會更小，功耗會更低 同樣麵積的芯片就可以放更多的晶體管進去 英偉達Blackwell 世代每顆B100超過2080億個晶體管 是史上最複雜的芯片之一 它使用颱積電的4納米製程 除了製程以外，就是先進的封裝 例如 CoWoS是颱積電在高性能計算和AI領域最關鍵的先進封裝技術之一 目前全球幾乎所有頂級AI芯片（如NVIDIA、AMD、Google TPU）都依賴它 CoWoS 是颱積電的王牌級2.5D封裝技術 通過矽中介層實現多芯片加HBM高速內存的超高帶寬互連 無論是先進的製程，還是封裝 目標都是讓芯片越來越小 集成度越來越高，讓同樣麵積的芯片 容納更多的元器件 再説白了，就是提高密度 密度的提高推動着芯片性能的提昇 英偉達的黃仁勳經常拿着一個大大的芯片 説白了就是把更多的CPU GPU，甚至是存儲 通信，集成到一個大大的芯片中去 集成的越多，密度越高，性能越好 這就是芯片行業的大趨勢 以上我説的是芯片 不同的芯片組成了AI服務器 AI服務器包括了CPU，GPU 內存和網絡 單個服務器的外殼就是一個機箱 英文是Chassis 類似於我們平時看到的颱式機 多個機箱可以裝在一個機架上 英文是Rack 形成更大的機架服務器 如果是機架之內 服務器之間的通信距離短 延遲低，效果更好 所以 我們需要在一個機架內放置儘量多的機箱 或者説放置儘量多數量的CPU和GPU 也就是説，一個機架 容納的CPU和GPU越多 越優化，性能越好 爲什麼通信距離短很關鍵？ 因爲通信距離越長，延遲越高 同時速率越低 所以，儘量把通信保持在一個機架內 而不是跨機架，這非常關鍵 説完一個機架 接下來就是數據中心會有大量的機架通過高速的網絡聯合工作 形成一個集群 在AI時代，這也叫AI集群 或者叫做AI超級計算機 甚至也可以叫做AI工廠 這些機架之間有大量的通信需求 同樣的道理 它們的距離越近 延遲越低，通信速率就越高 所以 最終我們在設計AI數據中心的時候 就是讓這些機架儘量靠近 每個機架儘量容納更多的CPU和GPU 每個CPU和CPU儘量容納更多的晶體管 所以，以上説這麼多了 其實就是一個關鍵詞，提高“密度”。 無論是在芯片級，還是機箱級 還是機架級 還是數據中心級，密度就是王道 提高密度，就是提高性能 黃仁勳所講的AI工廠 説白了就是一個統一的高密度的AI數據中心 聯合工作 它們的輸入和輸出都是Token 這就是未來的AI數據中心 除了要高密度 爲什麼一個AI數據中心的GPU越多越好呢？ 因爲現在的一個AI數據中心其實就是一颱超級計算機了 這個超級計算機擁有的GPU越多 自然他們的性能越強大 爲什麼不多建幾個數據中心 把它們通過高速的光纖連接起來呢？ 以前這種結構沒有問題 因爲數據中心的服務器之間通信任務不多 但是在大模型時代，GPU之間大量通信 長距離的光纖通信會增加時延 影響GPU之間的協調 換句話説，在AI時代 分佈式數據中心並不是最優解 這就導緻了這兩年巨頭門紛紛把越來越多的GPU放置到一個數據中心 導緻了AI數據中心規模越來越大 接下來介紹一下由馬斯克的 xAI所打造、廣泛被稱爲“世界上最大的 AI 數據中心/訓練集群”之一的Colossus 繫列 Colossus 的第一階段建在美國田納西州孟菲斯 第一版本上線時採用約 100 000 塊 NVIDIA H100 GPU 隨後迅速擴展，到 2025 年中期 GPU 數量達到約 200,000 塊以上 電力消耗已超過約 250-300 MW 該項目的未來目標規模爲“百萬 GPU 級”、功率 “約 1 GW 級” 的超級集群 在AI時代 數據中心距離用戶的距離已經不像以前那麼重要了 如果數據中心主要是用來訓練的 那麼放到哪裡都沒有問題 如果數據中心主要用來推理 例如回答用戶提出的問題 那麼推理所需要的時間遠大於網絡延遲所帶來的時間 所以，減少推理的計算時延的意義 遠大於數據中心距離用戶遠近所帶來的時延改善的意義 基於這個原因 AI數據中心已經不需要離用戶太近了 這和傳統的數據中心不同 數據中心的密度的大大提高 GPU數量大大的增加 這給數據中心的設計和運營帶來了巨大的挑戰 無論是供電，還是製冷，還是網絡 都使AI數據中心與傳統的數據中心大大不同了 工程難度不在一個數量級了 我接下來就一一説來 我先説供電 xAI 公佈其“Colossus 2”項目目標是達到約 1 吉瓦級別或更高的電力容量 1 吉瓦理論上可以同時供 100 萬戶家庭使用 相當於 一座中型髮電廠全力運轉 AI數據中心不僅“燒算力”， 還得“燒電來降溫”， 總功率往往比一般雲計算中心高 2–5 倍 微軟CEO Satya Nadella透露 公司已採購的芯片因電力和數據中心不足而閒置 而未來AI數據中心到底需要多少電力才足夠？ 沒人知道 即便是Sam Altman或Satya Nadella在採訪中透露他們也不知道 這種未知源於人工智能技術本身的高速演進 Gartner預測 人工智能和生成式人工智能正在導緻用電量飆昇 未來兩年數據中心的用電量預計將增長高達160%。 Gartner還預測，到2027年 40%的現有AI數據中心將因電力供應不足而導緻運營受限 剛才提到了 AI數據中心已經不需要離用戶太近了 那麼AI數據中心選址時 什麼更加重要呢？ 現在電力的充足性成了首要考量 中國的東數西算 也就是將東部産生的數據拿到西部能源充足的數據中心計算 也是同樣的道理來佈局數據中心的 現在很多時候有一個有趣的現象 就是衡量一個數據中心的大小已經不使用GPU的數量了 而是使用耗電量 黃仁勳在 GTC 等演講中通常用“吉瓦級功率”和“百萬 GPU 級規模”來描述未來 AI 數據中心的算力潛力 在 2025 年 7 月 OpenAI 與 Oracle 籤署協議 計劃開髮大約4.5吉瓦特的數據中心電力容量 這種數據中心GPU的數量應該是幾百萬了 電力是一個傳統行業，以往增速很慢 今年很多電力股都因爲這種巨大的電力需求而大漲了 AI數據中心對於電力需求的可能最終的解決方案 我認爲是核能 尤其是可控核聚變 這裡就不展開講了 總之 電力基礎設施會佔整個數據中心投資的20%。 下一個挑戰是製冷 剛才我一直在強調高密度 高密度導緻散熱更加睏難 使AI數據中心對於製冷的要求大大提高 傳統數據中心的散熱有些類似於辦公室空調加電腦的風扇 在高密度的環境下 根本就不夠 於是，液冷正在迅速普及 液冷是一種使用液體作爲散熱介質的技術 通過液體的高熱容和高導熱性 將電子設備産生的熱量帶走 以實現冷卻 數據中心需要液冷是因爲現代計算設備的功耗和髮熱量越來越高 傳統風冷已無法滿足散熱需求 而液冷能夠提供更高的散熱效率 並帶來節能、提高機櫃密度、運行更穩定等優勢 液冷同樣會消耗大量的水和電力 製冷繫統會佔到數據中心投資的15%左右 建設數據中心還需要網絡 包括數據中心內部的網絡 以及和外部連接的網絡 我不多説網絡了 我在講加速計算的視頻 有對於數據中心網絡的專門的講解 大家可以回看 例如 Broadcom和Arista就是專門做數據中心網絡芯片和繫統的 英偉達有自己的Nvlink和Infiniband技術 同時也支持以太網 網絡投資會佔到數據中心投資的5%左右 另外的50%， 就是芯片和服務器這些IT設備了 而芯片，主要就是GPU，TPU CPU，DPU，HBM等等 今天我就僅僅側重於建設一個AI數據中心了 總結一下 製冷繫統會佔到數據中心投資的15%左右 製冷繫統會佔到數據中心投資的15%左右 網絡投資會佔到數據中心投資的5%左右 另外的50%， 就是芯片和服務器這些IT設備了 運營一個AI數據中心同樣價格不菲 英偉達的GPU迭代非常快 導緻GPU每年産生大量的折舊費用 這就是爲什麼很多投資者擔心用於AI數據中心建設的資本支出增長過快 它和以前鋪設光纖，建設鐵路 公路的不同之處在於AI數據中心折舊太快 過時太快 好了，我今天就是要讓大家知道 這些互聯網巨頭巨大投入的AI數據中心都是什麼 爲什麼密度非常重要 爲什麼GPU的數量越來越大 爲什麼電力消耗和液冷都是十分關鍵 巨頭的資本支出持續加速 亞馬遜2025年預計總投入1250億美元 谷歌預計2025年的資本支出將在910億至930億美元之間 Meta預計2025年資本支出爲700億至720億美元 微軟在下一財年將投入1000億美元 蘋果預計未來4年在AI領域投資800億美元 可以預期 接下來幾年一座座嶄新的AI數據中心將拔地而起 AI工廠將成爲第四次工業革命的大腦 不管是不是泡沫，人類正在創造奇跡 它們都是偉大的工程 就和當年埃及人建金字塔 中國人修長城一樣被載入史冊 我們是有幸見証奇跡和見証曆史的一代人 同時 和大家一起學習AI數據中心最新知識 提高自己的認知 讓我們一起在這場AI數據中心的科技革命中找到逆風翻盤的機會 最後是廣告環節 如果你想了解到更多AI技術大趨勢 和大科技公司季報的詳細分析 一盃咖啡錢，加入我的Youtube會員 我會及時在會員社區留言分享我對於主要科技趨勢和科技大公司財報的看法的 我的特長是對於技術趨勢的深刻理解和科技股的長期投資經驗 都看到這裡了 關注老科的Youtube頻道 抓住AI時代的大機遇 讓我們的財富在這次科技革命中穩步增值吧