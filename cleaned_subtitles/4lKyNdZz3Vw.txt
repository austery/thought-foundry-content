Today, I want to talk to you about YouTubers who treat AI as if it stands for "apocalypse imminent." There is, I'm sure I don't have to tell you, a ton of AI-related commentary here on YouTube. After all, people who spend a lot of time writing scripts and making videos are likely to have a lot of opinions about software that generates scripts and videos. And of course, YouTubers make videos about their opinions on AI because YouTubers make videos about their opinions on absolutely everything. And there is some great YouTube content on how large language models work, like the series on 3Blue1Brown that is amazing that I've recommended many times and I've linked below. But a lot of the YouTube videos about AI, if you think about it, seem different. There are countless videos on YouTube, from a bunch of different creators, that present a surprisingly consistent narrative. But an odd one? It's the kind of story you'd expect to hear from those crazy preachers that keep insisting that the world is about to end. In any other context, without the money and the marketing behind all this and the way that the AI industry has co-opted so many of the sci-fi tropes from over the last century or so, we would all realize how preposterous this is. And it's bad enough when it comes from people who were making videos about the current trends in the most exaggerated way possible just to try to get clicks. What's worse is when the videos are from channels that claim to be, and have built a reputation for being, well-researched, responsible, factual and educational. And those videos sometimes even misstate the past and present in order to justify a sensationalist AI future. Now, there are a lot of channels that a lot of videos I could have picked. Today, I've just picked a few examples from SciShow, Kyle Hill and Kurzgesagt, each of which has accumulated millions of views across their AI-related videos. I'm going to show you some clips, I'm going to point out the misinformation and the unsupported assertions, and then I'm going to show you how that props up what the AI industry is trying to do, how harmful that is being to real people, and what we should be doing instead. And I'm going to try to do it as calmly as I can, which is going to be hard for me because I'm pi------------- Welcome to the Internet of Bugs. My name is Carl. I've been a software professional since the 1980s, and I'm trying to do what I can to make the Internet a safer, more reliable and less buggy place. And in my professional opinion, AI is the biggest current threat to the Internet, because the code AIs generate is still riddled with bugs and security problems, and because the narrative around AI is motivating companies to lay off competent software developers that could otherwise be fixing bugs and improving the online experience for all of us. But also, even though it's outside my particular software-related YouTube niche, because these AI companies and their creations are causing problems even worse than bugs, they're turning the Internet into a transportation medium for disinformation and worse, inflicting harm on innocent humans in the form of AI psychosis, encouraging trouble to keep dangerous to commit self-harm - allegedly - and likely causing problems in ways that we haven't even found out about yet. Okay, quick housekeeping note before we get started. I recently made a video that was a rant about a specific SciShow episode about AI that I thought was particularly egregious, but because that one SciShow video is really just a drop in the ocean that is AI disinformation on YouTube, and because too many people thought that that video was more about "SciShow is bad" rather than "AI propaganda is bad," I'm replacing that video with this one, and I've incorporated some of that video script into this one. So if some of this sounds familiar to regular watchers of this channel, that's why. Okay, the first consistent idea that's pushed in many of these YouTube videos is what I'll call the inevitability of AI. The idea that super AI is going to happen no matter what we do, and so there's no point in fighting it. Let me show you some examples. And make no mistake, chat GPT is the most visible of these generative AIs, but it's just the first. There will be more, and they will be better, and then those will get better, faster, and then what happens? These models are as bad as they are ever going to be, which I totally accept. Like that is true. In the past, AI was narrow and able to become good at one skill, but was rather bad in all the others. Simply by building faster computers and pouring more money into AI training, we'll get as new, more powerful generations of AI. These technologies are going to continue to develop and spread globally. If the United States stops developing AI, China is not going to. So there are a couple of variants on a theme here. One is the AI is going to get better and better and better narrative. And the other is, well, if we don't build it, China's going to. I find it very interesting that those ideas are presented over and over as if they're just obvious, with no better justification than, as St. Greene says, like, that is true. I don't buy that it'll only ever get better or narrative. In fact, I made a whole video about how I think that's the biggest lie we get from the AI industry. And I'll link that below. But it's not just me. AI critics have been saying that for years and people from the industry are finally admitting that just adding more computers isn't enough. But what's important for this discussion is that the logical conclusion of the inevitable way of thinking is that trying to stop AI development is pointless. Now, who might want you to believe that and why? Let's move on to the next one of the attributes that these AI hype videos seem to share, which is what I'll call exceptionalism. This is the idea that AI is something so exceptional, so new, so different than anything humanity has experienced before that previous roles and experiences just don't apply. So here's a clip from a Kurzgesagt video called, ridiculously: "AI, humanity's final invention." In 1997, an AI shocked the world by beating the world champion in chess, proving that we could build machines that could surpass us. Oh, okay. Let me get this straight. In 1997, a chess program finally proved that we could build machines that could surpass us. Really? You think so? Let me introduce you to page seven of "giant brains or machines that think," by Edwin Berkeley, published in 1949, quote, The kinds of thinking a mechanical brain can do. There are many kinds of thinking that mechanical brains can do. Among other things, they can learn what you tell them, apply the instructions when needed, read and remember numbers, add, subtract, multiply, divide and round off, look up numbers and tables, look at a result and make a choice, do long chains of these operations one after another, write out an answer, make sure the answer is right, know that one problem is finished and turn to another, determine most of their own instructions, and work unattended. They do these things much better than you or I, unquote. Now that sounds like a modern reaction to ChatGPT ChatGPT. Well, with the exception of "make sure the answer is right" because ChatGPT ain't doing that one. It turns out that the vast majority of the claims about how AI is going to change everything in ways we've never seen before are incredibly similar to the claims in the 1940s and 1950s about how giant brains, aka computers, were about to change everything in ways we've never seen before. If you read about the history of how technology impacts society, you'll find that older humans tend to freak out about newly invented technologies in exactly the same way that their parents or grandparents freaked out about tech that they consider normal. Later this quote, "The free access which many young people have to romances, novels, and plays has poisoned the mind and corrupted the morals of many a promising youth." That's from "memoirs of the Bloomsgrove family" by Reverend Ennis Hitchcock published 1790. Almost every generation insists that what's happening now is different, so different that it's changing everything. And yet, life goes on. But the fact remains that there are many, many, many ways that computers have surpassed us over the decades. We just take it for granted now that computers have been better than us at arithmetic and searching text since my parents were in elementary school. But arguments like "AI proves that X can happen" when in fact X happened in the 1940s is only one of the ways that these videos claim that AI is exceptional. Sometimes they also decide to just make things up. Here's Kurzgesagt again: "Humanity is not ready for what will happen next, not socially, not economically, not morally." Okay, and you know that how, exactly, and that's based on what exactly? Guess it must be the Hank Green School of, "like, that is true." So these next clips have another way of claiming exceptionalism, In this case, by just being disingenuous about past events. We really are at a precipice here. Nothing has happened before like this, and not so fast. Technology moves fast. We went from the Wright brothers to the first commercial airline in just 11 years. But even compared to aircraft, antibiotics, and nuclear power, the speed at which we are developing artificial intelligence beats them all. This is all happening really fast, faster than cell phones, faster than computers. So this is just blatantly false. Let's pick one example from the end there. The speed at which we're developing AI does not beat the speed at which we develop nuclear power. And it's not even close. The Manhattan Project was founded in August of 1942. By December, four months later, they had a nuclear reactor running at 200 watts and three years after that, in August 1945, more than 100,000 people were killed in nuclear explosions at Hiroshima and Nagasaki, 10 years after its founding in 1952. Both the US and the Soviet Union had nuclear-capable strategic bombers and the area of mutual assured destruction was beginning. OpenAI was founded in 2015. Had OpenAI accomplished anything as impactful as Hiroshima by three years after its founding? No. Has OpenAI or anyone or anything related to AI accomplished anything as impactful as Hiroshima ever? So no, thank God. Is there now, 10 years after OpenAI's founding, an AI equivalent of a strategic nuclear bombing fleet, as there was 10 years after the Manhattan Project? Again, also no. So how exactly is it that AI is supposed to be developing faster than that? Yeah, it's not. And SciShow should know better, and do better. But this nuclear comparison leads to the next attribute of this garbage AI claims by YouTubers, which often, but not always, is discussed in terms of making humanity extinct. So I'm going to call it lethality. This is the idea that AI is going to kill all of us, or at least almost all of us. You know, Skynet from the Terminator series, that sort of thing. This particular flavor of fabrication has gotten incredibly popular recently, since the release of a book called If Anyone Build It, Everyone Dies, which is just more an unsubstantiated nonsense. I made a video about that book, which I'll link below if you want even more information about that. The supposed lethality of AI is based on some really shaky assumptions, one of which is that artificial superintelligence is possible. Another is that artificial superintelligence will have incomprehensibly advanced mythical God-like powers of knowledge, deception, and prediction beyond what science allows. And another of which is that artificial superintelligence can be reached, probably in very short time scales, by a process called recursive self-improvement, which is where AI makes a smarter version of itself, and that version of a smarter version of itself, and that version makes a smarter version of itself, and so on and so on. It's a hypothesis that's taken straight out of the Hitchhiker's Guide to the Galaxy. "I speak of none, but the computer that is to come after me, a computer whose nearest operational parameters, I am not worthy to calculate, yet I will design it for you." I do not believe that the doomer idea of superintelligence, artificial or not, can possibly exist. And that's not my personal opinion, that's because superintelligence, as doomers usually describe it, is precluded by well-established mathematical and philosophical principles. I'm not going into that today though, that's going to have to be its own video, I'm working on it, so hit the subscribe button if you want YouTube to suggest it to you when I get it done. So here's a quick look at how some YouTubers push the idea of AI lethality. "Humanity's final invention, artificial intelligence." Many reasons why future AI might be impossible to control. AGI might be the last invention of humanity. It's possible that it could become the most intelligent and therefore most powerful being on Earth, a god in a box that could subvert civilization and bring about our end with humanity unable to come up with a way to stop it. If or when a super-intelligent AI decides to destroy us. So now we've seen some examples of inevitability and exceptionalism and lethality. Let's talk a bit about where that narrative seems to be coming from. So jumping to the end of the SciShow video, here's a disclosure that's a lot more important than you'd think at first. "Thanks to Control AI for sponsoring this video." So I've had Control AI reach out to me, and so I've done some research on them, and once I looked into them I started seeing their talking points everywhere. I really don't like it because they feel to me as if they are acting as a propaganda arm of the AI industry. Note that I'm not saying they are a propaganda arm of openAI and friends. It just feels to me like they are acting like it, let me tell you why. So here is Hank quoting from a statement that I'm going to put in the upper left of your screen. That's been signed by a bunch of AI executive scientists and famous people, but what Hank isn't going to tell you is that there are actually two different such statements. "In a statement signed by Nobel Prize winners, computer scientists, and even AI company CEOs, these people warned that addressing the risk of AI, quote, "should be a global priority alongside other societal-scale risks such as pandemics and nuclear war." So now in the lower left, I'm putting Control AI's website. You'll note that Control AI mentions the extinction statement that Hank quoted from in this video, and then immediately changes the subject to claims about AI boosting growth and innovation, specifically missioning medicine. I mean, notice the big abrupt subject change there? It's quite the pattern with them, as you'll see in a minute. Now on the right side, I'm putting up the site red-lines.ai with the other statement, which originated during the 80th UN General Assembly. Now let's look at what was omitted from the statement that Hank read. The redline statement talks about "widespread disinformation," and widespread disinformation from AI is already a real problem, but the SciShow statement completely ignores it. I'll put links about that below. The redline statement also talks about "large-scale manipulation of individuals, including children," and again, already a real problem, and again, links below, but the SciShow statement again just completely ignores it. Likewise, a statement that originated from the UN warns about "mass unemployment" and "systemic human rights violations." These are also already real problems that are actually occurring now, but again, the statement on the left ignores it, and again, I've put references to those problems actually happening now below. The closest the statement gets to talking about extinction at all is mentioning pandemics, which, while bad, are not extinction-level threats, because, as we found out in 2020, and 1918, and during the Black Plague in the middle ages and a bunch of other times, the human race, or at least enough of us, will take actions to limit the spread and impact of such outbreaks, such that many of us will survive, be that quarantine or whatever. But the most telling thing about this statement is, quote, "ensuring that all advanced AI providers are accountable." I wonder why the Control AI people might not want people talking about that or thinking about that. The Control AI webpage and SciShow in this video really, really want to make you think about and worry about the potential hypothetical someday maybe sometime in the future risk of humanity may be going extinct possibly because of AI. Whereas I, and I made a whole video about this that I'll link below, think that our attention needs to be focused on the "now" problems emphasized by the statement on the right, like disinformation, manipulation and employment, human rights violations, and holding open AI, and the other providers accountable. And I'm not the only one, so let's take a look at the signatories of these statements. Of the people that only signed the statement on the right are at least five Nobel Peace Prize recipients, three recipients of the Nobel Prize for Economics, one for Chemistry, at least three former heads of state, and a former president of the United Nations, as well as John Hopfield, who shared the 2024 Nobel Prize in Physics with Geoffrey Hinton, and Hinton signed both statements. Now, let's look at who only signed the extinction statement. Sam Altman, the CEO of OpenAI, as well as CEOs of Anthropic and Google DeepMind. Hmm, imagine that. The CEOs of three of the largest AI companies think that we should worry about extinction from AI, but don't think we should worry about AI causing disinformation, manipulation, or human rights violations, and they definitely don't want us to worry about ensuring that all advanced AI providers are accountable. Five Nobel Peace Prize recipients on the right, three CEOs of huge AI companies on the left. Which side are you on? Which side would you expect a group called Control AI to be on? Which side would you hope that SciShow would be on? Now, it's true that Control AI did spend seven months or so, until mid-2024, campaigning against DeepFakes. And that's a good thing, although they're no longer running that campaign, and DeepFakes are a bigger problem than ever. Oh well. And it's also true that Control AI is listed in the "partner" section of the red lines statement. From that, you would expect, or at least I would expect, that if Control AI was truly acting as a partner, that Control AI would be supporting, or at least mentioning, the initiatives in the UN statement. Alas, not so much. There's only one web page from Control AI that talks about the red lines initiative. Here's that page. Which isn't actually on the Control AI site, it's just on their "AI news" substack. And this page does include the red line statement, or kind of it appears to you. What it actually has is just a screenshot of the red line statement, so this statement doesn't exist as text on the Control AI site, and none of the risks from red line statement are mentioned in the text, and none of them can be indexed by a search engine on their site. So what does Control AI include about the red lines statement? Well, so this quote, "We urge our governments to establish clear international boundaries to prevent universally unacceptable risks for AI." Well, okay. It's odd for them to include the part about risks considering they don't actually mention any of the risks. And Control AI says about red lines AI, quote, "The statement doesn't specify what the red line should be," which, while technically true, I think is disingenuous because the statement does list several risks that Control AI fails to mention, as well as call for accountability that Control AI also doesn't mention. And then Control AI changes the subject to talking about an extinction threat, you know, the thing that the UN red line statement explicitly omitted. I told you there was a pattern of Control AI abruptly changing the subject. And I'm putting up here on the screen several Google searches showing risks from the red line statement that are not mentioned at all on the Control AI site, at least at the time I was recording this video. I've heard over and over from people that the doomers who warn about the existential threats also worry about the immediate issues that AI is causing. Like I just showed you, though, with the Control AI website, that has not been my experience at all. Most of what I've seen out of the "AI will make a extinct camp" downplays or ignores the near-term problems. So let's talk about why that matters. The problem with the extinction argument is that there's no clear, actionable path to solving the extinction problem, because it's only hypothetical at this point in time. The focus on a problem with no concrete characteristics at all leads to endless and fruitless debates, and it provides no help or relief for any of the actual victims, like the parents of the teenagers that have committed chatbot encouraged fatal self-harm, allegedly, or the teenagers that are currently probably allegedly being encouraged to harm themselves by chat bots, whose parents are unaware and unable to intervene, that we'll probably find out about later. By contrast, many of the immediate problems have clear-cut solutions. For example, severe, explicit criminal penalties for developing or hosting a service that can be shown to have generated content that manipulated or encouraged anyone to harm themselves or others. Regulations like that would immediately help the teenagers and the parents, because the companies would be forced to reckon with these problems or risk-roll penalties. And as a bonus, making companies liable for what their software does will force them to slow down and to understand how to control their software, which in turn will also greatly increase our understanding of how to control their output, which cannot help but improve our understanding of the alignment problem, and that cannot help but reduce the risk of any eventual hypothetically misaligned superintelligence that might lead to huge numbers of human casualties. So focusing on the immediate problem helps both cases, the near-term and the hypothetical long-term, while focusing on the hypothetical problems ignores the immediate case completely and still may not be useful in the future, even if the eventual hypothetical case were to come to pass, because we don't know what to do about the hypothetical case. But although "current-risk accountability" regulation is clearly in the public interest, according to me and according to the red line statement that came out of the UN, it's not in the interest of the people in the AI industry who are trying to put as much money in their pockets as they can. And so the CEOs of the biggest AI companies sign the extinction-oriented statement and don't sign the one calling out more immediate risks or calling for them to be held accountable. And with all these AI companies, nonprofits and large educational YouTube channels pushing this apocalyptic narrative, it's hard for those of us trying to be realistic to even keep up with all of it, much less make videos about it all. And putting together a video with facts and references is so much more work and so much more time-consuming than reading off AI industry talking points. But although I'm only one person, I'm going to keep doing what I can. Like giving you things to look out for, to tell when something is an industry propaganda, like the ones we've discussed in this video, namely inevitability, exceptionalism, and lethality. I wonder if there might be an easier way to remember that. Ah, there you go. When consuming AI content, look for the lie. Hmm. You know, now that I think about it, I should add superintelligence as another purely hypothetical thing that the AI industry propaganda won't shut up about, but that it's completely unsupported by any actual evidence. That should make it even more easy to remember. Lethality, Inevitability, Exceptionalism, and Superintelligence. When you see AI content, look for the L.I.E.S. And if you find them, there's a good chance it's more propaganda than it is practical. So wish me luck, and feel free to subscribe if you're so inclined. I would ask for people to be kind in the comments, but it wouldn't do any good so I'm not going to bother. Just always remember that the internet is full of bugs and misinformation, and anyone who says different might just be parroting industry talking points and trying to distract you from all the ways the industry is, allegedly, harming real people as you are watching this. Let's be careful out there. Thanks for watching. [BLANK_AUDIO]