people are spending a lot on these models. They're presumably doing this because they're getting value from them. You can maybe argue like, oh well, I don't think that value is real. I think people are just playing around, whatever. But like [music] whatever, they're paying for it. That's a pretty solid sign. We're almost giving you here the useful answer of like, I don't think it's a bubble cuz it's not burst yet. When it's burst yet, then you'll know it's a bubble. People often make the case, oh, AI hasn't been profitable yet, and they're spending more to make it profitable. In reality, they'll have paid off the cost of all the development they've done in the past very soon. It's just that they're doing develop more development for the future.
>> Will they regret that spending? How much are they spending? You can look at Nvidia and how much they're selling each year and you can see whether it keeps on growing and you can [music] see whether stuff is kind of looking good to continue. Math seems unusually easy for AI. I'm going to be honest. People often make claims about it being like this you know intuitive deep thing that it would mean that AI has achieved something some huge level of intelligence for it to solve. I think in practice this is just like you know making a piece of art. It turns out to be farther farther down the capabilities tree than people might have guessed. We sort of had this with chess decades ago, right? Like computers solved chess very well and everyone was thinking of this as the pinnacle of reasoning and everyone as a result kind of concluded like oh well of course computers can do chess. The like interesting scenario to think about you know 20% chance 30% chance something like this will happen [music] in the next decade is like you know a 5% increase in unemployment over over a very short period of time like [music] 6 months due to AI. The public's reaction to this will determine a lot. There will be very very strong feelings about AI once this happens. I think there will be a bunch of very strong consensus on what to do on things that we don't normally think of as things that people are considering. I know when this happened with co there was a several trillion dollar stimulus package in a matter of weeks to days. It was [music] break neck speed. I don't know what that will look like for AI but I think it's like everything else in AI. It's exponential which means it will pass the point of people sort of care about it to people really care about it quite fast. I just expect wherever we end up there will be this certain thing which we would have considered unimaginable a year ago. [music] Guys, there's a lot of conversation about the macro or are we in a bubble? How should we even think about this question? And we're going to get into forecasting later on, but why don't you just take your first stab at how you approach such a big general question?
>> Yeah, I mean, for me at least, um, the way that I thought about this a little bit is I look at kind of the big indicator being how much people are spending on stuff like compute. And I guess maybe some sense of will they regret that spending, that's relevant. the how much are they spending thing like you can see you can look at Nvidia and how much they're selling each year and you can see whether it keeps on growing and you can see whether stuff is kind of looking good to continue the will they regret it side I mean that's just to bec right like we'll actually have to wait and see it does seem as if most compute gets spent on inference that companies don't so far regret like using to offer their products so I mean on that side I'm like thinking not too bubbly yet. Um but yeah, I low confidence. So there's other stuff to think about.
>> Yeah. Uh right now the amount of money companies are actually earning in profit not including the cost to develop the models initially is seems to be like very positive such that if they stop developing bigger and bigger models and just stick with the ones they've had, they've be they'd have earned a profit pretty quickly at the current margins. Um and in this sense it doesn't seem bubbly. On the other hand, um at any given time they are investing in building even uh in building even larger and larger models and you know if that goes well then they'll earn more money and if that doesn't go well then no matter how profitable they are right now it'll be uh a small amount of money compared to how much they would have spent. Um so it's like uh I think right now there are not financial signs that that things are actually that there's a bubble. Um, a lot of people worrying about bubbles just like aren't necessarily used to the level of spending um, and just like the level of success that sort of happened in like scaling. Uh, but if it there is a bubble, it could happen very suddenly and be pretty bad. So,
>> yeah, I think we're almost giving you here the useful answer of like I don't think it's a bubble because it's not burst yet. When it's burst yet, then you'll know it's a bubble.
>> Yeah. Yeah. I do think like there's you could imagine a world which there's all the spending and the current level of success does does not like people often make the case oh AI hasn't been profitable yet and they're spending more to make it profitable but right now it's you know not making anything and in reality they're making you know they'll have paid off the cost of all the development they've done in the past very soon it's just that they're doing develop more development for the future uh so I think like there's this like underlying financial success so far that I wouldn't expect to see if there were the very least an obvious bubble.
>> Yeah, that does seem very relevant. People are spending a lot on these models. They're presum like, you know, users to use them. They're presumably doing this because they're getting value from them. You can maybe argue like, oh well, I don't think that value is real. I think people are just playing around, whatever. But like whatever, they're paying for it. You know, that's that's a pretty solid sign. I I guess one quick question on on related to this is like you talked in the report of the AI in 2030 basically that you haven't seen signs of of um basically these models kind of plateauing or like the capabilities keep increasing and you have the benchmarks you have the amount of data that is going the amount of compute do you think phases or parts of the models are plateauing though like for instance pre-training um are we seeing some sort of plateauing in that or do you think people are still exploring some innovations in that stage and u curious on what you think about that.
>> Yeah, I think this gets a bit harder to look at like we get to an area where there isn't as much public data to say a lot, right? It seems as if pre-training is comparatively less of a focus than it was before partly because like you have this exciting new direction of well new newish direction of postraining where they've done so much about reasoning whatever. Um, but then I don't necessarily take that as evidence of like, oh no, and that means pre-training you couldn't scale further, whatever. Like, it seems as if there is meaningfully more data out there. It seems as if plausibly like even, you know, a lot of this stuff is quite synergistic. You develop a better model. You like use post-training stuff to make it better. You get a load of data of the model actually being used uh successfully or not. A lot of that can probably go into pre-training next time. you you aren't projecting a software only singularity where AI is able to automate AI research uh because automated feedback loop why not
>> yeah I mean I guess like I might answer this and I'll yeah say more and it's like for me it's like that report it's no one person's kind of oh this is like the forecast this is the prediction right this report very specifically looks at what are the current trends are there reasons that they clearly like couldn't continue or might not and if they do continue where do they lead um I think whether you see this self-improvement thing that's very hard to do from a sort of trend extrapolation basis right like currently AI stuff does help AI R&D at least a little in terms of stuff like uh coding or selecting your data sets and creating those whatever but it's quite hard to actually measure and it's not really helping in some big way like this kind of self-improving thing suggest um there are reasons that you might think it could be very hard. Uh people have discussed before how possibly you know if stuff does just depend a lot on scaling up compute then maybe automating a lot of the R&D isn't that helpful. Um I find that somewhat compelling but I think it's also just it's pretty uncertain. It's hard to speculate about something that's quite out of regime like that. One thing that needs to happen in order for a software only singularity to occur is you need to be in this world where scaling up the amount of researcher R&D time basically um allows you to like improve AI enough that it makes up for the lack of being able to scale experimental compute or pre or pre-training. I think that something you would expect to see if this were the case is, you know, maybe not that much experimental compute being used in practice and instead all of the money is going towards researchers. Now, there's a very good case that there's a very large amount of money going towards researchers. But as far as we can tell, experimental compute, which you seem to need to do research, is al is receiving a similar amount of money and that in fact uh it's receiving many times more money than the final training runs that are actually of the models that are actually being released. I I think this is uh in my mind is a strong update towards oh you actually need this you need to do very large scale experiments to do research and that we don't really have large have good evidence that researchers and just researchers would be able to speed speed things up without doing more experiments. Um however um the actual there are like pretty good arguments on either side of this. I tend to lean towards no, you actually need to do more experiments and that means you can't get this software only uh singularity. Um but I don't think the people who claim otherwise are like crazy. I think they're making some like they have like very reasonable differences and we're both uh speculating on something where the data is currently pretty sparse. actually related to that like what do you think on so so if if you have like some of some of the exploration that researchers are trying I mean obviously like people are exploring a lot with RL trying to go beyond verifiable domains and um and what do you think about the argument for instance that gradient descent is is really good on learning in the current data set that you're giving right and just if you keep training this over and over it's going to start forgetting things that it was trained before right like catastrophic forgetting and and is this there's this argument right like well kids don't don't learn that way or like you maybe there's some imitation learning that kids do maybe there's some sort of exploration that they do and uh I wonder what you think about it. I mean if and it sounds right like if kids really would just learn on imitation learning I think parents would have a great time just raising kids but it seems like the reason why they have such a hard time raising kids is because they explore all these different things. What do you think about it in terms of the algorithms and like the things we need to keep improving these models over and over beyond the data and the compute? I am cautious about comparing the like how AI learn to how humans learn. Not because I don't think they are comparable, but because I think we know a lot more about how AIs learn right now than we know about how humans learn. And people like making sort of assumptions about how human learning works and saying, "Oh, AI doesn't do it that way." And I don't know, maybe that's true. Um maybe human kids learn via RL. Um I I I'm not very I I think that yeah I I I don't have strong opinions on whether or not like you know you need to change to a method that's more like what we think kids do right now. I suspect people will find some method that works to use the compute available because they've been able to do this in the past.
>> Yeah. I'm also sort of reluctant. I guess as well it's one of those things where when we point to particular issues like the example of catastrophic forgetting it's sort of well okay but as we've scaled up we have managed to do quite well at like having models that remember more and more things. Um, this isn't to say that hence the problem is solved, hence we're done, hence no more innovations necessary or anything like that, but I'm not exactly going to write it off.
>> Yeah, I don't I definitely don't think we've seen any slowdown yet in capabilities uh from any of these concerns people have. I think that people always have these sorts of concerns. I'm I'm reluctant to believe any given one of them until this actually shows up in numbers I can see on a graph. Uh which I just don't think has happened yet.
>> Dariothropic has has said he said in March 2025 that uh within 6 months AI will write 90% of code and of course that hasn't happened yet. He also said we have you know we could have AI systems equivalent of a country of geniuses in a data center as soon as 2026 or 2027. How do you evaluate why an anthropic is so is so bullish or or what is the crux of difference between what what they believe and perhaps what you believe?
>> My model at least which I don't know if it's right but what it is is that they think a bit more like the people who believe in uh you automate R&D and that gives you very quick takeoff. So they see it as like, yep, we're working on these AIs that are great for kind of research engineering type coding and at some point they're going to be useful and that's going to rapidly accelerate us to develop the next ones and then it's going to be quick progress. Yeah, I think that uh it's hard to tell uh the extent to which I I don't think we've gotten a lot of evidence that their sort of views of this like software only take off are wrong in so far as like they will taking a little bit longer to get to like the minimum level of competence for AI to get you there. Definitely seems to be the case, but it I I don't know. It's it's hard to tell the extent to which we've actually had significant updates on this. I know Dario often qualifies what he says by like saying as soon as or something like this. Um so this is like maybe the more more so the faster timelines he gives. Although I'm not sure. Yeah, there has also been I think sort of you know Talmud style commentary where people are carefully looking at his exact wording and then at wording of other people's discussion of how many lines of code that are generated by some teams at anthropic are generated by claude code and whether this does or doesn't satisfy what you said. So it gets a bit tricky. Yeah, I remember there was the uh the paper from um the the uplift paper that was claiming that actually models would slow you down, but I think like it mattered a lot what models they were using at the time because I think they were pretty outdated by the time the report came out. And uh I mean in my personal experience that you definitely become way faster and and and it just does so much more for you like you're just having the whole context on your codebase that's such a huge advantage that I think for a human just would be really hard to do. Um, I mean far more than 90% of the code I write is written by AI these days. Uh, but I know I'm not like the average coder uh at all, but it's definitely it's definitely I don't think it's like a wild prediction at this point that 90% of code is going to be written by AI. Um I mean for all I know somewhere at OpenAI there's someone just you know or that you know with alpha code doing evolutionary uh algorithms on having tons and tons of trials uh trying to you know million shot some hard problem uh that it's just like it's really unclear how many lines of code are actually being written by AI right now. I don't think it's such a wild I it's by a lot of like people's intuitive sense in terms of like oh is 90% of the job of a programmer being done by AIS definitely not but there's this more complicated sense of like how much is being written by AI probably not 90% but it's it's hard to tell
>> yeah and I think that is a very meaningful distinction you know like um if you were to measure how many lines of code are being written quote unquote by like tab completion then it's probably quite high but you don't necessarily expect that that's taking on that much of the programmer's really hard work that uplift paper that you mentioned like I find it really interesting and really good and it's also surprisingly recent in a way like you know you mentioned ah the models are outdated but I mean this was early 2025 so these were models that people actually did think were helping them and in the paper they even got them to say ahead of time like how much do you think this will speed you up and they said yeah I think however much They they then ask them afterwards, how much do you think this sped you up? And they're like, yeah, yeah, it sped me up. And I I feel it does reveal actually like it might be hard for us to judge uh whether we were sped up or not.
>> Yeah. One thing that might be happening here is that a lot of the code that's getting written by AI is code that wouldn't have been written otherwise. So, it's not really speeding up things that would normally happen. But, you know, there's a lot of simple graphs or simulations I run that might have not gotten written otherwise. Uh and so it's it it's hard to tell uh exactly what's going on here uh in terms of the impacts. I think at the end of the day, the most reliable indicator here is going to be how much money these people are making from programmers um and from you know subscriptions in general. And it's a lot of money. I think there's definitely indications that people are finding a use for them and probably a decent amount of that use is for coding but not exactly for the metric of doing 90% of an existing coder's job. Yeah, biology is this phrase that's been being used a lot which is um AI is an end to end. It's it's middle to middle um and which is meant to imply that um you know we're going to need a lot more human involvement than some people you know typically think. What what is your mental model of uh what AI is is going to do for for labor markets either on the sort of lower end and on the higher end in the next you know decade let's say. Oh, in the next decade I like on the higher end I'm definitely like you know probably I expect new jobs to be created. Everyone could still be influencers but uh on the higher end it's like there are not very good individual things that you can point to where it's very obvious that AI can't automate that job at this point. Yeah, you could argue okay but there's some unknowns and I think it's like pretty reasonable but those unknowns we sometimes you know we AI gets up against it limits and we figure out what they are and then it learn surpasses that and I don't know at the higher end it definitely seems plausible that it could just automate all of the basically all of existing jobs with the exceptions of ones that require manual labor that people actually care about being done by a human. Um it just like does not seem at all implausible to me that that can happen. Um or that that could happen very fast. Uh uh with the uh caveat there being like there's probably some regulatory push back if that happens. Um on the lower end uh I don't know could just you know could be a bubble and doesn't have any impact. Uh the thing I talk about when I'm talking about like the the like interesting scenario to think about which I'm not I don't know you know 20% chance 30% chance something like this will happen in the next decade is like you know a 5% increase in unemployment over over a very short period of time like 6 months due to AI being released to something that I think will have a very substantial impact on the world both in terms of how people think about AI um and sort of how much attention it gets and seems plausible to Uh but you know far from guaranteed.
>> Yeah, I think I strongly agree with being just highly uncertain. Uh it seems very plausible to me that you end up more or less kind of, you know, this generation actually is exactly where we run out of progress. It would be kind of crazy, but it could happen. Uh, and then it's like, oh, okay, everything is very much just generating more jobs for technical people to try to integrate it into doing kind of useful but janky things for all of the existing work people do. Uh, the stuff where it kind of becomes a crazy runaway thing that you can yeah really automate large swaves of remote work with. I mean, my timelines are, I guess, probably a bit longer than the others, but yeah, I mean, it seems hard to rule out that something really big happens in a decade. A decade's quite a long time.
>> I think I would be surprised if there were not 5% of jobs that exist now, which AI has automated away over the course of the next decade. I honestly I'd be surprised if it's not 10% um of the jobs that exist now. I think um how fast that happens and like the extent to which those people find other jobs is something which I don't think I have seen compelling evidence for either way. Um and we're and probably depends on how fast various things go and exactly what jobs are automated. I think that 10% over the next 10% of current jobs seems like a pretty reasonable uh lower it's not quite my lower bound but you know a pretty reasonable number over the next decade of uh but this might not show up in overall employment numbers. Yeah,
>> this is interesting. I mean definitely like the kind of to the extent there is a mainstream economics view of this stuff it would probably be that automation happens at the level of tasks rather than occupations and occupations can as a result you know go down quite a bit but uh a lot of the time you're automating these like similar tasks across lots of jobs. I think this is compatible with what you're saying. It's just that some jobs get really hit by it. I I don't know. I find it yeah quite hard to think about. I'm not sure what the even the historic base rate for kind of jobs ceasing to exist is. I know there are problems with this like the historic employment data series. There is actually quite a high I believe base rate of just the tasks in a job changing uh jobs themselves changing jobs kind of going away coming in. So yeah, even this 5% thing, I don't know what to think. Yeah, that would be like a big effect or kind of Yeah, that's actually roughly the size of effect you've already seen from something like software. I don't know.
>> Yeah, I I probably 5% of jobs that existed before software no longer exists. Uh it seems pretty reasonable. Um I but I'm not confident of this. It's definitely something which like I don't know. I expect especially if revenue trends continue I expect to know a lot more about this in a couple in a year or two um probably within the next year because it will just be the case that okay we'll have AI earning enough to substant to be like a substantial part of the economy if it's not showing up in unemployment then we've learned something about what it's doing we've learned that like it's able to do this without showing up in unemployment numbers or maybe it will show up in unemployment numbers and we'll see exactly what there's been like some early work looking at like uh indicators of this. There's a lot of things that complicate looking into this. Um because interest rates also have effects on like the sort of things you might care about or just like normal churn or also it's possible that tech companies, you know, maybe they'll lay off a bunch of programmers so that they have the capital to build data centers. And are those programmers being laid off because of AI? I I don't know. If you had a a kid that was a freshman in college and they were asking, hey, you know, what should I major in if I want to have a great career? You know, what might you tell them? And if they asked you about, you know, computer science or math or, you know,
>> promp engineer.
>> Yeah, exactly. [laughter] Yeah. What would you say?
>> Uh, I mean, I'd probably say not prompt prompt engineer. I think in general [laughter] people get better at using uh AI is very easy to use. Uh, yeah. Yeah, I I I think it's a good question. I think they should probably major in something where if they're majoring in programming, the thing that they should be or computer science, the thing that they should be looking for is not being a person who's going to like like the skills that are going to be useful are not going to be knowing a programming language. It's going to be more general purpose skills, um ability to like work with other people, um communication skills, this sort of thing. I don't really know entirely if this points to a particular major. Um, most majors are probably not majors that are like actually relevant for your job.
>> Yeah, I guess I'd sort of be like, well, there's not too much that you can do to plan around the super crazy futures. So, I guess go for something that you're passionate about that's useful in the worlds, but don't go crazy in that way. I actually think that yeah, computer science, maths, if you're passionate about them, they're very good because you'll learn interesting things that are valuable in many worlds. Um, but I don't know, I gave advice to a younger relative recently and they chose to study drama instead. So [laughter]
>> I do think that you know one of the things that if you have a better time in college that's like four years of your life you had a better time during and at the end of the day like you know if you if it's uh you have it's a crapshoot which of those things is actually going to give you a better time in the future. You planning for the present is a lot easier.
>> Yeah. I mean it's definitely becoming really hard to to know right. And I remember like the the problem engineer was obviously a joke because everyone believed two years ago that that was sort of some sort of viable thing and obviously models are phenomenally better at like just being great prompters. Uh so obviously like that's kind of like one thing that has been happening is really hard to predict what's what's happening as these models keep getting keep getting better. One one question that I have related to this is obviously code is such a big market and it has had such a big impact. one that I'm very excited about but it's still much earlier I think is computer use right it's basically automating all the digital tasks that you're doing on your computer uh and there's very few benchmarks around this like whether it's web arena or world and you talk a little bit on your report about benchmarks curious on like what do you think is missing in that space like why we haven't seen yet that moment where the moment for example when sonnet 3.5 came out or or cloud code or codeex where we saw significant improvement on coding In general, we haven't had that moment for computer use. What do you think is missing there?
>> Interesting. I mean, there have been improvements on computer use for sure. I do have I mean, this maybe I'm going out on a limb here slightly, but also I do think that there is a sense in which models are a little bit artificially hobbled by um their vision capabilities. Like it does seem as if a common pattern you see when you try to get models to do stuff with a gooey is they kind of get a bit confused about manipulating it and and you know in a way where it's like okay this is interacting with your general propensity to get infused in long as you would in like difficult long coding problems but it's kind of exacerbated cuz like you're not able to just easily look back on the thing and see kind of h I was wrong. you instead go down like some awful dead end of just I'm just going to click this again and again and again. Um, so I think that's part of it. I think there is something here or so probably about kind of long context coherence stuff like those tokens to represent the guey are pretty big and then you're filling up your context window as you go with like oh yeah well I had all of this stuff that's happened before and you seem to just run into a kind of spiral of increasingly less sensible outputs. So I feel like these are two of the big things but I don't know if that answers your question.
>> I I found computer use I know this was the first year I found computer use actually useful. I we use ChachiPT agent in our uh data center research because a lot of what we have to do is find permits which are all going to be on janky county bycounty databases of error permits for you know the county that Abalene Texas is in. Um and I don't know what databases exist for every county in the US. uh chatgpt does normal chatpt can't search them because it's these you know these actual user interfaces you can't just search them with you know URLs uh because they definitely don't work that well um and it's able to navigate this such that I can just ask it to find me permits on a data center in a particular city and it will come back with air pollution permits and like tax abatement documents and all of this stuff that let me learn a huge amount. Um, and this is just like because of the improvements we've seen in computer use over the past year or so. Um, I'm excited to Yeah, I think it's just just going to get better from there, but I've definitely found it starting to get to the point where it's actually useful.
>> What's your mental model more broadly for what is going to happen to uh productivity or or or just sort of e econom uh economy statistics in general? Are you some people say GDP growth would be you know 5%. And I think it's a Tyler Cowan view. I think some people would say, "No, no, we should get up to 10% growth or or maybe even higher if we truly um have have AGI in terms of how we understand it." What's your model of what happens to the productivity? I think my kind of baseline guessing would be, you know, I forecast out kind of if revenue keeps growing the way it has in theory for it to be worth spending that much on that, you know, those chips to do that inference, you should be getting something kind of similar to that value out of those chips by then. So then you could just draw from that kind of like oh okay so extrapolating to 2030 you need and I think for there it was in the report I don't know I calculated but I think it was on the order of like a percent kind of GDP increase that's in a few years right that's not presuming AGI that's presuming like if Nvidia stock not stock revenues keep like growing as they sort of previously have and you assume that they make roughly as much compute from it as before and so on. Um, if you actually get something I mean AGI is like, yeah, people use it to be emptying different things. I think if you actually get something that can do
>> any tasks that humans can do remotely, then presumably you see a lot of growth. It feels sort of difficult to guess exactly what kind of a lag you're going to see. I think there's reasons to think, oh well, maybe people will be slow to adopt stuff. How do they learn to trust it? whatever. There's other reasons to think, well, they're already using these technologies. A lot of it might actually be quicker than most growth. And indeed, adoption's been quicker for LMS than for many previous technologies. Um, so yeah, I think it sort of gets hard at that point to model. At some point on our site, we had some rough numbers where it was stuff like what if you, you know, doubled the virtual labor force, what if you 10 times it, whatever. Then you see these like crazy GDP boosts. Um, I don't know whether that's the most reasonable way to think about it. I sort of I think a lot of it comes down to whether you imagine that like yeah, you really get something that can do everything versus you get something first but can do a meaningful fraction of remote tasks but maybe can't do like an entire bucket of them and then it bottlenecks you more. So I guess it's again this thing of like my best guess on current trends is this fairly well- definfined you know few percent of GDP in 2030 thing which is already pretty crazy by economic standards. Uh but then once you go much further it's like god you know my predictions are just going to be even crazier. I I'm reluctant to make them.
>> I am going to be slightly less reluctant. um and make some claims. That's what we're here for.
>> Assuming in the next 10 years we get AI that is capable of doing any remote job as well as any human. Uh I think you know 30% GDP growth seems like a lower bound on something that's reasonable. Assuming you get this is a big assumption that a lot of people are going to that you know it's there's a lot going on in that assumption but uh assuming that happens I think you either are going to get like 30% GDP growth or you know negative 100% GDP growth because everyone's dead. Uh it's just like, you know, uh it it just like at the end of the day, it seems like you're going to have AI that can scale, but if you have AI that can scale there, you could probably have AI that scales even further. And right now, I think the like economic models I have seen of what happens if you get this sort of full replacement, you can automate a job. um are you know either show this sort of in extremely fast wild takeoff or with a couple of or um you know you have some people attempting to do this who then say and then you like look down through paragraphs and it's like assuming current levels of assuming AI is as capable as GPT3. Um, you know, I I think the the smaller numbers just like, you know, they're they're nearer they're either nearer-term predictions or predictions that aren't looking at like the full the the more the upper end of what sort of capabilities you might see in the next 10 years.
>> Yeah. I mean, it does seem hard to imagine a world where you have this supply of virtual labor that literally can do any stuff that humans can do and then it doesn't lead to crazy things. I definitely agree with that. I guess perhaps maybe some sort of a I don't know a heavy regulation situation but
>> there are they do yeah
>> I think there exist worlds in which things don't go crazy after that it does seem like those worlds are not in an indefinite stable state uh but you know it's not impossible but it does seem like the default there is you either go crazy up or you either go crazy down and it's probably going to be one of those two if you get to a world where it's like genuinely AI can do any job as well as any human. I think people I don't know it seems wild to me to claim that you know given that your default case should be you know not super ridiculous changes. It just like that's a lot of things that your AI can do right there and that's like yeah it just like seems like it should have fundamentally changed the economy in one direction or another. My intuition is a lot of the disagreement. I mean, probably some of it does come down to sort of cashed beliefs people already have, but I do also think some of it is that when people talk about like, oh yeah, AGI, AI that can do a remote job, whatever. Even though we feel like we're talking about the same thing, maybe sometimes we're not. I' I don't know. I've certainly had examples of conversations where it's like, yeah, AI can that can do any remote job. And then they discuss stuff that it can't do. and the stuff that it can't do. It's like well no like that's that's also a remote job. Like that's the kind of thing people currently do. So I think there is some of this.
>> What do you think like I mean you talk about benchmarks on your report but I I wonder like 2027 2028 what are going to be the right benchmarks to measuring the progress more than the economic growth more the capabilities on the model like intelligence on the model like we we had in 2012 Alex net obviously that that got solved long ago. Uh but that was probably not a measure of AGI by any any means. Um do you think the same would happen with the current benchmarks we have? So so Sweden, MMLU, um let's say we maxed out on those benchmarks, what comes after that? How would do do we measure that? Is it a sort of like GDP growth with these models? Is it sort of breakthroughs in science? How do you think is the right measure going forward? Yeah, I mean I think most of what we have is likely to be solved and indeed the examples you gave are like pretty close already. Uh like I don't know is basically solved. Sweet bench is like possibly close depends a bit on how ambiguous some of the questions are. There's some details but it's really getting there. Um I mean I think some directions are obvious. you kind of do similar things but harder and a bit better and trying to make them a bit more realistic and people are doing this. There are harder software benchmarks that people have made more of an effort to try to curate and that cover larger tasks for example. [sighs and gasps] Um I think there's also perhaps some question of kind of budgets involved. I I do think there's this kind of thing where like obviously if you just burn money, it doesn't intrinsically make the benchmark better, but probably you are going to see something where you're just going to have to devote more resources on average to them. Like if you're trying to prove a sort of higher level of capabilities to a higher standard of proof, probably it's going to involve kind of more effort in developing them. Uh I do also think though you're going to see examples of you know relatively small kind of small numbers of things that are just very impressive and these are also a valuable signal like when you see LMS being able to do things like oh yeah it just uh refactored this entire code base and it was really useful then this is going to be useful and even if it's not yet formalized into a benchmark if you've seen it for yourself it's going to be kind of useful for you as evidence. And then people are probably going to make benchmarks that cover things like this to try to systematize them.
>> I want to go back to our question on on timelines and I want to ask you about a few different uh sort of milestones and get your perspective on on on on timelines there. So so first is what what is a rough timeline for a a major unsolved math problem uh being solved by?
>> Oh, I actually wondered yeah because you had a few of these that you said for us to look at. when you say that it solves this I mean is this unassisted entirely it's uh or is it kind of a news you know report or someone tweets that hey like I dumped this at GPT and it solved it and what counts as major
>> um something that we would all agree uh like a substantive uh you know version of it not not a uh you know just a anecdotal you know person describing it
>> but does it have to solve it on its own?
>> Yeah, let's go with that. Sure. Yes. Honestly.
>> Oh yeah. Because I mean there's already cases it seems of LMS being Yeah. like people are debating a little bit but mathematicians who seem trustworthy are saying like wow I used this and it was really helpful during my proof.
>> Yeah.
>> I would not be surprised if AI solves like a major unsolved math problem like the Roman hypothesis or similar in the next 5 years. Um I'm not going to say that like that's my you know median case necessarily, but I definitely wouldn't be that surprised. It's like right now it doesn't look like math is that hard for AI. Uh it just like some things turn out to be hard and some things don't. And math is just like one of the domains where it's RL seems to work pretty well and where it's most other domains it's not at the point where it's like useful to a full professor to the same extent I think it is for math all or getting very close to for math. Um yeah, and at the end of and also it's like very unclear to what extent certain capabilities that it has unusually well might actually turn out to be very very useful. Like maybe it'll turn out that there's like four papers out there that it knows about that have obscure results in them that when combined solve some big conjecture which is the sort of thing that it like might be much more feasible to figure out with AI um than for a human to figure out um or something similar. There's a lot of uncertainty here, but it just like does not currently seem like something that AI is actually going to struggle with. People often make claims about it being like this, you know, intuitive deep thing that it would mean that AI has achieved something, some huge level of intelligence for it to solve. I think in practice, this is just like, you know, making a piece of art. It turns out AI could just do that before it could do a lot of other before it can, you know, remember things for more than a couple of days or whatever. Um, yeah, it turns out to be farther f farther down the capabilities tree than people might have guessed.
>> Yeah, I think I'm I'm also bullish though. I do think that yeah, it's one of those things where it's tricky and you really probably do need to define it quite well to get a good forecast on it to hope to get a good forecast on it. like I don't know we've had this experience that with benchmarking mathematics you know we got mathematicians to come up with problems that I think aren't as difficult as the kind of problems you're talking about but nevertheless they're like yeah if AI could solve this it would be like a big deal for AI progress it would mean something to me and then AI has solved them and usually their response has been kind of like oh yeah that updates me a bit although man when I look at it I just realize like yeah you can kind of brute force this you can kind of cheese this you can get through uh and it's a bit like oh okay I mean what if there's a problem that for humans we consider sort of oh this would be quite big and then yeah I solved it ah well it solved it whatever we sort of had this with chess decades ago right like computers solved chess very well and everyone was thinking of this as the pinnacle of reasoning and then they did and everyone as a result kind of concluded like oh well of course computers can do chess so yeah I don't know I I suspect that math is quite nice for AI to do. I'm reluctant to go out and assert like, oh yeah, definitely AI is going to like solve some of the Millennium Prize problems in the next few years, but it would not at all surprise me if it solves quite impressive seeming things in the next few years.
>> To to what about a breakthrough in biology or or medicine? And we've already seen some of that with uh the what's it called? Um alpha alpha fold. It a math seems unusually easy for AI. I'm going to be honest. So to the extent where I'm like a is it going to do the same exact level of like oh it on its own did this huge thing that seems to be a much bigger stretch to me. Um, it definitely seems plausible, but there's a lot of other concerns there where it needs to uh it needs to be able to like actually do experiments and get data and interact with the real world for a lot of these um in a way that does not need to happen at all for math. Um, in particular for certain uh yeah, it's just they in fact seem farther off. What is what seems more plausible to me is that we see like you know it become ubiquitous that some tools that like of using AI in some sort of aspect of like biology or chemistry or something useful like that that like certain aspects of it are enhanced. It also is possible that AI will you know make incredible strides without humans but it's it's harder.
>> Yeah. I think again it's a bit tricky for where you draw the line. I mean, I think you're not counting tools like AlphaFold because if you were, then probably you'd argue for that, right? Like the the inventors co won the shed Nobel Prize. Um, but yeah, I mean, I guess there's kind of different directions in biology. You could have AI being able to predict quite, you know, specific things like that. Or you could have something that's more general purpose, this so-called like co-scientist or whatever they want to call it approach where it's more about like, oh, it was able to look through the literature and have good ideas and there's different extents of human involvement. There already seem to be some results where impressive stuff is happening. I've not vetted them enough to really have a sense of like would this already count as having satisfied yeah the sort of level of impressiveness you're looking for. I sort of assume that finding things that end up being meaningful will happen pretty soon if it hasn't already happened. But then maybe there's a question of kind of okay, but is it doing as well as human researchers actually like prioritizing the best few ones to work on? Uh I think most of these co-scientist results have probably had pretty involved humans prioritizing though again I've not looked enough to say. La
>> lastly um how about for real super intelligence for for your definition of super intelligence? I have I have I I think I am I I am on the record as saying that the the median timeline I discussed uh or the modal timeline sorry uh I think it's modal yeah um which might be on the early side compared to where my median is um is you know 2045 was where when I did the podcast with Himeme we discussed like our forecasting breaking down and uh everything going bananas um is the terminology I have used um and that like looks like super intelligence. Um I you know um I think that it's like the case that if we get AI that can do every single job uh that a human can do as well as any human can do that job in the near future then this is you know means that scaling just works to get things much much better and probably means that you are not that many steps that you are just a bit more scaling away from getting AI that could do anything uh that humans uh sorry two things vastly better than humans. Um yeah, it gets hard to predict and I think as well it gets to be one of these things where the predictions get a bit unmed from the the stuff that you can like properly model. Like my my sort of you know guesses my like judgmental forecasts to use the fancy term for just kind of can do any remote work tasks probably have a median of about like 20 25 years. Um, I kind of struggle to imagine a world where that happens and people are like deploying it and doing research and yet they're not making further progress to being able to do stuff much better. So I guess I'd have to be like not too much longer after that for some definition of super intelligence. But yeah, all very uncertain and yeah, it seems to break down a bit. You you talk a lot about the progress in data centers, benchmarks, biology and there was one interesting part that I noticed just in the field that is robotics is making a lot of progress with let's say world models and like the physical space a little bit curious on like um what is your take here like what do you think it's uh it seems like a lot of the problems in robotics can be solved purely with imitation learning you might not need like a lot of sort of like breakthroughs in math or whatever like you can just basically learn it from a lot of data and I think in the last couple of years has being remarkable just in in robotics and world models overall. Curious on your take a little on this and if you did some kind of research in the space.
>> So we've looked into what sort of uh amount of compute is actually being used to like do these training runs. Um and what we found is that like computer it the training runs that are being used for robotics are like a 100 times smaller than the training runs that are being used for uh than the training runs that are being used for like frontier models. And so there's a lot of scaling you can do there. I don't think that until plausibly until very very recently there have been serious attempts to gather data for robotics at a massive scale. Well, it's just a case that you could hire a bunch of people to move around in motion capture suits if you need to. And there have been a lot of attempts to do that, although I think this might be changing. Um, I think of robotics as mostly a hardware problem. Um, a hardware and like economics problem of yeah,
>> if you if it costs $100,000 to build a robot, then, you know, it's not necessarily better than a human who could work for $20,000 a year uh or a very cheap human um in certain countries uh or something. uh sorry a the like sort of minimum wage in some countries uh that you might be able to afford labor for. Um it's just not obvious to me that there is a software problem here. Um the hardware it it does seem like unclear. It's it's very unclear to me how much of a hardware problem is left. In particular, there's certain tasks which robots might be able to do, but are they actually the tasks that you care about a robot being able to do? If you want your robot to be able to like nimily walk around while lifting up heavy things and moving fast and react, then that's that's hard. That's a hardware problem that I don't think we've seen solutions for yet.
>> Yeah, I think my impression roughly matches this. It's sort of I don't know. People fairly often talk about this distinction between remote work and physical work. I think because there's this perception of robotics progress lagging behind a bit and there even is some intuition that maybe maybe this physical manipulation stuff is actually just harder but I wouldn't conclude that with much certainty like Y has said it feels like you'd kind of also want to see well okay what what happens if it gets scaled up in a similar way to even get a sense of like oh okay was it actually harder versus was it just depp prioritized.
>> Is there is there anything we didn't get to that you feel is important that we uh leave our audience with?
>> We did discuss uh the data centers release you just did. I'm not sure if there's a good way to leave the audience with that.
>> Yeah, let's get into it. Okay. So, you guys just did a you release a project. Why don't you talk a little bit about what you were trying to achieve there and what you hope people take from it.
>> Yeah. So, we took uh uh 13 of the largest data centers we can find. Um these includes se a few from each of the major labs in the US. Um and we found permits. We took satellite images including new satellite images of all these data centers. We figured out how to determine how much compute is in them based off the cooling infrastructure that they're building as well as when they're coming online and their future timelines. So we understand this like real world data and it's all available um online on our website for free. um this like to give insight into this giant infrastructure buildout that's happening and the pace of it. Um there's some things about it that surprise me a lot. For instance, we learned that the most likely candidate to have the first gigawatt scale data center is anthropic which would not have been my pick. Um but Anthropic Amazon's new carile project reineer development seems on track to come online in January. Um followed shortly thereafter by Colossus 2. Um, we also learned a lot about what the largest concrete plans are rather than just like marketing plans. Some people will throw around numbers, but the one we found that's actually seriously underway and has permits and is, you know, setting up the electrical infrastructure for is one by Microsoft which is going to be used by OpenAI at least in part um in Mount Pleasant. Um, they're calling it Microsoft Fairwater. Um, and that one's going to be uh use a size use not quite as much power as New York City, but I think more than half the what's stopping us from significantly increasing the the the the cluster is is it the um is it is it cost? Is it supply lead times? Is are there any other engineering breakthroughs required? power.
>> I think that people are approximately uh wrong that there's something stopping us and we are scaling up as fast as there is money to scale up approximately. Uh I suppose they could want there to be all of the clusters literally today, but they're scaling up really quite fast. you're seeing these data centers which are using uh I think the one I mentioned for Anthropic Amazon is using about as much power nearly as much power as the state capital of Ind of Indiana which is where it's located. Um and the timelines on some of these uh like the Colossus 2 uh are you know 2 years or less which is just an insane thing to build this thing that's using as much power as a city. Um, I think that plausibly, you know, you don't want to buy chips now. You want to wait for there to be better chips. Uh, I I think that people think of there's a lot of noise about things being difficult and scaling up. And I think this is because people are having to spend a little bit more than they would ordinarily have to spend. You can't use the ordinary sort of power pipeline which is designed to deliver this affordable infrastructure um at a slow pace. you have to, you know, buy things that you wouldn't ordinarily have to buy and spend more than you would ordinarily have to spend, but not buy enough to slow it down. All of these things pale in comparison to the cost of your GPUs. Uh, so my actual takeaway from a lot of this has been, oh, we're not having too much trouble scaling up. But just like these plans are going really quite fast and it's not obvious that people would actually have the finances and desire to do them faster.
>> When when people are talking about energy as a as a as a a major potential bottleneck or having to, you know, increase our capabilities significantly, you're you're not worried that that's going to be a sort of durable sustainable bottleneck. that that's not
>> I think people like complaining because they can't just use the traditional plug into the grid for cheap affordable power um four years down the line pipeline. At the end of the day the day there are expensive uh technologies that exist right now. You could pay for solar power plus batteries. This is fairly small lead times. It might cost twice as much as normal power, but that's still way less than your GPU. So you're going to do it if you have to. And you see people doing these sort of emergency things that cost them a bit more. You know, starting up their data centers. A common thing we see is people starting their data centers before their data centers are connected to the grid. Um I think Abene was an example. XAI Colossus 1 is a prominent example of just finding ways around this that are expensive and you complain about it because you know it would be nice if you could do the cheaper way and no one's used to having to do it this expensive way. At the end of the day though, it's just like does not there seem to be enough solutions, especially if you are as willing to pay as you as people are in AI that I don't really expect it to be a significant bottleneck. Maybe let's close with this. If if if these systems get as powerful as we're as we're discussing as we're discussing, I'm curious to h how the sort of political system is going to respond. I'm curious if you're sympathetic to the Ashen Brener view that that um there's some potential nationalization that that that occurs. Um but but in general, how do you expect governments to to to respond? It's kind of remarkable of how um not in the political discourse uh it is given how how powerful it is already. I'm curious how you think about that.
>> I expect so the thing I calling back to what I mentioned earlier, this concept of you know the potential for 5% unemployment increase in like six months. I think that the public's reaction to this will determine a lot. There will be very very strong feelings about AI once this happens. I think there will be a bunch of, you know, a very strong consensus on what to do. I on things that we don't normally think of as things that people are considering. I know when this happened with COVID, there was a several trillion dollar stimulus package passed at like, you know, in a matter of weeks to days, it was break neck speed. Um, I don't know what that will look like for AI, but I think it's like everything else in AI. It's like, you know, exponential, which means it will pass the point of, you know, people sort of care about it to people really care about it quite fast if things keep going. Um, I I just don't know where we're going to end up. I just expect you know wherever we end up there will be it will look like oh everyone suddenly agrees that why that's that's to do this certain thing which we would have considered unimaginable a year ago and I I don't know what that will look like it might look like nationalization it might look like pausing um it might look like I don't know going faster uh guaranteeing better unemployment benefits who knows I I I I just think there's going to be some sort of like strong response of some sort and it's going to been very fast.
>> Yeah. I mean, you know, you make the point that governments are maybe less interested than you'd expect now, but I mean, the current impacts, I think, aren't really that large. I feel like the attention is getting larger, but it's not that AI as of right now is that powerful. And yet, governments are already talking about it a lot, right? and you have people meeting with heads of state uh from various hardware manufacturers and AI companies and like countries talking about their AI strategy stuff like this. So I feel clearly country national governments are going to be quite involved. It's just a question of how and yeah I also am a bit unclear on that. I think that right now we've seen this thing in revenue and finances where it's been doubling or tripling every year and my default assumption is that attention that AI gets from policy makers and governments is going to follow a similar trend where it will double and triple every year. Um this means that in the future there could if trends continue there will be a huge amount of attention and it means that right now there's a lot more attention than last year but you don't suddenly skip from very little attention to all of the attention. Uh although you do move quite we are moving I think quite fast.
>> I uh I think we made enough predictions that we'll have to have you back next year and uh at the end of the year check in and see where we're at and then make make it for next year. Um yeah David thank you so much for coming to the podcast.
>> Thank you. Thank you. too.
>> Thanks so much for having us. [music]
>> [music]