My co-founder was one of the people who made key contributions to the project of AlphaGo and he was one of the handful of people who flew out to the match with Lisa Doll. I think the thing that really made me internalize deeply there's a super intelligence here and imagine what those things might look like in the future and I think this this was true for many other people as well was move 37. This was a famous move where Alph Go in its match against Lisa Doll made a move that looked like a mistake initially. It looked like potentially a bug. Some members of the team thought that in the same way that language models hallucinate like they'll sometimes make something up that this was game agent basically making up a move that was incorrect. It seemed like Lisa Doll or at least the commentators thought that as well. Few moves later, maybe 10 moves or something like that later. It turned out that this was actually a brilliant move that was smarter than anything that any of the humans who were viewing the match could have imagined. It was so smart that everyone thought it was dumb. What that meant was that an AI system had discovered a strategy that was fundamentally more creative. And it made me think about what will a world look like when you have move 37s across every category of knowledge work. You know, a mathematician asks an AI to do a certain task for it and comes back to it with a move 37. It comes back to it with a proof that the mathematician had never even considered that was correct. What I think what end up happening is that in the same way that we're feeling some people are starting to feel the AGI now where there are kind of semblances of sparks of intelligence. I think we'll start getting into this point in time in the notsodistant future, probably in the next couple of years, where we're starting to feel the ASI, the artificial super intelligence, where move 37s start popping up across different areas of knowledge work and doing things that basically expand our creativity, right? Because when move 37 happened, it actually expanded our knowledge of the game of go and what was possible. So, it was a very it happened and now there's this new strategy that people are aware. How do people who play chess or go, how do they interact with AIs? They actually learn from them. They learn to get better from interacting with these AIs and I think we'll start learning a lot from these systems in the coming years and that's very exciting. I'm Misha. I'm the CEO and co-founder of Reflection. At Reflection, we are building super intelligence. The question is how? Our belief is that if you solve the problem of autonomous coding, you will solve the super intelligence problem more broadly. And that's kind of our path. Now in terms of you know how we got here the team pioneered a lot of breakthroughs in AI over the last decade my co-founder Giannis was one of the key architects of systems like deep Q networks alpho alpha zero and then Giannis and I worked together very closely on Gemini where we led a lot of the post- training work for producing Gemini 1 and 1.5 we kind of realized that two ingredients had come together that would enable you to take these language models create not just useful co-pilots or chat assistants, but intelligent capable autonomous systems. And these two ingredients, large language models are very broad in general. And the other ingredient was reinforcement learning as a technology which enables scaling up the autonomy of language models. We thought these two ingredients had come together were kind of mature enough technologically that you could combine them and produce something that would be a highly capable super intelligent autonomous system. [Music] I was born in in Russia and when the Soviet Union collapsed my family and I I moved to Israel. I mean this was I I don't remember this. I was one and then grew up first half my childhood in Israel, second half of my childhood in Washington state because we moved around fairly frequently. I didn't have the same, let's say, long form bonds that some other people do when they grow up. Lifelong kind of friendships going from childhood all the way to adulthood. As a result, I actually ended up spending a lot of time kind of especially when we were in the states alone and with books. I mean we had my parents brought a lot of books to the states and I did have friends but I also spent a lot of time after school just looking at my parents' library reading various things at the time you know there's definitely you know as a kid you start feeling pretty lonely about that but looking back I don't think I would have cultivated the interest that I did if I didn't have a lot of time on my own to be bored and think in some sense like boredom is a gift that you only appreciate in retrospect I was interested in physics and in literature when I kind of had all this time on my hands hands but ended up kind of hard committing to physics and the reason was that when when I read about kind of all the kind of most impactful science that had been done and the technology that produced. I would look back at the like technological artifacts that we have today and try to derive how are those originated and so an example is is a computer obviously incredibly impactful technology everyone uses today and and I was asking myself well how was that invented and you can trace it back you can go much further back as well but really there are some core components are invented that enabled this and one of them was a transistor and a transistor was invented by a theoretical physicist named John Bardin similarly when you think of technologies like GPS and you trace back to what is the ingredient that enables those technologies to work. It turns out it's also physics. GPS relies heavily on Einstein's theory of special relativity. And so I wanted to work at that root node of the science that will enable everything else that comes after it. I wanted to work on the stuff that if we look at the technology we have a few decades from now and we trace back to what was the breakthrough that enabled that. I wanted to be working on those things. That's what got me interested in physics. What I learned when I was in the PhD was that you have to kind of think about the science but not just the impactful science at any time. You have to think about what is the impactful science of the time today. The work in physics that I was reading about was basically done anywhere from 60 to 100 years ago. That's when all of the these kind of impactful inventions were made. And I realized that the field at least for me had crystallized a bit. It was hard for me to see how like what foundational breakthroughs I could be a part of. and not necessarily individually but as a team that would enable the next generation of technology. And at the same time I saw deep learning as a field taking off. And right around this time Alph Go happened and Alph Go was the first I would say major worldwide proof point of super intelligence of a neural network being trained to master a very complex board game go at a level that was more intelligent than the most capable human player. And I thought there was something really fundamental going on here. and I had to understand it effectively inside out. So I actually ended up dropping what I was doing and self-eing AI for it must have been four or five months and made some progress there where I started doing some independent research that opened up some doors after that. But it was really the realization that AI and in particularly deep learning and reinforcement learning were these kinds of building blocks of foundational ingredients of the science of our time that will lead to the most impactful technologies in within the next few decades. Like there there are a lot of similarities I think between entrepreneurship and research and science. But it is this kind of ability to look at a problem that looks really complex and messy and be able to reduce it down to some core set of principles that are actually guiding basically the direction of that problem. You know for research it could be you know the problem we're interested in let's say is is autonomy. Like we really care about getting these large language models to be capable and autonomous. And the question is how do you do that? How do you train them to do this? There are all sorts of ways to pursue this question. You can go in many ways, but it turns out there are typically only one or two things that really move the needle in a very major way. This sort of a framework that physics gives you for thinking about things allows you to one come in with that assumption and rather than looking for hundreds of solutions, really try to find the most impactful ones, but then have some rigor in your thinking that allows you to reduce it to to those base components. And I'd say that's true for all aspects of company building. There's the research part, there's a product part, there's a customer part. And typically in any one of the major buckets of company building, there's one or two fundamental problems. And everything else doesn't really matter. And the question is, how do you identify those one or two fundamental problems that will move the needle, that will solve your customer's problems, that will be packaged in the right way as a product, that will make it easy to use, that will yield the research breakthroughs that you're looking for. I think physics is very helpful for thinking about these problems. I think big labs have a lot of things going for them. There's a lot of compute. There are a lot of talented people. There are many problems that are suitable I think for solving in in a big lab. Ultimately we thought at the time this was you know after launch of Gemini 1 1.5 the paradigm for how people were thinking about things were basically building more capable chat bots. What we were deeply interested in since before this was kind of independent of our time at Deepmind. This is why we got into AI is the problem of autonomy and we really wanted to work on that and we felt that it is both a research problem and a product problem because suppose you build this really great highly capable autonomous intelligence. How do you know if it's actually working? How do you know if it's solving people's problems? One of the things we believe at reflection is that the evaluation that matters most is the real world evaluation. So if you're not working with customers and you're not building product, you're not actually evaluating your technology in the place that matters. We just felt that we'd be able to move faster on the research with a smaller, more focused team. And we wanted to be coupled very deeply with product and customers to make sure that we were steering our research in the right directions. And it's really hard to take a large organization that already has a product direction and is it's a big ship that is going in a certain direction. And if you internally believe that it should be going in a different direction, it's really hard to change to course correct. It's it's basically impossible. And so this was this was the main impetus for for starting as a company rather than doing it in a large lab. There are several insights that continue to resonate today from building Gemini and systems before that as well, but I can keep it specific to Gemini. One is that the things that tend to work at this level of scale, these are giant model. These are it's hard to comprehend how big these models are. Maybe to give a baseline the neural networks people were training say 5 years ago were 10 million parameter neural networks 100 million neural network was considered huge. Deep came out and it's a over 600 billion parameter neural network. These systems are massive. What ended up happening in AI in the before the era of scaling is that sophisticated complex ideas won like that you'd take a something small and you'd have like really complex kind of almost like mathematically sophisticated ideas and those seem to work and in the era of training these large systems and Gemini in particular it's the opposite. The simple ideas implemented at a great level of detail are the things that work. So you almost had to kind of flip a switch in your mind about how to approach research problems from adding increasing complexity until it works. Like an example of that is IBM blue like the system that beat Garrett Kasparov in chess. It was a very complex kind of uh basically treel like structure right that elicited all possible moves in chess then picked some best some of the best ones. The opposite is true for training these large language models. The objectives are very simple like predicting the next token or the next word is a very simple objective. The reinforcement learning algorithms tend to be pretty simple. My co-founder Giannis and I led a lot of the work in it's called reinforcement learning from human feedback or RLHF. And if you look at public kind of work on this that how other large scale models were trained like let's say Llama or DeepSeek, they're very simple algorithms like relative to what reinforcement learning researchers were thinking 5 years ago or a decade ago. These are very very simple algorithms. And so maybe that's a thing that stuck with me that doing simple things with a great deal of craft and attention to detail and building the right infrastructure to be able to support these large models and run them efficiently is probably the biggest takeaway. As you just heard from Misha, AI agents are freeing humans to focus on strategy, creativity, and real decision making. But here's the challenge for early stage founders. Where do I actually start? We suggest you to check out this AI adoption playbook from HubSpot for startups. This is the guide to systemically building real business value with AI without wasting time or money and hype. You'll understand how to start using AI without hiring a full tech team and automate repetitive work that's slowing your growth. It does this by providing a comprehensive 90-day roadmap that goes through the process week by week. We found the sections on every employee gets a chief of staff and the four layer agent architecture especially useful. The chief of staff idea shows how to start with personal AI tools before jumping into complex integrations and the agent architecture clearly explains what separates successful AI systems from random experiment. This ebook was made by HubSpot for Startups which is today's video sponsor. A big shout out to them for this free resource. Now back to the video. There are some things that I have an unconventional maybe opinion on that maybe some other people might not think about. I won't speak for other people but I do think that one of the premise that we deeply believe in in reflection how fundamentally important the coding problem is and specifically autonomous coding the ability for an AI system to autonomously co code something on a computer and kind of go from task to something that's completed and give that to the user I think it's a common way to think about such systems is that they're going to be useful for software engineers that makes sense coding is that software engineers do a point of view that I have and it's not just myself but our team at reflection is that coding is going to transcend software engineering. It's going to go much further beyond that and touch basically every other piece of work category of work on a computer. And the reason for that is that when we think about how a language model is going to do work on a computer, we have to think about what is its embodiment? What is the natural way for a language model to interface with a computer? Effectively, what are its hands and legs? For people, we have really strong spatial priors that were evolved through millions of years of evolution. And so we have hands. We're dextrous and we have really good kind of innate spatial reasoning. We're born with it. Language models don't have that. They were never evolved. They were trained on the internet. And so what's intuitive to us is not intuitive to them. Like they don't have the spatial reasoning that we do. But what's intuitive to them is coding. There's a lot of code on the internet. And the same way that we can very easily almost trivially without thinking about it reason spatially about objects, language models are that way with code, it's just intuitive to them. And so when we think about like what is the way in which a language model interacts with any piece of software in the future, likely not going to be by moving a mouse around like humans do and using the human UI, it's probably going to be through code. Most piece of software we think will open up these language friendly UIs or interfaces and they're going to be mostly programmatic. A thing that we believe that maybe not deeply internalized yet but it is definitely internalized by some is that if you solve autonomous coding you solve intelligence on a computer and it transcends software engineering. I think as I got started spending more time in AI, I think the ambition of what we'll be able to achieve, not me personally necessarily, but as a field and and this is something that personally drives me to be a part of, is that we are on the cusp of building a general super intelligence. Even a few years ago, this would have sounded like complete science fiction, but this is going to be the most impactful technology of our time. And it's hard to say well what the world will look like after it. But it's hard for me to imagine being part of anything more impactful or or exciting from a scientific perspective. I think the this is not necessarily even a nice to have. This is kind of a a property of this that is pretty remarkable is that it's not just research. It's not just science in a vacuum. These systems are really useful today. And you kind of co-develop instead of having to wait three decades to see your science have the impact that you're looking for. You're kind of co-developing the science and the product together. That's what drives me that the mission of building super intelligence because it is the impactful science of our time and the luck I would say that we have it that this research is useful today. My worldview on how humans interact with AIs as these systems become super intelligent and start impacting the labor market are coming from a position that this is not a zero- sum game. It's not like there's a fixed quantity of labor that either a human does or someone else does. Each time there's been a technological advance. It actually just increased the amount of things we could produce. With intelligence, the that increase is the amount of ideas and theories and experiments and software that you can build. What I think the world looks like a few years from now as these systems start becoming extremely capable is that they kind of lift everything up and we end up creating in almost every field of computer-based work and then and and in the future also physical work. We end up creating an order of magnitude more or even more than that. You know, it's it's hard I think it's at least an order of magnitude more than we're capable of creating today. What that means though is that today in the same way that we collaborate with colleagues like we have colleagues and we work with teams and ambitious projects take big teams to I mean not big in the sense of thousands of people but you need a cohesive team to accomplish something big together. I think in the future it'll be that take example of an engineer I think a software engineer will become more of a software architect. they have these this AI workforce at their disposal. And the same thing will be true for other areas of knowledge work where we kind of become architects that manage an AI workforce. The thing that will still be really important is asking the right questions because basically if you have a really competent AI system, it will do more or less what you ask it to do. The challenge will be how do you pick the right problems to work on? How do you pick the right questions? Which by the way that is the whole challenge today with starting a company or pursuing a career in research. The fundamental thing to ask is like what is the right problem to solve and then you also have to then execute it right. So you have to put in a lot of work to execute it and imagine in the future most the burden will be on asking the right questions and designing them projects problems correctly and the execution will be done by an AI workforce for you. I think that's roughly the paradigm that we're going into. I think on asking the right questions maybe I mean it more in the concrete sense of like you suppose you're you have a job in creative pursuit and you want to do a good job at it right you have to there's some uncertainty on it either you have to build a new product or figure out some research like breakthrough or make a piece of art that actually resonates with people right if you're I mean there's one thing of like making art for yourself but if you want to make art for you know that will resonate with people that's kind of another thing so how do you know what to pick how do you know you're going to be right that's kind of what I mean around asking the right questions. So for example, like in the next year there's going to be one or two breakthroughs in AI. Every year there are basically one or two breakthroughs in AI. How do you discover one of them? Like what question should you be asking to discover one of them? It's really hard. I mean kind we were talking about this earlier. I don't have an answer to it. But I guess I mean it in in sort of that way kind of if you suppose you had these like really super intelligent AIs, you just needed to point them in the right direction. What direction would you point them at? And how would you ask them the questions to elicit those behaviors? Like another concrete example I'll give is that during reinforcement learning before language models, this is like AlphaGo days. It would take like billions of steps for a reinforcement learning training to get like an agent that was competent. And so people are looking at and saying, "Wow, billions of steps, that's so long. How do we make it more efficient?" That was the question people were asking. How do we go from billions to make it 10x more efficient? So now it's hundreds of millions and 10x more efficient. So it's tens of millions. I was asking that question myself and that was the wrong question to ask. But you're kind of saying like the reason you wanted to make it more data efficient is because you wanted to get these general agents. And the way people thought about getting general agents is just make them really fast to train. And it turned out that the right question was to ask was basically to invent language models because language models without any of this kind of reinforcement learning training became very general. If I would have thought about it that way then I would have picked a different research question to work on. So right there are plenty of examples where I made the wrong like even if it was locally the correct answer. I think there was a question on the list around one of my papers called curl which highly cited paper. It's like cited a thousand times and it was an impactful paper locally. So it got citations but it asked fundamentally the wrong question which is why it was cited a thousand times and not a 100 thousand times. The people who are asking the right questions write the papers that become the like first sentence in every other paper. Like the first sentence of every language model paper is language models have become very powerful. Cite GPD4. You know picking the right thing is the hardest thing. So I I definitely will not say that will not claim mastery over this. It's something I think about a lot but if I was a consistent picker of the right things I would have discovered Imagenet built Alph Go uh basically right built every single breakthrough that's come out in AI. So and and you know more widely, right? So picking the right thing is is really hard. But some frameworks that at least I use to to think about it is at least for me it comes down to clarity of thought. And it's hard to just have like off-the-cuff clarity of thought. You need to have some way of formalizing what it is that you're thinking. And at least and for me personally, it's writing. Oftentimes when I try to express what it is that I'm trying to achieve or like a method that I'm trying to kind of approach, I'll express it in writing, I'll write it down and then kind of almost in like short essay format and and revise it because writing often times exposes lack of clarity and thought like in when you're writing kind of every sentence should should have meaning and should have a reason for being there. When you do a first pass on how you're thinking about a problem and you write it down, you realize how many holes there are in your thinking or unnecessary parts of your thinking and you can kind of strip those out and iterate with yourself through a writing process. So for me at least, this has come through writing and maybe it's because I used to as as a kid when I was really interested in both literature and physics, I spent some time writing short stories. I wouldn't say those are any good, but I'll say that the version of the short story that I had written after several iterations was way better than the initial version. Making making the right decisions and asking the right questions is a very hard thing, but I think that writing is one thing. and then discussing with with other people that you think are very smart and trust but in a in a critical way like in a way that you're not kind of looking for someone who will just support your idea but you're looking for someone who will challenge find the holes with you. So I think those are the two the two main ways at least for me. So for any startup I would say there it's not like there's I think one particular hardest time. I think that hard times accumulate over different stages of the company have different hard times and they're probably kind of you know all equally hard. But in the very beginning, the things that are hard is you come in and you have a blank slate. And it's sort of reducing that blank slate into something that is much more directed and focused and having clarity around that with how it aligns with your long-term mission, what you're trying to achieve, but also that short term it's a thing that will work and get you to kind of the longerterm objective. Having clarity on that is really important. And it's quite hard uh in the beginning of startup to develop that clarity. This is why when people call it something a pivot, it's really, you know, a startup took a bet on something that uh did not work and right and so then they developed some clarity and then they they pivoted to something. I think that's the first thing that's uh for us and I think for other startups uh is sort of the first trial that you go through a startup of figuring out what exactly is it that you're doing today. You have your long-term mission. I know what you want to achieve. What are the first steps to that? The second thing is how do you get the best people in the world to work on this with you? I mean there's a simple answer that's hard to execute which is the best way to get the best people to work with you is by hiring the best people. And uh what I mean by this is that it's really hard to build out a stellar team if you don't already have stellar people. Given that there's so much uncertainty around startups, people who tend to be attracted to startups are ones that are interested in building something from scratch and kind of partaking in the growth and the upside of that. But why would they bet on you versus another company at a very early stage? Often times it's, you know, if you have an excellent team that you've assembled, even if it's a colonel, five really, really strong people. Good people beget good people. And so it's really important to make the first three hires, hire extremely caliber people who you have a great deal of trust with. And so I would say that was a challenge. But once that was solved, sort of good people attracted good people. And it has these sort of compounding effects. In terms of in terms of motivation, I think it also comes down to sort of what's your long-term strategy and what's your short-term strategy and are both of those things um compelling. It's kind of almost like uh in AI there's this idea of system one and system two thinking. System two being more highle abstract planning, system one being kind of local reactive. And I think you need both of these components to build a company. The nice thing is that when you set a really ambitious mission that's exciting, I think building super intelligence is exciting to um a lot of practitioners in AI. I mean, that's why I got into it. Like I would join a company that that was trying to solve super intelligence. Having a really ambitious mission helps attract really good people. But that's not enough. You have to have clarity on what is it that you're going to do today that will get you there. And how is your bet? It's not about the bet being different, but why is your bet right? Like why do you think you're correct when others are wrong, right? because the alternative is to stay at a big lab that is also pursuing general intelligence and take you know these labs have cast a bet and you can join a big lab and ride that bet out. So you have to have good reasons for why you believe your short-term uh wedge into the broader mission is compelling like what why is this correct and in our case it's focusing solely on autonomous coding um and nothing else and we have reasons to believe why that is the kind of correct bet if you want to aim at the problem of super intelligence and so I think those are the kind of the way you motivate people is by doing something really ambitious I had a previous startup before this and we did something much smaller and it was actually really hard to attract good people to work with us because it was sort of not you know you have one life and people want to work on the thing that will be most impactful to them building super intelligence is a pretty I would say it's at a similar level ambition of like taking people to Mars right of building rocket ships that go into space and take people to Mars and even though that seems really out there and difficult to achieve really talented people are attracted to very hard problems and very concrete approaches to solve those problems the way to deal with setbacks. I think I mean there there are basically two main things. The first one is to deeply care what you're caring what you're working on. It depends on what inspires you but for us right it's kind it's the mission that we're going after and the approach. So we just deeply care and I I deeply care and that's really motivating. You have to deeply care about the problem and then you have to deeply care about the people who are working with you on the problem. I think with those two things, things that feel that would otherwise feel like setbacks, I don't know, don't really feel that way. Maybe it's because researchers have operated that their whole career is in uncertainty. That's the whole game is that you pick try to pick the right problem. There's a lot of uncertainty around it. There are a lot of setbacks and you persist so long as the problem is really interesting to you and you feel like the approach that you're taking is fruitful. Unless you learn, you know, there's some new evidence comes in that maybe you need to change your approach, then you need to do that. But then that's not really, if you're deeply interested in the problem, that's not really a setback. That's more of a learning. The setback would be doing the wrong thing, doing the incorrect thing forever. That's a setback. But doing the incorrect thing at first, then acquiring some evidence. Maybe, you know, from a company building perspective, it might be that you have an idea for what a product might look like. you show it to customers and then it turns out that they find something else valuable in it that you didn't think about. Some people consider that a setback, but that's actually you want to accelerate your time to that event. Like if you don't feel like you have like these sorts of setbacks, then you're probably not making progress. And so I think the important thing is to deeply care about what you're working on with the people that you're doing it with and be making and sort of have momentum like taking action and making progress. And so when I look at the past year, I don't really in in that sense like I I don't really see any setbacks like there were there's new information that was learned that change direction for us research and product. There are you know always setbacks in terms of you know maybe someone a really good candidate not really wanting to join. But you kind of have to take the aggregate view of like what is the sort of general vector that you're going on. Are you learning things constantly? Are you in aggregate hiring really good people even if some of them aren't converting? As long as there's that kind of momentum there, I think that setbacks don't really affect not necessarily just myself, but I think the setback doesn't feel as painful when you have kind of clarity of what you're pursuing. If I was giving advice to my younger self or you know I have two younger sisters as well. What advice would you be giving them or you know other people calling piece of advice might be sort of around picking the right thing, pursuing your passion like these sorts of things. But something that is underappreciated I think is sort of surrounding yourself with the right people. So if you have an internal kind of vector of interest you want to do something that like things that are impactful interesting to you. But so long as you have that in the same way that it's important in your personal life to surround yourself with very highquality friends. I think the most kind of when I when I look at how sort of my last decade has played out, the thing that has been most impactful to me was surrounding myself with right people who at that time maybe would have taken a chance on me like for example in in Berkeley I would say Peter Aiel when took me in as a postto lab I was a physicist I was not an AI person and AI was very competitive then but by being in that lab and surrounding myself with people like him and his PhD students that's what really enabled me to learn quickly and develop my thinking. I think often times a function of your what you're able to achieve is really who are the people who you're spending your time with. Very talented, ambitious people are also generally quite open to I think giving back. Now, it's of course hard to get in front of them and so it's not like I think just sending a cold email is not enough. You have to really demonstrate that you really want something badly and demonstrate it through not just words but actions. In in my case, it was I spent a few months and I went and did a research. I taught myself reinforcement learning. I did a research project. I had something clear to kind of concrete to bring to the table and get people's feedback on. But I think so long as you have that as like there's you're persistent, you're able to kind of show your desire to work on something through action and not words, that's a very rare thing for a person to do. People who have been successful in whatever industry that might care about, I think, will look positively on that. So there's a sort of I think you can get into almost any door that you want with sufficient effort and it's just really important to surround yourself with the right people to enable you.