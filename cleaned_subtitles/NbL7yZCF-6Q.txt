The companies at the center of the AI boom have been busy investing billions of dollars in each other. I'm sure you've seen the spaghetti diagrams in the media recently showing how companies like OpenAI are investing in their chip suppliers or how chip manufacturers like Nvidia are investing in their customers, enabling them to buy more chips. I first noticed how strange these deals were back in March when Coreweave, a company who buys chips from Nvidia, puts them in data centers, and rents out compute, filed to go public. Its IPO perspectives revealed that Nvidia owned about 5% of the company. When investor interest seemed tepid after a long IPO drought, Nvidia offered to anchor the deal at $40 a share with a $250 million order. Bryce Elder described the deal in the FT at the time as an oruraorus, an ancient symbol of a snake or a dragon eating its own tail. A similar metaphor might be an extension cord plugged into itself. If you don't know much about electricity, that might look like a perpetual energy machine, but trust me, I've tried it out and no matter how you configure it, it won't power your home appliances. You just need outside energy to get things going. That roughly speaking is the current state of AI infrastructure financing. While the sheer number and size of these deals have convinced some investors that the AI value chain is developing rapidly, others are concerned by the circularity. Two companies sit near the center of nearly every diagram. Open AAI and Nvidia. Each is likely trying to ensure that everyone in the ecosystem from suppliers to customers to cloud providers has a vested interest in their success. Open AAI recently announced a $300 billion cloud infrastructure agreement with Oracle, a $10 billion custom chip partnership with Broadcom, and strategic alliances with major memory suppliers. According to UBS analysts, OpenAI's memory commitments alone account for half of the world's current capacity. Nvidia, meanwhile, pledged up to a hundred billion dollars in investment to Open AI, who will, in turn, buy millions of Nvidia's AI graphics cards. AMD is also in on the game. Open AAI agreed to buy tens of billions of dollars worth of AMD chips. And in return, AMD gave Open AAI the right to buy 10% of its stock for 1 cent per share, contingent on AMD hitting certain share price targets and Open AAI deploying its chips. The way Matt Lavine explained the deal at the time was that if OpenAI announces a big partnership with a public company, that company's stock price goes up. So Open AAI could just pay for the chips in cash, receive stock, and when the deal is announced, the stock would rocket, effectively reimbursing Open AI for its purchase. Everyone wins. Amazon has its own version of the loop. It invested more than $8 billion in Anthropic, the company behind the claw chatbot, and in return, Anthropic committed to using Amazon as its primary cloud provider. That means training and running its models on Amazon's custom AI chips, renting compute from AWS, and integrating Claude into Amazon Bedrock, the company's enterprise AI platform. In effect, Amazon is funding a company that will use its chips, run on its cloud, and help sell its services. And now, Google is getting in in the loop, too. Anthropic just announced a deal to access up to 1 million of Google's TPUs, bringing over a gigawatt of compute capacity online by 2026. The arrangement is worth tens of billions of dollars and positions Google as both a major investor and infrastructure provider. Anthropic says it chose Google's chips for their efficiency and performance, but the deal also reduces its reliance on Nvidia and Amazon. Google has already invested $3 billion in Anthropic. Amazon has pledged 8 billion. Both companies now provide cloud services, custom chips, and strategic capital. Anthropic insists that it's just pursuing a multicloud strategy. But it's hard to ignore how deeply entangled it has become with all three of the largest US cloud providers, each of whom now has a financial interest in its success. Then there's Elon Musk who seems to believe that the best way to build artificial general intelligence is to have his companies date each other. His AI startup XAI acquired Twitter or the everything app which supplies realtime data to Grog, his chatbot, sometimes referred to as Mecca Hitler. Tesla, his electric car company, uses the chatbot in its cars and possibly its robots, which are coming next year. Musk owns a majority stake in XAI, which recently bought Twitter from him. He owns a minority stake in Tesla and now wants Tesla shareholders to invest in XAI. It's not incestuous exactly, but we'd have to get Errol Musk to explain why it's okay. The whole thing is starting to look less like a tech boom and more like a moious strip made of venture capital and electricity. And the electricity part isn't a metaphor either. Before we dig into that, let me tell you about this week's video sponsor, Delete Me. Delete Me is a subscription service that removes your personal information from hundreds of data broker websites where it's being sold online. Data brokers are businesses who collect information on individuals to sell. Some of the worst ones are people search sites that collect personally identifiable information. This is sold often as cheaply as a penny per record online and can be used by scammers, stalkers, and identity thieves. You have the right to protect your privacy by asking them to delete your data. But there are hundreds of data brokers, and they make it hard to get off their lists. I used Delete Me, who are recently named the number one data removal service by Wire Cutter, to reach out to hundreds of data brokers to request a deletion of my personal data and to deal with any objections. Delete Me even have privacy advisors who are real people who can take on custom requests with a yearly subscription. Delete Me data brokers don't read your data to their lists. Remove your personal information from the web using the QR code or the link in the description and use the code boil for 20% off. McKenzie forecasts $5.2 trillion in capex for chips, data centers, and energy over the next 5 years alone. Bane says that we'll need to see $2 trillion in annual revenue from AI companies just to justify that spending. Open AI has about $13 billion in revenues today and is essentially a money furnace. Anthropic is a smaller money furnace. Nvidia is very profitable but not $100 billion profitable. So the question becomes who's going to pay for all of this? The interconnected nature of these deals, the reason we need spaghetti diagrams to understand them has raised concerns about circular financing. Companies are investing in each other, buying each other's products, and pushing up each other's stock prices. Investors are now asking whether these interdependencies could pose risks if AI demand or monetization falls short of investor expectations. The AI industry's investment structure is starting to resemble something we've seen before, just not in Silicon Valley. In post-war Japan, large industrial groups known as kiritsu were built, usually around banks and trading houses with companies taking stakes in each other and coordinating supply chains. South Korea's chaiball system followed a similar pattern, but with families in control rather than banks. These models weren't about competition. They were, at least initially, about survival in capital constrained economies. It seemed to make sense to have tight financial relationships with the businesses you relied upon so that your supply chains were secure. The keratu and chibball models were often criticized for obscuring financial risk, misallocating capital, and propping up uncompetitive firms. When Japan's asset bubble burst in the 1990s, the tangled web of crossholdings made it almost impossible to unwind bad bets. Today's AI giants are by no means short on capital, but they are assembling these webs of mutual dependence. The question is whether today's AI giants are building a similarly fragile structure which looks stable from the outside but depends on a constant influx of new capital to keep the lights on. When we look at the numbers, they almost seem made up. Open AI Stargate project announced this January at a White House event is a $500 billion plan to build 10 gigawatts of AI data center capacity across the US. It's firstly kind of crazy that we're talking about data centers in terms of gigawatts. According to the US Department of Energy website, a typical nuclear power plant produces 1 gawatt of power on average. And that's enough electricity to power a million typical US households. The typical US household is 2.6 people. So 10 gawatt is enough power for 26 million average Americans. Open AI isn't just building Stargate. The Financial Times pointed out that the 6 gawatt deal announced with AMD is enough energy to power Singapore for a year. But there are other deals too. All in, OpenAI has committed to building 23 gawatt of new data center capacity, which they say will cost well over a trillion dollars to develop and it seems would require 23 nuclear power stations to power up. In Texas, where several Stargate sites are planned, electricity demand is rising so quickly that some operators are installing on-site gas turbines and exploring nuclear partnerships to avoid waiting for grid hookups. The XAI data center in South Memphis is running gas turbines with no emissions controls and no permits, creating enough pollution that according to Politico, the area surrounding it leads Tennessee in asthma hospitalizations. There's not just one or two of these firms. All of the big tech firms in the United States and a bunch of additional firms in China and elsewhere are building out AI capability. So, as I mentioned earlier, McKenzie now estimates that $5.2 trillion dollars in capex will be needed by 2030 just to build the data centers required for the projected AI workloads. On top of that, data centers powering traditional IT applications are expected to require $1.5 trillion in capital expenditures, meaning that we're talking about almost $7 trillion in projected data center spending over the next 5 years. The firms are not generating sufficient revenues to justify that spending and don't appear to have a path to profitability planned out yet. For a technology that was supposed to make scientific breakthroughs, cure diseases, and maybe even replace human cognition, a surprising amount of AI output looks like slop and sometimes worse. Open AI Sora can generate realistic video, but the most viral clips so far have been deep fakes of Taylor Swift and Spongebob as a character in Breaking Bad. Yo, Sponge, this stuff looks extra crystally, like restaurant quality. Then there's Elon Musk's X AI, which has built an anime girlfriend chatbot, which many feel is an improvement over the Hitler one, which should hopefully keep basement dwellers occupied for the foreseeable future. There's also a cartoony red panda version, if that's what you're into. As easy as it is to make fun of this, there are many less widely discussed breakthroughs. The 2024 Nobel Prize for Chemistry went to two Google DeepMind researchers for their pioneering work on AI powered protein folding, which promises to expedite drug discovery and development and is already being used to combat cancers and other diseases. A number of my viewers think of me as being anti- tech and anti- AI as I've made fun of many of the more ridiculous claims out of Silicon Valley. And there are a lot of them to keep up with like the Hyperloop, the metaverse, AI enhanced water bottles, the general usefulness of the blockchain, and passing off short-term office rentals with free beer as a tech business. There are plenty of uses for AI which don't involve generating slop, but people do seem to love slop. Open AI is not profitable. It's spending much more money than it brings in in revenue and is doing so at a pace that would give most CFOs post-traumatic stress disorder. To fund its infrastructure buildout, the company has secured a $4 billion revolving credit line from a consortium of banks. This is very unusual. Historically, high growth tech firms raised capital through equity, especially if they were burning cash as lenders like to see predictable earnings. The shift from equity to debt and from public listing to private investment is happening across the sector where data center providers are borrowing against assets like racks of GPUs which might quickly become obsolete. This creates a strange dynamic. The companies building the infrastructure are borrowing to serve customers who are also borrowing or being subsidized by their investors. The whole system appears to be leveraged on optimism. For now, the money is flowing and for users, as I argued in my Blitc Scaling video from a few years ago, it probably makes sense to make the most of these expensive AI tools that we're currently getting for free. It's not clear how long that can last for though, or who will be left holding the bag if AI providers can't flip to profitability. The GPU rental market is already showing early signs of stress, and the buildout is only getting going. According to FT Alphavville, the price to rent Nvidia's B200 chip has dropped from $3.20 an hour to $2.80 per hour in just a few months. Older chips like the A100 are now available for as little as 40 cents per hour. That's below break even for many operators even under ideal conditions. They calculate in the article that a cluster of eight chips which would have cost around $200,000 5 years ago and has a 5-year useful life would need to have generated about $4 an hour in rental fees just to break even. Back in 2020, the average rental price for an A100 was $2.40 an hour. That's now fallen to around $1.65 per hour. And to make it worse, the average is being skewed by hyperscalers who are continuing to charge more than $4 when their competitors go as low as 40. If demand for all of this infrastructure doesn't materialize, it could become stranded. Data centers built for 5 years of peak usage might sit half empty. The FTP suggests that many pandemic error GPUs will end up in liquidation, never having earned back their cost. There is a precedent to this. Telecom firms in the early 2000s built out fiber optic networks that were never used. Railways in the 19th century similarly laid track to nowhere, much of which was later removed. The AI industry is now laying down gigawatts of compute, betting that someone will not just show up to use it, but actually pay to use it. If they don't, the fallout won't be limited to a few startups. It'll hit lenders, landlords, and the public utilities that signed up to support the boom without necessarily understanding the bet that they were making. Nvidia's stock market valuation is based on the idea that demand for its chips is massive and will keep rising not just this year but for the foreseeable future. The question we have to ask is how much of that demand is real and how much is driven by Nvidia's investments in other companies. Open AI is buying and renting billions of dollars worth of Nvidia chips. Nvidia is investing in Open AI. Coreweave rents Nvidia chips to OpenAI and Nvidia owns a stake in Coreweave. The same dollars are circulating through the system, possibly inflating purchase orders and revenue projections. It's hard to tell where the demand ends and the subsidy begins. The circularity makes it difficult to assess the quality of revenues, and that's why there are so many people asking if we're in an AI bubble. If Nvidia's biggest customers are also its investment targets and those customers are using Nvidia's money to buy Nvidia's products, then the margins may not be quite what they seem. There's also the question of how well this infrastructure is being used. Open AI claims to have 700 million weekly users, but only 5% are paying customers. Most of the revenue in the sector comes from enterprise contracts, not individual subscriptions. And even among business users, the success rate of AI pilot projects is low. McKenzie puts it at less than 15%. We're not seeing the mass AIdriven layoffs that many were predicting a few years ago. Labor data shows no clear relationship between AI deployment and trends in employment other than for freelance graphic designers and copywriters who have seen sharp declines since the arrival of Chat GPT and some junior coding jobs which have been in decline. Not long ago, the complaint was that American companies were no longer investing. they were hoarding cash or using it to buy back stock and just avoiding risk. Now the complaint is that they're investing too much and possibly in the wrong things. The circular deals are big, but they're not overwhelmingly so. The Open AI Nvidia deal, as an example, should account for around 13% of Nvidia's expected 2026 revenue according to UBS. And that's assuming the full gigawatt deployment goes ahead. That would mean 50 to $60 billion in total capital investment with Nvidia receiving $35 billion of it back. Nvidia says that it might reinvest $10 billion into Open AI, but only if monetization keeps pace. That's a performance-based approach which is smart and gives plenty of room to back out. The financial health of the big players is solid, too. The mega cap US tech firms are expected to generate over $200 billion in free cash flow next year alone, even after capex. That's enough to fund the infrastructure buildout without leaning too hard on debt or requiring new financing. The balance sheets are strong and the earnings are real. This isn't the same as the telecom bubble. Valuations are elevated but once again not absurd. In the late 1990s, internet stocks traded at 60 times forward earnings. Today's AI leaders are closer to 35 times. And the ones everyone is excited about actually have earnings. The market isn't pricing in infinite growth. It's pricing in a bet that AI will be big and that the companies building it will make a lot of money. Now that bet might not pay off immediately. Monetization so far has been slow. Adoption is uneven and some parts of the value chain, especially cloud renters and AI labs are more exposed than others. But the fundamentals are better than they were in past cycles and the investment strategies are more cautious. There's one constraint that doesn't show up on balance sheets. Electricity. Open AI Stargate project alone will require 10 gawatts of power which as I mentioned is around 10 nuclear power stations. The full buildout just for open AI not for the others is expected to need 23. For context, the last new nuclear reactor in the United States took more than a decade to complete and came online in 2024. There are no new nuclear sites currently under construction. Permitting for solar and wind has been tightened and tariffs have raised costs for those power sources. Even fasttracked projects face multi-year delays. Some developers are installing gas turbines on site just to avoid waiting for grid connections. The chips might arrive on schedule. The electricity probably won't. High private market valuations like we're seeing for firms like OpenAI, XAI, and Anthropic only make sense if one of them ends up dominating the space. That's what tech investors are expecting as that's what's happened in the past with Google dominating search, Amazon dominating e-commerce, Meta dominating the metaverse and those uh glasses that Zuckerberg wears. If AI turns out to be a winner take all market, then paying up for the winner could be a great investment. But owning them all might not as a bunch of them could fail. If you invested in all of the big search engines in the mid 1990s, it wouldn't have worked out for you as Google arrived late but then dominated search. The deepseek moment earlier this year caused a bit of panic in AI as it showed that models can possibly be replicated quickly and cheaply. Elon Musk's rapid deployment of Grock showed the same thing. These systems might require a lot of really expensive R&D, but they may not be very hard to copy. And if the models are all roughly the same, then the market may not reward any one player. Instead of a big winner and a monopoly, we might see a very competitive market for AI tools where none have any pricing power. On top of all of that, there's the question of who profits. It might not be the model builders. It might be the businesses that use the models. AI could end up boosting productivity across the economy while the labs themselves struggle to monetize. So while this might be a bubble, the fundamentals of the biggest companies involved are stronger than in past bubbles. The investment strategies are also more cautious where a lot of the big deals that have been announced leave lots of room for backing out, but the outcome is still really uncertain. Someone has to pay for all of this, and it's not clear who wins or if anyone does. If you found this video interesting, you should watch this one next. Don't forget to check out our sponsor, Delete Me, using the link in the description. Have a great day and talk to you in the next video. Bye. [Music]