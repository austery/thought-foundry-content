maybe the very last thing that these systems will be able to do these llms will be able to do is given the laws of physics as we understood them at the turn of the last century invent general relativity yeah from that uh so I think that's probably the terminal step and then once once it can do that if it can do that then there won't be much else to do as far as uh humans are concerned it's pretty extraordinary I mean particularly coming from a physics background in which progress is pretty slow uh to come to the AI field and see progress being so extraordinarily rapid day by day uh week by week year by year um looking at it it certainly looks like these llms uh and these AI systems in some sense they're just interpolators but the level of abstraction at which they're interpolating keeps going up and up and up uh and we keep sort of writing up that chain of abstractions and then presumably from a sufficiently elevated point of view the invention of generativity uh from Newtonian physics is just interpolation at some sufficiently grandiose level of abstraction that perhaps tells us something about the nature of intelligence human intelligence as well as uh as well as about these large language models if you ask me how many years until we can do that uh that is not totally clear but um in some sense General general relativity was the greatest leap that Humanity ever made and uh once we can do that perhaps in 10 years uh then then we will have fully encompassed human intelligence will it have the same will it be of the same character as what Einstein did clearly there's some there are many disanalogies between human intelligence in these large language models but I think at the right level of distraction it it may be the same do you think AI mathematicians physicist will have advantages over humans just because they can by default think in terms of weird dimensions and manifolds in a way that doesn't natively come to humans ah um you know I think maybe we need to back up to in what sense the humans do or don't think natively in high Dimensions obviously it's not our natural space there was a technology that was invented to think about these things which was you know notation tensor notation VAR other things that allows you to much using just even writing as as Einstein did 100 years ago allows you to of naturally move between Dimensions uh and then you're thinking more about manipulating these mathematical objects than you are about thinking in higher Dimensions I don't think there's any sense I mean in which large language models naturally think in higher Dimensions more than humans do you could say well this large language models have billions of parameters that's like a billion dimensional space but you could say the same about the human brain that it has all of these billions of parameters and is therefore billion dimensional whether that that fact translates into thinking in uh billions of spatial Dimensions I don't really I don't really see that in the human and I don't think that applies to an LM either yeah I guess you could imagine that um you know if you just seen like a million different problems that rely on uh doing this uh weird tensor math then in the same way that maybe even a human gets trained up through that to build better intuitions the same thing would happen to AI just sees more problems and develop better representations of these kinds of weird geometries or something I think that's certainly true that you know it it is definitely seeing more examples than any of us will'll ever see in our life and it is perhaps going to build more sophisticated representations than we have yeah often in the history of physics a breakthrough is just you know how you think about it what representation you do it is sometimes jokingly said that Einstein's greatest contribution to physics was his uh a certain notation he invented called the Einstein summation convention which allowed you to more uh easily express and think about these things in a more compact way that strips strips away some of the other things you Penrose one of his great contributions was just writing down um a inventing a new notation for thinking about uh some of these space times and how they work that made certain other things clear so clearly coming up with the right repres presentation has been an incredibly powerful tool in the history of physics and and many incredibly large developments somewhat analogous to coming up with a new experimental technique in some of the more applied physic uh applied scientific domains and yeah one would hope that uh as these large language models get better they come up with better representations at least better representations for them that may not be the same as a good representation for us so there's a there's an interesting question here clearly these models know a lot and that's evidenced by the fact that even professional physicist can ask and learn about FS that they're less familiar with but this um doesn't this raise the question of we think these things are smart and getting smarter if a human that is reasonably smart had memorized uh basically every single field and knew about the open problems knew about the open problems in other fields and how they might connect to this field knew about um potential discrepancies and uh connections what you might expect them to be able to do is um not like Einstein level conceptual leaps but there are a lot of things where just like hey mag uh magnesium correlates with this kind of phenomenon in the brain this kind of Phenomenon correlates with headaches therefore maybe magnesium supplements cure headaches um these kinds of like basic connections you would anyways does this suggest that llms are as far as intelligence goes even weaker than we might expect given the fact that given their overwhelming advantages in terms of knowledge they're not able to um already translate that into new discoveries yes they definitely have different strengths and weaknesses than humans and obviously one of their strengths is that they have read way more than any human will ever read in their entire life um I think maybe again the analogy with chess programs is is a good one here they will often consider way more possible positions that's Monte research than any human chess player ever would and yet they even at human level strength they're if you fix human level strength they're still doing way more search so that their ability to evaluate is maybe not quite as natural as a human so the same I think would be true of physics uh if you had a human who had had read as much and retained as much as as they had you might expect them to be even stronger Scott arenson recently or it was a year or so ago posted about the fact that the uh gbd4 got like a b or an A minus or something on his insured qu Computing class which is definitely a higher grade than I got and so I'm already below the waterline um but uh yeah you know you teach a bunch of subjects including gr at Stanford um I assume you've been querying these models with questions from these exams how how how has their performance changed over time yeah I um I take an exam I gave years ago in my graduate generativity class at Stanford and give it to these models and it's pretty extraordinary three years ago zero zero uh a year ago they were doing pretty well maybe a weak student but in the distribution and now they essentially Ace the test in fact that that I'm retiring that that's just my own little private eval I don't you know it's not not published anywhere but I just I just give them this thing just to follow along how they're doing and it's pretty it's pretty strong um they you know may be easy by the standard of graduate courses but a graduate course yeah uh in general relativity and they get um pretty much everything right on the final exam that's just in the last couple of months that these these have been doing that what is required to a a test I obviously like they they probably have like read about all the generality textbooks but I assume to AC test you like need something be on that is there some you would characterize physics problems compared to math problems tend to have two components one is to sort of take this word question and like turn it using your physics knowledge into a a maths question yeah and then solve the maths question that's that tends to be the typical structure of these problems so you need to be able to do both the bit that's maybe you know only llms can do and wouldn't be so easy for other things is is step one of that is like turning into a math problem um I think if you ask them hard research problems you certainly can come up with problems that they they can't solve that's that's for sure but it's pretty noticeable as we have tried to develop evaluations for these models that as recently as a couple of years ago certainly three years ago you just scrape from the internet any number of of problems that are standard totally standard high school math problems that they couldn't do and now we need to hire phds in whatever field and you know they they come up with one great problem a day or something you know the difficulty as these llms have got stronger the difficulty of evaluating their performance has has increased