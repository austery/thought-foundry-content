大家好，这里是最佳拍档，我是大飞 今天我们来聊一个听起来有点冷门的技术 但是实际上却是GPT-5背后的隐藏武器 它就是通用验证器（Universal Verifier） 早在GPT-5发布之前 知名科技媒体The Information就报道过 它的性能提升主要就来自于这个验证器 虽然后来GPT-5的能力升级没有达到预期 但是所有大厂都开始把通用验证器 当时了下一个战场上要争夺的圣杯 今天我们就来讲讲 这件可能会决定AI未来的隐形武器 理解了它，我们可能才能真正看懂 接下来一轮AI技术竞赛的核心关键在哪里 在深入讨论具体的技术之前 我们得先回答一个关键问题 为什么通用验证器突然变得这么重要了呢？ 这还要从之前支撑大模型能力飞跃的核心技术 可验证奖励的强化学习RLVR（Reinforcement Learning with Verifiable Rewards）说起 RLVR的逻辑其实很简单 就是找那些有明确标准答案的领域 比如数学和编程任务 答对了就给模型加分，答错了就扣分 训练的效果可以说是立竿见影 这也是为什么过去一年 AI在数学推理、代码生成上的进步特别明显 但是问题来了 现实世界远比对/错这样的二元判断要复杂 比如在医疗场景中 医生给癌症患者解释治疗方案 既要保证医学知识的准确性 又要考虑患者的心理状态 可能还要结合患者的经济条件来推荐方案 这时就没有唯一正确的答案 只有更合适的答案； 再比如创意写作 一篇小说的开头好不好 取决于故事性、情感共鸣、风格适配 这些都是主观且多元的评价维度 在这些领域，RLVR几乎完全失灵了 因为它没法定义什么是对 自然也没法给出奖励 甚至会让模型为了追求标准答案而变得十分的机械 给出的回答缺少同理心 这显然不是我们想要的AI 想要让AI突破这个瓶颈 就必须得跳出对/错的奖励限制 让它能够像人类专家一样 在不同领域判断所谓的好/坏 把海量非结构化的经验数据 转化为有效的学习信号 而通用验证器 正是为了解决这个问题而生的 它被学界认为可能引发强化学习的下一次范式革新 这也是为什么它比GPT-5的功能升级 更值得关注的原因所在 接下来，我们会分开两条技术路径 来介绍一下当前通用验证器的研究进展 虽然这两条路各有侧重 但是最终都指向让AI学会自主判断优劣的终极目标 第一条路径，也是目前最主流的方向 那就是让已经具备通用能力的大模型来当裁判 但是把过去简单的评价标准变得更复杂、更贴合开放领域的需求 其实用大模型当裁判 也就是LLM-as-a-Judge的概念 早在2024年初就有了 当时它更多是作为评估AI能力的一项工具 比如让GPT-4判断两个模型的回答哪个更好 但是并没有直接用到训练中 也没法给模型提供实时的优化反馈 直到2024年8月 DeepMind发表了题为《生成式验证器》的论文 第一次尝试把语言模型直接训练成强化学习用的验证器 也就是生成式奖励模型GenRM（Generative Reward Model） 早期的生成式奖励模型主要用在数学、算法推理这些逻辑性较强的领域 它的核心版本GenRM-CoT（生成式奖励模型-思维链）会先拆解问题的解决步骤 比如在计算一道微积分题的时候 它会一步步检查求导是否正确、代入的数值是否有误 从而精准的找出推理过程中的错误 但是后来随着OpenAI的o1模型和可验证奖励的强化学习技术的兴起 生成式奖励模型一度遭到冷落 因为在数学、编程这些有标准答案的领域 训练数据本身就包含了验证信息 再去建立一个复杂的生成式奖励模型 反而显得画蛇添足了 直到大家发现可验证奖励的强化学习技术 在开放领域开始失灵 生成式奖励模型这条路径才重新被重视起来 最近的几篇关键论文 都是在生成式奖励模型的基础上 针对开放领域的复杂性做了较多的深化 其中有三篇代表性的研究论文 首先是Scale AI在2025年7月23日发表的《作为奖励的评分细则》（Rubrics as Rewards） 简称RaR框架 它的核心思路特别的直接 那就是既然开放领域没有标准答案 那我们就给AI制定一套多维度的评分细则 把好答案的标准拆解开 让裁判模型照着细则来打分 RaR框架把这个过程分成了三步 分别是专家立法、模型释法和AI执法 第一步专家立法 就是让人类专家和大模型一起 为特定领域制定一个元框架 比如医学领域 专家会明确评价的维度必须包含事实的正确性、同理心和帮助性 还会给这些维度分等级 比如必要标准、重要标准、可选标准、陷阱标准等等 这一步的关键是定下原则 确保评价的方向不跑偏 但是问题来了 如果每个具体问题都要专家来写细则 根本没法大规模的应用 比如医学领域里有上万个病症 每个病症的问诊场景都不同 专家不可能一一都覆盖 所以RaR的第二步是模型释法 就是让大模型把元框架落地到具体的场景上 比如给模型一个问题 如何诊断肾结石 再给它一份专家写的参考答案 模型会自动生成7到20条具体的评分项 比如必要标准是 必须指出非对比螺旋CT对肾结石的敏感性 重要标准是 需要提到尿常规检查的辅助作用等等 这样一来 专家只需要写元框架和少量的范文 模型就能把细则扩展到成千上万的场景 从而解决了可扩展性的难题 第三步AI执法 就是把这套细则用在强化学习里 需要训练的学生模型会针对一个问题生成多个答案 然后让裁判模型照着细则给每个答案打分 学生模型再根据分数来优化自己的生成策略 比如这次因为没有提到CT的敏感性被扣分 下次就会主动补充这个信息 实验的结果很有说服力 那就是用RaR框架训练后的Qwen2.5-7B 在医学领域的得分 从原始模型的0.0818飙升到了0.3194 性能几乎翻了四倍； 即便是经过指令微调的Qwen2.5-7B-Instruct 也从0.2359提升到了0.3194 相对提升了35%。 此外 在HealthBench-1k医疗基准测试中 它比让模型直接打1到10分的Simple-Likert方法 相对提升了28%， 甚至超越了让专家为每个问题写参考答案 然后模型对着答案打分的方法 展现了明显的优势 不过RaR的方法也有局限 它还是需要专家为每个领域去编写元框架 在没有完成元框架的领域 还谈不上通用 但是它至少提供了一套可复制的工具 只要有领域专家 就能快速搭建这个领域的验证体系 已经是很大的突破了 在RaR之后，2025年8月18日 蚂蚁集团联合浙江大学发布了Rubicon论文 它在RaR的基础上做了两个关键的升级 解决了通用验证器落地的两个大问题 第一个升级是细化评分细则的过滤与激励机制 Rubicon构建了一个包含10000多个评分标准的系统 新增了否决机制和饱和度感知聚合 否决机制是一道硬约束 比如模型回答中出现Reward Hacking行为 不管其他的维度多好 直接否决所有的奖励； 而饱和度感知聚合则是为了避免模型出现偏科 如果模型在逻辑清晰度上已经得了满分 再往这个维度优化，边际得分会递减 所以要倒逼它去提升同理心、风格适配这些短板 同时 Rubicon还会用非线性函数放大高分区间的差异 比如两个答案分别得85分和90分 实际奖励差距会拉大 让模型更有动力优中选优 第二个，也是更关键的升级 那就是解决了强化学习的跷跷板效应（Seesaw Effect） 过去我们用强化学习同时训练模型的多种技能时 经常会出现一种能力提升 另一种能力下降的情况 比如训练模型逻辑清晰和同理心 模型可能越练越会讲专业术语 但是对患者的情绪视而不见 这对通用验证器来说是致命的 因为如果验证器要覆盖多个维度 训练后模型反而有了短板 就谈不上通用提升 Rubicon的解决方案是分阶段训练 第一阶段用基础验证标准打地基 比如检查模型是否遵循输出格式、回答的长度是否符合要求 还有通用性强的标准 比如逻辑是否通顺、信息是否完整等等 先让模型养成不跑偏的习惯； 第二阶段再用针对具体领域的细化评分细则来训练 比如医学领域的同理心临床准确性 这样一来 模型在第二阶段优化专项能力的时候 不会丢掉基础的能力 实验显示 用这种方法训练的Qwen-30B-A3B模型 在人文、创意、社交等开放式基准上的平均性能 提升了5.2%， 甚至比体量更大的671B的DeepSeek-V3模型 还要高2.4%； 更意外的是 它在AIME 2024这类推理任务上还提升了4.1%， 完全没有出现跷跷板效应 Rubicon还有个很贴近用户体验的改进 那就是解决了模型回答的AI味儿太浓的问题 团队针对语言风格设计了专门的评分标准 比如是否使用口语化表达 是否有个人视角 是否避免了“作为AI我没有情感”这类话术 比如要回答你生命中最有活力的时刻是什么 原始模型会说作为一个人工智能 我没有人类那样的情感或个人时刻 但是我可以帮助你探讨这个问题 巴拉巴拉 而Rubicon训练后的模型会回答 虽然我没有真实的生命体验 但是我能想象那些让人感到活力的瞬间 可能是清晨第一缕阳光洒在身上时的清醒 也可能是专注做一件事时忘记时间的投入 如果是你 会觉得哪个时刻最有活力呢？ 显然，后者更有人味儿 互动感也更强 如果说RaR和Rubicon是把评价标准变的复杂 那阿里Quark团队在2025年6月18日发表的Writing-Zero论文 则走了另一个方向 那就是不再纠结于细化的标准 而是把裁判模型变的强大 让裁判更擅长在开放领域区分好答案和坏答案 过去的奖励模型有个通病 容易出现Reward hacking 被模型钻了空子 比如在写作任务中 模型发现写得越长分数越高 就会故意堆砌文字 而Writing-Zero的解决思路是 让裁判模型在打分前 必须先写一段批判性的分析 相当于让裁判讲道理 为什么会给这个分数 从而避免凭感觉来打分 具体来说 Writing-Zero构建了一个成对的生成式奖励模型（Pairwise GenRM） 在给两个答案打分之前 会先进行分析 比如答案A的优势在于紧扣主题 用了具体案例支撑论点 但是结尾有点仓促； 答案B虽然结构完整 但是案例和主题关联性弱 还出现了事实错误，等等等等 这段分析必须结合通用标准和任务特定的标准 而且要经过前期微调与强化训练 确保分析逻辑和人类偏好是一致的 之后，裁判再根据这段分析来打分 分数的区分度就会出现显著提升 比如两个看似都不错的写作答案 通过分析能看出案例关联性的差异 从而给出不同分数 在训练学生模型的时候 Writing-Zero用了改进版的GRPO算法 比如学生模型在生成一组答案后 随机选一个当临时的参考答案 裁判模型把其他的答案和这个参考案对比 给出更好+1或者更差-1的信号 学生模型再根据这个信号来进行优化 实验结果显示 Writing-Zero训练的模型 在内部写作测试集和人工评估中 表现都要显著优于传统的奖励模型 尤其是在内容质量逻辑连贯性这些主观维度上 人类评估者（evaluator）更认可Writing-Zero模型的回答 因为它很少会出现凑字数自夸这种问题 好了，以上就是第一条技术路径 让模型当裁判，方向要么是细化标准 要么强化裁判的能力 接下来我们看第二条更反直觉的路径 不让外部的模型当裁判 而是让模型自己来评价自己 也就是自己给自己打分 实验证明它真的有效 第一篇代表性的论文是SEA Lab在2025年5月发表的《无验证器的强化通用推理》， 核心逻辑很简单 那就是与其相信外部的验证器 还不如用模型自己对答案正确性的自信度来当奖励 具体流程是这样的，给模型一个问题 比如求解x²+3x-4=0 先让模型只生成推理过程 比如第一步因式分解，(x+4)(x-1)=0； 第二步让每个因式为0 得x=-4或者x=1； 然后把数据集中的标准答案 x=-4或x=1，贴在推理过程的后面 让模型计算基于自己刚才的推理 能说出这个标准答案的概率是多少 这个概率就是模型的自信度 直接作为奖励信号 为什么这个逻辑会成立呢？ 因为模型自己能判断 如果推理的过程逻辑通顺、步骤正确 那么说出标准答案的概率自然高 就该给高奖励； 如果推理的过程混乱或者错误 那么说出标准答案的概率低 就给低奖励 从结构上看 这种方法和DeepMind的GenRM-CoT很像 但是两者的核心区别在于 GenRM-CoT需要外部模型来判断推理的过程是否正确 而SEA Lab的方法是让模型自己来判断 自己的推理能不能导出标准答案 相当于自己验证自己 实验结果出人意料 在Qwen3-8B模型上测试 这种无外部验证器的方法 得分和有外部验证器的传统强化学习方法相当 甚至在某些数学推理任务上 还超过了有外部验证器的版本 但是它的局限也很明显 那就是高度依赖数据集中的标准答案（Ground Truth） 如果没有标准答案 就没法计算说出标准答案的概率 奖励机制直接失效 比如当面对如何写一篇关于春天的散文 这种没有标准答案的问题时 它就无能为力 而且，它还没法处理等价答案 比如标准答案是8/5 模型算出1.6或1又3/5 按照这个方法会被判定为自信度低 但是实际上这三个答案是等价的 针对这个局限 UC Berkeley在2025年5月发表的论文《学习在没有外部奖励的情况下推理》中 提出了一个INTUITOR方法 把模型自评推向了更彻底的方向 连标准答案都不用 完全靠模型的内部信号打分 研究团队的出发点是一个观察 大模型在处理难题或者缺乏相关知识的时候 输出的自信度会降低 比如让模型去解一道它没学过的高等数学题 它生成的内容会断断续续，用词犹豫； 而当它面对熟悉的问题 输出会更连贯和确定 我们常用的困惑度（Perplexity）就是一个用来衡量自信度的指标 但是困惑度有个问题 容易偏爱更长的文本 导致模型写得越啰嗦 困惑度可能越低 但是内容质量未必高 所以INTUITOR提出了一个新的自信度指标 自确定性（Self-Certainty） 它的计算方式是 模型生成每个token的时候 会预测下一个词可能是什么 形成一个概率分布； 然后计算这个分布和均匀分布之间的KL散度 最后把所有词的KL散度取平均值 就是自确定性分数 这个指标的优点在于 它更关注推理过程的连贯性 而不是结果或长度 KL散度越大 说明模型对下一个词的预测越确定 推理步骤越通顺； 反之则说明模型在犹豫 推理可能有问题 同时 它还能鼓励模型覆盖合理的路径 并且坚定选择最优解 接下来 INTUITOR把传统可验证奖励的强化学习框架 比如GRPO算法中的外部奖励 全部替换成自确定性的分数 形成了一个新的框架 也就是从内部反馈的强化学习RLIF（Reinforcement Learning from Internal Feedback） 和RLHF相比 RLIF完全不需要人类的标注 既不用专家写标准答案 也不用标注者来判断哪个答案更好 只要给模型一个问题列表 它就能通过自我评估进行优化 实验的结果非常惊人 在数学推理任务上 INTUITOR训练的Qwen2.5-3B模型 在MATH500测试集上准确率达到61.2%， 接近使用标准答案的传统GRPO算法 要知道 INTUITOR完全没有用标准答案 能达到这个水平已经超出了很多人预期 更关键的是，它具备一定的泛化能力 在MATH数学基准上训练后 把模型放到LiveCodeBench代码任务上测试 居然实现了65%的相对性能提升； 而传统GRPO算法在代码任务上完全没有提升 这说明INTUITOR训练的是模型的通用推理能力 而不是针对某个领域的应试能力 同时 INTUITOR还能让模型自发生成更长、更结构化的推理过程 比如解一道几何题 原始模型可能会直接给出答案 而INTUITOR模型会详细写第一步 确定三角形的类型是直角三角形； 第二步：用勾股定理计算斜边； 第三步，验证结果是否符合题意 这样的步骤更清晰 也更接近人类的思考方式 看到这里，可能有观众会问 这两条路径看起来都有进展 但是它们能真正实现通用吗？ 答案是，目前还不能 因为它们都存在根本性的瓶颈 第一条路径（RaR/Rubicon/Writing-Zero）的核心瓶颈 在于手动搭建的脚手架 不管是RaR的领域元框架 还是Rubicon的评分标准库 本质上都需要人类提前为每个领域搭好架子 进入医疗领域要搭医疗架子 进入教育领域要搭教育架子 所以永远无法覆盖所有的复杂场景 如果未来要处理跨领域的任务 现有的架子可能完全不适用 还得重新搭 这和通用的目标还有很大差距 第二条路径（VeriFree/INTUITOR）的瓶颈 是无法超越预训练的知识 模型的自确定性、自信度 都基于它在预训练阶段学到的知识 如果遇到一个它没有学过的新领域 它的自确定性就会很低 根本没法自我评估 更没法凭空创造超越预训练数据的知识 因为它的内部反馈 始终跳不出预训练的知识囚笼 那真正的通用验证器终局会是什么样？ 强化学习之父理查德·萨顿（Richard Sutton）最近提出的OaK架构（Option as Knowledge） 给我们描绘了一个更宏大的蓝图 OaK的核心思想是 让Agent完全通过与环境的实时互动构建认知 摒弃所有设计时注入的知识 也就是说 不用人类提前写评分标准、不用预训练数据里的知识 让AI从零开始 在和世界的互动中自己学会什么是好 什么是坏，自己构建验证器 OaK的运作分为8个循环步骤 我们可以理解为Agent的学习闭环 1、学习主策略，首先 AI需要明确核心的目标 如何最大化最终的奖励 这是最基本的强化学习任务 2、生成的新特征 AI 不断地从环境中发现和创造新的“状态特征”， 也就是新的概念或看世界的新角度 3、特征排序 对新发现的特征进行排序 判断哪些是重要的、有用的 4、构建子问题，基于那些重要的特征 创建新的“辅助子问题”。 5、为每个子问题学习一个解决方案 这个方案就是“选项 (Option)”。 6、学习知识模型 为每个“选项”学习一个模型 预测执行这个选项会带来的各种后果 这就是“知识 (Knowledge)”。 7、执行规划，利用学到的“知识”， 在更高、更抽象的层面上进行思考和规划 从而改进整体策略 8、维护元数据 持续跟踪和评估系统中所有元素的有效性 为后续的学习提供指导 通过这个闭环 AI就能自主构建一个动态进化的验证器 而且这种能力会随着互动不断升级 不需要人类干预 不过，目前OaK还处于理论蓝图的阶段 最大的挑战是 当前的大模型缺乏主动学习和持续反思的能力 比如模型不会主动去发现新特征 也不会持续跟踪选项的有效性 这些能力的突破 可能需要大模型架构的根本性变革 而不只是算法优化 不过 我们不用因此否定当前通用验证器的研究价值 因为现在的两条路径 其实已经是OaK架构的早期雏形 RaR和Rubicon中的评分细则 本质上是OaK中子问题的手动外部版； INTUITOR的自确定性 则是OaK中价值函数的简化静态版 现在的研究 其实是在为未来的OaK架构测试其中的零部件 比如RaR验证了多维度评价的有效性 INTUITOR验证了内生奖励的可行性 这些经验都会成为构建终极通用Agent的基石 最后我们总结一下 通用验证器的本质 就是解决AI如何在没有标准答案的世界里 判断优劣的问题 它不像GPT-5的多模态能力、长文本理解那样直观 但是却是AI从处理结构化任务 走向适应复杂现实世界的关键一步 当前的两条技术路径各有突破 但是都还在中途 而OaK架构描绘的终局虽然遥远 但是已经为我们指明了方向 未来几年 谁能先突破通用验证器的瓶颈 无论是让手动脚手架实现自动的扩展 还是让模型具备主动的学习能力 谁就能在AI竞赛中占据主动权 好了，今天的内容就到这里 感谢大家的观看，我们下期再见