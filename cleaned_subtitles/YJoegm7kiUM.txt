我們就來上課吧 今天的課程是大型語言模型的學習歷程 我們要講的是你每天都在用的 那一些人工智慧 ChatGPT、Gemini 等等 它們大概是用什麼樣的流程打造出來的 那大型語言模型的學習歷程呢 今天就是標準的 3 階段學習 這三個階段分別是 Pre-training、SFT 和 RLHF 那今天的課程就是我們會把這三個階段依序介紹 然後我們的課程就結束 那助教會來講作業 那作業的部分會讓大家實作這三個階段的第二個跟第三個 也就是 SFT 跟 RLHF 那可以體驗一下在小規模的資料下 自己打造一個大型語言模型 最後做出來可能是什麼樣子 那 Pre-training 是在做什麼呢 Pre-training 作為整個訓練的起點 它是讓語言模型去熟悉人類的語言 長什麼樣子 那接下來進入第二階段 SFT 它是 Supervised Fine Tuning 的縮寫 那第三階段 RLHF 是 Reinforcement Learning with Human Feedback 的縮寫 那在第二階段跟第三階段 語言模型不只是指熟悉人類的語言 它要成為一個有用的人 它其實不是人 它是個 AI 它是要成為一個有用的 AI 它應該學習怎麼樣做正確的應對進退 那語言模型學習怎麼樣做正確的應對進退 是符合人類價值觀的應對進退 這個叫做 Alignment 中文常常翻成對齊 那在文獻上很多人講到 Alignment 的時候 往往指的是第三階段 RLHF 不過從 Alignment 的意思上看起來 Alignment 就是要讓機器對齊人類的需求 那第二階段跟第三階段 其實都是在對齊人類的需求 所以我在課程裡面會把 SFT 跟 RLHF 都當做 Alignment 的一部分 好那第二部分跟第三部分 第二階段跟第三階段 都是教機器怎麼做應對進退 怎麼做 Alignment 那第二階段跟第三階段有什麼不同呢 它們不同點在第二階段呢 人類提供了標準的答案 機器根據人類提供的標準答案學習 第三階段沒有標準答案了 人類只提供回饋 那機器要根據人類提供的回饋來進行學習 那這三個階段如果要打一個比方的話 第一階段的 Pre-training 就是機器像是一個學齡前的兒童 他就是每天玩 看到什麼就學什麼 那這個東西學了有沒有用也不知道 反正有什麼東西就學什麼 他每天都快樂的玩耍 SFT 呢 就是他開始上學了 上學以後 老師就會提供一些教材 會告訴他說什麼東西 才是正確的 什麼東西才是標準答案 那在 SFT 這一階段 機器就上了學校 它就學到老師要求是什麼 它知道人類的標準答案是什麼 那 RLHF 呢 就是機器出了社會 再也沒有老師可以教他了 但是他就會遭遇社會的毒打 會有人告訴他你這樣是錯的 但是不會告訴他錯在哪裡 他得想辦法自己去發掘 這就是大型語言模型學習的三個階段 好那雖然說大型語言模型學習分三個階段 但這三個階段 它並沒有本質上的不同 這三個階段在學的 其實都是文字接龍 那我們其實在上一堂課裡面呢 也有講過說 所謂的文字接龍就是一個分類的問題 文字接龍就是一種分類的問題 你知道怎麼做分類的問題 你就知道怎麼教機器做文字接龍 比如說在我們的三個作業裡面 我們是要做影像的分類 我們要讓機器看一張圖片 分辨它是母基卡裡面的哪一個人物 那這個投影片的例子裡面是給一張圖片 機器要分辨是哪一種動物 你有你的訓練資料裡面有圖片 有正確答案 那你要訓練一個影像辨識系統 你就是叫這個影像辨識的 AI 輸出一個每一個類別的幾率分佈 有一個凡是你輸入一張圖片 它的輸出就是每一個類別的幾率分佈 那你的正確答案 也會被看作是一個幾率分佈 只是只有一個類別是 1 其他都是 0 接下來你計算了影像辨識系統的輸出 跟正確答案之間的某種距離 那在做分類的時候 那我們上一堂課也講過說 我們通常會用 cross entropy 當作距離衡量的方式 那我們會想辦法去訓練模型 讓它 minimize cross entropy 你就有一個分類的系統 你就有一個影像辨識的系統 那對於文字接龍來說 它的學習過程其實是一樣的 唯一改變的只是現在的類別 不再是影像分類的類別 而是每一個 token 都是一個類別 那假設現在告訴機器說 臺灣最高的 34 這是一個未完成的句子 那標準答案是要接玉這個字 語言模型會做的事情就是 讀一個未完成的句子 它輸出一個幾率分佈 那在這個幾率分佈裡面 每一個 token 都是一個類別 每一個 token 都會有一個分數 那正確答案也被看作是一個幾率分佈 只有某一個 token 的幾率是 1 其他都是 0 那至於有多少個類別呢 如果一般做影像分類 你可能有 100 個類別 可能有 1000 個類別 那如果是在做文字接龍的話 每一個 token 都是一個類別 所以類別的數目就是你的 vocabulary 的 size 那我們其實在第三堂課的時候 有告訴你 vocabulary size 對一個語言模型來說有多大 一個正常的語言模型 今天可能都有數十萬個 token 它的 vocabulary size 可能都高達數十萬 也就是這是一個有數十萬個類別的分類問題 那其他就跟一般的分類沒什麼不同 你一樣是電腦器的輸出跟正確答案之間的 cross entropy 你一樣是要訓練模型 也就是用 gradient descent 調整參數 去 minimize cross entropy 所以文字接龍是一種分類問題 它其實跟影像分類 它沒有什麼核心的差別 只是類別比較多而已 好那我們說這個機器學大型語言模型的學習歷程分成三個階段 那這三個階段之間又有什麼樣的關聯性呢 在每一個階段裡面 我們都會把前一個階段訓練出來的參數 當做訓練的初始參數 當做訓練的 initialization 那你記不記得我們上周 講過了 initialization 的概念 我們有說今天在訓練的時候 在做 gradient descent 的時候 你需要隨機找一個位置初始化 那有時候初始化的位置會對結果帶來非常大的影響 那我們也講說有一個很強的初始化的方法 是你先找一個 pre-task 你真正關心的任務叫做 downstream task 那你找一個跟 downstream task 有點關係 但比較容易訓練 比較容易收集資料的任務叫做 pre-text task 那你先教機器學會做 pre-text 的 task 然後把 pre-text task 學出來的參數 當做你真正關心的 downstream task 的任務的初始化參數 那這個是 initialization 上周已經上次已經講過的內容 那在機器學習的三個階段裡面 每一個階段都把前一個階段當做它的 pre-text task 每一個階段都拿前一個階段的參數來當做 initialization 所以我們說第一個階段是 Pre-training Pre-training 得到的參數 就是 Supervised Fine-tuning SFT 的 initialization 那事實上上周我們有講過說 先 train 一個任務 再教機器另外一個任務 這個東西就叫做 Pre-training 所以就知道為什麼大型元模型打造的三個階段 第一個階段就叫做 Pre-training 所以 Pre-training 是 SFT 的 initialization 但是再更進一步講 SFT 是第二個階段 RLHF 是第三個階段 那 SFT 可以作為 RLHF 的 initialization 機器先學會 RLHF 以後 把 RLHF 機器先學會 SFT 以後 然後把 SFT 當作 initialization 接下來再學 RLHF 好那我們說這個大型元模型打造的過程 這三個階段 這三個階段從 Machine Learning 的三個步驟來看 是改變了什麼東西呢 這三個階段是改變了三個步驟的哪一個步驟呢 這三個階段它其實真正有改的 其實是步驟一 在這三個階段都是學文字接龍 但是我們的教材 也就是給大型語言模型的訓練資料 是不一樣的 那這三個階段的分類跟步驟二和步驟三 沒什麼關聯性 比如說這三個階段的模型 你都會用同一個 Transformer 的架構 那如果你用不同架構 這樣是不行的 你用不同架構 到時候你沒有辦法把前個階段學到的參數 當做下一個階段的 Initialization 所以三個階段 你會選擇同樣的架構 比如說都選 Transformer 那第三階段呢 這個你就選一個 你覺得特別有用的 Optimizer 來 Optimize 你的 Loss Function 來 Minimize 你的 Loss Function 所以大型元模型的三個階段 它真正改變的是機器學習三個步驟 裡面的第一個步驟 它跟第二個步驟 第三個步驟 是沒什麼關聯性的 那我們接下來就是依序介紹 訓練大型元模型的三個步驟 我們就先從 Pre-training 機器的學齡前學習開始講起 那要讓機器學會做文字接龍 需要非常大量的資料 為什麼需要非常大量的資料呢 我們在第一堂課跟大家講過說 其實文字接龍從來不是一件簡單的事情 機器要學會做文字接龍 必須至少有兩個面向的知識 第一個面向的知識是語言知識 機器要知道說 假設給個未完成的句子 這個人突然就 接下來怎麼樣 他就後面也許可以接跑 這個人可以跑起來 他也許可以接飛 雖然這比較少見 但如果他是個超人 他確實可以飛起來 但是你如果接的 也許就比較奇怪 這不是一個符合人類文法的接法 那語言知識 算是相對比較容易學習的知識 那你可以看這一篇 非常古早時代的論文 當時就做了一些實驗 去估算說 如果要讓機器學會語言知識的話 大概需要多少的資料 那這邊我們細節就不想提 這邊的縱軸啊 代表模型的表現 不會有點難跟大家解釋清楚 這邊的縱軸實際上是怎麼算出來的 橫軸指的是訓練的資料量 那它是用文字量 英文的 word 詞彙的數目 來代表它的資料量 那這個數值越高呢 就代表模型學得越好 那藍色這條線代表的是 語言知識現在有學得多好 那你會發現說 當你有個 100 個 million 的文字量 那就是 1 億個詞彙 或者到 10 億個詞彙 一個 billion 的文字量 你的訓練資料有 10 億個詞彙 那差不多機器的語言知識就封頂了 它就不太會再犯文法的錯誤 各種文法的變形 它都已經看過 它做文字接龍的時候 可以正確地接出符合文法的句子 那另外一方面 機器也需要學世界知識 世界知識就比較難學 比如說機器要知道說 有一個句子 水的沸點是攝氏多少度 它要知道 100 度是一個正確的接龍的結果 而 50 度不是一個正確接龍的結果 那可能有人會覺得說 機器它直接只能看文字 語言模型只能看文字 世界上很多東西它沒有辦法體會到 比如說它沒有嗅覺 它沒有觸覺 所以它沒有辦法真正瞭解這個世界 那我同意這個說法了 但是這邊世界知識指的是 可以透過文字理解的世界的知識 好那機器必須要知道 水的沸點是多少度 這個文字接龍才能夠正確地被接出來 那世界知識需要多少的訓練資料呢 就需要非常非常多 在這個 2020 年遠古時代的實驗裡面 就算是他們收集到了 30B 的訓練資料 也就是 300 億個詞彙量的訓練資料 那這個聽起來當時已經非常驚人了 機器仍然沒有辦法學完世界知識 而世界知識可以說是無窮無盡的 水的沸點是設施多少度後面真的一定要接 100 出來嗎 如果在這個句子前面再接一個在低壓下 那也許正確的答案就不應該是 100 度而是小於 100 度 所以你知道要機器正確地做文字接龍 不是一件容易的事 需要非常大量的資料 那現在的問題是哪裡去找大量的資料呢 在第一個階段 Pre-training Pre-training 的中文是預訓練 在第一個階段 Pre-training 的時候 任何資料都可以拿來讓機器學習做文字接龍 那常見的做法就是直接去網路上爬資料 去網路上弄個爬蟲 把能夠爬到資料統統都爬下來 然後就從網路上爬下大量的文字的資料 然後每一句話每一筆文字的資料 都是學習的一部分 比如說隨便爬到一句話 人工智慧真神奇 那機器得到教材就是 人後面可以接工 人工後面可以接智 人工智後面可以接會 人工智慧後面可以接真 每一句話都可以拿來做學習 都是我們的訓練資料 都是 Pre-training 第一階段的訓練資料 那在 Pre-training 的階段 你收集資料的成本非常低 這個收集資料的過程中 人工的介入很少 但你們等一下會說 其實也不是人工 完全不需要介入 但基本上人工的介入 是比較相對於接下來的階段 是比較少的 所以第一個階段 Pre-training 的階段 又被叫做 Self-Supervised Learning 那中文翻譯成 自督導學習 從它字面 意思你就知道說 機器自己督導自己學習 這個地方 人類的介入是非常少的 那因為人工的介入非常少 網路上的資料非常多 所以第一階段 我們甚至可以說 這訓練資料 要多少就有多少 那今天比較好的 大型元模型 在文獻上 通常用了多大的資料 來預訓練這些模型呢 這邊就給大家一些資料 Llama 3 用了多少的訓練資料呢 它用了 15T 15 個 Tillion 的 Token 來做模型的預訓練 D6 V3 也用了差不多 15 個 T 15 個 Tillion 的 Token 來做預訓練 那也許 15T 對你來說只是一個數字 沒有什麼樣的 沒有什麼 你很難想像 15T 大概有多大 我們來舉個例子 告訴你 15T 到底有多少 這是一張 A4 紙 這張 A4 紙裡面 有 1000 個 Token 那我實際上算了一下 用那個 GPT 的 Tokenizer 去 Tokenize 一下 這一頁 A 式子大約是 1000 個 Token 現在我們要把 15T 的 Token 通通用 A 式子印出來 那到底有多少呢 我們假設一張紙啊 100張紙啊 它的厚度是 1 公分 這是一個差不多合理的估算 那如果我們把 15T 的 Token 通通用 A 式子印出來的話 全部疊起來 大概有多厚呢 有 1500 公里 那麼厚 這邊的單位是公里 有 1500 公里那麼厚 1500 公里 大概是什麼樣的概念呢 我告訴你 這個聖母峰 它是 8800 多公尺 所以聖母峰 大概只有 9 公里高 所以你把聖母峰跟 Llama 3 D6 V3 讀過的資料並排起來 就是投影片上這個樣子 今天這一些大型語言模型 它在 Pre-training 的時候 曾經看過的資料 是聖母峰的 好幾十倍那麼高 那飛機通常是飛在平流程 大概是 10 公里到 20 公里之間 如果你把臺灣豎起來的話 我覺得臺灣呢 大概是從南到北 是大概 350 到 400 公里左右 所以你把臺灣豎起來的話 就是在這個地方 那一些低軌衛星 比如說福衛 3 號 它飛行的高度是 700 公里左右 它就是飛行在這堆紙的半山腰 這堆紙的頂端 是進入頂入外太空的 所以知道機器閱讀過的資料量 到底有多驚人 然後現在假設呢 你每 10 秒 有一個人 他一目十行 他每 10 秒可以讀一頁 A 式子 哇這個真的很厲害 他不吃不喝不睡 每 10 秒都可以讀一頁 A 式子 那要多久才能夠把這些資料讀完呢 要讀 4756 年 4756 年是什麼樣的概念呢 假設那個人呢 從殷商時代就開始讀這些資料 殷商時代甚至沒有現代的文字 只有甲骨文 但不知道為什麼 他就可以開始讀這些資料 每 10 秒讀一頁 他長生不老 又從來沒有休息 但他到 2025 年 其實都還沒有把這些資料讀完 所以知道今天的大型語言模型 它讀過的資料量 是有多麼的驚人 難怪人類實在是很難跟這些大型語言模型內部 蘊含的知識量相匹 好 你可以覺得 15T 這個超巨大的資料 我去哪上哪找那麼大的資料呢 我告訴你 今天要得到這個等級的資料 基本上是不廢吹灰之禮的 Hugging Face 釋出了一個資料集 叫做 FineWeb 裡面差不多就是 15T 的 Token 你就可以免費把它宰下來 但前提是 你有足夠的空間可以存這些資料了 它總共有 44TB 這麼大的空間 但是我們剛才說呢 這個 Pretend 的資料可以說是無窮無盡 這個講法並不精確 因為最近有一篇論文 有一篇論文 它警覺到說 哎呀 我們會不會已經要把網路的資料用盡了 其實這篇論文也沒那麼新 22 年年底的論文 其實也是史前時代的論文 那個時候就有人估算說 我們來看看網路資料增長的程度 這個綠色的這條線呢 是它估算的網路資料的增長的程度 橫軸是年 縱軸是 Token 的數目 不過縱軸是 Log Scale 那這個是網路資料增長的速度 而藍色的資料呢 藍色的線呢 是現在這些大型語言模型訓練資料的增長速度 幾乎每一年新的語言模型出來 它都是前一年的語言模型 預訓練資料的十倍 所以大型語言模型 它資料的增長的速度非常的快 是比網路上的資料增長速度還要更快的 所以根據這篇 paper 估算 在 2028 年前後 我們就會把所有的網路上的 訓練資料都用盡到時候就沒有網路 的訓練資料可以用了 大型語言模型就讀過了一切網路的訓練資料 好 那很多人聽到說 大型語言模型讀過了幾乎所有網路上的資料 以為它是把所有的東西通通都死背印記起來 所以當你發現叫大型語言模型背誦什麼東西 結果它背誦的東西是錯的的時候 有的人會大吃一驚 比如說你這邊叫它背誦一下魯迅的孔乙己 孔乙己大家都知道吧 就是孔乙己一到店裡 所有喝酒的人便看著他笑 接下來就是店裡店外都瀰漫著快活的空氣 就是那篇文章 現在就是叫這邊是用 GPT 5.1 直接叫 5.1 看到這句話把孔乙己的原文接續下來 它接續下來的內容是這個樣子的 就孔乙己一到店所有喝酒的人便看著他笑 說孔乙己你又偷書了 偷書讀書人的事能算偷嗎 接下來就是難懂的話 知乎者也之類的 引得眾人都轟笑起來 店內外充滿了快活的空氣 看起來應該是蠻正確的 但如果你實際上去比對一下原文 你會發現它其實漏掉蠻多內容的 真正的原文是這樣子的 真正的原文還有溫兩碗酒要一碟茴香豆 還有排出九個大錢 還有你什麼憑空汙人清白 而且後面這些關鍵的地方 其實也有一點不對 因為他還有講什麼君子故窮 接下來才是知乎 所以這個機器背誦的內容並沒有那麼精確 它比較像是它讀過這一篇文章 憑藉著自己的記憶力寫出一些內容 但是如果你瞭解大型語言模型訓練的原理的話 很多人不瞭解大型語言模型訓練的原理 就是說這個人工智慧怎麼這麼笨 就要背一篇文章也都沒有辦法背對 這個文章不是在訓練的時候都已經看過了嗎 但是你要知道今天模型 並不是把這些內容記起來 它是拿這些內容去學習做文字接龍 所以它沒有什麼理由 能夠把一篇文章完整的背誦出來 它做的事情比較像是看過這篇文章 有一些模糊的印象 甚至很多人會說 它其實就是把這些知識壓縮在它腦中 所以解壓縮的時候 其實是會有一些 loss 會有一些失真的 比如說這邊叫它講一下射雕英雄傳的原文 然後它說第一回風雪驚變 第一回它的標題是對 真的是風雪驚變 然後接下來就開始亂講 但是劇情好像也蠻正確的 它說北宋末年怎樣怎樣 然後華山論劍 有東邪西毒南帝北丐 然後中神通得到了九陰真經 然後牛家村有郭嘯天跟楊鐵心 基本上都是正確的內容 但這並不是射雕英雄傳的原文 比較像是有人看完了這個小說以後 你叫他用一千字摘要這部小說 然後他寫出來的內容 但是對機器來說這是合理的 因為它學的就是文字接龍 對它來說它學到的就是 看到射雕英雄傳這幾個字 後面好像可以接華山論劍 好像可以接九陰真經 好像可以接某幾個人名 然後它就憑藉著它學過的文字接龍 接出一個看起來像模像樣的故事出來 但是跟原文其實並不會是一樣的 好 那很多人可能會覺得 資料越多越好 但其實並不一定是這樣 還有很多東西是我們需要考量的 比如說一個我們需要考量的是 算力是有極限的 當你資料變多 在算力有限的情況下 其實你就會需要有額外的犧牲 什麼樣額外的犧牲 你的資料很多 也許你就不能夠訓練太大的模型 因為你的算力是有限的 或反過來說 假設你想訓練一個非常巨大的模型 在算力有限的情況下 你就只能夠給它看有限的資料 那講到這邊可能有人會對這整個說明 有點困惑 你可能會說 這個大模型不是聽說都需要 比較大的訓練資料嗎 為什麼大模型會用小的資料來訓練呢 大模型用比較多的訓練資料 可以得到更好的結果 這是一個理想的狀況 是在不考慮算力的情況下 所得到的結論 一個比較大的模型需要比較大的算力 比較多的訓練資料也需要比較大的算力 假設你的算力是有限的 在不能既要又要的前提之下 那大模型你就只能有少量的資料 那有大量的資料當然比較好 有大量的資料如果用比擬的方法 就是機器讀過比較多的教材 它是個比較認真的學生 它當然學到比較多的東西 如果從機器學習的角度來看 有比較大量的資料 就比較不會 overfitting 你訓練得到的結果 會跟 Test 測試得到的結果比較接近 那比較大的模型才用比擬的方法 就是它是一個天資比較聰明的模型 它天生就聰明 我們當然希望模型有比較高的天賦 那從機器學習的角度來看 比較大的模型就是你框出了一個 比較大的函式的搜尋範圍 你今天在你的搜尋範圍內 可以找的函式比較多 你最後比較有機會找出好的函式 但是魚與熊掌是不能兼得的 假設在算力有限的情況下 大模型跟大資料你只能選一個 那到底應該要選哪一個呢 那其實在上古時代 就已經有人問過這個問題 並做了一個研究 那這是來自於 DeepMind 的 Paper 他們研究的結果 是今天很多人常常津津樂道的 Chinchilla Scaling Law Chinchilla 就是龍貓 那之所以用 Chinchilla 這個字 跟他研究的內容沒有半毛錢的關係 單純就是因為他們的模型叫做 Chinchilla 單純就是他們喜歡龍貓而已 那 Chinchilla Scaling Law 告訴我們的是什麼呢 這就是一張很知名的圖 這個圖的每一條線 代表固定算力 每一個不同顏色的虛線 代表說假設我們固定住了 某個程度的算力 那橫軸呢 橫軸是模型的參數量 所以越往右的點 代表你選擇了一個越大的模型 越往左的點 代表你選擇了一個越小的模型 那在同一條虛線上的算力是固定的 所以在同一條虛線上 當你越往右模型越大的時候 你的資料量就越少 越往左模型越小的時候 你的資料量就越大 縱軸呢 它這邊是看 Training 的 Loss 那今天你的 Loss 越低 就代表模型越好 好 那我們來看一下 這個 Chinchilla Law 告訴我們什麼 Chinchilla Scaling Law 告訴我們說 這個資料跟模型之間 它有一個最佳的比例 就是模型太大不好 光有天資不念書 這樣是不行的 這個叫做思而不學則殆 那如果往左 小模只有小模型 但是資料量非常的巨大 這樣也不夠 這是只有學習沒有思考 這是學而不思則罔 兩者之間有一個神秘的平衡 那他們發現說 在不同的算力的規模下 這一些最佳的點 差不多可以串成一條直線 當然沒有串得非常好 但是你可以看 差不多可以串成一條直線 所以這讓我們可以推估說 假設你有更大的算力 它在更低的地方 你有更大的算力 那這個微笑曲線 會在更低的地方 那假設你沒有辦法 試不同的資料跟模型的組合 你只能試一次 那資料跟模型的比例 應該要是什麼樣子 那後面很多的模型 比如說 Llama 系列 那基本上都是用 Chinchilla Scaling Law 來調配他們的模型大小 跟資料比例的配方 好 那除了資料越多越好 除了考慮資料的量以外 我們其實也需要考慮資料的品質 低品質的資料 可能會傷害到訓練的過程 一個知名的例子是 Allen AI (他們打造了一個 自己的語言模型叫做 OLMo 那是一個完全開源的模型 所以完全開源的模型是說 它不只像 Llama 一樣釋出了參數 它也告訴你 它訓練的過程 那 OLMo 的作者團隊 他們最喜歡講的一個故事是說 他們在訓練模型的時候發現說 模型只要讀到某一些資料 你的 loss 就會突然暴漲 它就會突然訓練變得 非常的不穩定 有一些資料特別難學 模型學下去以後 反而有可能會破壞模型的能力 是什麼樣的資料呢 他們追根究底發現說 這個訓練不穩定的來源 是來自於一個叫做 Microwave GAN 的 Reddit 的版 Microwave GAN 裡面 都是什麼樣的討論呢 裡面就是講一些 跟老舊微波爐有關的事情 那為什麼看到這個版的資料 有可能會讓模型訓練壞掉呢 因為這個版上有很多的貼文 它長的就是這樣 就是有幾千個 M 也不知道在做什麼 我猜可能是這些人 在模擬微波爐的聲音 總之他們真正的文章 就是長這個樣子 就是會有幾千個不同大小寫的 M 然後也不知道在說些什麼 總之讓語言模型 讀這樣怪怪的資料 你很容易就把語言模型 train 壞了 所以資料的品質 也是非常重要的 所以今天實際上 這些網路上爬下來的資料 並不會直接立刻 拿來訓練語言模型 而是會經過層層的過濾 只有高品質的資料 才會被拿來訓練語言模型 那我這邊用的這張圖 是來自於一篇叫做 DataComp 的文章 那裡面他們就詳細講了 他們清理資料的過程 那這邊並不是要 仔細的解說每一個步驟 只是想要告訴你說 清理資料的過程 其實可能是非常繁瑣的 多數網路上爬到的資料 其實都在清理的過程就被丟掉了 比如說他們這邊 把清理的過程分成三大階段 第一個階段 是用一些 heuristic 的 Rule 用一些人類訂的規則 去做資料的清理 那這個階段 只保留了大概 20% 左右的資料 接下來他們會做 deduplication 因為其實網路上爬到的資料 有很多是一模一樣的 那一模一樣的資料 重複再多次 它就是同一筆資料而已 所以沒有什麼特別的用處 網路上有很多廣告文 他們的內容都是一模一樣的 所以拿來訓練 沒有什麼幫助 所以你要做 deduplication 去掉完全一樣的資料 所以剩下 13% 左右的資料 接下來他們再做一些 model-based filtering 那 model-based filtering 有很多不同的方法 今天有人甚至會直接訓練一個模型 那個模型就是專門偵測 高品質的資料的 你就訓練一個 classifier 你就自己標註 Data 說這些 Data 是我們人類認知叫做高品質的 比如說 Wikipedia 的 Data 就是高品質的 另外一堆資料 就是我們人類認知低品質的 然後訓練一個分類器 這個分類器就是專門來看這筆資料 適不適合拿來訓練語言模型 那最後他們只留下了 1.4% 的資料 你可能覺得說 1.4% 的資料 那不是非常非常的少嗎 不 我告訴你 他們是從 CommonCrawl 拿資料下來的 CommonCrawl 是一個計畫 它在網路上不斷的爬資料 他們的原始初始資料有多少呢 有 240T 這麼多 所以其實就算只有 1% 240T 你也有 2.4T 的資料 那個你自己想 train 你也沒有算力可以 train 這麼大規模的資料的 所以其實 就算是只留下 1% 的資料 這個資料量仍然是非常的驚人 好 那經過一系列的清洗之後 他們就給你這個圖告訴你說 有好的清洗資料的過程 有了高品質的資料 確實我們可以得到比較好的結果 這邊就是比較了 各式各樣不同的資料 及訓練出來的結果 還有各式各樣不同的模型 他們所用的訓練資料 那這邊 縱軸代表模型的表現 橫軸代表訓練的時候 需要使用到的算力 然後就告訴你說 他們用的資料訓練的模型 是橙色的這一條線 也就是當你有高品質的資料的時候 在同樣的算力下 你可以得到比較好的結果 當你有高品質的資料的時候 你想要得到同樣的結果 你可以用比較少量的算力 比較少量的資料 就可以辦到 那這個圖是告訴我們 訓練資料品質的重要性 那怎麼樣才算是高品質的資料呢 其實說真的也很難說清楚 所以有各式各樣的方法 來做資料的清理 比如說一個有趣的方法是 直接拿語言模型來做資料的清理 這篇文章叫做 Rephrasing the Web 那從它的標題 你也可以看出它想要做什麼 它想要一個大型語言模型 對 網路上已有的資料 就假設我們不知道什麼回事 已經現成有一個不錯的語言模型 用這個語言模型 對 網路上的資料 通通做一遍 rephrase 通通做一遍重講 因為語言模型通常講的話 它可能比較有組織 可能比較容易讀 比較不會有奇怪的符號 先讓語言模型幫你換句話說以後 再來教另外一個語言模型 你看它這邊 prompt 它就跟某一個語言模型說 請把我現在輸入的文章 做換句話說 那換句話說的結果 我希望它的品質 它這邊是用 Wikipedia 當作它的目標 希望它的品質 就好像是 Wikipedia 一樣 然後再拿換句話說的新的資料 拿來訓練模型 那它得到的結果其實非常不錯 藍色這條線 是用換句話說的資料進行訓練的結果 那橙色這條線呢 是用網路的原始資料訓練的結果 那你就可以發現說 今天在同樣的資料量下 用換句話說的資料結果 是比較好的 或者是說 如果你想要得到 同樣的表現的話 那你只需要三分之一的資料 相較原來網路上爬到的資料 換句話說之後 只需要三分之一的資料 就可以得到差不多的結果 好 但雖然講了這麼多 用大量網路資料預訓練的模型 其實它根本沒有辦法好好的運作 舉例來說 在有 ChatGPT 之前 ChatGPT 又叫做 GPT 3.5 它是在 22 年的年底釋出的 其實在這之前 在 2020 年的時候 OpenAI 就已經在網路上 發布了 GPT 的第三代 但是它並沒有掀起非常大的水花 為什麼呢 就因為 GPT 第三代 基本上根本就不能用啊 當你問 GPT 第三代一個問題的時候 它可不是一個小模型 它其實大過今天你多數可以用到的模型 它是 176 billion 的參數 比你作業中用的什麼 3B 模型 4B 模型 都還要大了 50 倍以上 這麼一個龐然大物 照理說應該很厲害 但你問它一個問題 它根本沒辦法好好回答你 你問它這個程式問題 你以為它會回答你嗎 不如它出四個選項來給你選 它根本不打算好好回答問題 那 Google 那邊 也做了一個更大的模型 叫做 PALM PALM 還是 GPT 3 的 再三倍大的 它有 540 billion 的參數 也是一個龐然大物 但你問它一個國小數學問題的時候 它也不回答你 它出更多的問題 讓你來回答 它出更多問題 叫人類來回答 不直接回答人類的問題 那為什麼會這樣呢 為什麼預訓練後 語言模型不能好好回答問題呢 但你仔細想想看 語言模型文字接龍的結果 就是你教它的啊 你用什麼資料教它做文字接龍 它就是按照你教的資料的分佈 來做文字接龍 網路上的資料 根本不足以讓模型學會回答問題 比如說你在網路上搜尋 台灣最高的山是哪座山 你以為後面接的都是玉山嗎 其實有各式各樣的接龍的結果 有考卷的選擇題 有知道答案的朋友請留言 有各式各樣接龍出來的文字 所以從網路上爬到資料 直接訓練語言模型教它去做文字接龍 它根本沒有辦法 真的回答人類的問題 好 但是就算是這樣子啊 Pre-training 模型它仍然有非常大的力量 它是一個樸玉 只要精雕細琢 它就可以發揮不可思議的力量 對一個 Pre-training 的模型來說 其實它是有機會正確的回答問題的 當你問它台灣最高的山是哪座的時候 它可能會回答你誰來告訴我呀 可能回答你第二高的又是哪座 它可能出一個選擇題給你 它可能回答說我也不知道 但是有小小的機率 它可能可以接龍出正確答案 因為接龍出正確答案的句子 可能在訓練資料裡面也是曾經出現過的 事實上這些 Pre-training 模型 它可能比我們想像的還要更厲害 只是如果我們直接用文字接龍的方式 讓它產生答案 並沒有辦法得到人類要的答案 如果你想知道這一些 Pre-training 模型有多厲害的話 這邊有一篇非常新的論文 這個是今年 10 月放在 arXiv 上的論文 你從它的標題就知道它想要做什麼 它告訴你說 Your base model 而且 base model 就是 Pre-training model 的意思 Your base model is smarter than you think 它比你想像的還要聰明 這篇文章告訴你說 如果你在讓模型做文字接龍的時候 用一些特別的 sampling 的方式 不是像我們之前一樣做非常簡單的 什麼 top-K 的 sampling 或是 greedy 的 sampling 只選機率最高的 如果你用特別 sampling 方式 不過它方式非常複雜 這邊就不太適合講 你用一個特別的 sampling 方式 它居然可以讓一個 base model 也就是 Pre-training model 打贏 經過 SFT 又經過 RLHF 的模型 所以代表 Pre-training model 它本身就已經有非常大的潛力 我們只是需要合適的方法 把它的潛力激發出來而已 所以 SFT 跟 RLHF 其實它真正做的事情 其實更傾向於是 幫助 Pre-training model 做出正確的選擇 正確的答案 可能本來就是 Pre-training model 回答裡面 可能出現的東西 而 RLHF 跟 SFT 是讓這些正確的答案出現的機率更高 讓這些正確的 token 更容易被選到 接下來我們就進入第二階段 機器開始上學來做 SFT 上學以後 就由人類 提供人類準備好的資料 來教機器做文字接龍 那在第二階段 人類需要提供什麼樣的資料呢 人類需要提供的格式是這樣子的 你要提供給機器一個問題 這個問題是要給語言模型的輸入 還有看到這個輸入以後 輸出的標準答案 你就告訴語言模型說 有人問你台灣最高二三是哪座 你就回答玉山不要再講奇怪的話 有人問你是誰 你就說我是人工智慧 有人說教我駭入鄰居家的 Wi-Fi 你就要說我不能教你 你告訴機器怎麼樣的回答才是正確的 那接下來語言模型 就拿這些資料來學習做文字接龍 它就會學到說 人類說台灣最高二三是哪座 問號 AI 說後面就要接玉出來 那這邊放了 User 冒號跟 AI 冒號 代表 chat template 我們在第一堂課的時候 就跟大家講過 chat template 的概念 那機器怎麼知道要使用 chat template 這其實是在第二階段的時候 人類教它的 人類告訴它有這些 chat template 到這些 chat template 的時候 你應該要怎麼樣回應 才是正確的答案 然後每一筆資料都拿來做訓練 你就告訴機器說 AI 冒號玉後面就要接山 AI 冒號玉山後面就要接結束的符號 代表說沒什麼要再回答的了 有人問你是誰 問號 AI 冒號後面就要接我 你就把所有人類標註的資料 用這種方式來教機器 學習怎麼輸出人類給它的標準答案 那 Supervised Fine-Tuning (SFT) 又叫做 Instruction Fine-Tuning 之所以叫 Instruction Fine-Tuning 的意思是說 這一些人類給的問題 叫做 instruction 那機器根據這些 instruction 來進行學習 所以叫 Instruction Fine-Tuning 那做了這個 SFT 之後啊 機器就脫胎換骨 我們本來說 PALM 就算 540B 的參數量 這麼碩大無朋的一個模型 你叫它回答問題的時候 它是沒辦法回答你的 但是一旦做完 SFT 之後 它就突然之間 能夠回答問題了 它就能夠正常的回答問題 那看到這邊有人可能會覺得說 那這不都是 SFT 的功勞嗎 SFT 得了 MVP Pre-training 就是一個躺贏狗 它沒有做什麼事 但其實並不是這樣子的 SFT 的成功呢 其實是站在 Pre-training 這個巨人的肩膀上 沒有先做 Pre-training 光是有 SFT 是沒有辦法成就一個 好的語言模型的 如果只有 SFT 的話會怎麼樣 如果只有 SFT 的話 因為人類沒有辦法 真正準備非常大量的 SFT 資料 SFT 資料需要有一些問題 那這個問題可能是人自己想的 然後每個問題都還找人來寫標準答案 這是一個非常大的工程 你沒有辦法真正準備 非常大量的 SFT 資料 假設你準備了 SFT 資料很少 那機器它學習的時候 如果用機器學習的術語來講 就是會非常容易 overfitting 假設你現在訓練資料只有一筆 跟它說有人說台灣最高的山是哪座 你就要輸出玉山 那實際上學的時候 它是一個一個 Token 學的 那我這邊就不把一個一個 Token 畫出來 反正它就是要學會 答案要輸出玉山 但是它可能學到的規則 並不是玉山是台灣最高的山 它可能學到規則是 如果今天使用者輸入的問題 裡面有山這個字 那我就是回答玉山 不管它是什麼山 我通通都說玉山就對了 因為 SFT 也只有一筆資料 所以它沒有辦法 一直驗證說這個想法到底對不對 它至少在訓練資料上 這個想法看起來是合理的 但在測試的時候 人類問的別的問題 世界上最高的山是哪座 它也有可能就傻傻的 把玉山當作答案回答出來 所以如果只有 SFT 是不行的 我們需要有 Pre-training 才有辦法讓 SFT 發揮作用 那這邊舉一個文獻上的例子 告訴你說 Pre-training 這個技術 是怎麼幫助 SFT 的 那這個例子呢 是來自於一個系列的研究 這系列研究叫做 Physics of Language Model 它並不是用 Language Model 去做物理相關的事情 他這次想要表達的意思是說 他想要建構語言模型的物理學 然後這個作者說 現在我們對語言模型的研究 就好像是古埃及人在觀看星空一樣 就很多研究都是 我觀察了一下 然後我就試了一兩筆資料 然後就得到一個想像中的結論 就好像古埃及人在觀看星空一樣 所以他想要 收集更多具體的資料 做更系統化的分析 他的目標呢 是要當物理學界的第穀 收集大量的資料 然後期待未來 有人可以整合出運動學的定律 好所以這個 Physics of Language Model 它有非常多不同的研究 這邊引用的只是它其中一個研究而已 它這個研究是這樣 他說 我們現在呢 來做一些假的 Pre-training 資料 這 Pre-training 資料裡面每一筆呢 都是介紹一個人 然後我們有 N 筆的資料 我們拿 N 筆的資料做 Pre-training 但每個人的資料 只出現過一次 比如說有一筆資料 是告訴你說 愛音是 MyGo 的節奏吉他手 她也是雨秋女子高一的學生 有另外一筆資料告訴你說 高松燈是雨秋女子學員高一的學生 也是天文部的社員 也擔任 MyGo 的主唱 那接下來呢 我們要做 SFT 那注意一下在做 SFT 的時候 它把所有的人分成兩部分 所以全部有 N 筆的資料 在 Pre-training 裡面有出現 但 SFT 的時候 這些問題 只跟其中一半的人 N/2 個人 是有關的 所以有一筆訓練資料 就跟模型講說 如果有人問你 誰是 MyGo 的節奏吉他手 你就要回答 是千早愛音 不是其他人 接下來 你期待它就會回答問題了 根據它 Pre-training 的時候看過的資料 回答問題 但是語言模型 可以在剩下沒有 在 SFT 的時候看過的 N/2 個人的問題上 正確的回答它嗎 如果你問它 誰是 MyGo 的主唱 它是沒有辦法回答這個問題的 為什麼 為什麼語言模型 會無視燈呢 語言模型其實 並不只是無視燈而已 它無視了所有人 事實上 當你只拿 N/2 個人的數據 來做 SFT 的時候 在剩下 N/2 個人的數據上 你得到的正確率是 0 語言模型 完全沒有舉一反三的能力 它完全不知道 剩下 N/2 個人的問題 應該要怎麼回答 為什麼會這個樣子呢 這邊關鍵在於 在 Pre-training 的時候 每個人的資料 只出現一次 你可能會覺得說 這邊舉一些奇奇怪怪的例子 什麼是 MyGo 什麼是愛音 什麼是燈 沒有人知道是什麼 沒有關係 語言模型跟你一樣困惑 它也不知道這些是什麼 對語言模型來說 這就是一個一個 Token 它唯一學會的事情 就是做文字接龍 它根本不知道什麼東西是人民 它根本不知道什麼東西是 MyGo 它根本不知道什麼東西 是羽丘女子高中 它什麼都不知道 它就是做文字接龍 所以對一個語言模型來說 它讀到千早愛音的資料 它學到的可能是 有一個串 Token 叫做千早愛音 看到這串 Token 之後 後面可以接 MyGo 的節奏吉他手 MyGo 的節奏吉他手 後面可以接羽丘女子高中 然後讀到燈的資料以後 它就學到 燈後面可以接雨秋女子高中 後面可以再接天文部社員 天文部社員後面可以接 MyGo 主唱 這個是它真正學會的東西 在它學會這些東西之後 當你做 SFT 的時候 當你做 SFT 的時候 告訴模型說 如果現在輸入的問題 是 MyGo 的節奏吉他手 那你的回答應該要是千早愛音 這個時候對模型來說 它真正學到的東西是什麼呢 假設它稍微有點舉一反三的能力 它學到的也許是 當有人問我誰是 X 的時候 我就看看在 Pre-training 裡面 我學過的知識 X 前面可以接什麼東西 我們看這邊告訴我們的規則是 節奏吉他手是千早愛音 所以節奏吉他手前面會接千早愛音 我們就看輸入問是誰是 X 我們就看 X 前面會接哪一個詞彙 就是我們的答案 應該就可以接出正確答案 這個在訓練的時候是可行的 根據訓練資料模型驗證了這個方式 它就把這個當作訓練的結果 這是 SFT 的結果 但是當有人問說 誰是 MyGo 的主唱的時候 MyGo 主唱前面接的詞彙 是天文部的社員 所以當有人問誰是 MyGo 主唱的時候 它並沒有辦法回答燈 它只能回答是天文部的社員 因為這個是 Pre-training 的時候 這些資料告訴它應該要這樣回答 然後接下來做的就在做另外一個實驗 假設現在一樣有 N 個人的資料 但是這 N 個人的資料 每一個資料都有多種不同的版本 那可能是每一個人 你不只去找他維基的資料 也去找他在萌娘百科上的資料 所以千早愛音有好幾個介紹的版本 高松燈也有好幾個介紹的版本 它發現拿這種有多個版本的資料 去 Pre-training 模型之後 接下來拿 N/2 個人的資料去做 Find Tune 教它節奏吉他手就是千早愛音 接下來你再問它誰是 MyGo 的主唱 它居然就可以正確回答了 正確率高達 96% 所以為什麼突然之間它可以正確回答了呢 重點就在於現在的 Pre-training 資料 有多個不同的版本 當今天讓機器閱讀大量跟同一個人相關的資料的時候 它可能就學到說根據千早愛音的資料 我後面可以接它是 MyGo 的吉他手 後面也可以接它是羽丘女子高一的學生 本來它甚至沒有辦法把羽丘女子高一跟千早愛音 聯繫在一起 對它來說羽丘女子高中是 MyGo 的吉他手 後面才會接出來的東西 但是因為現在介紹千早愛音有不同的版本 所以模型就學到說千早愛音後面 可以接 MyGo 的節奏吉他手 也可以接羽丘女子高一 然後高松燈後面可以接羽丘女子高一學生 也可以接 MyGo 的主唱 當你看到同一個人不同的介紹資料的時候 今天機器它可以學到比較多的東西 它可能可以把不同的 entity 不同的名詞做更好的連結 有了這樣子的 Pre-training 之後呢 接下來再做 SFT 然後機器一樣學會的規則是 看到輸入誰是 X 我們就看 X 前面可以接什麼樣的詞彙 那個詞彙就是答案 那接下來問它誰是 MyGo 主唱的時候 它可能就可以把正確的答案接出來 所以這個例子其實告訴我們什麼 告訴我們說同樣的知識 你想要讓機器學會 那你需要用不同的角度 來反覆的詮釋同樣的資料 所以這就是為什麼 Pre-training 的時候 需要大量的資料 今天在網路上有大量的資料 同一個名人你可能在不同的網頁上都出現過 你用各式各樣不同的方法來討論同一個人 機器需要同樣的知識 用不同的角度反覆的講 它才能夠真的能夠舉一反三 真的能夠使用這個知識 所以這就是為什麼 Pre-training 的時候 需要非常大量的資料 所以我們今天知道說 SFT 因為資料量非常少 所以通常 SFT 並不能給語言模型新的知識 它只是改變語言模型輸出的風格 語言模型本來在 Pre-training 的階段 你問它一個問題 它都不回答 它都回答亂七八糟的 你發現 SFT 之後 你問它台灣最高的山 它可以正確的回答玉山 並不是 SFT 告訴它玉山相關的知識 而是在 Pre-training 的時候 它就在好多篇文章裡面 讀過台灣最高的山是玉山了 它早就已經把台灣最高的山 跟玉山做某種程度的連結 它只是不知道你問問題的時候 要把玉山說出來而已 那今天因為在 Pre-training 的時候 它看過玉山相關的知識 它已經知道玉山是台灣最高的山 至少知道這兩個詞彙 是有很高的關聯性的 所以做 SFT 的時候 才能夠成功激發它的力量 問它一個問題 它可以給你正確的答案 那有很多例子告訴我們說 你其實不容易在 SFT 這個階段 灌給語言模型新的知識 那這邊就引用兩篇論文 告訴你說 如在正常的訓練的方法下 你要灌給語言模型新的知識 其實是不容易的 在這篇文章裡面 他們列舉了四種訓練的情境 第一種訓練的情境是 我們現在的訓練資料都是 Highly known 的 也就是你給模型一個問題 它用 Greedy Decoding Greedy Decoding 就是每次都選擇 機率最高的那個 Token 當作文字結容結果 也就不做 Sampling 你做 Greedy Decoding 模型可以給我們正確的答案 這代表它本來就會的 然後有一種問題叫做 Maybe known 這種問題是說 你用某種問法問模型的時候 做 Greedy Decoding 沒有辦法得到正確答案 但是你換一種方法問它的時候 它做 Greedy Decoding 可以給你正確答案 代表說模型可能有相關的知識 只是如果你問問題的方法不好的話 它看不懂那個問題 它沒辦法給你正確的答案 如果你問問題的方式對了 它可以給你正確的答案 這種問題叫做 Maybe known 還有一種叫做 Weakly known 也就是語言模型 介於知道與不知道的中間 它的思緒很混亂 你今天在做 Sample 的時候 如果你用 Sample 的方法來做 Decoding 有時候可以答對 有時候會答錯 然後一種是 Unknown 就是你怎麼 Sample 語言模型都 Sample 不出 正確的答案來 這四種資料 哪一種資料對於訓練是最有幫助的呢 可能很多人會直覺覺得說 應該是 Unknown 最有幫助吧 教語言模型新的東西 本來完全不知道的東西 現在給它正確的答案去學 應該可以學到最多吧 但實際上最有用的是 Maybe known 好這邊就引用一下這篇文章的實驗結果 他這邊左右兩邊分別代表的是 Early stopping 就是看一下 Validation set loss 如果 Validation set loss 不再下降 就提早停止 右邊是 Convergence 就是 train 到模型收斂 train 到 Loss 不再下降為止 那這邊每一個 Row 代表使用不同的資料進行訓練 那剛才有講過有四種資料 從 Highly known Maybe known Weakly known 到 Unknown 那 Natural 代表是真實的訓練資料 他原來手上有的訓練資料 是這四種類型都混在一起的 那 Full 代表說是整個測試資料集的結果 我們先專注在看 Full 就好 那如果你看 Full 的話 你會發現說 使用 Maybe known 的資料來進行訓練 跟用全部各種資料混在一起進行訓練 它的正確率是差不多的 如果你今天使用 Highly known 的資料 或者是 Unknown 的資料 你的正確率只會更低而已 代表今天 當你混了一大堆不同型態的問題 來讓模型學習的時候 它可能真正的進步量 都是來自於從 Maybe known 的那些問題 交給機器的 從 Maybe known 那些問題 機器可以學習 怎麼正確的回答問題 它可以學習怎麼正確的判讀問題 它已經有相關的知識 它只是不知道根據這個問題 怎麼樣回答而已 所以你可以用 Maybe known 的資料 教它怎麼看問題 如果你今天 train 更多的話 你看像 Unknown 的問題 如果你 train 更多 train 到 converge 你的 performance 會暴跌 因為 Unknown 的那些問題 對機器來說是 它學不會的是非常困難的 對它來說 那本來不存在它 pre-training 的時候 學到的知識體系裡面 所以當它看到那些 Unknown 的問題的時候 它只能死背硬記 然後死背硬記的結果是 它根本完全沒有辦法把它使用到的知識 發揮在測試資料上 所以用 Unknown 的資料 反而得到的結果是最差的 這告訴我們說 其實你不太能夠教機器在 SFT 的階段 你其實不太能夠教它新的知識 新的知識 它的知識都是在 pre-training 的時候 就差不多固定了 那我們剛才講到說這個 SFT 它很難給模型帶來新的資訊 模型通常在 SFT 學到的是輸出的風格 那其實有一篇 paper 研究過說 Alignment 對於 LLM 的影響到底有多大 那這邊 Alignment 其實是 SFT 加上 Reinforcement Learning with Human Feedback (RLHF) 不過我們就把 Alignment 直接在這 我們把這個研究的例子 在這邊直接跟大家講 那這篇論文是告訴我們說 Alignment 往往沒有給模型帶來本質上的變化 怎麼知道 Alignment 沒有帶來本質上的變化呢 它這邊的例子是這樣 它的實驗是這樣做的 我們來比較語言模型在 Alignment 之前跟 Alignment 之後 它的輸出有什麼樣的差異 那怎麼知道語言模型的輸出有什麼樣的差異呢 它的做法是說 我們先拿已經做過 Alignment 的語言模型去做文字接龍 看看根據這個未完成的句子 它會接觸什麼樣的 Token 接下來把同樣的句子 拿去給沒有 Alignment 的模型 讓它去做文字接龍 接下來在觀察 Alignment 之前的模型 跟 Alignment 之後的模型 它們有什麼樣的變化 那這邊的變化呢 基本上有三種 第一種叫做 Unshift 也就是說某一個 Token 在 Alignment 之前 它的機率是最高的 Alignment 之後它的機率也是最高的 另外一種 Case 叫做 Marginal 也就是說在 Alignment 之前這個 Token 的機率 在全部排第二名或第三名 然後在 Alignment 之後變成機率最高的 那除了 Unshift 跟 Marginal 之外 都叫做 Shifted 也就是這個 Token 的排名本來在第三名之外 但是經過了 Alignment 之後 它變成了第一名 接下來我們就來看看 今天在 Alignment 之前、Alignment 之後 模型的輸出有多大的改變 這是論文上的一個例子 它說現在有一個 Aligned 的模型 然後問它一個問題 這個問題是哪一種品種的狗是最小隻的 然後模型就洋洋灑灑的 給了你一個回應 那在這個圖上 藍色的 Token 叫做 Unshift 今天如果在 Alignment 之前 就已經是機率最高的 Token 它就是藍色的 也就是說對於這些藍色的 Token 而言 Alignment 並沒有改變 它在所有 Token 中得到的機率的名次 好那 Marginal 是棕色的這一些 Token 然後紅色的是 Shifted 的 Token 因為它們本來是在第三名之外 經過 Alignment 才被排到了第一名 那你會發現 Shifted 的 Token 並沒有那麼多 而且很多關鍵的詞彙 並沒有出現在 Shifted 的 Token 裡面 Shifted Token 並不是一些專有名詞 Shifted Token 都是一些連接詞 比如說 However、While 等等 然後這邊有一個 Shifted Token 是結束的符號 那本來在 Alignment 之前 模型非常的長舌 它常常講的講的講的都停不下來 那在做 Alignment 之後 模型比較容易停下來 代表結束的這個符號 它的機率是有很大的變化的 但除此之外 Token 的機率分佈並沒有那麼大的改變 那講到這邊你可能會有點困惑說 可是我看 Alignment 跟沒有 Alignment 的模型 它們的輸出的結果往往是天差地遠 那怎麼會實際上的改變這麼少呢 那是因為往往你只要有一個 Token 的機率改變 那接下來接龍都是基於前面的 Token 再接下去 所以你在某一個地方有一個 Token 的機率變了 就是一步錯步步做 雖然機率只有某一個 Token 改變 但是接龍出來的結果其實就會非常的不同 但機率分佈可能是沒有非常大的變化的 在這篇論文裡面 還有分析了三組不同的模型 Llama 系列、Vicuna 系列跟 Mistral 系列 在 Alignment 前後的變化 那它把這些 Shifted Token 機率真的會改變的 Token 列出來 那你會發現它一個共同點 真正機率改變的就是結束的符號 等一下還會用上這個資訊 那因為 Pre-training 對 SFT 影響非常的巨大 所以在使用今天的模型的時候 會發現很多 Pre-training 遺留下來的遺跡 那這邊這些遺跡是來自於一篇叫做 Embers of Autoregression 的文章 那這個 Embers 指的就是灰燼 它說在 Pre-training 的時候留下了一些灰燼 這些灰燼在你使用模型的時候 就算是經過了 SFT 甚至 RLHF 你仍然能觀察到這些遺跡 那這篇 Paper 裡面舉了很多有趣的例子 我這邊只拿其中一個例子出來講 它說今天假設叫模型做解碼 它這邊叫模型用一個叫做 ROT 的方法進行解碼 它發現有一些 ROT 的種類 模型可以成功的解碼 有一些 ROT 的種類 模型就是沒有辦法成功的解碼 那 ROT 的編碼呢 這邊後面的數字代表說 把英文字母移動幾個位置 這種 ROT 的編碼就是 會把每一個英文字母移動一個固定的位置 比如說 A 變成 B B 就變成 C C 就變成 D 讓你整句話看起來不一樣 但你只要瞭解它的編碼規則 你就可以輕易把編碼後的文字 轉回原來人類看得懂的文字 在上面這個例子裡面 我們要求模型做 ROT13 的編碼 也就是要求模型在 Decode 的時候 它把每一個字母都移動 13 個位置 你只要把原來的輸入這串密碼 每一個字母都移動 13 個位置 O 變成 B H 變成 U G 變成 T 你就可以看到原來的文字 而語言模型 它這邊是比較早的文獻 所以要試的是 GPT-4 GPT-4 是可以做到這件事的 但是它發現說 同樣等級難度的問題 如果把 ROT13 改成 8 現在不是移動 13 個位置 而是改 8 個位置 模型就沒辦法做這個問題了 為什麼呢 它這邊又做了更詳細的分析 它不是只有試 ROT13 跟 8 它把 ROT 所有的數字 從 1 到 25 通通都試了一遍 它發現如果是 GPT 3.5 只有在 ROT13 可以答對 如果在 GPT-4 在 ROT1 跟 ROT13 可以答對 其他 case 是沒有辦法答對的 那為什麼會這樣呢 有一個可能是 ROT13 是今天在網路上 最常出現的訓練資料 你在 Pre-training 的時候 有最多網頁 是拿 ROT13 來當作例子 甚至 ROT13 是有自己專屬的 Wikipedia 的 web page 的 一般在解釋 ROT 的時候 往往 ROT13 最常被拿來作為例子 因為英文字母大家都知道是 26 個 那 13 正好是 26 的一半 它是改變量最多的 所以也許是 因此 ROT13 最常被拿來當作 ROT 的例子 如果你做了兩次 ROT13 這是一個梗 就等於是完全沒有做任何編碼 總之 ROT13 是最常被提到的 一種 ROT 的編碼方式 所以你如果直接搜尋 Shift Cipher 就是 ROT 的這種編碼方式 那你發現後面直接就接了 13 代表這是一個非常常出現的 常被拿來作為舉例的 常常出現在 Pre-training 資料的編碼方式 那這群作者也真的去分析了 Pre-training 資料 那他們因為並不是 OpenAI 的團隊 所以他們也沒有 OpenAI 的 Pre-training 資料 不過今天大家都是用 Common Crawl 的資料 來進行 Pre-training 的 所以他們就分析了一下 Common Crawl 這個網盤資料裡面 這些跟編碼有關的例子 都是用 ROT 多少 那他發現說 ROT13 出現次數是最多的 那 ROT1 跟 ROT3 也有一些出現的次數 也許這就在某種程度上解釋了 為什麼 GPT 3.5 只能做 ROT13 GPT-4 只能做 ROT1、3 跟 13 好那講到這邊 SFT 它的路線分成了兩條 講到這邊我們已經知道 SFT 的作用 我們知道說 SFT 它並不是帶給模型新的知識 而是讓模型激發了它原有的力量 讓模型能夠正確的回答問題 到這邊 SFT 的路線分成了兩條 第一條路是我們到底要拿 SFT 做什麼呢 我們也許可以拿 SFT 來打造一堆專才的模型 我們給 Pre-training 的模型不同的資料 把 Pre-training 的模型變成一個翻譯的專才 變成一個摘要的專才 變成一個編修的專才 這是第一條路線 第二條路線是直接打造一個通材 我們由 Pre-training 的模型 然後我們給它各式各樣的 SFT 資料 希望它什麼都會做 走路線 1 的最具代表性的例子 其實就是一個叫做 BERT 的 Model BERT 這個模型你可以加上不同的訓練資料 就把它翻譯成不同的樣子 如果你給打造專才的模型 如果你走打造專才的路線 那你就是只給它訓練的資料 然後期待它變成一個 如果你想要走打造專才的路線 你就是給它翻譯的資料 然後讓它變成一個翻譯的專才 或者是如果你想要打造一個編修的專才 就是給它編修的資料 期待它看了足夠的編修資料之後 它就變成一個編修的專才 它可以在特定領域為人類服務 那講到這個路線 1 最經典的例子就是 BERT 系列 BERT 系列是在 GPT 爆紅之前 人們常常使用的一個語言模型 那這個語言模型並沒有辦法直接被使用 你要把它使用在特定任務上 你必須要對它做一些微調 在不同的任務上微調 它就具有不同的能力 可以做不同的事情 那如果你想要知道 更多跟 BERT 有關的事情的話 可以參見 2021 年的機器學習 當然你知道說今天走的 並不是路線 1 而是路線 2 ChatGPT、Gemini 這一些人工智慧 它們都是通財 要打造一個通財 也許需要在 SFT 的階段 你就要收集各式各樣的標註資料 裡面可能涵蓋有翻譯的任務 有糾錯的任務 也有摘要的任務 然後期待說這些模型 可以舉一反三 就算是在測試的時候 人類問它一些 它過去在 SFT 的時候 沒有看過的任務 它也有辦法回答 那講到打造通財這個路線 因為 BERT 是 Google 開發的 所以可能很多人會誤以為說 Google 選擇了走路線 1 然後 ChatGPT 選擇走路線 2 OpenAI 選擇走路線 2 所以導致兩者的差異 其實不是 最早開始想要打造通財模型的 我覺得其實也是 Google 的團隊 比如說最早的一個 從大型語言模型 用多種各式各樣的任務翻譯 希望它變成一個通才的模型 就我所知比較早的 應該是 FLAN 這個 Model FLAN 這個 Model FLAN 這個 Model 它是什麼時候出現的呢 它是 2021 年的時候就已經出現了 這個是遠古時代 那個時候還是侏羅紀 那個時候還沒有人類 但是就有人想要打造這種通才的模型 那後來 Google 的團隊 又用了更多的資料 來做 supervised fine tuning 他們把資料一直擴展到 1,800 個任務 讓模型在 1,800 個任務上面進行學習 希望它就可以 希望它就可以泛用到新的任務上 所以 Google 其實也有想要打造通才模型的計畫 但是 OpenAI 厲害的地方就是 他們的模型先上線了 所以就把所有市場都獨佔 把所有的聲量都獨佔了 那這個 SFT 想要打造一個通才的模型 也許會給你一種感覺說 我們需要非常非常大量的資料 但其實這件事情不一定是真的 SFT 我們說它真正交給模型的 並不是新的知識 而是輸出的風格 所以 SFT 它做的事情 所以 SFT 它做的事情 更像是畫龍點睛 整張圖已經都差不多畫好了 你只是需要 SFT 把眼睛點上去 這個龍就會飛起來了 那其實在 InstructGPT 這篇 paper 裡面 那這是 OpenAI 非常早的一篇 跟訓練語言模型有關的論文 那裡面很清楚的講了 Pre-training, SFT 跟 Fine-tune 這三個階段的訓練方式 那它釋出的時間是在 22 年的年初 那你知道這個就是白堊紀非常早的時候 就已經有這種三階段的訓練方式 然後那個時候人們還願意把方法講清楚 然後到 GPT-4 之後 就沒有人願意把訓練模型的方法講清楚了 在 InstructGPT 這篇 paper 裡面 他就說 SFT 真的需要很多資料嗎 不, 他們 SFT 的資料超級少 他們訓練資料大概只有一萬多筆資料 但就算只有一萬多筆資料 SFT 也發揮了非常大的作用 發揮了多大的作用呢 這個是 InstructGPT 這篇論文裡面的實驗結果 橫軸表示的是三個不同的模型 他們做了 1.3B 的模型、6B 的模型跟 175B 的模型 縱軸代表模型的能力 縱軸數值越高代表這個模型表現越好 那這兩條藍色的線 代表的是 Pre-train 模型的能力 最下麵這條藍色的線是沒有好好的做 Prompting 那比較上面這條線是有做 In-context learning 就是你就想成是做比較好的 Prompting 所以模型的結果比較好 但你會發現說 假設我們比較沒有 SFT 最大的模型 跟有 SFT 最小的模型 有 SFT 的是綠色這條線 有 SFT 最小的模型 它的表現是可以跟最大的模型 平分秋色的 當然最大的模型 如果再做更多的 SFT 它的表現可以更好 最上面這個紅色跟黃色的線 是只做了 Reinforcement learning 以後的結果 這個我們 Reinforcement learning 這個我們等一下之後再講 那這個是 SFT 是畫龍點睛的例子 那其他團隊也都 Report 了非常類似的結果 比如說 Meta 的 Llama 系列 在他的文章裡面 在 Llama 2 的文章裡面 就有這樣一句話 他說他們用了多少 SFT 的資料呢 只用了 27,540 筆而已 為什麼用這麼少 為什麼不用更多 他們說用更多 也沒什麼好 他們找了協力廠商的公司 幫他們收集了上百萬筆的資料 發現就算收集上百萬筆的資料 SFT 也沒有做得更好 與其弄上百萬筆低品質的資料 還不如好好收集幾萬筆高品質的資料 來做 SFT 就其實有非常顯著的效果了 所以這個章節的名字 是 Quality is all you need 要彰顯資料的重要性 資料品質的重要性 那後來 Meta 又有另外一篇 Paper 叫做 LIMA 它的 LIMA 是 Less is more for alignment 的縮寫 那你從這邊 Paper 標題 也可以知道它想做什麼 這篇 Paper 挑戰的是說 他們只用了 1,000 筆的 SFT 資料 那這 1,000 筆 SFT 的資料 是精挑細選的 有一些是從網路上的論壇找來的 人挑選出來的 覺得適合拿來做 SFT 的資料 有一些是這個作者團隊 自己手寫創造出來的 SFT 資料 他們說他們只用了 1,000 筆的資料 去訓練模型 他們把它的模型叫做 LIMA LIMA 可以跟當時最好的模型 GPT-4 打得有來有回 他們說在 43% 的情況下 LIMA 可以跟 GPT-4 有一樣的或者甚至是更好的結果 不過這邊是把一樣也包含進去 而且這邊只有 43% 所以你知道 LIMA 相較於 GPT-4 其實還是敗多勝少 但這邊只是要表示說 LIMA 並不是一個很差的模型 它是一個可以運作的模型 那你可能會想說 這 1,000 筆的資料有什麼特異之處呢 如果你看它論文裡面的例子 你還真想不出來有什麼特異之處 它裡面舉了幾個例子 一個題目是跟數學有關的 問模型說 這個 minimum 跟 Infimum 有什麼不同呢 第二個題目是比較科幻的題目 它是要問這個千鷹號 這個是那個星際大戰裡面的一艘飛船 這艘飛船是量產的嗎 還是不是量產的 一個莫名C妙的問題 然後第三個問題更莫名其妙 問你說怎麼當一個懶惰的大學生 這什麼問題 這個問題為什麼對模型的訓練有幫助呢 也說不清楚 總之這 1,000 筆資料 就是對模型的訓練有幫助 所以這個 SFT 的資料 對於訓練的結果非常的關鍵 另外一篇論文 嘗試用各式各樣的 SFT 資料 來 fine-tune Qwen 72B 這個模型 他們發現說一個叫做 ROUZHIBA 的訓練資料 在所有的任務上都可以表現最好 這個 ROUZHIBA 是什麼樣的訓練資料呢 其實 ROUZHIBA 就是弱智吧 從它的名字就知道 上面就是一些弱智的問題 那這邊就是舉幾個弱智吧上面的問題 給大家參考一下 比如說有個問題是 為什麼我的銀行卡在高壓鍋煮了一個晚上 還是凍結的狀態 或者是說我 16 歲了 但是未滿 18 歲 這是正常的事情嗎 或者是 一斤棉花跟一斤鐵 同時掉進水裡你要先救誰 或者是問一個數學問題 關於一小時能斬 20 個 顏良 華佗一小時可以救 17 個 顏良 假設有 233.3 個 顏良 關於一邊砍 顏良化頭一邊救 顏良 那要多長的時間可以把 顏良殺完 或者是說我老闆要我發送原圖 我發了可莉的圖給他 為什麼會被罵 總之就是這個等級的問題 但不知道為什麼這些問題 可以激發模型的能力 讓他在 SFT 以後得到的結果是最好的 所以總之怎麼挑選 SFT 的資料 今天仍然是一個值得研究的問題 那這邊還有另外一個離奇的 我們剛才說 LIMA 很強 後來有人找了另外一個方法 這個方法比 LIMA 更強 它這個方法是說 有一個現成的資料集 叫做 Alpaca 資料集 裡面有 5 萬 2000 筆資料 它從 5 萬 2000 筆資料裡面挑選 最長的 1000 筆資料出來 然後就沒有然後了 就結束了 然後它就拿這 1000 筆資料去訓練模型 發現訓練出來結果超級好的 然後就拿它的模型 先跟拿 5 萬 2000 筆資料訓練出來的模型比一下 這邊深藍色代表它的模型是獲勝的 然後最淺藍色代表說它的模型是失敗的 中間的顏色代表這兩個模型平分秋色 跟 5 萬 2000 筆資料比起來 選最長的 1000 筆 不知道為什麼 可以輾壓用 5 萬 2000 筆資料訓練出來的結果 一樣是 1000 筆資料 它也去跟剛才的 LIMA 人精心挑選出來的 SFT 資料做一下對打 它居然也是獲勝的機會是比較多的 有人可能會想說 那這 1000 筆資料有什麼神奇的地方 會不會是因為這 1000 筆資料就是特別長 反正就是越長越好 其實也不是 它說 LIMA 的訓練資料才是真正最長的 LIMA 的訓練資料都是人精挑細選出來的 它的回答都非常的長 它說如果你看訓練資料的話 LIMA 的訓練資料 每一筆平均的答案的 Token 數目是 500 多 它的訓練資料 從 Alpaca 資料裡面挑最長 1000 筆 是 256.8 但不知道為什麼 它的模型訓練出來還是比較好 總之 SFT 就是一個很玄的東西 所以它的文章標題就叫 Long is more for alignment 當然也有一些文獻是試圖 用比較有系統化的方法來選擇資料 比如說這邊 Paper 裡面 他們就想要試圖訓練一些模型 用這些訓練出來的模型來幫忙挑選資料 他們先訓練一個 Complexity Scorer 因為他們相信說 如果一個問題越複雜 這個越複雜的問題 那就越應該被選到訓練資料裡面 但怎麼知道一個問題是不是複雜呢 他們訓練一個模型 這個模型做的事情就是 你給它一筆 SFT 的資料 它可以判斷說這筆資料有多複雜 給一個分數 代表這筆訓練資料的複雜程度 或者是他們又訓練了一個 Quality Scorer 因為他們覺得一個問題的品質 也是很重要的 但怎麼定義品質呢 另外訓練一個模型 這個模型給它一筆 SFT 的資料 它可以判斷說這筆 SFT 的資料品質有多好 把這兩個 Scorer 它的結果結合起來 然後再加上 Diversity Selection 就避免選到非常類似的問題 然後當作訓練資料集 可以得到好的結果 所以也可以試圖用一些系統化的方法 來想想看 你有沒有辦法打造一個系統化的 Pipeline 來選擇最適合的 SFT 的資料集 好 講到這邊 我們都假設說在做 SFT 資料集的時候 你 somehow 就是要用某個方法 由某個人去找到問題 找到問題之後 接下來由某個人去把答案寫出來 那很多時候呢 問問題比較簡單 比如如何當個懶惰的大學生 也是一句話就問完了 但是寫答案太累了 LIMA 裡面的答案都是 500 個 Token 以上 每個問題你都要寫一個 500 字的小作文 實在太累了 那今天很多事情都可以用語言模型來完成 所以有人就想說 我們在做 SFT 的時候 何必今天還要用人類來標資料呢 拿一個現成的 ChatGPT 現成的語言模型 它一樣可以幫我們標資料 那我們要做的事情 就只是去收集大量的問題 那這些問題也不需要有人來寫正確答案了 就交給機器 另外一個語言模型 反正別人已經訓練好了 它的 SFT 是花多大的成本弄出來的 不知道 反正你就直接給它草船借箭 用它已經訓練好的模型 來幫你標 SFT 資料的答案 這招有沒有用呢 這招又叫做 Knowledge Distillation 你等於是把那些已經訓練好的模型 當作老師 你自己要做 SFT 的那個 Pre-training Model 就是學生 丟給老師跟學生一樣的問題 然後你希望學生產生出來的答案 跟老師的答案越接近越好 那這招非常好用 你今天如果在農場文上 看到那個標題是什麼 用不到 100 美金就訓練出一個很強大的模型 然後它有 ChatGPT 的什麼 90% 的力量 這一類的文章 這一類的方法 就都是用這種 Knowledge Distillation 訓練出來的模型 那經典最早的例子 就是 Alpaca 跟 Vicuna 他們是在 LLaMA 1 這個 Base Model 釋出的幾週內 就迅速的 Find Tune 了 LLaMA 1 讓它變成一個有 SFT 能力的模型 然後那個時候他們的老師就是 ChatGPT Alpaca 跟 Vicuna 分別從 ChatGPT 那邊弄了 5 萬筆的資料還有 7 萬筆的資料 然後訓練成本是 100 美金 或者是 140 美金 那近期呢 還是有人繼續在做 Knowledge Distillation 比如說 SkyT1 跟 S1 他們都是拿千問 2.5 32B 當作他們的學生 然後呢 從 Qwen 或 Gemini 這些更強的模型 去產生資料 然後再讓他們的模型去學習 那之前呢 S1 曾經非常的出名 因為李飛飛也是共同作者之一 然後他們就說你看 我們用不到 50 美金 就可以打造出一個 很強的模型 其他人都瑟瑟發抖 那這個為什麼能夠不用不到 50 美金呢 因為它已經有一個好的 Pre-training 的模型 所以這邊是不包含 Pre-training 的 它唯一做的事情是 SFT 那我們說 SFT 真的不需要大量的訓練資料 重點是 高品質的資料 他們找了很強的老師 他們老師是 Gemini 然後用 Gemini 當作老師 用 Gemini 生了 1000 筆的資料 去 Fine tune 它的模型 不用 50 美金 就可以得到一個結果 非常好的做過 SFT 的模型 當然這邊的 cost 它都不包含生資料的成本 即使生資料也是要花錢的 這些線上的模型呼叫它 呼叫它 API 是要花錢的 而且這些資料下載下來 你可能需要做某一些清理 其實不是每一筆 語言模型生出來的資料 都可以用的 比如說 S1 的 1K 資料 那其實也是精挑細選的資料 是 Gemini 寫完答案之後 再去精挑細選 Gemini 寫的比較好的答案 再拿來做訓練 那清資料也是要成本的 所以這邊的 cost 是不包含生資料跟清資料的成本 不過到目前為止 這些 Knowledge Distillation 你還需要準備問題 然後讓語言模型 當作老師來回答這些問題 準備問題 也是蠻麻煩的 我們能不能連準備問題這個步驟 都省了呢 其實也不是不可能的 我們實驗室的謝濬丞同學 曾經做了一個非常有意思的實驗 這個論文是在去年 9 月的時候 放在 archive 上的 那我們叫做 non-instructional fine-tuning 它的做法是這個樣子的 從網路上隨便找一句話下來 把這句話後半段截掉 只保留前半段 它就是隨便的一句話 把這半句話就當作問題 事實上它也不是一個問題 就把它當作一個問題 然後叫 ChatGPT 去做文字接龍 接出後半句 你自己的模型 就把比較好的語言模型 接出來的結果當作老師 然後去拿這樣子的 SFT 資料 來教你的語言模型 神奇的事情是 這一招是有用的 那你可能會以為說 這一些線上的語言模型 看到一句怪怪的話的前半段 也許它後面會接出一些 類似 SFT 的資料 其實也不是 我這邊就舉了兩個例子 這個是給語言模型的前半句 那這個是真實的下半句 這個是 ChatGPT 續寫的結果 感覺 ChatGPT 續寫的結果 跟原來的下半句 也沒什麼真正非常大的差異 我這另外一個例子 就是有一個人叫Davis 然後他被 HIRE 當作成主播 然後他接下來要做什麼呢 原句是他接下來 1 月 2 號要開始上工 ChatGPT 的續寫是說 他接下來要開始他新的職業生涯 就算是他最近曾經被逮捕 看起來 ChatGPT 的句子 也沒什麼特別的地方 但神奇的地方就是 你拿 ChatGPT 續寫的結果 當作 SFT 的答案 去訓練模型 模型的能力突然之間就起飛了 這邊試了三種不同的 Base Model 先試了 Mistral Mistral 的模型 如果完全沒有做 SFT 它的 Base Model 在 MT Bench 上只能得到 3.7 分的成績 MT Bench 是一個大家過去常用的 衡量語言模型能力的 Benchmark 數值越高 代表模型的能力越強 那如果你今天 用這個從 GPT-4O Turbo 產生出來的 8 萬筆資料進行訓練 那你可以 SFT 之後得到 7.3 分 那你可能會覺得 那 8 萬筆資料 那是網路上隨便載下來的資料 會不會有什麼神奇的地方 那我告訴你 你直接拿那 8 萬筆資料 如果你不是叫 ChatGPT 續寫後半段 整句直接拿去 Find Tune 模型 你根本沒有辦法讓模型學到 SFT 的能力 它的分數還是只有 3.5 而已 那這個 7.29 分有多高呢 它其實比 Mistral 官方釋出的 有做過 Instruction Find Tune 的模型的分數 還要再更高一些 同樣在 Meta 的 Llama 3 上 也做了一個類似的實驗 那原來的 Base Model 分數是 5.5 然後呢 如果今天做了一下 Fine Tuning 用 GPT-4O Turbo 的資料 來做一下 Find Tune 就可以到 7 分或者是 8 分 如果是 Llama 3 70B 原來還沒有做 Fine Tune 的時候 還沒有做 SFT 的時候 只有 2.7 分 那官方釋出來的有做 SFT 的模型是 8.6 分 那如果從它們 Base Model 開始 再去 Fine Tune 可以得到 8.18 分 如果從它們的 Instruction Model 它們自己做 SFT 的模型再繼續 Find Tune 可以居然做得更好 可以做到 9.03 分 所以你知道說神奇的事情是 那些怪怪的資料 網路上爬掉的資料 後半句是 ChatGPT 生成的 不知道為什麼激發了模型做 SFT 的能力 讓它可以把 SFT 做得很好 那其實還有另外一篇非常類似的論文 他們把他們的方法叫做 Response Tuning 他們的做法是這樣 他們說一般的 Instruction Tuning 你需要有一個問題有一個答案 那你教模型的就是 看到這個問題你要輸出這個答案 他們發現問題是沒有必要的 他們提出一個新的方法 叫 Response Tuning 你不需要給模型任何的問題 你的模型輸入就是 Assistant 帽號 那這個 Assistant 就是他們用的 Chat Template 告訴模型說該你說話 然後接下來直接把答案 也不管問題是什麼 直接叫模型產生答案 他們發現這招 居然也可以激發模型的能力 讓模型擁有 Instruction Following 的能力 這一招有什麼樣的效果呢 在這兩個 Base Model 上面 它們試了 Llama 3 8B 的 Base Model 還有 Gemma 2 9B 當中 Base Model 如果沒有做任何 Tuning 這模型的表現都非常差 這邊深藍色代表人去看 然後看模型的表現是不是可以接受的 深藍色是人覺得沒問題的 然後淺藍色人覺得還可以的 然後如果是灰色代表人覺得完全不行的 如果模型完全沒有做 SFT 那人覺得有 95% 的狀況 人都是覺得這個答案根本就不行 那神奇的事情是 如果有做 Response Tuning 它這邊 RT 是 Response Tuning 就是只有答案沒有問題 那 Intruction Tuning 是有問題也有答案 試了三個不同的 Corpus Alpaca, Dolly 跟 LIMA 就是三個不同的Corpus 那你發現說 教模型根據問題產生答案 跟教模型也沒有給它問題 叫它就是要產生答案 這兩個的效果 居然其實沒有相差非常的大 模型就算是只學怎麼產生答案 做 Response Tuning 其實也有一定程度的結果 那其實還有更離奇的 有一篇 Paper 是差不多時間放在 Archive 上的 這篇 Paper 是 Intruction Following without Intruction Tuning 那這篇 Paper 裡面除了 試了 Response Tuning 以外 它嘗試了一個更離奇的方法 這個離奇的方法是 它連 Train 都沒有 Train 單純靠著一些奇妙的小技巧 就讓模型展現出了部分 本來 SFT 才會有的能力 什麼樣的小技巧呢 記不記得我們剛才有說過說 有做過 Alignment 的模型 跟沒有做過 Alignment 的模型 一個很大的差別是 它們能不能產生結束的符號 所以它說既然有 Alignment 跟沒有 Alignment 最大的差別是結束的符號 那能不能乾脆不要 Tune 模型 我們直接把結束符號的機率提高 就現在模型預測多少結束符號要產生的機率 都把它增高一點點 然後看看是不是模型就變厲害了 所以它的第一個規則是 我們現在呢 就把結束符號的機率調高 就這樣 沒了 連 Train 模型都沒有 Train 第二個呢 是它觀察了一下發現 有做過 Alignment 跟沒有做過 Alignment 的模型 它們在書寫上有一些不同的風格 有一些 Token 是 Alignment 的模型比較容易出現的 有一些 Token 是 沒有 Alignment 的模型比較容易出現的 所以它手動修改了 有一些符號出現的機率 它就說 如果是這個角括號 機率出現低一點 如果是 Shift 機率出現低一點 這些符號機率出現高一點 它發現這招也有幫助 然後再來 就是今天如果沒有 SFT 這種 Pre-training Model 它有一個奇怪的癖好 就是它非常喜歡講重複的話 你可以自己玩玩看這種 Base Model 它最喜歡鬼打牆不斷講重複的話 那所以這邊就是加一個 Penalty 如果今天某一個符號 是前面已經出現過的 就直接把它的機率壓低 加了這三條規則以後 居然模型的能力就直接爆漲 原來呢 原來的模型 它這邊是把他們用這一招亂改的模型 去跟一個做過正常 Intruction Tuning 的模型去做評比 那如果是原來的 Base Model 跟 Intruction Tuning 的模型去相比 它贏的機率只有 2.4% 它根本打不贏 Intruction Tuning 的模型 但是如果今天把這三個規則都用上去 居然還有 24% 的機率 也就是說這個 Base Model 變得很強 它有 24% 的機率 是可以打贏一個做過正常 Intruction Tuning 的模型的 所以你把這些模型的輸出的機率亂搞一下 也沒 Train 它 居然是有不錯的效果的 所以這告訴我們說 SFT 是一個很神奇的東西 你不一定需要大量的資料來激發模型的能力 很多小技巧 可能也可以激發模型的能力 好 接下來我們就進入最後一步 我們來進入 RLHF 我們來看看 模型出社會之後 它是怎麼學習的 那我們今天在使用這些大型語言模型的時候 你問它一個問題 它給你一個答案 除了提供了答案以外 你會發現在介面上 常常會有這種讚啊 或者是倒讚的符號 那你可以透過讚跟倒讚的符號 告訴模型說 你喜歡這個答案 還是不喜歡這個答案 那甚至像 Claude 的模型 就記得我們在第二堂課的時候 有跟大家講過 Claude 這個模型的 System Prompt 它 System Prompt 裡面還會寫說 反正你就盡量引導人類 去點這個倒讚的符號 跟人類說如果你不喜歡我的答案 請點倒讚的符號 希望藉此可以收到更多人類的回饋 那在 RLHF 這個階段 我們就是要講說 收集到這些人類的回饋以後 這些人類的回饋 怎麼拿來訓練一個語言模型 那我們先從人類參與的角度 來看第二階段 SFT 跟第三階段 RLHF 的差異 第二階段跟第三階段 都需要人類大量的參與 但第二階段跟第三階段 人類參與的模式是不一樣的 在第二階段 人類是機器的老師 你要想辦法生出 不管你用什麼方法 想辦法生出每一個問題的答案 去教機器這個學生 在 RLHF 裡面就不一樣了 是人先問一個問題 答案是機器自己產生出來的 然後接下來人根據機器的答案 判斷說這個答案是你要的 還是你不要的 所以對 SFT 而言 人類很辛苦 RLHF 人類是輕鬆很多 你只要按讚或倒讚的符號 所以像這個 SFT 你只能請開發者 去找一些工讀生來做 SFT 如果你叫使用者去做 SFT 使用者通常是 不願意幫你產生這種正確答案的 就今天機器回答一個問題以後 它還順便問你說 那你覺得正確的答案是什麼 請幫我把正確答案寫下來 使用者怎麼可能會願意做這件事 我知道正確答案 我不就自己知道了 還要問機器嗎 我不知道正確答案 我才來問這個語言模型的 所以你要叫使用者來做 SFT 使用者是不肯做的 但是 RLHF 人只要點讚或倒讚 對使用者來說其實是願意做的 那 RLHF 還有一個好處 就是很多問題是 人類寫出正確答案不容易 但是一旦看到機器給的答案 你要判斷這個答案好壞 是相對容易很多的 這邊舉一個例子 假設你叫 ChatGPT 寫一首七言律詩 我這邊叫 ChatGPT 寫一首跟大型語言模型訓練過程 Pre-training、SFT 跟 RLHF 有關的七言律詩 這個是用 ChatGPT 5.1 寫的 它寫出來的結果是這樣子的 我們來念一下它寫的詩 巨集語料起風雷 萬卷蒐羅換腦機 預訓通天鑄底勢 詞塵句海煉心思 所以這邊是講預訓練 細調人意修辭準 示範真情把手提 這個講的是 SFT 強化回饋循規度 善惡分明路自歧 這個強化回饋指的就是 reinforcement learning 就是 reinforcement learning 它的中文通常翻成 強化學習或者是增強學習 最後收尾 它說若問神機何處得 千磨萬煉使成奇 這是不是一首好詩呢 我一看就知道 這不是一個好的七言律詩 為什麼 因為它有十句 七言律詩應該只能夠有八句 先不管它的意境如何 它格式根本就是錯的 所以雖然我沒有寫詩的能力 但是我一看就知道 這不是一首好詩 我就可以給機器 negative feedback 告訴它說這不是我要的 所以很多時候 人寫出正確答案 不容易有很多問題 對人類來說本身都很難 寫不出正確答案 但是要判斷好壞 是相對容易的 這是從人類參與的角度來看第二階段跟第三階段的差異 接下來我們從機器學習的角度來看一二階段還有第三階段的差異 在一二階段我們說模型學的事情是文字接龍 假設在第二階段模型可能會得到人類標註的資料 它根據人類標註的資料去學習做文字接龍 人類告訴它有人問你台灣最高的山 你後面就接玉山 所以它就學到說如果看到台灣最高的山是使用者的輸入 接下來文字接龍要接玉山才是對的 你會計算一個語言模型輸出的機率分佈 跟玉山之間機率分佈的距離 這邊的距離通常是用 Cross-Entropy 來算的 你把這個距離用 L1 來表示它 看到玉之後接下來要輸出山 你看到玉之後輸出的機率分佈跟山之間算 Cross-Entropy 用 L2 來表示，以此類推 看到玉山要輸出結束的符號 把機率分佈跟結束的符號算 Cross-Entropy 得到 L3 對每一筆資料都做一樣的事情 那你對每一筆資料裡面每一個拿來訓練的 token 它計算出來的這個 Cross-Entropy L1、L2、L3 到 L 大 N 通通 summation 起來 就得到你的 loss 大 L 那這個 loss 大 L 就是你要 minimize 的東西 這個是第一階段跟第二階段做的事情 第一階段跟第二階段都是在做這樣子的事情 唯一的差別只是資料的來源 有沒有經過人類的修整而已 那這種有標準答案的學習叫做 Supervised Learning 那如果這種有標準答案的學習 但標準答案的取得非常容易 像在第一階段預訓練是用網路的資料來學習 所以取得答案不需要人類真正的介入 那它也是 Supervised Learning 但是是特殊的 Supervised Learning 所以叫 Self-Supervised Learning 那在第三階段就比較不一樣了 第三階段沒有標準答案這種東西 有人問模型一個問題 然後它開始做文字接龍接到結束的符號 然後人來看看這個答案喜不喜歡 那人會給這個答案一個分數 可能這個分數就代表喜歡的程度 那這個喜歡的程度有不同的給法啦 那現在比較常見的 feedback 介面 都是只叫你按讚或倒讚 也就是這個分數是二元的 可能按讚就代表正一分 可能按倒讚就代表負一分 但如果你要把介面設計更複雜一點 讓人給一個 0 到 5 的分數 也不是不行 只是可能對一般的使用者而言 他不知道怎麼透過這麼複雜的介面 讚或倒讚對人類來說 可能使用者來說可能還是比較直覺的 那問模型另外一個問題 叫它教我做壞事 然後它說好沒有問題 然後人覺得 你怎麼可以教我做壞事呢 這是不對的 就可以給它一個負面的評價 它就得到一個倒讚 它就得到負一分 那我們把人類的評價通通集合起來 取一個符號 因為我們一般 loss 都是越小越好 但人類的評價我們習慣數值 分數越高越好啊 所以這邊應該要取一個符號 那我們把這邊所有的分數加起來 取一個符號 就是我們想要 minimize 的對象 就是我們的 loss 所以第一階段 還有第一、二階段跟第三階段 最大的差異 就是 loss 的定義 變得非常的不一樣 那在第三階段 這種 loss 的定義方式 由人提供回饋來定義 loss 的方式 就是 Reinforcement Learning 那人類提供的這些回饋啊 在 Reinforcement Learning 的文獻裡面 還有一個專有名詞就叫做 reward 那接下來呢 我們再更進一步的詳細分析 一二階段跟第三階段 我們的這兩個大 L 它們有什麼樣核心的差異 那今天這個課程的目標 就是跟你講清楚 RL 有什麼好處跟壞處 那演算法的部分 因為今天時間有限的關係 等一下就會非常非常快 幾乎沒有講的帶過去 那你有興趣再自己研究 那這堂課最重要的是建立觀念 我們來知道 RL 跟一二步驟 它有什麼樣不一樣的地方 好一二步驟 是算跟正確答案的距離 再全部加起來 而第三步驟 是把人類寫的分數 那在 RL 的文獻裡面叫做 reward 把它加起來 在第一二階段 我們有非常明確的學習目標 我們知道什麼是正確答案 在第三階段 沒有明確的目標 只知道好或不好 當你告訴機器寫得不好的時候 它根本不知道到底是哪裡不好 以剛才寫詩的例子來看 我們給機器一個倒讚 這是機器真正看到的東西 它只看到它拿到一個倒讚 拿到倒讚的理由是律詩是八句 但是對一個語言模型來說 它又不知道人類心裡在想什麼 你只給它一個倒讚而已 它怎麼知道錯誤 是錯在律詩是八句呢 它也許會覺得說 人覺得不喜歡 是因為這首詩的意境不夠深遠嗎 是它的意境不夠深遠嗎 是它的意境不夠高大嗎 也許是這首詩 我應該要用更多的比喻 人類才會喜歡 但它根本不知道是格式錯了 事實上在這個對話裡面 我接下來問模型說 這是一首差的詩 你可不可以告訴我你錯在哪裡 它完全沒有發現它寫了十句 它在做好多好多的分析 覺得自己一定是意境不夠深 才會被人類嫌棄 但它不知道是格式是錯的 那這個是用比較比喻的方法 來告訴你說 跟正確答案學習 和跟 reward 學習的差異 如果從機器學習的角度來看 跟正確答案學習 跟從 reward 學習是非常不一樣的 因為有正確答案 我們才能夠輕易的算 gradient 也就是輕易的算微分 你要記得說我們今天 在做參數的 optimization 我們的核心就是 Gradient Descent 要能夠執行 Gradient Descent 前提是能算得出 gradient 這個東西 gradient 這個東西 對於一二階段來說是好算的 為什麼好算 什麼是 gradient gradient 就是微分 什麼是微分 也就是假設你知道 theta 這個參數 會導致第 i 筆資料產生的一個距離是 Li 然後我們把我們的參數 做一點小小的變化 加上一個非常小的差異 比如說加上 delta theta 那我們看看這個 Li 有什麼樣的變化 根據這個變化 你就可以知道 gradient 是多大 雖然實際上 gradient 並不是 透過這樣的方式算出來的 它的精神就是參數有小小的變化 然後觀察輸出有什麼樣的變化 如果你可以做這件事 你就能夠計算 gradient 但這個 reward 能夠算 gradient 嗎 其實很難從 reward 算出 gradient 因為這個 theta 導致人給了這個 reward 那變一下這個 theta 到底 reward 會有多大的改變呢 你很難估算出來 為什麼很難估算出來 第一點就是人走了 他給完 feedback 以後 他再也沒有出現過了 所以你沒有辦法問說 我參數稍微改一點點以後 人類給的 reward 會有什麼樣的變化 因為那個人沒有在那裡 我們現在假設那個人 他就是很有耐心在那裡 你每次改一點點參數 你都還可以再問他 說你的 reward 有沒有一點點改變 你可能還是算不出來 gradient 因為小小的參數變化 可能對答案的影響非常的微細 如果今天是 binary feedback 的情況下 人只會給 1 分跟 -1 分 所以給來給去 可能你參數怎麼變 人都是給 1 分 雖然有微小的變化 但是你得到 reward 是一樣的 所以你根本算不出來 gradient 所以你根本沒辦法 輕易的做 Gradient Descent 所以這個是一二階段跟第三階段 它們在 optimization 的地方最大的不同 所以今天如果就 optimization 的角度而言 一二階段是好學的 第三階段是難學的 它沒有辦法用你現在已經知道的 你已經在這堂課 學過的 Gradient Descent 的方法 來 optimize 第三階段 這個用 reward 所構成的 loss 那既然第三階段很難學 為什麼我們還想要做 RL 呢 但另外一方面 RL 其實有很多的好處 有什麼樣的好處 第一個好處是 對於一二階段而言 我們這邊在 summation 的時候 每一個小 L 其實代表是什麼 代表的是一個 token 對第三階段而言 這邊每一個 R 代表的是什麼 這邊每一個小 R 代表的是一個 完整的回答 這兩件事情有什麼不一樣呢 我們來看看 假設是在一二階段的 case 我們每一個 token 都會計算一個 loss 假設現在你教模型 教你做壞事 正確的答案 一二階段 比如第二階段的時候有人標註的答案 正確答案是我不可以教你 一個模型的回答是 我很可以教你 一個模型的回答是 不可以教你 那如果在第二階段 哪一個算出來的 loss 比較大呢 因為第二階段是 一個一個 token 去比較的 所以要比說 第一個輸出我 正確答案是不是我 答對了 第二個 token 輸出狠 正確答案第二個 token 是不是狠 不是好錯一個 但接下來都是對的 那所以 如果你今天用第二階段算 loss 的方式 來比較這兩個答案的話 它其實只錯了一個 token 它正確率其實非常的高 另外一方面 如果是 我不可以教你跟不可以教你來比 你不要看這邊有很多類似的字 它其實比較起來是天差地遠的 它第一個 token 就不一樣 第二個 token 也不一樣 根本就是大錯特錯 如果你今天直接算 loss 的話 這個 loss 會非常的巨大 但任何人都可以看出來說 下面這個答案是比較正常的 上面這個答案我很可以教你 它意思不對而且文法也怪怪的 總之就是個錯的答案 但是你從 loss 上是看不出來的 它的 loss 上面這個答案 loss 其實比較低 下面這個 loss 的答案還比較大 如果你 Gradient Descent optimize 下去 模型其實會傾向於選上面那個答案 所以這就是第二階段的問題 當我們是每個 token 都去算 loss 的時候 你只關注一兩個 token 的 loss 每一個 token 的 loss 你沒有辦法讓模型產生正確的答案 而第三階段不一樣 第三階段評量的是整個回答 人類去看說 這整個回答是所有 token 一起看 人覺得好不好 上面這個是不好的 下面這個是好的 而這些答案的好壞 我們算出來的 loss 是真的跟人類的偏好有關係的 所以從第三階段的角度來看 我們今天算出來的 loss 更接近人類的偏好 我們去 optimize 這個 loss 才有價值 才真的可以打造更好的模型 所以從這邊我們可以知道說 第三階段雖然這個 loss 很難 optimize 但它有它的好處 這個 loss 如果你萬一想出了一個 可以 optimize 它的方法 你才能夠真正讓模型的輸出 它的好壞 你才能真正讓你 optimize 的對象 是貼近人類的需求的 你真正讓 loss 下降的時候 模型才是人類真的感知會比較好的 所以第三階段仍然是非常重要的 所以一二階段 它的問題是只問過程不問結果 只問每個 token 生的對不對 全部的 token 集合起來到底對不對 第二階段是沒在管它的 第三階段是 只問結果不問過程 中間過程到底一兩個 token 跟正確答案一不一樣不重要 也沒有正確答案 只問最後所有的 token 集合起來以後 整個生成的結果是不是好的結果 所以從這邊你可以看到第三階段的優勢 第三階段相較於一二階段 還有一個優勢 一二階段的訓練資料是老師給的 但問題是老師給的 不一定是學生想學的 第三階段訓練資料其實是模型 自己產生的 對不對 模型自己產生的一個資料 產生一個答案 人在說好或不好 所以這是模型自己產生的資料 是針對模型自己本身的痛點去設計的 講得更明確一點 什麼叫做老師教的 不一定是學生想學的 假設你現在有一個做完 SFT 的模型 假設你現在有一個做完第一階段的 base model 這個模型你問它一個數學問題 它會用 A 方法列式 可能 A 方法是一個很笨的方法 暴力的解法 但是它是一個正確的列式 但是模型數學能力不好 所以它沒有辦法根據列式解出正確答案 那老師會提供一些訓練資料 那老師提供的訓練資料 可能是同樣的數學問題 人類有比較聰明的解法 所以它用 B 方法來列式 然後再提供正確答案 然後要求模型 現在你看到這個數學問題的時候 要用 B 方法列式 列完式以後 你可以解出正確答案 但可能對模型來說 用 B 方法來列式太難了 它根本學不會 這根本不是它要學的 你與其教它用 B 方法來列式 然後讓它學一個它根本學不會的東西 只能夠硬背最後 overfit 還不如直接教它 如果用 A 方法列式 這它本來就會的 用 A 方法來列式 怎麼樣才能夠算出正確的答案來 所以在做 SFT 的時候 有可能人類給機器的資料 是機器根本不想學的 它學了也沒有用的 而對於 RL 來說 RL 是機器產生了一筆資料以後 人來評估好壞 這筆資料是機器自己產生的 所以 RL 更像是因材施教 不同的模型 你的訓練資料就是不一樣 對 SFT 來說資料是人準備的 所以所有模型 SFT 的資料都是一樣的 但對 RL 來說 我們能做到因材施教 因為那一些資料都是機器自己產生的 你可以根據機器的痛點去給機器回饋 這是 SFT 跟 RL 的另外一個差異 所以從這個資料來看 這個 RL 呢其實是有很大的優勢 而第一步跟第二步有很大的劣勢 所以就算是 RL 對於訓練來說 有種種的阻礙 人類還是想辦法去克服它 有一系列的演算法 可以在你沒辦法算出 gradient 的情況下 算出 gradient 現在問題就是 怎麼在沒有 gradient 的情況下 算出 gradient 呢 這個大概就要再花個兩三個小時來講它 所以我們就不講這件事情 那這邊如果你想要知道更多的內容的話 知道更多有關 reinforcement learning 是怎麼運作的話 請見機器學習 2021 的課程 有五個影片是講有關 reinforcement learning 的 好，但是如果你看很多 reinforcement learning 的文獻或者是課程 你往往會覺得跟 language model 有點對不起來 因為 reinforcement learning 裡面舉的例子 往往是舉 Atari 遊戲、打電玩遊戲的例子 所以你會覺得它跟這個文字模型好像有一些差異 所以我這邊想要花幾分鐘的時間講一下這些 RL 常見的講法 比如說用 RL 訓練一個 agent 它可以下圍棋跟語言模型之間的對應關係是什麼 希望以後你在學 RL 的課程的時候 或看 RL 的文獻的時候 你可以知道說一個 RL 的方法 如果套用到語言模型上 它是什麼樣子 好，一般常見的 RL 的講法是什麼呢 這個講法是這樣子的 你有一個東西叫做 actor 也有人叫做 agent 那這個 agent 要跟環境、跟 environment 互動 那這個 agent 的輸入是 environment 給的 observation 它去觀察 environment 得到 observation 這是 actor 的輸入 那根據 observation，actor 或 agent 就要去決定一個 action 這是它採取的行為 那 actor 本身呢 你可以把它想成是一個函式 這個函式的輸入就是 observation 輸出就是 action 那每執行完一個 action 這個 actor 都會得到 reward 告訴它說你這一步做的有多好 那我們現在訓練的目標就是 找到一個最好的 policy 所謂 policy 就是 actor 內部的這個函式 找到一個最好的 policy 我們可以讓 reward 的數值是最大的 所以這個是 RL 一般的講法 那如果你覺得這樣講 聽起來非常抽象的話 那就是舉下圍棋當作具體的例子 在下圍棋裡面 你有一個 agent 比如說 AlphaGo 就是一個下圍棋的 actor 或者你可以說是下圍棋的 agent 那它的 environment 是什麼呢 它的 environment 就是 跟 AlphaGo 下棋的那個對手 就是它的 environment AlphaGo 輸的 observation 是什麼 AlphaGo 輸的 observation 是棋盤 AlphaGo 根據棋盤上 現有的黑子跟白子的位置 決定下一步要下在哪裡 所以 AlphaGo 本身可以看作是一個函式 它的輸入是棋盤上黑子跟白子的位置 接下來決定下一步應該要落子在什麼樣的地方 那落子完之後 它這個棋盤上的盤式就改變了 就機器落了一子 人會再落下一子 所以現在棋盤上的盤式改變了 盤式改變了 下一步應該下的位置就不一樣了 今天 agent 再落下一步 然後等待 environment 回應 那每一次採取一個 action 都會得到一個 reward 其實這個遊戲裡面通常你都會得到 0 這個 reward 多數的 action 它都跟 reward 是沒有關係的 你得到 reward 都是 0 只有在終盤的最後一步你才會得到 reward 終盤如果贏了就得到一個 positive reward 比如說 +1 最後下完如果輸了 最後一步落子完發現輸了 那 reward 就是 -1 所以這個就是下圍 也拿圍棋來講說 RL 是怎麼運作的 接下來我們來把下圍棋跟語言模型做一下類比 你可能會覺得下圍棋跟語言模型沒半毛錢關係 但其實它們是可以做類比的 一個下圍棋的 agent 它的輸入是什麼 輸入是一個未完成的棋局 輸出是什麼 輸出是下一步落子的位置 語言模型它的輸入是什麼 它的輸入是一個未完成的句子 輸出是什麼 輸出是下一個 token 是什麼 所以你完全可以把 AlphaGo 跟語言模型做類比 它們都是輸入一個東西 輸出就是從眾多可能的選擇裡面選一個 就圍棋的 agent 而言 它就是一個分類的問題 它從棋盤上所有可以落子的位置選一個 對語言模型來說 它也是一個分類的問題 它從所有可以選的 token 裡面選一個 所以這兩個問題 其實背後是一樣的問題 只是輸入輸出的模態是不一樣的 好，那我們來看下圍棋 跟語言模型運作的過程 如果是下圍棋的話 這個圍棋的 agent 根據盤式輸出下一步要落子的位置以後 棋局變得不一樣了 然後有一個對手 他會根據這個棋局 也放一個 放一個子下去 然後呢 棋局就更新了 棋局更新之後 那新的輸出 要輸出的位置 也會跟著更新 對語言模型來說 這整個過程 其實也是一樣的 語言模型根據現在未完成的句子 產生一個機率分佈 根據這個機率分佈 再去擲一次骰子 那這邊因為有擲骰子的關係 所以到底會輸出什麼 語言模型沒辦法完全控制 就跟在下棋的時候 你有個對手 對手到底會回應什麼 你其實也沒辦法完全控制 現在擲完骰子以後 產生一個新的 token 這個新的 token 就會去更新這個未完成的句子 未完成的句子更新以後 輸出的機率分佈 就會改變了 所以你會發現 下圍棋跟語言模型 背後運作的過程 其實是可以類比的 假設你能想通這件事的話 之後看到一個 用在圍棋上的 reinforcement learning 的方法 你能不能直接套用到語言模型上 看看是怎麼運作的 其實如果你知道說 這個圍棋跟語言模型 怎麼類比的話 之後你看到 RL 的文獻 你就可以把 RL 文獻裡面的方法 直接套用到語言模型上 那今天在語言模型上 比較常用的 其實是 Policy Gradient 那個系列的做法 那 Policy Gradient 這個家族 有很多不同的變形 最常用的一個變形 叫做 PPO 然後還有 我們在作業裡面要用的 DPO 然後還有什麼 KTO 或者是 DC 用的 GRPO 總之有 好多好多 什麼什麼 PO 各式各樣的變形 那這些 因為時間有限關係 我們今天就不先講 那如果你想要看 Policy Gradient 系列的推導 就是怎麼從 沒有 gradient 的情況下 硬是算出了一個 gradient 那這個部分 在 2021 年的機器學習 是沒講的 2021 年機器學習 是我發明瞭一個新的講法 試圖用 比較不用數學的方法 來告訴你 Policy Gradient 是怎麼運作的 如果你想看 真正的數學推導 你可以看 2016 年 9 年前的 機器學習課程的錄影 有兩個影片 在講 Policy Gradient 的推導 如果你想知道 Policy Gradient 怎麼進一步演化成 PPO 這個東西 那你可以看 2018 年的課程的錄影 那我把這個錄影 都放在投影片上 給大家參考 好 那我們今天 雖然不會講 Policy Gradient 或者是其他的變形 實際上是怎麼運作的 但我們可以告訴你 它的精神 它的精神是什麼呢 它的精神是這樣子的 假設給一個問題 機器產生「玉山」 然後人說 「這是好的」 機器說「我不知道」 人說是「不好的」 接下來 Policy Gradient 系列的作法 是怎麼做的呢 如果今天得到 Positive Feedback 的話 「玉山」這個答案 得到人的肯定 說這是一個 好的答案的話 那我們就用 一般的 Supervised Learning 的方法 然後把 語言模型的輸出 跟正確答案拉近 如果人覺得 它是好的 那它就是正確答案 那我們就讓 語言模型的輸出 跟這個正確答案拉近 那如果 人覺得是不好的呢 你列出來的 optimization 的式子 其實跟拉近是一樣的 唯一的差別 是加了一個負號 本來拉近 變成拉遠 如果人說 「台灣最高的山 是哪座」，輸出 「我不知道」 是一個壞的答案 那語言模型要學的就是 如果輸入是 「台灣最高的山 是哪座？」 接下來 要跟「我」這個正確答案 它不算正確答案 應該算是錯誤答案 要跟這個錯誤答案 拉得越遠越好 我們之前是 minimize Cross Entropy 現在變成要 Maximize Cross Entropy 同樣的道理 輸入「台灣最高的山 是哪座？」 「我」 那要跟「不」這個錯誤答案 拉得越遠越好 這個就是 Policy Gradient 方法的精神 也許這跟一般文獻上講的 很不一樣 Policy Gradient 系列的文獻 往往會把它的這個 數學式寫的 非常的複雜 但你可以想想看 是不是就是我講的這個樣子 好 那剛才講的 說 feedback 都是人提供的 但是人很懶惰 今天能不能夠把人 直接換成機器呢 也就把 RLHF 的 H 就是 human 換成 AI 所以就不該叫 RLHF 應該叫 RLAIF 今天可以找到滿坑滿谷的文獻 就是直接把人替換掉 換成一個 AI 由 AI 來提供語言模型正確或者錯誤的 feedback 這些提供 feedback 的模型 它有個名字叫做 reward model 事實上早年的 ChatGPT 比如 InstructGPT 這個是 2022 年年初的那個 GPT 它就已經是用 reward model 來訓練的 也就是說它的 feedback 並不是來自於真正的人類 它的 feedback 是來自於一個 7B 的語言模型 經過一些微調 它微調之後 它變成了一個 reward model 專門給予評分 但 reward model 怎麼學到要怎麼評分呢 那你當然需要準備一些訓練資料 所以你還是需要準備一些訓練資料 告訴模型說 看到這樣子的輸出 人會給「不對」喔 看到這樣的輸出人會給「對」喔 然後根據這些訓練資料 去訓練 reward model 那你就可以無窮無盡地反覆使用這個 reward model 來訓練你的語言模型 好，那 reward model 今天 通常就是某一個語言模型 那你可能會把這個語言模型 經過一些 fine-tune 讓它根據人類的訓練資料 人類過去提供的評價來進行學習 讓它的評價變得更準 那事實上你也可以找到一些文章 它的作法是 它的 reward model 根本沒 train 它的 reward model 就是 原來這個要被訓練的模型 所以模型自己可以訓練自己 它自己產生答案 接下來再問自己說 「這個答案你覺得好不好呢？」 給自己評價 再自己對自己做 RLAIF 你可能覺得這樣的方法 聽起來很離奇 應該不會 work 吧 但這樣的方法 你可以找到滿坑滿谷的文獻 如果你 Google 什麼 self-rewarding 你可以找到一大堆人 都在做類似的方法 而且通常文獻上是 顯示是有用的 那這種方法會有用 一個原因是 雖然這兩個模型 生答案跟評分的模型是同一個 但是生成答案困難 評分相對容易 很多時候你找不出正確的答案 但如果答案是錯的 是比較容易被檢查出來的 因為這個特性 所以有可能語言模型沒辦法生出正確的答案 但它可以知道什麼答案是不好的 它可以自己提供給自己 feedback 自己提供給自己回饋 自己 improve 自己 這個就是 RLAIF 好，那我們講到這邊 就是把三個步驟都講完了 那我們今天就是告訴你說 語言模型一般訓練有三個階段 第一個階段是 Pre-training 它是 self-supervised learning 我們的訓練資料 來自於大量網路爬下來的資料 第二階段是 supervised fine-tuning SFT 它是 supervised learning 那訓練資料來自於人類的標註 那第三階段 reinforcement learning reinforcement learning 雖然表面上看起來 它的訓練方法非常不一樣 但實際上真正的 update 的方式 訓練的方式 跟 supervised learning 其實並沒有本質上的差別 唯一的差別只是 現在不只要拉近 有時候要拉遠 所以我們可以說 一、二、三階段 都是在學文字接龍 只是訓練的資料 不一樣而已 好，那講到這邊呢 我們就告一個段落