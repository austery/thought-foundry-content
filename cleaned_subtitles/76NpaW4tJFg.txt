Technology doesn't exist in a vacuum. It's constantly shaping us and we are constantly shaping it. And with that comes a mix of huge possibilities and some real risks. Stephanie Dick thinks about this often. She is an assistant professor in the school of communication at Simon Fraser University. She studies the history of mathematics, computing, and artificial intelligence. I asked her what the past can teach us about the moment we are in right now. the kinds of questions we should be asking about where to go next.
>> Stephanie, great to have you on the rundown. How are you today?
>> Hi, I'm so well. Thanks so much for having me. It's great to be with you.
>> Yeah, I'm excited to learn about AI and and its history. So, let's delve into it. There's a lot of talk these days of the risks of AI, but on the flip side, a lot of hype about what uh how industries can benefit. I'm hoping you can help us explain. Has there ever been a time in the history that of tech advances that have been so binary?
>> I would say especially where the history of information technology is concerned, we have these moments of fear and excitement fairly regularly. And part of the reason for it, I think, is that information technologies come with really big promises about how we're going to learn and how we're going to work. But they also often come with really big infrastructural changes. So I have my students often read a little letter that went into the New York Times in 1857 about how sure the telegraph is going to allow us to communicate a lot faster. We're laying these cables across the ocean that are going to reduce the time for communication. But what is it going to do to our sense of truth, our sense of time, our sense of connection? Even the telegraph had people excited about economic revolution and it had other people really concerned about the impact of speed and quantity on information landscapes and human experience. So I think we have a lot of historical precedent for the excitement and concern that we're all hearing and experiencing today.
>> You mentioned the printing press. Are there other earlier experiences with technology that can help us navigate the advancement of AI and what we've can learn um from those early lessons?
>> Absolutely. Here I think the printing press is such a helpful historical example because we often imagine I think and we hear a lot of stories that tell us that it's technology that drives social change that we develop technologies and because of what they do or the way they work they force certain modes of being on us that we have to reckon with. And an example of this that is often pulled up is the printing press which comes from early modern Europe. And the story often told is that along came this genius Johannes Gutenberg. And he invented the printing press. And with the printing press, the printed word became faster and cheaper and there was more information that circulated more widely. And as a result of that, information was democratized and that led to the overthrow of monarchies and the democratization of Europe and the Protestant Reformation and the scientific revolution. We've attributed a huge amount of cultural transformation to the printing press. But it turns out that historically that story is just not true. Every part of it is false.
>> What is the truth?
>> Johannes Gutenberg didn't invent the printing press. He was a metal worker who improved movable type. And the printing press itself did make, you know, print cheaper and faster, but only for a very small amount of the population who were able to read. And it wasn't actually until almost 400 years after the printing press that more than 50% of Europeans could read. And I think that the actual revolution that led to the democratization of information and profound transformations in our society was actually public education as much or more than the printing press. And I think we're in a really similar moment right now where a lot of people are saying that AI is going to do all of these things. But I keep asking myself, what's the public education part of the moment that we're currently in? What's the social and political work and the pedagogical work we need to do in order to align this new technology with the kind of society we all maybe want to live in? Well, let's talk about those leaps and bounds because it seems like AI has has come so far in a very short period of time. Some people will even say it feels like a sci-fi film. I'm not going to lie, the amount of conversations I have with people who say it feels like an episode of Black Mirror. That that is seems to be a a constant conversation I have. In what ways might technology be going too fast?
>> Yeah, it's a really good question. And with the history of computing, one of the things we notice again and again is that the people who develop computer technologies often first figure out what they can do, what they can get this [clears throat] technology to do, and then they figure out how to sell that to us as the solution to our problems. It hasn't been going the other direction for some time where we articulate the problems that we want solved as particular communities or academic disciplines or organizations and then technology is designed in response to how we articulate our own problems. The computer is leading right now where we're sounding out its capacities and then we're supposed to all follow those. And I think it's worth mentioning that the current AI that we have, generative AI powered by machine learning models, large language models, and these other datadriven forms of behavior are just one iteration of what's been called artificial intelligence. And in the 20th century, the whole point of AI was to try to automate human intelligence. M
>> they wanted to be able to articulate what it is that our intelligence involves and reproduce that in the computer. So is our intelligence about reasoning? Is it about knowledge? Is it about intuition? What are those faculties? How might we automate them? And it turned out our own intelligence was too hard to automate. Those projects all failed to make good on their promises. And so this current iteration that we have was sort of born out of a moment where we said, "Okay, well, we don't know what our intelligence is like. We can't model it as rules for the computer to follow. So, we're just going to throw data at the computer and let it find its own rules. Let it determine its own behavior based on what it finds there." Uh, and so we're kind of redefining intelligence as we go through this history. It comes to mean really different things to different people. And what I fear in this moment is we're going to redefine intelligence all together to just mean machine intelligence, datadriven intelligence, and we'll lose some of the other forms that have been so essential to our development and our collective life.
>> I want to talk a little bit about some concerns that have been uh hitting the headlines uh with technology like chat GBT. Uh there's the issue of syncopency. Uh basically telling the user that anything that they want to hear is probably the easiest way to sort of define it and uh rightfully so has raised some concerns and want to know are we stuck with that or is there something that we can can we work around that?
>> It's such an important question and one that I'm really struggling with too while I navigate AI myself. So the first thing I think it's really important to acknowledge is that the sycopancy of Chad GBT is a business model or it is a part of a business model in which technology companies are trying to capture our time and attention as much as possible and they're trying to motivate our data producing engagement and behavior as much as possible. So even before AI, companies like Meta and Facebook and Google and you know uh Microsoft and X have been trying to figure out how to get us to stay on their platforms as long as possible, how to engage with as much content as possible, how to produce more data as much as possible. So having these AIs tell us that we're great all the time, tell us we're the next best thing is yet another mechanism for keeping us on the sites. And it really concerns me because at this historical moment socially, I think we are losing some of the skills that are required to navigate difference and conflict. We're struggling, students are struggling, teachers are struggling with how to navigate critical feedback. People are very sensitive. Conflict is really difficult. And then when you introduce a technology that just tells people they're awesome all the time, it really reinforces both the capture of our attention and the erosion of some of our interpersonal skills. And definitely AI need not be like that. the personality if we want to call it that uh of sick fancy I think is a part of this business model of attention capture but that it's not an inherent feature of datadriven chat bots we could develop them to behave really differently as well but there would have to be a business model that fueled those alternatives I think
>> when you mentioned the the business model and and sort of the business playbook of you know getting engagement getting the data getting us to uh interact with these AIs There's probably a lot of red flags for a lot of people who are like, "Oh, okay. I do not want to do any of that." And I'm sort of uh wondering what what will it take to get a sort of high rate of adoption of AI when there are some concerns even when you when you're mentioning that it's like okay yes when you see the business model in the back end you know there might be some hesitation but there are some benefits that will outweigh some of that.
>> Absolutely. I mean, I will never be a person who says there isn't extraordinary power and potential in our data. And it's a a really interesting moment because across academic disciplines, business sectors, organizations, facets in society, everyone is asking a shared set of questions right now. And I'm really excited about that. Namely, what are the data that we have? What kinds of insights might that data afford that aren't otherwise available? And how can we create new kinds of data, ask different questions of our data? And in so far as that's a project that AI invites us all to participate in. I'm very excited. I also think there are rightfully deep and profound questions about trust and accountability. Uh we know that artificial intelligence fabricates and constructs or hallucinates fairly regularly. It lacks contextual information. It uh has bizarre moral reasoning in a lot of contexts. So I think trust is going to be a really significant issue and it's wide reaching. There's the question of whether we can trust this technology and under when what conditions whether we trust people who are using this technology and under what conditions and perhaps most importantly do we trust the people who are developing this technology and under what conditions and so I think that widespread adoption is going to come from a lot of trust negotiations but right now people are afraid and they're angry I think in part no one in a certain sense sort of asked for this And now all of a sudden everyone is being told we have to reckon with this technology in our own spaces. So those are are all issues that we're going to have to do a lot of social negotiation around. You had mentioned uh the people developing these technologies and I want us to sort of go back in time and I am curious would the 20th century uh technologists be surprised um at how these applications that they were working on in the early days and see how they're being applied to now because we know uh AI really became a field of study in the 50s. you had sort of mentioned that but seeing how it is being applied now from what it what they thought in the early days would they be uh shocked?
>> I don't actually think so. Um, I've been studying history of AI for a long time since before AI was something that people were talking about meaningfully in the world. And one of the dominant themes of the history is this overwhelming sense of inevitability among technologists and I think also among a broader public too. We seem to have this sense that these things are inevitable. They're going to happen. we better do them first before somebody else does them or there's no possibility that we aren't going to have these technologies. I even have and I think this is an important comparison especially where the environmental repercussions of artificial intelligence are concerned. I have one historical uh thinker who talked about the manifest destiny he called it of computing that eventually everyone would have a computer that was intelligent by any definition available to them at all times and that it was a kind of manifest destiny uh in the same sense that American colonization was imagined to be both morally necessary and inevitable. So that same inevitability has been assigned to artificial intelligence in the narratives that surround the technology for a long time. I think many of them however would be surprised to know that the approach to AI that ended up generating success was the artificial neural networkdriven datadriven paradigm that really didn't look as promising in the 1950s and60s when these conversations started. Stephanie, we have about 30 seconds left and I am curious. I I have to imagine that you're quite excited for what the future looks like here. Uh so with that being said, what's next for AI?
>> What I'm the most excited about with AI is that it has got us all asking these deep existential questions about who we are, what kind of work we want to do, how we want to live, where meaning in our lives come from. And I'm not necessarily excited about the technology as much as I am about the way that it's forcing us into these deep conversations about how we want to live. And I think they are overdue.
>> Stephanie, we're going to leave it there. Thank you so much, Stephanie Dick, assistant professor in the school of communications at Simon Frasier University. Thank you so much.
>> Thanks so much for having me.