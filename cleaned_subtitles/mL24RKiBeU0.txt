AI bubble is already deflating and there are signs and sometimes the most powerful one is not in the stock market. It's in the words full self-driving full self-driving supervised. Can you tell the difference? It's almost the same phrase but entirely different products. That single word tells you everything you need to know about how tech bubbles are born and why we always believe them. And the question is, if the signs were always there, why did we fall for it? How did investors fall for it? And the answer isn't about tech at all. It's because that's what people do. There is so much noise about AI bubble right now. whether you believe it or not, whether it'll burst. But the question that kept bothering me is why? Why do they form? We've seen the dot. We've seen 2008. Why are we repeating the same mistakes? Today isn't about predicting or speculating when or whether it'll burst. It's about understanding why tech bubbles are born in the first place. Let's dive in. You might have noticed how the industry began shifting from autonomous agents, metaverse to co-pilots, virtual assistants, AI companion. The linguistics of AI have really shifted lately. But remember how just two years ago, Meta has aggressively pushed the narrative of metaverse as its flagship direction. And remember the videos with Mark Zuckerberg wearing the glasses? Those videos were everywhere. The famous Facebook became Meta and they invested billions into reality labs, VR sets, avatars, virtual ecosystems. This was the first time since the company's inception when Facebook did a fullscale rebranding and all of it happened because of AI. But fast forward to today and the whole meta narrative is slowly becoming a thing of the past. The current vibe is personal super intelligence. You can smell the effect of Agentic AI theme. The interesting thing is that they still invest and produce hardware. You must have seen the AI enabled glasses that they launched recently. The glasses and a lot of AI hardware is their thing now, but the metaverse language is being slowly swept under the rug because super intelligence took over the entire branding. And this linguistic rebranding, if I may, can be spotted in other industries as well. For example, robo taxi market and especially the giant of the American automotive industry, General Motors. And within the GM, they have a division called Cruz. Cruz's primary focus is autonomous vehicles and robo taxi services. Three years ago, the original positioning was autonomous robo taxis. The promise and the narrative was fully driverless level four cars in San Francisco and other major cities. Cruis's PR investor decks press releases all described as a breakthrough in widely available uncrrewed ondemand robo taxi service. But then two years ago in October 2023, the cruise division got in trouble because of the multiple incidents. There was even one when a taxi was dragging a pedestrian after a collision. And as you would imagine, this was followed by a major backlash. Cruz's permits got suspended, public safety concerns, and the whole narrative immediately shifted to operation suspended, investigation ongoing, and root cause analysis. You may ask, but how did Whimo pull it off? Then Cruz and Whimo both fought for dominance in autonomous driverless taxi business, but there was a chasm in branding, PR, and most importantly, operational decisions. Cruz aggressively marketed as a pioneer of fully driverless. Level four, autonomous robo taxi. Just in case you're unfamiliar with this whole leveling thing. Level zero is no automation. You drive the car. You drive everything. You're responsible for everything. Level one is driver assistance. The car can do one thing automatically. Cruise control, lane keeping, or braking, but you still control everything else. Now, level two, partial automation. The car can do two things. Steer and brake or steer and accelerate. So, it can drive itself on a highway in ideal conditions, but you must stay alert and be able to take over at any point. You are supervising and you're the one responsible. Level three, conditional automation. When the car can handle most driving tasks in specific conditions like highways, but it can demand you take control if something goes wrong or conditions change. In this case, you're partially responsible and you're on standby. Level four, high automation. This is what Cruz promised and what Whimo was able to achieve in the end. The car can drive itself without human input. A human driver is not needed for normal operation, but the system has limits. For example, only in good weather or only on certain roads. Level four means that you can sleep in your car and not pay attention. And finally, level five, full automation. The car can drive itself anywhere in all conditions. No human needed ever. This is complete driverless and this does not exist yet. So for cruise, the narrative was focused on rapid commercial rollout. A lot of bold claims about removing a human driver entirely and then scaling up to wider geography. The business model aimed for mobile only operations all done through a ride app and their whole business model and go to market strategy was all at once disruption. There were no public talks about a phase roll out. Whimo on the other hand is Google's product. Whimo is a child of a technative company. Whimo was clearly a lot more prepared knowing how tech launches work. They market it as the world's first commercial also level four self-driving service but with a measured safety first pilot in Phoenix, Arizona. They put a big emphasis on the user experience and the messaging kept reassuring about ongoing safety oversight. They went with a phased roll out and framed autonomy as incremental innovation, not fast disruption. And unlike GM, Whimo stayed afloat with its autonomous claims and their clearly communicating limitations and suspensions. And lastly, my favorite example. I rarely share personal facts about myself, but here's one. As you know, I do a lot of research. I love dissecting data and working with stats. I actually enjoy reading scientific papers at 2:00 in the morning. But when I need to compile my findings and present, whether it's on video or if I'm presenting a product release at work, making all of that data readable, digestible, and aesthetically pleasing, is a torture for me. I am really not creative when it comes to visuals. And as someone who works in product, I don't get to just opt out of presentations. I have to do companywide updates. I have to do team presentations. I have to do feature releases and customer webinars. And I need slides that don't just contain bullets. they need to land. And if I'm presenting to five different audiences, they need to land five different times in five different ways. That is why a tool like Gamma AI is a lifesaver for me. I just take everything I need. I dump it into Gamma and I tell it what to do. I do it professionally and I do it with my YouTube team. On the YouTube side, we're preparing for a massive relaunch in January. There is so much that needs to be done behind the scenes. And Gamma is how I turn all of my data, all of my messy notes, everything that's on my mind into beautiful presentations that I can share with my team. And I know I'm not going to be judged for the lack of my design skills. I just need to focus on what I'm good at. And Gamma handles the rest. Unlike the products that we're talking about in this video, Gamma doesn't overpromise. It's not claiming to replace your entire workflow with AI magic. It's doing one thing exceptionally well. Tanking your content and making a presentation ready. If you've tried Gamma at least once, you know how satisfying it feels to see your data transform into something actually beautiful. There's no learning curve. There's no disappointment. Just polish presentations exactly when you need them. Huge thanks to Gamma for sponsoring this portion of the video. You have no idea how much easier my January prep has become. Tesla's rebranding from full self-driving to full self-driving supervised, which if you really think about it, there is a world of a difference between autonomous and supervised autonomous. They sound similar, but they're completely different user experiences because supervised autonomous essentially admits that the product is level two driver assistance, not level five or level four autonomy. Because level four or level five implies that you can fall asleep in a driver's seat, let alone in the back seat. This false advertising on Tesla's part triggered a number of lawsuits in the US, China, and Australia over deceptive marketing. The problem with this marketing is that when you say full self-driving, you imply level four or level five autonomy, meaning that the vehicle requires no human input, no monitoring under any conditions. The media and marketing loves the word driverless. And what do you hear as an average consumer when you hear driverless? You hear the car can drive itself. But full self-driving supervised in reality implies that the system fully requires constant human supervision with the driver ready to take control at any point. This is level two autonomy and level two is functionally equivalent to cruise control. The difference is not semantic, it's categorical. Level two means you're always responsible. Level four or five means you can sleep in your seat. This is not just scaling back. It's a fundamentally different product. Tesla's case is mine if I may for a much larger pattern across AI, across autonomous vehicles or adjacent tech sectors. In August this year, there was a lawsuit against Tesla in regards to a wrongful death case with Tesla ordered to pay almost $243 million. There was a crash. It happened 6 years ago when a driver was operating a 2019 Tesla Model S with autopilot engaged. The car approached a T intersection with multiple stop signs and flashing red lights. The driver took his eyes off the road and neither the driver nor Tesla reacted to what was happening in the intersection and the car drove straight through it. Now listen to the verdict breakdown. This is important. The liability was split like this. 6 to 7% of fault assigned to the driver, 33% assigned to Tesla. That 33% assigned to Tesla means that the system itself was found to be a meaningful cause of harm, not just a passive tool that was misused by a driver. And this was the signal to the entire automotive industry. And now they have to be careful with their marketing. And if you're listening to this and thinking, okay, so Meta was excited about the whole metaverse, but now they're kind of shifting away from that. Tesla said we wouldn't need taxi drivers by 2020 and had to take it back. But now everybody's talking about the bubble. How did this bubble form? If there were signs prior to the explosion of Chad GBT, why did it happen? So, I spent the last week studying why bubbles form, why we don't learn, and why even though we know that the bubble is coming, we keep buying AI stocks. And the answer to that is that it's human nature. And the reason this bubble has formed is a perfect storm of technical inaccuracies, economic pressures, and a mountain of psychological biases. Let me prove it to you. The entire AI progress until recently has relied on the notion of scaling laws. Scaling laws are essentially principles that assume that increasing computing power and by computing power I'm referring to how fast and how much a computer can process data meaning the amount of text images and other information that are fed into AI and model size which is how many parameters AI has think of it as like the number of knobs that you can have in a machine. So scaling laws assume that increasing all three improves performance predictability. The underlying idea is the more the better. The more data the better. The bigger the model the better. The more computing power the better. You may ask who came up with that. Where does this causal relationship come from and does it even exist? And the answer is somewhat surprising because nobody really did. For 30 years researchers and mathematicians just kept observing that scaling worked. Bigger models meant bigger results. More data meant fewer mistakes. It was pure empirical pattern recognition, not a law of physics. So they kept doing it, but nobody was really digging into why it worked and what happens when you run out of things to scale. And they did run out of things to scale. They ran out of high-quality data because nearly all highquality diverse human text on the internet has already been collected and used to train large AI models. They started reaching physical limits of computing hardware. Remember the Moors law? Essentially the observation that chip power doubles every couple of years. And that law is slowing down dramatically because those chips simply approach the limit of atomic size. And they're now hitting the ceiling. Scaling laws worked for years, but now each 1% improvement requires 10 or even 100 times more resources, which makes additional gains outrageously expensive. But there is a deeper problem that makes the inaccuracy of mathematical assumptions somewhat irrelevant. And it's the data itself, the data quality and the data scarcity. The internet contains roughly 500 trillion tokens of text data. But a lot of it is garbage. And on top of it, the Moors law is slowing because the silicon chips approach physical limits. AI compute has been doubling every three months since 2012, but semiconductor manufacturing capacity is fully booked through 2026. When you hear 95% AI project failure, the root cause is almost always poor data. The foundation that a lot of LLMs are based on, the notion of scaling laws, has diminishing returns. To get to the first unit of improvement requires one unit of data, but the next one requires 10 and then 100. But everybody expecting revolution. But the revolution isn't happening because it cannot happen technically. So this was the technical layer. And now let's move on to the psychological layer. Humans are prone to a series of biases. But when it comes to bubbles, this is exactly the moment when our linear minds meet the world of exponents. And this bias cuts both ways. Investors underestimated how fast AI could progress. For example, they missed Nvidia's exponential data center revenue growth from 600 million to $41 billion. They dismissed transformer models as incremental improvements when they were discontinuous leaps and as a result massive FOMO among those who enter the market late and rush to catch up. And the opposite way when the public overestimates how close we are to this magic entity of AGI because they extrapolate rapid progress that happened very recently linearly and completely ignore the fact that LLMs have technically reached their peak performance. AI researchers fully admit it. Multiple groups of AI scientists told the press that scaling laws are breaking down and IASKver Openi co-founder clearly said that the results from scaling up pre-training have plateaued. The fact that there was a rapid improvement from GPT3 to GPT4 does not mean that it will linearly extrapolate to AGI within years. And the reason it's not a linear function is because scaling laws are logarithmic, not linear. And each improvement requires 10 to 100 more resources. This mental framing that if AI progressed so much in two years, imagine where it'll be in 10 completely dismisses physical limits, data quality, and S-curves. Chad GBT's viral moment in November 2022 became the most available example in tech consciousness. Cassie Kazerov called it a UX revolution because that's when AI became the subject of discussion in every room. A 100 million users in two months. Can you imagine that? The fastest consumer app adoption ever. Every conversation, every news cycle, earnings calls reference Chad GBT. And Chad GBT's explosive adoption became the mental model for all AI and investors extrapolated. If Chad GBT grew this fast, all AI will grow fast. Which again completely ignores the fact that Chad GBT is a consumerf facing app with unproven ROI. And for something to really become dominant and change the world and become a revolution, it has to be adopted in enterprise and B2B which follows completely different adoption curves. And again on the psychological level, what happens is that people see the products that almost work. Tesla's full self-driving supervised, which isn't level five supervised, and mentally fast forward to works. And what they do is they ignore that the gap between 95% and 100% is often exponentially harder than the first 95%. The single biggest constraint preventing almost works to works is the need for human supervision. And this breaks the entire unit economics equation. I want to drill a little bit more into Tesla's full cell driving promise because the linguistic branding around it contributed to the fact that we have a bubble. Elon Musk's 2016 prediction full self-driving will be solved within 2 years. That became the mental anchor for the idea of autonomous vehicles. Tesla's investors still till this day reference Musk's promises as the baseline. From October 2016 to September 2022, the price for full cell driving rose from $3,000 to $15,000. That's a 400% increase in just 6 years for software that was never delivered. The market wanted to believe that the FSD would achieve full autonomy within a reasonable time frame. Each price hike reflected Musk's promises and product previews, beta releases, street testing. But again, it wasn't tested in the wild as somebody who has built a career in software product management and development. In Tesla's defense, it is normal for a tech industry to demo a product that is 80% ready. As long as you can make it work end to end. If you can demo your product in its basic form and it will solve a real problem end to end, 80% is sufficient for a demo because time to market is everything. You show the 80% and if it works, you continue patching up the remaining 20 and you prepare for the release. But FSD is a piece of software inside hardware, a car. You can't deliver 80% of the car. You can show 80% but people are paying for 100. Consumers paid 5x premium because they believed that if I buy an early it'll be worth hundred thousands of dollars later. But the operative word here is believed. The price people were willing to pay for FSD was directly proportional to their belief that full autonomy was imminent. The same thing happened with OpenAI. Their $80 billion valuation anchored expectations for all AI startups because investors judge new AI companies against OpenAI baseline. If OpenAI did it, everyone else will do it. Which again completely ignores that OpenAI has unique advantages. OpenAI is not a typical AI company. To put this in perspective, I want to give you an example of how this manifested in the past. And I wanted to make it a little bit more creative and instead of using the.com or 2008 analogy, I want to recall the railway mania in the United Kingdom during 1830s. In 1830s, the first passenger railways succeeded beyond expectation and investors saw huge real returns. Then the investors extrapolate. If these first lines made us rich, more rails, more wealth, parliament says yes, do it. Approves hundreds of lines. Stock market goes up, new companies multiply, money floods in. Many schemes are poorly considered or outright dubious, but everyone is doing it, so why miss out? But by 1846, 33% of promised railways are never built. Tightening money, scandals, failures, all start to pile up and then they crash. The railway share prices collapse. Lots of investors lose fortunes. projects are cancelled or absorbed by stronger or real companies. Despite the losses, the UK is left with the world's best rail network for the next centuries, but only after the mania and plenty of losses. And back to our bubble, when it comes to echo chambers, social media didn't help. X, LinkedIn, YouTube, Reddit, all filtered content to match user interests and the content creation market followed to deliver to the demand. And we ended up seeing hundreds or thousands of people screaming AI will transform everything who googled what AI was the day before. I mean even these guys are talking about AI. And this myriad of biases keeps creating network effects and creates this self-reinforcing delusion. You've got a positive feedback loop. Adoption begets more adoption. You've got social proof because the number of AI adopters signals value regardless of what these guys who actually understand what they're talking about screaming because yeah, why listen to a three-hour podcast by an AI scientist when there is a plethora of 20 second clips on TikTok? And to top it off, the fear of being left behind or FOMO. And as a result, this AI adoption race seeps through multiple levels and levels of people and companies. Every Fortune 500 company announced AI initiatives in 2024, not because they had validated ROI, but because competitors were doing it, the boards demanded what's our AI strategy. Regardless of the fit, AI powered became mandatory in marketing. Smaller startups started adding AI to their pitch decks even when using very basic automation. And it's not because people don't understand what they're doing. They're not idiots, but because the industry demands it. Thousands of companies rebranded chatbots as intelligent or smart agents. And the result is hundredsome companies doing genuine AI and thousands of AI lookalikes with massive FOMO. When basic critical thinking goes out the window because you feel pressured to simply follow the crowd and the FOMO is so strong that it runs on institutional scale because the fear of somebody else winning the market has never been higher. And you may ask, but what about investors? If this is so obvious, why are they pouring so much capital into something that hasn't proven the ROI? And this is the phenomenon of sophisticated investors, institutional investors and CFOs falling victim to extrapolation bias. This is one of the most counterintuitive findings in behavioral finance. The paradox is that professionals extrapolate just like amateurs. There is a Nobel Prize caliber economist whose name is Andre Schifer who published a great piece of work on how professional investors and CFOs extrapolate from their past returns when forecasting future returns. His key finding is that past performance nearly perfectly predicts future forecasts. There is a 78 correlation in the experiment that he ran and in that experiment he surveyed professional investors about their predictions for the next 12 month stock and the predictions were highly correlated with the last 12 months returns. In his findings, the same problem affects CFOs because their forecasts for their own company stocks were almost always a reflection of their past performance, not fundamental or objective analysis. There are multiple studies done on this topic that confirm that individual investors over extrapolate earnings and that the extrapolation bias exists on so many levels. But why? Why do they fall for it? Research proves that institutional investors do exhibit less extreme behavioral biases than retail investors, but they still engage in hurting momentum trading. They have mood swings and overconfidence bias. In the data that I found, 74% of fund managers think that they're above average at investing. Overconfident investors believe that they can outplay the market through research and active trading. But in 2023, for example, only 25% of actively managed mutual funds outperformed the market over the previous 10 years. Behavioral biases are present among everybody and even professional money managers because the bias is not eliminated by professional status. This well-known Goldman Sachs report referenced in multiple sources shows that despite all evidence, AI theme can run for years because bubbles take a long time to burst. And experienced investors know this but can't resist participating and money keeps flooding in despite knowing that we are in the hype phase. If we use a.com analogy, we're probably in the 9798 equivalent. We know it's a bubble, but we still think we have time before it bursts. Every tug bubble feels unique while it's happening. But if you zoom out far out, the pattern is painfully familiar. A new technology arrives, the language, the linguistics inflate it. Money amplifies it and belief does the rest. Then reality catches up and the words start to change. Autonomous becomes supervised. Metaverse becomes super intelligence. Agents become assistants. And then the markets follow. We call it a correction, but in reality it is human nature resettling itself. We really hope that this gave you a glimpse into what's happening under the surface and a reminder that we have seen this before. As always, thank you for listening. We'll see you next time. Bye.