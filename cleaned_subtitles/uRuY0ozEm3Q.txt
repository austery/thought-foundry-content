Any LLM that was trained on pre-1915 physics would never have come up with a theory of relativity. Einstein had to sort of reject the Newtonian physics and come up with this space-time continuum. He completely rewrote the rules. AGI will be when we are able to create new science, new results, new math. When an AGI comes up with a theory of relativity, it has to go beyond what it has been trained on to come up with new paradigms, new science. That's by definition of AGI. Martin, um, I know you wanted to have Val on. What What do you find so remarkable about him and his contributions that that inspired this? Vashan and I actually have very similar backgrounds. We both come from networking. He's a much more accomplished networking guy than I am, but that's a high bar given you uh invent but but uh but we come from and so we we actually view the world in an information theoretic way. It is actually part of networking. Um, and you know, with all this AI stuff, there's so much work trying to create models that can help us understand how these LLMs work. And in my experience over the last three years, the ones that have most impacted my understanding and I think have been the most predictive are the ones that Vishall has come up with. Um, he did a previous one that we're going to talk about um called Matrix. Is it uh beyond the black box? But yeah, beyond the black box. So actually if yeah, you know, and we should put this in the notes for this, but like the single best talk I've ever seen on trying to understand how LM's work is one that Fishall did at MIT, which uh um Harvey Baller Christian pointed me to and I watched that. So So he did that work and then he's doing more recent work that's actually trying to scope out not only how LLM's reason, but like it has some reflections on humans reason too. And so I just think he's doing some of the more profound work in trying to understand and come up with models, formal models for how LLM's reason. Well, just on that note, um you you said his most recent work uh helped you change how how how humans think. Why don't you flush that out a little bit? How did it sort of Well, okay. So, can I can I just try to take a rough sketch at it and then you just tell me how how how wrong I am? Right. I am. you know, you're trying to describe how LLMs work and one thing that you found is that they reduce a very very complex multi-dimensional space into basically a a geometric manifold that's a reduced state space. So it's reduced degrees of freedom, but you can actually predict where in the manifold the reasoning can move to roughly. So, so, so, so you've reduced the dimensionality of the problem to a geometric manifold and then you can actually formally specify kind of how far you can reason within that that manifold. So it it it and the the articulation is that we or one of the intuitions is that we as humans do the same thing is we take this very complex heavy tailed stoastic universe and we reduce it to kind of this geometric manifold and then when we reason we just move along that manifold. Yeah I think you captured it accurately. That's that's kind of the spirit of uh the work. Yeah. Wait. Can I just hear it in your words because you know I'm this lay I'm a VC so you're you're a VC with an H index of what 60 something true uh yeah so so you know ultimately what all these LLMs are doing whether you know the early LLMs or the LLMs uh that we have today with uh you know all sorts of post training RHF whatever you do at the end of the day what they do is they create a distribution for the next token, right? So, given a prompt, these LLMs create a distribution for the next token or the next word and then they pick uh something from that distribution using some uh some kind of algorithm to predict the next token, pick it and then keep going. Now what happens uh because of the way we train these LLMs the architecture of the transformers and the loss function you know the the way you put it is right it it sort of reduces the world into these beijian manifolds. Yeah. And as long as the LLM is going uh in sort of traversing through these manifolds, it is confident and it can produce something which is which makes sense. The the moment it sort of veers away from uh the manifold then it starts hallucinating and starts spouting nonsense. Confident nonsense but nonsense. Yeah. So, so it creates these manifolds and the trick is you know we the distribution that is produced you can measure the entropy of the distribution right entropy the way uh Shannon descri entropy yeah not not thermodynamic entropy so uh so so suppose you have a vocabulary of let's say 50,000 different tokens and you have a distribution next token distribution over these 50,000 tokens. So let's say the cat sat on the right. If that is a prompt, then the distribution will have a high probability for map. Yeah. Or hat or or table and a very low probability of let's say ship or whale or something like that, right? So because of the way it's trained, it it has these distributions. Now the distributions can be low entropy or high entropy. Yeah, a high entropy distribution means that there are many different ways that the LLM can go with a high enough probability for all those paths. Yeah, low entropy means that there are only a small set of choices for the next token. And the prompts also you can uh categorize into uh two kinds of prompts. One prompt is uh is you can say uh high information entropy. Yeah. And one prompt is low information entropy. Y so the way these manifolds work the the LLM start paying attention to prompts that have high information entropy and low prediction entropy. So what do I mean by that? So, so when I say I'm going out for dinner, yeah, right? So, when I say I'm going out for dinner, that phrase, the the LLMs have been trained, you know, they've seen it a lot and there are many different directions I can go with it. I can say I'm going for dinner tonight. I'm going for dinner to McDonald's or I'm going to dinner blah blah blah. There are many different but when I say I'm going to dinner with Martin Casado you know the LLM now this is informationri this is sort of a rare phrase and now the the sort of realm of possibilities reduces because Martin is only going to take me to Michelin star restaurants y I'm not going to go to a McDonald's you know you you you get what I'm saying the moment you add more context you make the prompt information rich the prediction entropy reduces. Yep. Yep. Yep. Yep. And another example that uh I I often but but but just quickly what so but what is your takeaway? What is your implication on that which is of course as as you're in so yeah so you're um uh yeah so sorry I forgot how you described it but like so the the more precise you are the more tokens you are I presume the less options you have for the next token is that correct or not correct yeah yeah essentially so you're red you're reducing it you're reducing it to like a very specific state space when it comes to confidence in an answer and like this is kind of a manifold that you can go on and then I mean do you do you have kind of a conclusion of what that means for systems or what that means for reasoning or is it just a nice way to articulate the bounds of LLM? No, that there is something uh I don't know I don't know if I should say profound but but there is something about it which tells what these LLMs can or cannot do right so it it uh the one of the examples that uh I often tell is suppose I ask you what is 769* 1 you have no idea you you can have some vague idea given the two numbers And so in your mind the next token distribution of the answer is going to be diffuse right you don't know you have maybe a vague guess if you are you know mathematically very good maybe your guess is more precise but it's still going to be diffuse and it's not going to be the correct answer but if I if you say can I write it down and do it the way we have learned multiplication tables now you know exactly what to do next step right you write 769 and then 1025 and then you know exactly so at each stage of that process your prediction entropy is very low. you know exactly what to do because you have been taught this algorithm and by invoking this algorithm saying okay I'm not going to just guess the answer but I'm going to do it step by step then your prediction entropy reduces and you can arrive at an answer which you're confident of and which is correct and the algorithms are pretty much the same way you know that's why chain of thought works what happens with chain Chain of thought is you ask the LLM to do something chain of thought. It starts breaking the problem into small steps. These steps it has seen in the past. It has been trained on maybe with some different numbers but the concept it has been trained on. And once it breaks it down then it's confident. Okay. Now I need to do A B C D and then I arrive at this answer whatever it is. Let's zoom back out. I want to get into LLMs. But but f first Michelle maybe you can um give more context on your background and how that informs um your your work here. Okay. So yeah as uh Martin said my background is very similar to his. We you know we come from doing networking. So my PhD thesis my sort of early work uh at Colombia has all been in networking. But there's another side of me, another hat that I wear, uh, which is, uh, both an entrepreneur and a cricket fan. I was going to say, don't you own a cricket team or something? I am a minority owner at your for your local cricket team, the San Francisco Unicorns. That's right. I'm very proud to have you. So, but uh, uh, so say in the 90s uh, I was one of the people who, uh, uh, started this uh, portal called Cricket Info. And uh Craig Info at one point it was the most popular website in the world. It had more hits than Yahoo. That was before India came on. It's remarkable. And so you know uh we built cricket is a very stat sport. You think baseball multiplied by a thousand. And we had built this free searchable stats database on cricket called stats guru. And this has been available on trien for since 2000. But because you can search for anything, everything was made available on stats guru. And you know, you can't expect people to write SQL queries to query everything. So how do you how did we do it? Well, it was a web form, you know, where you could formulate your query using that form that and in the back end that that was translated into SQL query, got the results and got it back. But as a result that because you could do everything, everything was made available. The web form had like 25 different checkboxes, 15 text fields, 18 different drop downs. The interface was a mess. It was very daunting. So, and ESPN acquired Cricket Info uh in the mid 2006, I think, but they still kept the same interface, and that has always sort of nagged me. And so I still know the people. Wait, wait. What nag what nagged you is that cricket info did not have informal language. It had a web form for doing queries. That web form was terrible. Because of that only the real nerds. Of all the things in the world that bother you. The fact that an old website was a web form. I appreciate I appreciate your uh commitment to aesthetic. So, so I I'm still friendly with the people who run ESPN cricket for the the editor and chief uh whenever he comes to New York, you know, we meet up, we go out for a drink and so he was here in 2000. So now the story shifts to how LLM and me sort of met. So January 2000 right before the pandemic he was here uh and I again said why don't you do something about stats guru and he looks at me and says why don't you do something about stats guru. He was kind of joking but uh he he he thought maybe you know I had some ways to fix the interface. So anyway then the pandemic hit the world stopped but in July of 2020 the first version of GPD3 was released and I saw someone uh use GPD3 to write a SQL query for their own uh database using natural language. And I thought can I use this to fix stats guru? So I got early access to GP3 you know getting access those days was difficult but somehow I got it but soon I realized that you know no I cannot really do it because stats guru the the backend databases were so complex and if you remember GT GPD3 had only a48 token context window. There was no way in hell I could fit fit the complexities of that database in that context window and and GP3 also did not do instruction following at that time. But then in trying to solve this problem, I accidentally invented what now called rag where based on the natural language query I created a database of natural language queries and structure of the structured queries uh I I created a DSL which then translated uh into a rest call to stats guru. So based on the new query I would look through my set of natural language queries. I had about 1500 examples and I would pick pick the six or seven most relevant ones and then that and the structured query I would send as a prefix and the new query and GPD3 magically completed it and the accuracy was very high. So that had been running in production since uh September 2021 you know about 15 months before Chad GPD came and you know the whole revolution in some sense started and rack became very popular. I didn't call it rack but this is something sort of I accidentally did in trying to solve that problem for quick info. Now once I once I built it you know I was thrilled that this worked but I had no idea why it worked. You know I stared at that I stared at that transformer architecture diagram. I read those papers but I couldn't understand how or why it worked. So then I started in this journey of developing a mathematical model trying to understand how it worked. So that's been sort of my uh journey through this world of AI and LLMs because I was trying to solve this cricket problem. Yeah. Amazing. And and so maybe reflecting back since since the release of GP3, what has most surprised you about how LLMs have have developed? So what has most surprised me? the pace of development. M so GPD3 was you know it was a nice parlor trick and you had to jump through hoops to get it to do something useful but starting with the you know chat GPD was an advance over GP3 and then you had all these things like chain of thought instruction following GP4 really made it polished and uh you know the pace of development has really surprised me now you know when I started working with GP3 I could sort of see what its limitations were, what I could make it do, what I couldn't make it do. But I never thought of it as you know what it what these LLMs have become for me now and what what have become for millions of people around the world. We treat uh these uh models as our co-workers almost like an intern that you know you're constantly uh chatting with them brainstorming making them do all sorts of work which we could imagine you know just when chbd was released you know it was nice it was it could write poems it could write limmericks it could answer some hallucinated uh questions but the capabilities that have uh emerged now that pace has been very sort surprising to me. Do do you see progress plateauing or how do you either now or or in the near future or how do you see it going? I yes uh in some sense progress is plateauing uh it's like the iPhone you know with the iPhone came out wow what is this thing and the and the early iterations you know constantly we were amazed by new capabilities but the last you know 7 8 9 years it's maybe the camera got a little bit bit better or you know one thing changed here or memory is more but there has been no fundamental advance in what it's capable of, right? You can sort of see a similar thing happening with these LLMs. And this is not true for just one one company and one model, right? You look at what OpenAI is coming up with or what uh Anthropic, Google or all these open-source Sanies model or Mistral the capabilities of LLMs has not fundamentally changed. They've become better, right? They've improved but they have not crossed into uh a different realm. Michelle, this is something that I really appreciate about your work. And so um the thing that really struck me is as soon as these things showed up, you actually got busy trying to have a formal model of what they're capable of, which was in stark contrast to what everybody else was doing. Everybody else is like AGI these things are going to you know recursively self-improve like or or or they'll say oh these are just stochastic parents which doesn't mean anything. So everybody had rhetoric and sometime this rhetoric was fanciful and sometimes this rhetoric was almost reductionist like oh it's just a database which is clearly not true. And the thing that really struck me about your work is you're like no let's figure out exactly what's going on. Let's come up with a formal model and once we have a formal model we could reason about what that means and then you know in in my reading of your work I kind of break it up in two pieces. There's the first one where you basically you came up with this you know matrix abstraction I think it's worth you talking through and then you took in context learning as an example and you mapped it to Beijian reasoning which to me was incredibly powerful because at the time nobody knew why in context learning worked. So I think it'd be great for you to discuss that because again I think I think it was the first real kind of formal effect on like like how are these things working and then the more recent work that you're working on now is a kind of more generalized version of of of what is the state space that these models output when it comes to comes to confidence which is the manifold that we're talking about uh before. So I would it would I think it would be great if you just described your matrix model and then how you use that to just to to provide some balance what in context learning is doing what's happening. Okay. So so yeah let's start with that matrix abstraction. So, so the idea behind the matrix is you have this gigantic matrix where every row corresponds to a prompt and then the number of columns of this matrix is the vocabulary of the LLM. The number of tokens it has that it can emit. So for every prompt, this matrix contains the distribution over this vocabulary. Yep. So when you say the cat sat on the you know the column that corresponds to matt will have a high probability most of them will be zero but you know reasonable uh continuations will have a non-zero probability and so you can imagine that there's this gigantic matrix. Now the size of this matrix is you know if you just take just the old uh first generation GPD3 model which had a context window of 2,000 tokens and a vocabulary of uh 50,000 next tokens or 50,000 tokens then the size of it the number of rows uh in this matrix is more than the number of atoms across all galaxies that we know enough. So clearly we cannot represent it exactly. Now fortunately a lot of these rows are do not appear in real life right an arbitrary cor collection of tokens you are not going to use that as a prompt. Similarly you so so a lot of these rows are absent and a lot of the column values are also zero. Right? when you say the cat sat on the it's unlikely to be followed by the token corresponding to let's say numbers or you know an arbitrary collection of tokens only a very small subset of tokens that can follow a particular prompt. So this matrix is very very sparse. But even after that sparity and even after removing the sort of gibberish uh prompts the size of this matrix is too much for these models to represent even with a trillion parameters. So what in an abstract sense what what is happening is the models get trained on certain you know data from the training set and certain some a subset a small subset of these rows you have reasonable values for the next token distribution. Whenever you give the prompt something new, right, then it'll try to interpolate with what it has learned and what's there in the new prompt and come up with a new distribution. But it's basically so it it's more than a stoastic par. It is sort of basian on this uh uh on this subset of the matrix that it has been trained on. So, so when I say, you know, I'm going out for dinner with Martin tonight. Now, I'm reasonably sure that it has never encountered that phrase in its training data, right? But it has encountered variance of this phrase. And given that I'm going out with Martin, it it can produce a Beijian posterior. It uses that evidence that Martin is the one that I'm going for dinner with and it'll produce a next token distribution that will focus on the likely places that we are going. So, so this matrix because it's represented in a compressed way yet the models respond to everything every prompt. How do they do it? Well, they they go back to what they've been trained on, interpolate there, and use the prompt as sort of some evidence to compute a new distribution, which So, right. So, so the the context of the prompt impacts the posterior distribution. Exactly. Yeah. Right. And this is and this is you you mapped to Beijian learning where the the the context is the new evidence new evidence exactly to learn from. So, so I I'll give you so, so for instance, uh the cricket example that I spoke about earlier. Yeah. So, I I created my own DSL. Yeah. Which you know mapped a natural language query in cricket to this DSL which then I can translate into a SQL query or a REST API whatever. But getting the DSL is important. Now these LLMs have never seen that DSL. I designed it. Yeah. Right. But yet after showing a few examples, it learned it. How did it learn it? And this is this is in the prompt. You didn't no training. 100% in the prompt, right? So like it's the same. Yeah. Yeah. Yeah. Yeah. This is this was happening in October 2020, right? I had no access to internals of OpenAI. I could just, you know, access their API. OpenAI had no access to internal structure of stats guru or the DSL that I cooked up in my head. Yet after showing it only a few examples, it learned it right away. So that's an example where it has seen DSLs or structures in the past and now using this evidence that I show. Okay, this is what my DSL looks like. Now a new natural language query it is able to create the right posterior distribution for the tokens that map to the example that I've seen now the uh the other beautiful thing about this is this is an example of few short learning or in context learning right but when I give that prompt along with this these examples to this LLM I'm not saying to the LLM okay this is an example of few short learning so learn from these examples Right? You just pass this to the to the LLM as a prompt and it processes it exactly the way it would process any other prompt which is not an example of in context learning. So that really means that the underlying mechanism is the same right whether you give a set of examples and then ask it to complete a talk a task like an incontext learning or just give it some prompt for continuation that I'm going out for dinner with Martin tonight. there's no in context learning there but the the process with which it's generating or doing this inferencing is exactly the same and that's what I have been trying to model and come up with a formal model of what I've found very impressive is you've used this basic model to show a number of things right to describe in context learning and to map to basian learning but you did it for another one where you kind of you've sketched out this almost glib bib argument on Twitter on X where you made this um uh you you made a rough argument for why recursive self-improvement can't happen without additional information. And so maybe maybe just walk through very quickly how like the same model you can just very quickly show that a model can never se recursively self-improve. So uh you know another phrase that uh uh we've been using uh recently is you know the output of the LLM is the inductive closure of what it has been trained on. Yeah. So when you say that it can recursively self-improve uh it could mean one of two things. So, let's get back to the Well, actually, you know what's kind of interesting is like often the mo most people agree that if you have one LLM and you just feed the output into the input, like it's not going to do anything. But then often people will say, well, what if you have two LLM, you have no external information, but you have two LLMs talking to each other. Maybe they can improve each other and then you can have like, you know, a takeoff scenario. But again, you even addressed this even in the case of like n number of LLMs using kind of the matrix model to show that like you just aren't gaining any information uh entropy. Yeah. Yeah. So, so you can represent the the sort of information contained in these models and let's go back to that matrix uh analogy that I have the matrix abstraction. So like I said you know these models are uh represent a subset of the rows right? Yeah. So a subset of the rows are uh represented but some of these rows are able to help fill out some of the missing rows. For instance, you know, uh if the model knows how to do multiplication, doing the step by step, then every row that is corresponding to let's say 769 * 125 or whatever all those it can fill out the answer because it has those algorithms sort of embedded in them. You just need to unroll them. Yeah. So it can sort of self-improve up to a point. But beyond a point uh these models can only uh sort of generate what they have been trained on. So let me give you I I'll give you three examples. Yeah. So any model any LLM that was uh trained on pre915 physics would never have come up with a theory of relativity. Einstein had to sort of reject the Newtonian physics and come up with this space-time continuum. He completely rewrote the rules. Right? So that is an example of you know AGI where you are generating or generating new knowledge. It's not simply universe right it's not computing it's actually discovering something fundamental about the universe fundamental and for that you have to go outside your training set. Similarly, you know, any any LLM that was trained on it would not have come up with quantum mechanics, right? That that's wave particle duality or this whole probabistic notion or that you know energy is not continuous but it is quantized. You had to reject Newtonian physics. Yeah. Or get incompleteness theorem. Yeah. He had to go outside the axioms to say that okay it is incomplete. So those are examples where you're creating new science or fundamentally new results. That kind of self-improvement is not possible with these architectures. They can refine these they can fill out these roles where the answer already exists. Another example you know which has received a lot of press these days is these IMO results international math olympiad. Yeah. you know whether it's a human solving it or the LLM solving it they are not inventing new kinds of math they are able to connect known results in a sequence of steps to come up with the answer so even the LLMs what they are doing is they are exploring all sorts of solutions in some of these solutions they they start going on this path where their next token entropy is low. So that's where I say they they are in that Beijian manifold. Y where you have this entropy collapse and by doing those steps you arrive at at the answer but you're not inventing new math. You're not inventing new axioms or new branch branches of mathematics. You're sort of using what you've been trained on to arrive at that answer. Yeah. So those things LLMs can do, you know, they'll get better at it of connecting the known dots. Yeah. But creating new dots, I think we need an architectural advance. Yeah. So Martin was talking earlier about how the discourse, you know, was it was either statastic par statcastic parrots or, you know, AGI recursive sol. How are you how do you conceive of sort of the AGI discourse or or or even this the the concept? what what does it mean to the extent that it's it's useful? How do you think about that? So, the way you know I think about it, the way we have tried to formulate in our papers is it's it's beyond a stocastic parrot, but it's not AGI. It's doing Beijian reasoning over what it has been trained on. So, it's it's it's a lot more sophisticated than just a stoastic parrot. So, how do you define AGI? Okay, so AGI uh so how do I define AGI? So the way I would say that LLMs currently navigate through this known Beijian manifold, AGI will create new manifolds. So right now these models navigate, they do not create. AGI will be when we are able to create new science, new results, new math. When an AGI comes up with a theory of relativity, I mean it's it's an extremely high bar, but you get what I'm saying. It has to go beyond what it has been trained on to come up with uh new paradigms, new science and that's that's my definition of AGI. Michelle, can you do you think that based on the work you've done, can you bound the amount of data computer or data or compute that would be needed in order for it to to evolve itself. So, so one of the problems if if you just take LMS as they exist is there was so much data used to create them to create a new manifold will need a lot more data just because of the basic mechanisms right otherwise it'll just kind of like you know get kind of consumed into the existing set of data like h have you found any bounds of of of what would be needed to actually evolve the manifold in a useful way or do you think we just need a new architecture? I personally think that we need a new architecture. The more data that we have, the more compute we have, we'll get maybe smoother manifold. So, it's like a map. Yeah. Because I mean, there's there's there's this view that people have. They're like, well, Vish, this is all this is all this is all, you know, good and well, but you know, I could just take an LLM and I can give it eyes and I can give it ears and I can put it in the world and it'll gain information and based on that interfation, it'll improve itself. um and therefore it can learn new things. But the counterpoint that I've always just intuitively thought to that is the amount of data used to train these things is so large. How much can you actually evolve that manifold given an incremental? I mean almost none at all. Right? There has to be some other way to generate new manifolds that aren't evolving the existing one. I I completely agree. There has to be a new sort of architectural leap that is needed to go from the current, you know, just throwing more data and more compute, you know, it's going to plateau. It's it's, you know, the iPhone 15, 16, 17. And are are there any research directions that are promising in your mind that might help us, you know, go beyond LLM limitations? So, so I mean u again I love LLMs. They are fantastic and they are going to increase productivity like nobody's business but I don't think they are the answer. So you know Yad Lick famously says that uh LLMs are a distraction on the road to they're a dead end. They're a dead end. They're dead. I don't think I'm not quite in that camp but I I think we need a new new architecture to sit on top of LLMs to reach AGI. You know a very basic thing you know what Martin just said you know you give them eyes and you give them ears you make them multimodal they of course they'll become more powerful but you need a little bit more than that you know the the way human brains learns with with very few examples that's not the way transformers learn and you know I'm not saying that we need to create an Einstein or a ger but there has to be an architectural leap that is able to create these manifolds and just throwing new data will not do it. It'll just smoothen out the already existing manifolds. Is that something? So, is is your goal to actually help like think through new architectures or are you primarily focused on putting formal bounds on existing architectures? A bit of both. I mean, the the former goal is the more ambitious one that uh everybody is chasing and yeah, I I I think about that constantly. Are are there any new even like uh sort of hints at a new architect or like have we started to make any progress on on on new new architectures or is it uh you you know um YAN has been pushing at this JA architecture. Yeah. Energy based architectures. Uh they they seem promising. the the the way I have been sort of thinking about it is you know you there's this uh set of uh benchmarks or the arc prize right that Mark Mike Koop and France have have and if you understand why the LLMs are failing on this test maybe you can sort of reverse engineer a new architecture that will help you uh succeed in that right and I agree with a lot of what several people say that you know language is great but language is not the answer you know when I'm looking at uh catching a ball that is coming to me I'm mentally doing that simulation in my head I'm not translating it to language to figure out where it'll land I I do that simulation in my head so a way you know one of the new architectures architectural things is how do we do how do we get these models to do approximate simulations to test out that idea and whether to proceed or not. So, so, so yeah, we have, you know, another thing that I've always wondered about is did we develop as humans, did we develop language because we were intelligent or because we developed language, we accelerated our intelligence. So, I I don't know which side of the camp uh you follow on that question. I mean what's interesting is like you have these anecdotal examples of humans developing languages denovo that have been recorded right like it's it's either the Guatemalan or Nicaraguan sign language right where there is these students that develop their own language without being taught and so that would suggest that languages follows intelligence. The problem is is they're all anecdotal, right? Like who knows if somebody didn't teach them sign language? Like nobody really knows. There is no controls. So this is all these observational studies and there's so few of them you have to wonder if it's just kind of sloppy observation. And so I think that the question is still outstanding. Yeah. So I I mean language definitely accelerated our intelligence. There's no question about that. Yeah. But which followed which we don't know. But I view it as I view it as I view it as a networking problem naturally which is once you have languages you can communicate and when you communicate store you can replicate. Yeah. Yeah. Exactly. Exactly. Right. Cool. Again this is kind of a wonky question but um uh you know I think one thing that you've brought to the discourse and for those that are listening to this I really think that you should look up Val's work and read it. I just think it'll give you a really really especially if you have a systems background like a networking or systems background give you a really really good understanding of kind of the bounds on these um but like the toolkit that you draw from is like information theory and like more formal have you found that the AI community is receptive to this or is it like two different cultures two different planets trying to communicate and not a lot of common ground like how have you found like bringing like the networking view of the world to the AI realm. Some of them are receptive to it definitely but uh you know uh these large conferences and their reviewing process it's so random and the kind of questions they ask you know I'm a modeling person I like to model things and you know I submitted one version of this work to one very famous uh uh machine learning or AI conference and the reviewer said okay this is the model. So what so so that is uh that's absolutely remarkable. So like you you've actually taken a system that nobody understands we have no models for you actually provided some model that we can use to analyze it and uh that alone wasn't sufficient. They're asking so where are the large scale experiments to to prove this? I do listen I I honestly I mean I I find there's so much empiricism in like the the current you know AI community exactly because we don't understand the systems you know it kind of reminds me I I I feel like um I feel like systems went the other way right it's like we had all of these models but then we didn't understand how the systems worked and then we just like actually did measurement it feels like ML and or the AI stuff is the opposite which is like we know we don't understand them and so we just measure them but now we're trying to like come up with the models Yeah, exactly. So, it was so easy in some sense to build these uh artifacts and then just measure them that people have been going around trying to do that. And you know, one one term I really dislike is prompt engineering. Why? You know, engineering used to mean sending a man to the or providing 59's reliability. Prompt engineering is prompt twiddling. Yeah. Right? You you fiddle with a prompt and the model changes and the the inference uh the output changes and you know you have like hundreds of papers just just you know doing one experiment on the other changing a prompt this way that way and writing their observations and as a result you know lots of these papers are being written are being submitted for review. reviews get busy looking at all this kind of empirical work and my personal taste is to first try to understand model it. Yeah. And then you can do the other sound like a true theory guy. I don't know about this bit twiddling like let me ask one more LM question which is are there any benchmarks or real world tasks that if if they if they occurred you'd sort of reevaluate and say hey maybe LLMs are you know closer to the path to AGI than I thought if there were any real world task good you know which uh for LLMs uh or these models the one domain where you have the most training data is probably coding right and coding is where uh you can also have the most structure Yeah. And yet anyone who has used uh these tools whether it's cursor or whatever or cloud codes LLMs continue to hallucinate continue to generate unreasonable code you know you have to you have to constantly uh babysit these models. So the day an LLM can create a large software project without any uh babysitting is the day I'll be a little bit convinced that it's towards easy. But again I I don't think uh it'll be able to create new science. If it does that's when I'll be convinced. I you know I think that you can almost take a definitional approach to answer this question. Vash like the problem with these types of questions is is if you have billions of dollars and you can collect whatever data you want you can make a model do anything you want right and so like you know what I'm saying like it's at some level you've got this entire capital structure machinery behind these models so you're like oh it can be good at science well sure you put a billion dollars in solving material science and collect all this data you'll be good at material science or or whatever it is and so but there is a definitional answer which is and and and and I'm going to draw from your work which is there is a manifold that's in there based on the data it's been training on and then the question is is if it ever produces something that's off like a new manifold. So considering the existing trained data if it ever does that if it does something that's outside of that distribution then clearly we're on a path to to learning new things and if not then everything is just a computational step from what's already known. Yeah. That's all I mean. And I guess I guess the count I guess the counter to that would be maybe all humans do is work on their own manifold and Einstein you know was lucky or something I guess would be the counter to that but yeah so so you know there are several Einstein examples and yeah it's creating this new manifold. I didn't want to use that definitional answer. I thought it might sound too too wonky too mathematical but essentially uh if LLM's really created this new manifold then I would be convinced but so far they have just gotten better at navigating the existing manifold the existing training set which is hugely powerful and is going to change the world which is hugely I'm I'm not denying that I think they are extremely extremely good at what they can do but there's a limit to what they can do so I have one quick question what's next for you? I mean, you've uh you you you've tackled in context learning, you've got a model for LLMs, and now you've got a generalized model for like, you know, like their solution space. What are you thinking about tackling next? Yeah. In terms of uh modeling or academically an LLM? Academically, yeah, academically, I'm uh you know, I'm I'm I'm thinking of this. What is the architectural leap that is needed? Oh, that's exciting. to create this uh new manifold and how do we use you know multimodal data awesome to expand that come back talk to us that's right we'd love that so I mean uh you know even with uh LLMs you know the in the paper we say that you can improve uh the inference by following this low or minimum entropy path so so that's a very sort of small step that We are taking you know we are building and training models that will do inference based on the entropic path. Yeah. By the way is is model probe still up? Token probe. Yeah. Yeah. Token probe is still up and and you can see actually the you know token probe is a software that we built and thanks to Martin and A6Z's generosity is running on your servers and anyone can go and test. And what we have done there is we actually show the entropy. Yeah, it is so enlightening. I recommend anybody listening to this who's interested actually check out token. It shows you the limit. Yeah. As you go along, it's remarkable. Yeah. So in context learning, you know, you create your new DSL and you give it to the prompt and you can see the confidence rising with each new example, the entropy reducing and that sort of is a validation of the model. You can see it sort of unfurling and right in front of your eyes. The token probe is running. Thanks. Thanks again. Michelle, thanks so much for coming on the podcast. This is a great conversation. It was great fun. Thank you. Thank you so much again. [Music]