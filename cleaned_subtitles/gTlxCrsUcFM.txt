The talk about AI bubbles seemed very divorced from what was happening in frontier labs and what we were seeing. We are not seeing any slowdown of progress. We are seeing this very consistent improvement over many many years where every say like you know 3 4 months is able to like do a task that is twice as long as before completely on its own. It's very hard for us to intuitively understand these exponential trends. If you manage to make everybody in society 10 times more productive you know what kind of abundance can we achieve? What will we be able to unlock in the next 5 years? I think we can go extremely far.
>> Welcome to the Matt podcast. I'm Matt Turk from First Mark. Today my guest is Julian Shitwezer, one of the world's most impressive AI researchers. Julian was a core contributor to DeepMind's legendary AlphaGo Zero and Muse projects and he is now a key researcher at Enthropic. We covered the exponential trajectory of AI and his predictions for 2026 and 2027, the frontier in reinforcement learning and AI agents and the science behind AI creativity and the famous move 37 from Alph Go. Please enjoy this fantastic conversation with Julian.
>> Hey Julian, welcome.
>> Hey Matt, thanks for having me. A couple of weeks ago, you wrote uh an incredible blog post uh that broke the internet entitled failing to understand the exponential again. What is it that so many people are missing about the current trajectory of AI?
>> Yeah, it's funny that you bring up that blog post. I really didn't expect it to blow up that much. I actually had the idea when I was on holiday in Kyrgyzstan a few weeks ago on a very long car ride and then I started thinking about this and like all the talk about oh AI bubbles I seen on X and like you know this discussion and it seemed very divorced from was you know what was happening in frontier labs and what we were seeing and that made me start to wonder a bit like is it that things are moving so fast that people maybe struggle a bit to extrapolate and understand intuitively. Oh, you know, maybe it's far away now, but you know, it's doubling every so many months, which means that once it gets close to us, it's going to move past and become really good very quickly. And that reminded me a lot in like a different way, but like what happened during early
>> co
>> where we had a similar situation where, you know, at the beginning it's like very few cases. It's like, wow, you know, it's never going to happen. It's only a few hundred people. Who cares? But if you understand the math and if you look at it, right, it's like, oh, it's going to double every, you know, week, two weeks. Clearly, it's going to be a massive scale. But it's it's very hard for us to intuitively understand these exponential trends because it's just not what we're used to in our normal environment. And so that's what got me thinking oh is something similar happening here with AI right
>> we are clearly if if you're looking at many benchmarks we have many evaluations we have we are seeing this very consistent improvement over many many years where every say like you know 3 4 months is able to like do a task that is twice as long as before completely on its own and so we we can extrapolate this right and we see that oh in a year from now maybe two years from now It's the top models are going to be able to work completely on their own for like a whole day or more. Combined with this, combined with the fact that there's a huge number of knowledge based jobs in the economy, knowledge based tasks and combined that in the frontier labs we are not seeing any slowdown of progress. Just extrapolating those things together over a very short time like you know half a year, one year already that is enough to know that there is going to be massive economic impact. That means if you look at current like if you look at OpenAI if you look at Enthropic if you look at Google those evaluations those revenue numbers are actually fairly conservative. I think some more thoughts, some things more I've seen more recently is that it's maybe actually even more interesting and more complex that you know while those frontier labs and frontier models are clearly very capable and on like an extreme trajectory, there are like a lot of other companies, right, that are trying to follow into the same AI sphere that may also have very high evaluation but not necessarily the revenues to support it. And so it's possible that there may simultaneously be like some sort of bubble in you know the wider ecosystem while at the same time the frontier labs on a very solid trajectory having a lot of revenue making a lot of money. I think that may be quite of an unusual situation
>> that in the past you know maybe in the com bubble maybe people were talking about like you know the railroad rush and stuff like this we did not see this bifocation. So I think it yeah I've been thinking about this more and I think it's it's getting more and more interesting the situation.
>> Fascinating. So you alluded to some of your predictions or extrapolations for 26 and 27. Do you want to unpack that? You had you had three of those.
>> Maybe you know calling it my prediction is giving myself too much credit, right? I will just say this. If you look for example at meter eval and you very naively extrapolate the linear fit, that's what you would expect to happen. And so I'm just going to be humble, right, and say like, "Oh, most of the time, right, I'm not going to be smarter than statistical models, statistical extropolation of past trends that have been very consistent. So I'm just going to be very humble and like despite, you know, what I might know all about research and what's happening." Probably like, you know, the most likely the best prediction I can make is actually just follow that data, that extrapolation and see where it's going to take us. And yeah, in that case, if you if you roll this out, if you look at other benchmarks, I think we would have something like next year, maybe the models will be able to work on their own for a whole day worth of tasks. If you think of software, right, you might say like, oh, implement this entire feature, build out this entire set of the app. If you think of knowledge work, right, if maybe do like a whole research report, this kind of scale. The reason I think why task length specifically is interesting is because that's what allows you to delegate more and more work to language models to agents. Even if you have a very clever model, but if it needs feedback or the interaction with you very often, then it really limits what you can delegate to it. If you need to talk to it every 10 minutes, right? Versus if you have something that can go for hours at a time, obviously, right? Then you cannot just have one copy of it. you can have a whole team that you delegate tasks to and manage them. And so I think that's why it's really critical that the models are actually smart enough, the agents are smart enough to work on their own to correct their own errors to you know iterate because that's really what allows you to delegate indeed. Uh test length and time to complete as the metric for progress. So by mid206 you mentioned agents can work all day autonomously. late 2026 at least one model matches industry experts across many occupations and then by 2027 models frequently outperform experts on many tasks. So is so it's more time running and then generalization across the economy and you mentioned GDP val the the open AI metric as um as as a benchmark to to um uh already see the progress towards multiple professions. Yeah, I think the GDP is like a super cool evaluation from OpenAI where they collected a lot of like you know real world tasks from real domain experts to make sure it is actually representative of what you might do in the economy and then they evaluated a lot of models on those tasks. They compared them against real experts performance to give us like a really good indication of you know how close how far are we from having significant economic impact. So I think that's that's like a super cool evaluation. The sort of obvious question is that GGB val and meter are carefully uh designed benchmarks. How do they predict production value once you add compliance, liability, messy data, the messy world, tool friction and and all the things.
>> So I think like messiness and task length like you know time duration that you're able to work independently are very similar or very correlated. So I think that's why it's interesting that meter tries to measure how long can the model go on its own because if you know if you think about like you know how do you come up with a task that you know takes a human 8 hours 16 hours right you will have to include all these messiness and all this real world mess to even be able to measure it but I think you know ultimately to go further we really want benchmarks we really want evaluations that come from the actual users whether it's the industry whether it's private users because that's that's what ultimately matters, right? Like is the model helpful to you? Do you get something out of it, does your paperwork, helps you write something, fixes your codes, helps you study, right? I think that's the real proof. If you release a new model, right, do people start using it more? Do they really enjoy it?
>> Is there anything that would change your mind? any kind of signal whether that's real world adoption or benchmark performance something that would make you more um cautious about um that exponential is there anything that would change your mind
>> I mean many things yes I think like you know many of these things are like you know internal only right I might look at our model pre-training I might look at our fine tuning I might look at RL things you know how do new runs go compared to past runs do they match our expectations the scaling continue then you know I might look at more public things of like are people actually able to use those models to be more productive for example at the beginning right there's always some adaptation period of oh you have like a new tool like cloud code takes you some time to figure out how to use it but then in the medium term in the long term do people keep using it are they getting more and more productive using it I think that's like you know one of the things I look at many many signals I think when you do RL when you do research I think you get very much in the habit of like looking for signals to prove yourself wrong because you know you often have ideas that you get attached to, but that's not a good way to do research, right? You most of your ideas are not good and they're not going to work.
>> So you really want to figure out as quickly as possible whether this idea is any good or whether it's actually wrong. So if you really get into this habit of like oh finding the fastest thing that will show that oh no this is actually not true. So by 2026 2027 in your extrapolation framework AI becomes as good as humans. A key question of the moment is um can it be to which extent can it become better than than humans? There's um uh some chatter these days around uh move 37 and uh whether AI can create those like alien new path to think and solve hard problems. So first of all maybe remind the audience what move 37 is and then do you think that AI in its current state uh is is going to be increasingly able to to provide move 37 type thinking?
>> Yes. So I guess yeah to give background move 37 that was when we were building AlphaGo AI program to play the game of go that was like in the year 2016 I think and we're playing one of the best players in the world at the time because at that time you know no AI program no computer program had ever beaten the top human players at go and it was considered to be you know one of the most difficult board games sort of like you know a real test of intelligence. Move 37 happened during the second game of the 5K match where Alph Go played like a really unexpected unconventional move that surprised many professional Go players. I think you know the commentator said it was you know truly creative unexpected and then ultimately Algo ended up winning that game. And so I think that was for many people an early sign that AI is not just you know purely calculating following an optimal path but it can also do something that is truly novel and creative that you might not expect you know just from imitating his training data. Yeah, I think I think that's very relevant in a modern context as well, right? Because as you alluded to, there is a lot of discussion of, oh, are LM just paring the training data? Can they actually do novel things? For me, as somebody who has been doing research a long time, I think it's pretty clear that these models can do novel things. And that's why they are so useful to many people whether it's you know writing code for you because obviously right you're not just writing code that you already have that wouldn't be very interesting or like helping you you know write a paper or the way those models are trained they're literally trained to generate a whole probability distribution which means that when we sample from them we can generate you know infinite amount of novel sequences from them. for the question of something like move 37. I think there it really comes down to like you know is it something that is sufficiently creative and impressive that we can easily recognize it in the game of go right that was pretty ideal conditions because it's you know very clean very abstract you can really each move is a very impactful so you can really see it clearly I think to have the equivalent for our modern models you need the combination of a task that is like sufficiently difficult and interesting and a model that is both able to create sufficiently diverse and creative ideas and also able to evaluate accurately how good they are so that it can go down you know increasingly novel path while making sure that this novel path is actually you know interesting and useful. Creating novel things is actually very easy with language models. The hard part is creating novel things that are useful and interesting.
>> Extrapolating this further, there's the idea of creating novel science. So not just one move but like a whole new ID, new concept. What's your current take on on this? So I think alpha code and alpha tensor proved that you can discover novel programs and algorithms. Uh very recently I think last week there was some news of uh Google de mind and yell uh in the biomedical uh field coming up with with brand new things as well. So do you think that's uh accelerating and that AI is in the process of discovering novel science?
>> So I think we're absolutely at the stage where you know it is discovering novel things and we're just moving up the scale of how impressive how interesting are the things that it is able to discover on its own. And so I think it's highly likely that sometime next year we're going to have some discoveries that people pretty you know anonymously agree that you know this is super impressive. I think at the moment we're more at the stage of you know oh it came up with something but there's debate about and like but yeah I'm not very worried because I see this process continuing and then once it gets clear enough there's less need to argue about it. How far do you think we are from an AI winning the Nobel Prize?
>> Yeah, I think that's a really interesting question, right? Because we had a Nobel Prize for AI with Alpha Fold, of course. And so I think the next very interesting point is going to be when can AI on its own make a breakthrough that is so interesting that it would win a Nobel Prize. And I think my guess for that level of capability might be maybe 2027. I think we're we're probably not going to find out for quite some time afterwards because of the delay in getting prices, but I think by 2027 2028, I think extremely likely that the models will be smart enough and capable enough to actually have that level of insight on that level of discovery.
>> Amazing. I
>> think yeah Nobel Prize, right? It's like the Fields Medal for math and all these kind of like advances. I think you know that's that's what I'm truly excited about actually is like AI that can help us advance science and really unlock you know both all the mysteries of the universe and all the improvements in living standards and abilities for us that we could have if we understood the world better.
>> All right. So extrapolating this even further then we get into the AI 2027 thing that that you you you probably saw. So this general idea of um if AI uh can create novel science then AI can create uh AI researchers and basically AI can create itself which um effectively leads to a discontinuity moment. So I don't know if that in the blog post that's the singularity or whatever. uh but does that strike you as somebody who's uh you know as deep in the field as possible as something that is possible in the short term or are they counterbalancing forces that makes that path to discontinuity uh harder as you get closer? Yeah, I think a true disconnectuity is extremely unlikely from you know obviously AI researchers are already using AI to accelerate themselves and so what's what's already happening and like what is likely to continue to happening is that we see like a smooth improvement of productivity and then the main open question is how does the difficulty of improving AI keep scaling because a very common effect in a very common issue in many scientific fields is that we find all the easy problems first and then you know as we continue exploring the problem the field it gets more and more difficult to make advances. So in my mind the main question is do these two trends balance each other out so that you know the AI makes us increasingly more productive so that as it gets more difficult to make advances we just sort of about stay on trend and then we keep improving roughly linearly or it is still too difficult and then you know eventually after some time we still see a slowdown but it seems quite unlikely to me that we improve in productivity so much that we can actually accelerate That would be very unlike any other scientific field. The normal course in many scientific fields is that we actually need to exponentially increase the research effort just to keep making progress and find new insights. For example, if you look at pharmarmacology discovering new drugs, it's nowadays in the range of billions of dollars to discover a new drug versus you know maybe 100 years ago a single scientist could discover the first antibiotic right by accident. It's not that we will be surprised by sudden takeoff in progress where you know oh we're just doing our research and suddenly our model is like 10x better. We will be seeing advanced signs of oh we're making faster progress every single week. we can see something is happening. Maybe we decide to pause if you don't understand what's happening.
>> Do you think the current approach to modern AI systems which effectively is pre-training plus RL does that take us to where we want to be? Whether we call it AGI SI, it's unclear what any of the things mean. But um do you feel that this paradigm is the right one or do we need to come up with a different architecture all together post transformers or otherwise?
>> I think that's a great question and I think it hugely depends on what you mean by where do we want to be. So I think if you're thinking of oh we want some kind of system that can perform at roughly human level in basically all tasks that we care about productivity wise then I think yeah it's extremely likely that the current approach pre-training or you know transformers is going to get us there. If what you care about is oh we want to have a model of intelligence that is conscious in the same way we are or you know more abstract qualities like this I think that's maybe more uncertain right and I think this is where a lot of the confusion and disagreement comes from as you alluded to know AGI ASI like you know people talk about very different things and they have very different things in mind when they say oh the current paradigm is going to get there it's not going to get there I often yeah like to not use the term AI or ASI and just talk very concretely about you know what problem are we solving what task are we solving what quality are we interested in because I find that often makes the actual dis disagreement much more obvious but yeah I think if you're just thinking of in terms of like is this going to help us be massively more productive is this going to massively accelerate scientific progress then I think definitely the current approach we'll get there
>> and given how extremely deep you are in I cannot resist asking you sort of the the the the trendy question uh duour uh based on Richard Sutton's recent appearance on Dorish podcast uh do you think that the models of the future will be trained in RL from scratch and that actually having pre-training um in addition to RL is the wrong way to go?
>> Personally, I think that's unlikely. Not not because pre-training is strictly necessary. I think we may well be able to train something completely from scratch as we've been able to do in other domains. But more because pre-training on this vast data sets we have just brings us so much value that we would from a practical point of view not want to give it up. So we, you know, we might well do some agents that are trained from scratch out of scientific interest. It could be very interesting to learn about what would a non-human intelligence look like. But from a programmatic point of view, I definitely think we would keep using pre-training data. not just from an efficiency point of view as well, but also I think there is interesting safety angles because by pre-training on you know all this human knowledge we're implicitly creating an agent that has similar values as we do and I think that is quite valuable for aligning uh you know highly intelligent agent if you already start out by caring about sort of you know the same rough set of values that makes things much easier then you create an you know arbitrary alien intelligence that may have completely different values. Despite having, you know, done a bunch of from scratch RL in the past, I think I'm often quite pragmatic about this.
>> I'd love to put a pin in that specific discussion about um alignment uh and what we do to ensure safety to later in the conversation because I think that's a super interesting uh vein. Um, but maybe to switch tacks uh for for a minute. Um, I'd love to go into a little bit of your uh story and then uh the monumental body of work that you've done uh at Google Deep Mind before joining anthropic around uh Alpha Go Alpha Zero M0. Uh so maybe uh just the 3 4 minute version of of your personal story from when you were a kid. What was the path that led you to become a world-class AI researcher?
>> Yeah, actually when I was a kid, I didn't have any expectations of becoming an AI researcher. I was always very interested in computers and I grew up in the Austrian countryside in a small village. So, you know, it's not like there was a huge amount of things happening, but computers were always very interesting to me. You know, it's like this connection to the wider world to all these other interesting things. And I was very interested in computer games as well. And I think that's the first time I became interested in programming because I wanted to make my own games which I think is very common in people who get into programming. But I somehow I always got distracted by the technical aspect of I'm going to build a very general game engine that you know can run any kind of game. And so I never actually ended up making any game. I learned a lot about making game engines and different technologies and that's how I ended up studying computer science eventually in Vienna. Yeah, there was like a classical computer science degree and then by chance after my first year in my first summer holidays I had an internship with Google and that's when I realized oh wow these guys are doing really interesting things they you know that's where their big clusters the tens of thousands of machines are that's first time I radically changed my plans from wanting to stay in academia and you know I had originally thought oh maybe I do a PhD That's when I changed, oh no, actually I just want to join these guys at Google and I will finish my degree as quickly as possible. And so that's actually yeah when I got my full-time position at Google, finish my degree the next year and then move to London. So I was just working as a normal software engineer at Google working actually like in advertising which I wasn't super excited or interested in. So like the technology was interesting, right? is like these huge systems and Google has you know famously great technology but actually after you know a yearish of this I was pretty done on board of advertising and so I was actually planning to leave Google and thinking of maybe joining a hedge fund going into finance when by chance I saw an email in my work inbox that this guy Demis was going to come to the office and give some talk about Atari and video games and AI And it was actually a day off because I was visiting a friend somewhere else in England.
>> But that email looked so intriguing that I was like, "Oh no, I'm going to have to like take the train back to the office right now and like see this talk."
>> And yeah, like I'm really glad like that I saw this email and I did go back
>> because that's like the moment where I decided, oh no, like no, I'm not going to go into finance. I'm going to move to Deep Mind.
>> I'm going to join these guys because this looks clearly, you know, super interesting, super amazing. They are doing really interesting research.
>> All right. Tell us a story of um Alph Go, Alph Go Zero, Alpha Zero, Mu0. Uh because it's it feels like it's fundamental AI knowledge that everybody who has an interest in the space should know about should understand the progression in in particular. So um starting with the beginning of Alph Go, you alluded to it a second ago, but like what did it do? what how was it trained and then how that how did that evolve with each version
>> alpha
>> go I think at that moment in time go in the machine learning community was this really big target where everybody felt like oh you know it's this big unsolved challenge image ImageNet had just happened before so clearly you know models were starting to do something with images and being able to recognize them and predict them and if you look at the go board you know the right way it looks a lot like one of those images that you classify. So there was a lot of momentum around using neural networks to somehow play go and then at the time David Silver and Ashang deep had been working on go I think like both of them had been working on go for quite a while had published some very interesting papers and that's when the idea of using Monte College research with deep networks came together so the idea was that to train a deep null network to predict which moves you might want to play and you know whether you're winning or losing the game and then use the tree search to really make a big plan of what are all the possibilities in the game. How would it go for you if you chose a certain move or a different move? How would the opponent respond?
>> And to explain this in super plain English, uh the term search in this case is as you said is research is not what people normally think of search which is searching a corpus. This is searching a series of options effectively. Is that is that the right way to think about it?
>> Yes. It's it's quite literally what you might do when you play a game of chess when you play any board game. It's quite literally thinking of you know what move am I going to do? What move is my opponent going to do in return and then thinking about many possible moves like that and mapping out all the possibilities in the future.
>> So deep learning plus search. What was Alph Go trained on?
>> Initial training phases of Alph Go were on some human amateur games if I remember correctly.
>> Mhm.
>> So basically just predicting if you have humans playing many games of go try to predict at each turn in the game what move would they have played? And it turns out that you know if you train a deep network to do that you can get something pretty decent like amateur go level.
>> Mhm. Mhm. but not good enough to actually beat a really strong player.
>> And by the way, just for the lore of it, uh did you guys have any uh sense that uh was going to crush Lee Doll? So, the the famous Go player that you mentioned earlier in the conversation? Was it was it obvious before? Was that a surprise?
>> We thought we had a pretty good chance, but we were very nervous about like, you know, are we going to win? Are we not going to win? Are we going to lose? Yeah. We actually had some bets beforehand of like how many games we're going to win or lose. Like I think it was very ambitious to put the match as early as we did. If we had wanted to be a bit more safe, we may have like tried to do a few months later. And I think if you had done it a few months earlier, we would have probably lost. So it was very knife edge of I guess which also made it much more interesting for us, right? Because
>> it really means that each game is like a nailbiter of oh what's going to happen? Are we going to win? Are we going to play you know dumb move? What's going to happen? So that was very exciting.
>> Alph Go Zero which was I believe the year after. How was that different? What was the progression?
>> Main change between Alph Go and Alpha Go Zero was to remove all the human Go knowledge. So instead of starting by imitating human go games, we were training it just from scratch playing only against itself and rediscovering basically all go completely figuring out from scratch how to play.
>> Did you give it the rules of the game?
>> We didn't give the rules of the the game to the network per se, but we used the rules of the game to score the result. So basically you know he would play and he would tell it you know who won who lost or you know you cannot make this move.
>> So the next hop was uh alpha zero which was a year or two later. How is that different? So alas zero the idea was well obviously go is really beautiful game but ultimately we would like to do something more general right so can we remove anything that is go specific and verify that the algorithm can actually solve more problems and in that case we did that by trying to solve both chess go and shroggy which is a Japanese chess basically with the same algorithm you know the same network structure just by running it in different games and also making it you know much simpler, elegant and faster. So basically that was really laying the groundwork for applying the algorithms to solve real problems. And then the next um stop in the journey was Mu0 and and just to bring it home for uh people, you were uh I believe second author on AlphaGo Zero and you were the lead author on M0 which um in the world of of I'm I'm sure you're going to be very humble about it, but like in the world of AI is like as as big a deal as it gets. So um I'll say it so you don't have to say it. Um so Mero, what was the next um what was how was that different? So the main motivation I had for making M0ero was that if you want to solve many real world tasks, you have no way of perfectly simulating what's going to happen. And you know, if you play a board game, obviously you know if you make this move, you know what's going to happen. It's like the piece is going to go there, it's going to take a piece, whatever, right? But if you actually want to solve something like a robotics task or anything more complicated, it's impossible for you to simulate what's going to happen accurately. And also we as a human we don't do this right we just imagine in our head of oh if I'm going to say this then he's probably going to respond in that way this meant that alpha zero as it was could not be applied to such problems because it required some way of you know simulating the game scoring the outcomes and the idea with m0 was that well we already have a deep neural network right these networks can learn a lot of things so why not let it why not teach it to predict the future of the environment, the future of the world. Why not make the model be able to learn for itself, what is going to happen after each action it takes?
>> After that, you also uh applied this to uh code and math. So that was alpha code and alpha tensor. So like zooming out a little bit that evolution of um reinforcement learning in games and then code and then math. What did you learn about the general power of search and learning that is today relevant in modern agentic AI systems? How did that that whole body of work translate to what uh you are doing today?
>> So games are a really good sandbox to learn very quickly about a lot of the reinforcement learning science. you know the algorithms that work well, the kind of problems that we encounter, the even from a technical point of view, how do we build a learning system that spans, you know, many data centers, uses tens of thousands of machines because games are very clean sandbox, very clean environments, so we can make many good experiments. And then now that we have a much more general model, right, the language models can do almost any task, but they're much more complicated. They're much slower to experiment with, we can apply those same lessons of ah, you know, we know how to build a really robust reinforcement learning infrastructure. And now we can build the same one for language models or like you know we know if you do this kind of RL then the model will learn how to exploit the reward and so we can apply the same lessons the same mitigation techniques to the language models.
>> If I understand correctly I think Muse had um a learn world model.
>> Mhm. So basically sort of rehearse the future for for lack of a better expression. Uh so do do modern LM agents have have anything like that? Do they have an internal world model that lets them preview actions before they commit? So I think yes I would say that language models have an not an explicit world model but they do have an implicit model of the world because to be able to predict you know what is the next likely word in this sentence how is this paragraph going to continue they need to internally model you know what is the state of the world that makes this person say that thing and so it's it's actually somewhat similar to m0 in the sense that mu0 zero also only had an implicit world model. You know, it was never trained to predict, you know, what does the screen actually look like if you take an action. It was also only trained to implicitly predict if I take this action, you know, what is the next action I should take or is it going to be good or bad for me. So in both those cases, you have an implicit representation of the world in your model that you can use to make predictions, but you're not actually reconstructing the full state of the world because reconstructing the full state of the world, you know, that can be very expensive and complex.
>> If you think about, you know, super high resolution video, audio signals is a very large amount of data that probably you don't actually need. If you think of human attention, we are only aware of a very small subset of what's actually going on all around us all the time because that's you know the most relevant information that we actually need to make decisions and that goes back to the the prior discussion about so retraining. So the reason why pre-training and RL work well together is that you have that world model that's um implicitly embedded into the the the the corpus. Although the argument against it is that it's what humans think the world model is uh as uh embodied by language versus what the world model actually is. And that's that that's my understanding of the of the of the of the debate. I mean for the debate I think different people have different points of view so I don't want to speak for anybody but
>> yes yes yes
>> but yes I think like pre-training on this rich knowledge gives you some representation of the world already so that when you actually start to act and interact with the world you can very quickly make you know meaningful decisions meaningful actions I yeah I like to think of it you know in the similar way if if you look many animals when they are born they very quickly know how to move how to run even Right? If you look at gazels for example in the savannah in a way that is like you know clearly they did not have time to really learn this from scratch right few minutes or hours and you know in their cases they did not do pre-training but they have some evolutionary encoded structure in their brain
>> because clearly it is very beneficial to have some sort of knowledge to make your learning more efficient.
>> Yeah. just RL in nature would uh would lead to not so good results. Like if you're a gazelle and like you have to AB test whether to run towards the lion or away from the lion.
>> Exactly.
>> It's like the you know thousands of generations of gazels acquired this knowledge over time.
>> Y
>> it was encoded in their genes and their brain structure in some way right and then you get to start on top of that. I think the main you know the main challenge or the main thing you need to watch out for is that you don't overenccode or you don't restrict your search base too much. If if your pre-training if your prior knowledge prevents you from exploring something that might be the correct course of action that will be bad. So there you know there is some danger there you have to be aware of.
>> So this general idea of making pre-training and RL work together in modern AI systems seems to be the the big idea or topic of 2025. Although of course I know it's it's it's been years in the making. Uh why did it take so long? U it feels like RL you know progressed uh in in in its own direction and then pre-training worked in its own direction and those were slightly separate. Why did it take take so long to put them together? Is that just purely practical and economic or or anything else? Scaling up the language models to the massive degree that we scaled them up took a lot of effort on its own. And from a science point of view, from an engineering point of view, retraining and supervised training is more stable and sort of easier to debug because you don't have this feedback cycle. you basically, you know, have a fixed target and you're trying to learn this target and so then you can focus on like, you know, is my training working and like is my infrastructure working and then, you know, does it scale as a fallover versus if you compare to RL in RL you have this feedback cycle of oh I learn something and then I use that to generate my new training data and then I learn from that training data and now if you have you know something is not working it's very hard to figure out where in this cycle your problem is coming from. No, maybe your training update was bad and that's why you suddenly started behaving badly or maybe the way you decided, you know, the way you select actions to behave is not correct and so you generate a bad training data and that's what messed up everything.
>> So it's just much more complicated to get working correctly. And so I think it makes a lot of sense to you know first scale up the pre-training the architectures figure out something that works pretty well especially if you can already get pretty far by some fine tuning some prompting and then when you know when it's clear that these models are really general they are really useful and we have them in pretty stable state then you know you can ramp up RL and take them even further even in our own work right if you look at alpha go alpha zero we always follow a similar split as well when we first set up the architecture of the network, the training using fixed supervised data. And only when we had that working really reliable, only then did we do the full RL loop and the full training just because like debugging all of it at the same time, you're just setting yourself up for failure. It's really useful to be able to isolate the component and say like, you know, I have known good data over here. I have a known good target there. If the thing in between is not working, I can isolate it. And then, you know, we can isolate all parts of the system. How compute intensive is it to scale RL and are there scaling laws for RL the same way you do in pre-training?
>> There's less published literature about it. But I think if you if you look at all the RL literature over time, we see very similar returns on compute in pre-training and in RL where we can invest exponentially more compute in RL and keep getting benefits. There's going to be some interesting research to come to figure out what are the trade-offs between pre-training and RL compute. we know how what what should be the split for a big model for example it could be 50/50 should be like 1 to 10 which way should it be 1 to 10 so I think that's going to be extremely interesting but so far yeah we definitely see good returns on both
>> what's the latest stateofthe-art or or thinking uh in the field of um rewards so in what you described uh for alpha zero alpha go that was basically win loss as a reward board. Then it sort of feels like we went into kind of like fuzzy human matching. This is good, this is not good. And now that we expand um as as as per the the above into more general fields where it's sort of unclear whether you win or lose, uh how does how does that work? What what parts of the evolution are you working on? Are you excited about?
>> Personally, I don't work that much on reward modeling. I mostly work on sort of reasoning, planning, search time, search compute, ways of making the model smarter by spending more computation. Yeah, thinking about rewards. I think the reinforcement learning process per se doesn't really care where the reward comes from. The algorithms are very happy to use any source of reward. Whether that's like a human feedback signal, it's like some automated signal from like you know winning, losing the game or passing a test. Whether it's something more model generated for example anthropic we had this paper about constitutional AI to you know have the model itself score whether you're following some guidelines.
>> Mhm.
>> So it can be very flexible to what kind of reward you follow. the RLVR like all all the things are those those are at this stage um stuff that that you see commonly common commonly uh used um any any many any thoughts
>> yeah I think we're seeing like huge mix of rewards and environments and I think it's very much much people working very hard in figuring out what are the best reward sources and how do we scale it up and how do we you know get more rewards more reliable rewards that will be one of the key ingredients in scaling up RL further.
>> And so switching from from rewards uh what is the latest thinking uh in terms of uh data training data for RL uh again following the uh evolution from like AlphaGo where it used to be human data and then like self selfplay uh what how does that uh how does that work where does the data uh come from and what kind of data works best to train modern RL
>> yeah I guess the great thing about RL is that the data is generated by your model itself. So the smarter our models become, the better RL data we can generate, the more interesting and complex tasks they can solve, which then gives us more and more data that we can train on cuz like you know the more the more complex the task, the longer it takes to solve the task and the more data it generates that we can then use for training. I think part of the challenge is to find tasks that are really representative of what people actually want to do with the model because now language models are so general people are using them for so many different things. There's more and more of a challenge of you know we need to cover as many of those as possible in a to make sure that you know the model is actually able to do this diverse set of tasks. What uh matters more for training data? Is that quality? Is that quantity? Is that recency?
>> I think that's like a very interesting question that maybe doesn't have like a super clear answer yet or it's maybe still interesting research to be done. I think we've seen papers arguing for different things or we've seen different benefits, right? Like clearly we see pre-training as we scale up the data. we can keep improving but we've also seen very interesting fine-tuning results papers published for with a very small amount of examples you can teach the model how to do an interesting skill and I think we don't have any good scaling laws yet that tell us the trade-off especially I think because it's very hard to measure what is the quality of a data point right like how good is this example compared to this other example without being able to measure this it's very hard to quantify the trade-off in any Okay, I think intuitively it's definitely true that if you have bad data, RL doesn't work that well and if you have very high quality data, it becomes much more stable. For example, I think that was like a it's very clear in like Alpha Zero days where you we spend Alpha Zero spends a lot of computation. It does a lot of planning and search to decide which move to take and so that generates very high quality data to train on which then resulted in RL training that was incredibly stable. So you know you can run it across continents take a long time to generate the data and then train on it and is very robust versus in modern RL with language models. You know the difference in how good is the model and what data it generates that we then train on is not so large. because we more directly sample from the model and then train it which then results in reinforcement learning that is less stable. And so one direction of scaling RL and making it more stable is by improving this by for example putting more reasoning into your language model to generate much more high quality training data that can then give us training that is much more stable and that we can scale up much more easily. I'd love to spend uh a little bit of time now on the general topic of RL and agents. So the famous augent AI that everybody's been talking about uh you know breastlessly for the last year. Uh so for people listening and you know as as often in an effort to make this broadly accessible by by by you know a group of general people in tech um could you drive home the the uh sort of intersection and overlap between RL and and and agents? Does RL power agents? How does that work?
>> Yeah. So I guess like maybe first let's take a step on what do we actually mean by agent?
>> Yes.
>> As compared to like as like a general language model, right? The second most debated question after agisi is what is an agent? Yes,
>> I guess. Yeah, for our purposes, let's just say that an agent is an AI that can act on its own. You know, maybe take some actions on a computer, save some files, edit some files, send an email, whatever you want, right? But the main characteristic is that it doesn't have to interact with the user all the time. It can do things on its own. The reason why RL is very important for this actually connects back to pre-training because our pre our pre-training data is not very agent-like. If you think of the pre-training data right there is like websites and books and you know all kinds of recent text that has a lot of information but it doesn't have a lot of actions. It doesn't really capture how the humans actually interact with the world. So if you take a raw pre-trained model, it's not a very good agent. You know, maybe you can prompt it a bit and like, you know, sort of push it in the right direction, but it's not going to be very good at interacting and especially it's not going to be very good at correcting for its own errors because the pre-training data has no examples at all of how is our agent going to fail. And that's exactly where reinforcement learning comes in because in RL we can take our agent let it interact with the environment and then directly train on that interaction. So for example, if the agent did well, we can reinforce those actions. And if the agent did badly, we can push it away from those actions. And if the agent sort of did badly at the beginning, but then recovered and managed to well, then we can also reinforce that recovery. And so that's super important because it allows the agent to actually learn from, you know, its own distribution of behavior.
>> Mhm. And that just makes it much more robust because now he doesn't have to generalize to something he has never seen before. He can actually learn you know on the actual problem that is trying to solve. And that's why you know RL is really unlocking so much agentic capabilities. Now
>> if I'm an AI builder today building an AI app and I I build it on top of anthropic. Uh anthropic is whatever model is going to come with some of this sort of batteries included. uh but as a builder on top do I need to do my own RL uh there is this emerging space of like RL as a as a service where you know for this task or that task that I build on top of a general model that sort of like offers you ability to do RL or or can I do a lot of damage just through prom or like maybe like supervised fine-tuning first
>> I think nowadays with the capabilities of like you know topic cloud models top OpenAI GPT models. You don't need to do any fine tuning. You can take the model as is, write your own tools, your own harness, and benefit from that agentic training because doing good agentic fine tuning is actually very hard. And so it's it's quite hard to do better than the top frontier models that you might get. But on the contrary, coming up with good tools and a good represent representation of your task makes a huge difference. So like you know depending on how you express your problem for the model can make it way harder or way easier and so you can get a lot of mileage out of that.
>> What's currently missing to achieve the big dream of a gentic AI? Is it model capabilities at the core or is it sort of like boring uh quote and of quote engineering around reliability tool use safety uh what needs to happen? I think we have sort of there's basically improvements needed around the whole space. Make you know the model better able to correct its own errors. Make the model better able to continue going for long times without getting distracted. Making the model just smarter in general. Maybe making the model faster. Like there's basically like, you know, a whole set of things that we know that we can improve. There's probably yeah not one individual blocker and that's why we will continue to see sort of smooth incremental progress over model releases but sort of given how many things we know there are that we can do better on and improve. Yeah, I'm quite excited about where models are going to end up. I think that's actually, you know, one of the reasons why AI is a very fun field is that there are so many lowhanging fruits that, you know, you can do much better on, but already the current models are so good that it's very fun to work on it. It's like, oh, I can fix this thing. It'll be even better
>> versus, you know, if you're in a place where everything has already been solved and it's like really hard to figure out how to make it better, it's a very different story. Let's spend a minute on on eval uh and um we we we touch upon this a little bit but just to give it some some some proper space. So there was um you know in your blog post that we talked to at the very beginning of this conversation this this this concept of external benchmark and then you quoted your piece goodart law. Uh so what does first of all what is good hard law and then uh how should labs compare results uh so that it doesn't end up with this kind of leaderboard theater that we've seen a little bit in the last you know couple of years.
>> Yeah. As a good law basically says that any measure that becomes a target stops being a good measure and you know you can think of that intuitively that if you start paying for example programmers based on how many lines of code they write well suddenly they will discover many ways to add more lines of comments which is you know completely useless and this is a very yeah general effect that obviously right if you give people an incentive that they should optimize they will try very hard. Yes.
>> And we also see this with language model benchmarks. Of course, people want to get promoted. They want to launch their model. So any benchmark that is too easily measured or that has a lot of attention on it, people will optimize very hard for it. Which means that probably the model will look very good at that benchmark. But if you then use it for your own task, you might get different performance. Yeah. You ask about like how what do we do about this? It's very hard to prevent people from optimizing on the benchmark. So one possibility is just periodically create completely new held out benchmarks that nobody has seen before and that gives you a fairly you know good estimate of model performance. So I know for example like a lot of researchers have their own toy problems that they use to test all the models precisely for that reason. So that you know this is a problem a set of problems that nobody has seen. you have a pretty good guess that it's going to give you an unbiased estimate. If you're like, you know, an individual of your company trying to decide which model to use, it's probably, you know, something similar. Just make your own internal benchmark that really represents what you care about and then measure on that. And I think that's likely to be the most objective, most accurate way of measuring
>> internally. What does that look like at a place like anthropic or previously uh deep mind? Uh is there um I mean I know there there are teams that are focused on on on evals. How do you how do you think about what works, what doesn't in terms of internal evals?
>> It definitely used to be easier to have good evals. You know 5 years ago the tasks were doing I think it was easier to measure model performance. I think nowadays it's much more difficult and I think we try to not over rely on evals so much because it's quite hard for example to measure you know how good is this model really at writing code. Yeah, I think it's one of the big unsolved or very important problems in the field of you know making really good evals that are both cheap to run reliable and accurate because it's easyish to make an evil that takes one of those but to get all three is quite hard for example like you know in the beginning we were talking about open GDP evol GDP evol and that one is like you know it's very accurate and unbiased but it's very expensive to because what it actually involves is like taking human experts having them do the task
>> and then compare the model task to the experts and like you know rate it with multiple people. So it's very accurate but it's like extremely expensive to do.
>> And related to that topic of of evals what what's the latest in terms of our ability or I should say your ability uh to truly understand uh how models work. to the general field of mechanistic interpretability. You alluded to the fact earlier that um RL if I understood correctly sometimes make it a bit harder because it does uh things occasionally in a more inscrutable way. My my words maybe not not yours. Um so what is the latest and indeed does RL make things harder or easier? Oh, so what I meant before is that debugging RL in general know completely unrelated to interpretability
>> is harder because there are more moving parts. But it is also true that if you're not careful with RL, you can make interpretability harder. For example, one common thing with modern models is they they do reasoning with the train of thought. You could look at the chain of thought to you know see what are the model internal thoughts and then you could also have a thought that oh maybe I should use that as a reward signal in RL and punish the model if it thinks the wrong thing but then suddenly you completely destroyed your interpretability angle. So you sort of have to be careful that yeah you don't do RL on the signals that you want actually want to use to interpret what the model is thinking of doing. That said I think yeah there's some extremely exciting interpretability things happening including mechanistic interpretability. I think like actually last year I think before Johnic maybe even there was a super cool golden gate claw model where you know they found the neurons in claw that were responsible for the golden gate concept and then modified them to make a version of claw that really love the golden gate bridge in San Francisco and so that's like a really vivid example of ah you know we really understand what's happening in this model and like you know what better way is there to verify that understanding than actually changing the behavior of the model and so I think that's like a super important direction for safety. As the models get smarter, we really need to be able to understand what is the model thinking internally. You know, what is the values it has? Is it lying to us? Is it actually genuinely following the instructions? And so I think like you know definitely extremely important area to invest in and work in. I think that especially if like you know people interested in working AI or doing AI research, I think interpretability is a great area to get into. Yeah, perfect segue for last for the last part of this conversation. I'd love to to zoom out and and talk about um the impact of AI. So if we think that we are on the exponential and that things are going to only accelerate from here, what does that mean? Uh and certainly safety and alignment which is a core value at anthropic hopefully in other parts of the field as well, but like anthropic is particularly uh vocal about safety and alignment, let's say. um how how does that actually manifest? So we just talked about interpretability. What what uh for people who are concerned that this is going too fast and that we collectively are creating a a monster quote end of court. Can can you give us a glimpse into the kind of work uh that is done for alignment and safety at a place like anthropic? Yeah, I think like the focus on sort of safety alignment pervades all of anthropic and there's very rigorous processes where we train a model whenever we want to release a model both to you know analyze the capabilities of the model verify the alignment of the model. Ensure that it you know does not do harmful things on its own. ensure that it does not enable you know malicious users to do harmful things and to the point where if we are unsure about the safety of model we will delay the launch and like you know until we're sufficiently sure that is actually harmless we will not launch and release a model which you know may you know I guess shows that you know people take the safety much more seriously than any financial return or revenue. I think yeah also in terms of research and resources the teams working on safety and interpretability are a big focus of the company which you know gives me a lot of confidence that we're actually care about this and put a lot of effort into it. and at a more technical level and to uh tie back an earlier part of the conversation um around uh when when we're discussing the you know pre-training and and and and safety. So is safety and alignment an RL problem? Uh and by that I mean uh the the beauty of having pre-training uh is that you import that world model as we're discussing but arguably you also import into your brain uh a lot of bad stuff if you collect data from the internet as we know uh there's good things but also a lot of toxic content. So is uh alignment largely using RL to get rid of the bad stuff that is built into the pre-training.
>> We can definitely use RL to like shape the model behavior and ensure that for example given adversarial given bad input it sort of behaves safely or knows that he can refuse or is you know robust who attempts to pack the model. Yeah, I wouldn't view it alignment just like an RL problem. I think it sort of it goes throughout the whole stack. You might you know for example filter the pre-training data in some way. You might after training you might have classifiers that like you know look at the model monitor the model behavior to ensure that it is actually aligned. You might when you write the system prompt for the model that you use. You might put safety guidelines in there. So I think safety alignment it really pervades the whole of research and the whole of you know product and deployment it's not just isolated into any one part
>> and then another super interesting topic in the same vein of like the impact of AI is obviously the discussion around jobs. So if as per the GDP discussion the uh agents are becoming just as good or better than humans obviously what does that mean uh for all of us in terms of our jobs? what what have you learned um after the experience of Alpha Zero, Alpha Go uh that that could give us a glimpse into uh what may happen once we all have super powerful agents do our jobs.
>> So I think the first thing that we didn't talk about yet so far is that artificial intelligence is quite I mean this may sound a bit simplistic but it's quite different than human intelligence. So we can see that right that the model may be much better at us on some tasks like you know calculation obviously and like much worse than us at other tasks. So it is not I don't think it is at all going to be any like one for one replacement. It's going to be much more complimentary of you know the model is really good at something that maybe I really don't like doing or I'm not interested in or I'm very bad at and then I'm much better than the model as a mother part. And so I think it's going to be like a gradual process of we're all going to incrementally start using models more and more to improve our own productivity rather than you know have a model that one for one is able to do exactly the set of things we can do. And so for example you know I use cloud all the time to for example you know refactor code or maybe write some front end code that I don't want to write or at the same time there's other parts where I'm clearly much better putting than cloud still. So there is a synergy of you know use the best most productive skills I think I guess economists call it like comparative advantage but like there is this you know long process of we'll both sort of improve our productivity incrementally and I think that process is going to give us some time of figure out politically and figure out economically how do we want to you know benefit from this massive productivity increase you know even independently from AI, right? The promise of technology has long been that, oh, we're going to be all so productive, so wealthy that we need to work much less.
>> Yet, mysteriously, right, we all have like 40 hours working week for decades. And so, you know, I think it's much more like a political social problem of like figuring out how do we actually benefit from all these improvements and like, you know, bring the increases in wealth and productivity to everybody and it's much less a technological problem.
>> Mhm. also means that we can't really solve it with technology. We have to solve it at like a sort of a democratic political level. How do we spread these benefits?
>> Do you think that that uh increases inequality? So, uh as you think about the impact of Alph Go and and and and Mu Zero, what happened to the top go players and what happened to the top chess players? Did they do they disappear or did they get enhanced and better? Yeah, I think at least in the case of chess and go there has been like more interest and it has become much easier for people to study how to play go how to play chess cuz now you don't need to find you know an expert tutor you anybody can practice on their own right spend a lot of time and I guess like chess streamers are very popular on Twitch right here and similarly right like a lot of students are using language models to study I think also for coding right cloud code these agents they raise the bar of what anybody who has an idea can accomplish on their own. I think you know the larger picture whether it increases or decreases inequality is quite hard to forecast. It both sort of raises the floor of what any person can accomplish but it also gives very productive people an ability to be even more productive. It's possible that we see quite a difference between countries depending on the taxation social redistributive system that they have in whether inequality increases or decreases for example overall I'm quite excited that it is very much nonzero sum is very much you know increases the total wealth available in society I think if you think about progress if you think about prosperity that is the most important thing like redistributing the pie is kind of a losers game. To get more wealthy, we really need to grow the pie. You know, if you think of the agricultural revolution, the industrial revolution, the reason why we have much better lives nowadays is because, you know, we're so much more productive. We have so much more wealth. And so that's the key step we want to unlock. If we manage to ma make everybody in society 10 times more productive you know what kind of abundance can we achieve? I think that's the key question, right? What advances does that unlock in medicine? You know, curing diseases, halting aging. What does it unlock in terms of energy? Obviously, right, we have like climate crisis. We need more energy rights to sustain our lifestyle. What advanc advances in material science can we have? All of those are basically bottlenecked on how much intelligence we have access to and how can we apply it. So I'm yeah incredibly optimistic about like what will we be able to unlock in the next 5 years. I think we can go extremely far.
>> Well that feels like a a wonderful place to leave it. Uh thank you so much Julian. This was absolutely fantastic. Thank you for spending time with us.
>> Yeah thank you for all the exciting questions and uh giving me the time. Hi, it's Matt Turk again. Thanks for listening to this episode of the Mad Podcast. If you enjoyed it, we'd be very grateful if you would consider subscribing if you haven't already or leaving a positive review or comment on whichever platform you're watching this or listening to this episode from. This really helps us build a podcast and get great guests. Thanks, and see you at the next episode.