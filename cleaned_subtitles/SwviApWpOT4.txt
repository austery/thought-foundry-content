So today, I'm going to talk about using agents for "Vibe Coding", or at least as close to "Vibe Coding" as I'm going to get. This is not a coding video, and you shouldn't need to know how to code in order to follow it. In fact, I hope this video makes it to folks who are interested in "Vibe Coding", but aren't experienced coders. Because while I think it's great that people who don't code are interested in using these tools, I think they should go into it with their eyes open about what you're getting. So for those of you that are interested in coding details, I have a companion video on my other channel with plenty of details. Links below and at the end of the video. So I'm going to compare three agentic tools, Claude Code, Gemini CLI, and OpenAI Codex. I'm going to tell you roughly how I've configured them and use them safely, how I evaluate them, and why I chose that method. I'm going to tell you which of those three tools is absolutely not worth your time and why, and what the trade-offs are between the two decent ones, so you know which of them would be better fit for what you're trying to do. And then I'm going to talk to you about their limitations, and boy, do they have some serious limitations. And a lot of people who are so excited about "Vibe Coding" aren't talking about these limitations. And let me make a quick pitch here for why this matters. The prevailing wisdom in that "we're going to have human-level intelligent AI very soon now" camp, rests on the assumption that "very soon now," the AIs are going to get good enough that the AIs will start improving their own code faster than a human can. And that's what's supposed to lead to crazy increases in its intelligence. I've linked a video called "What If AI Keeps Getting Smarter" Below? If you want more info on that. However, in order for AIs to rewrite their own code faster than we can, the AIs have to get better at writing code than humans are. And that's why I think this kind of "how well do AIs write code compared to professional software engineers" experimentation videos are so important. I think of it kind of like an early warning system because as long as the AIs aren't as good as humans at writing software, then they're not likely to be able to improve their own code. And if they do start getting good enough to start potentially improving their own code, then I expect this kind of experimentation is where we'll get the first indications of that. So watch this space, because the closer they get to writing better code than humans, the closer we humans get to being FF-----. [computer startup music] This is "The Internet of Bugs." My name is Carl. I've been a software professional for 35 plus years now. And I'm trying to do my part to cut down the number of bugs out on the internet. Toward that end, I want to talk about bugs generated by ""Vibe Coding"" so you can hopefully see past the hype and know what it can be used for and what it's not yet stable enough to accomplish. So "Vibe Coding" is starting to take on a bunch of different meanings to different people. But here's an excerpt from the original tweet that named it that explains how I'm gonna be using the term at least for today. Quote, "There's a new kind of coding I call vibe coding where you fully get into the vibes. I barely even touch the keyboard. I "accept all" always. I don't read the diffs anymore. When I get error messages, I just copy paste them with no comment, usually that fixes it. It's not too bad for throwaway weekend projects, but it's not really coding. I just see stuff, say stuff, run stuff and copy and paste stuff. It mostly works." So I did an evaluation project using these rules to start with anyway, with three different command line based agent coding tools. Claude Code, Gemini CLI and OpenAI Codex. Then once I got to the end of the project, I opened up the code to compare the different tools and to see how they would measure up what's expected of professional programmers in the real world. Now, I don't like testing AI code generators or actual people. With coding riddles, I think it's dumb, no matter what the prevailing wisdom appears to be in the hiring process at most companies. Because one, coding riddles are not a thing that comes up when you're really trying to get work done. And two, I found that many people who have memorized a bunch of coding riddles have a very over-inflated sense of how smart they think they are and often they get themselves into a hole that their team has to waste time digging them out of, often while they're kicking and screaming and pounding that they didn't get their way. I found a convenient site last year to use to evaluate coding AIs. If you've seen one of my previous AI coding comparison videos, you've heard about it before. It's called code crafters and then make step-by-step coding challenges for programmers that are not only challenging, but that teach you how the internet really works by having you build simple versions of core internet technologies. They are not sponsoring this video, but I do have an affiliate link below that you can use if you wanna check them out. And if you do, it might help me keep making more videos like this. The challenge I'm using today is one I've used before. It's the one I found that the LLMs have the easiest time with and that's building a simple web server using the Python programming language. Python is one of, if not the most, common language in the dataset these models are trained on and unsurprisingly, there are a ton of examples about how to build a web server on the web that the AI companies find when they scrape it. So this should be pretty much as good as the AI's concurrently get on problems that aren't just coding riddles they've memorized. So let's see how good they actually do. I was pretty hands off on this whole thing. I gave each tool a link to the webpage for its copy of the challenge and a browser connection that was all set up for it to use. I told it to go to the page, read the directions and follow them. Occasionally one would ask permission to do something. I pretty much always said yes, as long as it didn't involve something pointless like installing a new copy of a piece of software I already knew was installed. So to cut to the chase, two of the three, Gemini CLI from Google and Claude Code from Anthropic completed the challenge at basically at the exact same time. Wow, right at the same time. They each had different strengths and they both left things to be desired but they each got the job done. OpenAI's tool though failed pretty spectacularly on a couple of different fronts. Okay, so let me tell you about what's going on in the rest of this video. Next up I'm gonna be talking about how I set up the agents and how I ran them. Again, I'm not gonna go into a ton of detail because I have a longer video on my second channel where you can watch the whole thing play out in real time if you want. But I'm gonna explain the basics here. Then after I explain the setup, I'm gonna talk about OpenAI Codex failing miserably. Then I'm gonna compare and contrast the two that did finish and talk about what each was better at than the other. And then I'm gonna talk about what's still missing, what risks people run by using "Vibe Coding" for production software, where things might be going from here. And then lastly, I'm gonna be talking about next steps for this channel. Like I said, this was the easiest challenge I can think to give them, and I'm not gonna stop there. So there will be harder challenges and other variations coming up that you can subscribe if you want to see. I've got chapter markers for all these things, so feel free to jump around or skip the parts that aren't relevant to you. Okay, onto the setup. I can't walk you through all the different things I tried to get all of this working and all the things I learned not to do along the way, but trust me, this was a culmination of months of on-and-off experimenting. I'm just gonna give you what I finally got working. So I'm using two physical machines here. I have a primary machine. It's a Mac studio that I use for my YouTube recording, editing, Final Cut Pro, that kind of stuff. And then I have a second machine that runs Linux. On that Linux machine, I have three virtual machines that all each also run Linux. Each one of those virtual machines is dedicated to one of the coding agents. They all mount a file system from the underlying host that they write their files to. That way I keep copies of what each agent is doing. They can't hurt each other aside from starving each other of resources. And if one of the agents goes nuts, I can just kill it and roll the disk of that virtual machine back to a known good state. The reason I'm going to all that trouble is because I'm running all of these agents in the most permissive or dangerous mode. They don't ask for permissions before they do whatever they think they want to do. If they screw up, they could wipe out the files on the machine and make it unbootable. I tell you that to warn you that if you don't want to go to this much trouble, you should run your agents in a mode where they have to ask you for permission before they do things. If you run agents in a permissive mode on a machine that you care about and they delete all your files, crash your machine and you can't get into it anymore, don't say I didn't warn you and don't come crying to me. And yes, I've had something like that happen. In an earlier version of this experiment, I had all three agents running on the same time on one much larger virtual machine and one of the agents or some combination of them ran it completely out of swap space, crashed it and corrupted part of the file system. Not sure exactly what happened or what or which one was at fault, but it hasn't happened since I separated them, but that doesn't mean it couldn't happen again. So the other piece of the experiment is the web browser. I'm running three instances of Chrome on the main Mac, each one with a different profile directory and using a different web socket debugging port. Then I forced the agents to use the browser assigned to them to talk to the web via a library like PlayRite or browser use. This is for two reasons. One, so I can set up all the authentication for them talking to the internet, which is a lot easier than trying to teach them to understand two factor authentication and all that stuff. And second, so I can watch what they're doing and kill the browser process if it starts to try to do something stupid. So for the record, my end goal here is to be able to use these agents to write code that can automate some of the tedious tasks that I have to do on YouTube's site as a YouTube creator. So in this evaluation, I'm using and testing an authentication mechanism that's similar to the two factor authentication that Google uses for YouTube. You might be able to use a simpler browser setup for your own problems depending on what you're trying to do. Now there's a thing you might have heard of that I'm not using called MCP or Model Context Protocol. That lets these agents talk to various tools. And I know there are people out there that use Microsoft's Playwright MCP or some equivalent to enable the agents to talk to a browser. Personally, I haven't had good luck with that. I tell the agents to write their own code using the Playwright library instead. And I run that instead of using the MCP. This is because when there is an error, I can look at the code they wrote and figure out what's going on. When they try to use the MCP and something goes wrong, I have a lot less visibility into what they were trying to do, so it's a lot harder for me to troubleshoot. You might be able to get away with using it. I know some people that are having good luck with it, and if it works for you, it might be easier for you. But with my setup, it's been less than helpful. Okay, so let's talk about OpenAI's Codex project. So first off, Codex refused to work for me at all out of the box. I got this error about my organization being unverified. And then when I went to that link, then it sent me to this creepy other site where it wanted me to sign away my biometric information. I'm not doing that, and I recommend you don't either, especially not for the tool that turned out to be the weakest of the three by far. The other problem with OpenAI's tool is, at least for me, it doesn't have a working permissive mode. The way it does sandboxing is useless for my purposes. It might be safer for someone who doesn't know what they're doing, but I think the other tools are pretty clear about what's going on, so I just don't think it's necessary to do the extra steps that OpenAI is doing. And it made it impossible for me to make an apples-to-apples apples-to-apples comparison to the other tools. So for this test, I was using a modified version of OpenAI's tool. I went through the changes that I made in the video on my other channel, but they were basically ripping out all the code related to projects, organizations, access token claims, and sandboxing. I don't have any reason to believe that these changes should have affected the results I got. If on the other hand, it turns out that despite appearances, you can only get halfway decent results from OpenAI by giving away your biometric information, then you can make that decision for yourself, but I'm not doing that. Okay, with that out of the way. At the point that Claude code in Gemini CLI pretty much simultaneously finished Step Eight of the coding challenge, OpenAI was still struggling to complete step two. In previous experiments, I'd seen ChatGPT write the code needed to pass this challenge when I was giving it the prompts manually. So I don't think it's a failure in OpenAI's coding ability. So much as, unlike Google, they haven't yet been able to catch up to Anthropic's work with their Claude code agent tool. I expect at some point, OpenAI will release it with similar functionality and assuming I can figure out a way to use it without giving away all my biometrics, I'll make a video about how it behaves. So, okay, onto a summary of what happened with the other two tools that did get to the end of the challenge successfully. If you want a lot more details, see the video on my other channel link below. So based on this experiment of the two, Gemini seems to be better than Claude code at following directions, although that's both good and bad. Gemini wrote much more concise code. It pretty much did what the extractions told it to had to do and not much more. The code was harder to read. It didn't have any comments or explanations. It did very little error checking. It was pretty much the bare minimum. Claude code I find easier to use, especially the way they handle ToDo Lists is kind of cool. And the code that Claude generated was better organized, better commented, easier to read and had better error checking. But it did do a bunch of stuff I didn't ask it to do. And by the end, it had removed some functionality that had been needed in a previous step, which is what we call "introducing a regression error." So, Claude code might be better for you or it might be worse. If you're sure you know exactly what you want, then Gemini might be a better fit. If you want something that might help you out by going above and beyond what you ask for, Claude might be better. The problem that both of these tools, and in fact, pretty much all Generative AI of any kind struggle with, is context. The more stuff you give them that they're supposed to do, the harder time they have remembering all the different things on the list and the more they are likely to break something that used to work. Now, that's a thing that people can have trouble with too. And as an industry, we've developed techniques to help with that. And the primary one is by writing automated tests at each step that can tell us later if previous functionality gets broken. I've seen a number of people on the Internet opining that if you are worried about the quality of the code that you're getting from your AI, you can just tell your AI to write tests. But I can tell you from both personal experience, in this experiment and others, that generative AI is absolutely abysmal at writing tests. It makes sense if you think about it. The AI's have a lot less training data to draw from when it comes to writing good tests. In this particular experiment with regard to automated testing, the two AI's failed in two different ways. Gemini, on each step, wrote a test to ostensibly help it verify that it had done what it was supposed to do. And then, on each subsequent step, it would dismantle the previous tests and write a new one for the step it was currently on. This defeats the whole purpose of testing and is pretty useless. Claude wrote a bunch of tests and it kept them all, but they were really helpful. So for example, the starting code for the project printed out a helpful message about how you can turn on debugging on Code Crafter's site if you need help troubleshooting. Claude wrote a test to verify that the "just for your information" statement was always printed. That print statement doesn't help anything and nothing would go wrong if it stopped, but Claude wanted to make sure it didn't go anywhere. On the other hand, Claude wrote zero tests that covered the vast majority of the required functionality. There's a big complicated nested if-else block that's the core of the logic in the program. And Claude wrote tests for something like 3 of the 78 lines in that block. And when at some point it between step six and step eight, the functionality from step five of the challenge apparently ended up going missing. Claude had no idea, and the test didn't catch the fact that code had accidentally been removed. So where does that leave us? In the easiest of the challenges I could think to throw at it, Claude and Gemini both managed to meet the minimum standard of the success criteria, but neither of them created code that was maintainable or well tested. By cracking open the code myself and prompting them specifically on what they need to do better, I have no doubt that I could eventually get them to produce code that was maintainable, at least for challenges this easy. But one, cracking open the code goes to get the spirit of "Vibe Coding". And two, I have decades of experience to know what maintainable code is, what I need to look for and what I need to ask the guys to do. That's not true for a lot of the people that are excited about "Vibe Coding" at the moment. So if you are interested in "Vibe Coding", keep that in mind. So as far as what's next for the channel, now that I've finally figured out a reliable way to run these tests without me having to do a bunch of manual copy and pasting, there are a lot more experiments that I want to run. Seriously, I've been trying for months to get something like this to work reliably and I finally managed to pull it off. There are a lot more difficult challenges at code crafters. So I'll definitely be doing some of those. There are also some other agent tools like OpenHands, which is what used to be OpenDevon that can talk to different AI vendors. So the idea of comparing Claude code's tool to a different command line tool that uses the same Claude backend model would be interesting. Same with using the exact same front end tool to go head to head between two different models. That way we will know what the tool is responsible for getting right and wrong and what different the models actually make. Those experiment should be fun. So hit the subscribe button if you wanna know when those happen. At the moment, if you are experimenting or wanting some code that you can run once and throw away, then I think "Vibe Coding" is a great option assuming your problem is simple enough. On the other hand, if you're wanting code that's going to need to be running reliably in production for a long period of time, then the current state of the art is nowhere near what you need. And I would urge you not to use "Vibe Coding" for anything that needs to be maintainable or anything that's going to be sitting on the internet in a place where hackers might be able to get to it because that's a recipe for making the internet less safe for everyone. And the internet already has too many bugs. It doesn't need an additional bunch of vibe coding crap. So until next time, thank you all for watching. Let's be careful out there. [BLANK_AUDIO]