大家好！ 在AI炙手可熱的今天 以AI爲中心的數據中心 正在變成第四次工業革命的核心 AI數據中心又和AI服務器，超級計算 和雲計算相結合 成爲了新一代計算平颱 什麼是AI數據中心？ AI數據中心的結構是什麼？ 它和AI服務器，超級計算 雲計算的關繫是什麼？ 英偉達的AI數據中心解決方案是什麼？ 它爲什麼是第四次工業革命的核心？ 我今天就給大家説清楚 我先説什麼是數據中心 數據中心在颱灣叫做資料中心 在公司裡 你可能去過你們公司的機房 機房裡有可以遠程訪問的服務器 有網絡設備，還需要製冷，需要供電 你可以把數據中心理解成一個更加大號的機房 在數據中心，有大量的服務器 服務器放置在機架上 而機架又和機架通過網線或者光纖密密麻麻的連接起來 最終它們通過電信運營商的光纖連接到外部的世界 有些公司，有自己的數據中心 如銀行或者政府 往往是出於安全的考量 大部分公司是租用數據中心 僅僅是把自己的服務器放置到別人管理的數據中心 於是就有了專門做數據中心的運營商 如Equinix和Digital Realty 在中國 數據中心主要是電信運營商提供的 有些公司甚至連服務器都不是自己的 而是租用的 這就是雲計算的方式了 這種數據中心也可以稱爲雲數據中心 提供服務器出租的就是雲計算運營商 如亞馬遜的AWS 微軟的Azure和Google的Google Cloud 如何理解服務器呢？ 你把服務器理解成高性能的計算機就好 傳統上，服務器就是放網站的 於是世界各地的客戶就可以訪問你公司的網站了 當突然很多人訪問你的網站該如何處理呢？ 可以使用負載均衡的方法 當很多人訪問時 你可以同時複製和啟動多個服務器 把一些訪問讓這些新的服務器處理 甚至在世界各地複製服務器的內容 這就減少了以前服務器的負擔和訪問延遲 如果你讓雲計算運營商給你做這種負載均衡的話 雲計算運營商已經自動化了這些過程 你根據使用量給雲計算運營商付費就好 同樣 Youtube在世界上每天有大量人同時訪問 Google也是這樣動態的管理來自世界各地的服務器 最終處理大量的需求的 這種服務器的結構 往往是以CPU爲主的結構 是Intel和AMD的天下 這種服務器結構和我們平時用的計算機差不多 對於網站這類內容，即使訪問量很大 這種服務器結構處理起來輕車熟路 爲什麼呢？ 雖然可能需要很多服務器 但是服務器和服務器之間的通信要求卻不多 這是剛才講到的網站 和Youtube視頻訪問 這些任務的特點 十多年以前，就有人把很多服務器 通過高速通信連接起來 密密麻麻的放置到一個機房裡 形成服務器集群 英文叫Cluster，來完成一個大的任務 這就是超級計算機的概唸 爲什麼要密密麻麻地放置到一起呢？ 原因是短距離通信延遲低，速率高 所以，服務器們必須儘量靠近 什麼樣的任務需要超級計算機呢？ 在以前，這些例子往往包括天氣預報 導彈軌跡的仿真等等 所以 以前隻有國家和國防才需要超級計算機 超級計算機所進行的計算也叫做高性能計算 超級計算機也可稱爲高性能計算機 超級計算機雖然很多國家在競爭 但是其作用並沒有那麼大 因爲大部分公司的業務是網站 是不需要超級計算機的 早期的超級計算機 用的基本上都是Intel的CPU 當時中國，美國 日本和歐洲 都競爭超級計算機的規模 不過，超級計算機的實際用途不大 尤其商業化程度不高 這種以計算爲主的具有超強計算能力的數據中心在中國往往稱爲算力中心 生成式AI的出現 對於計算和數據中心有了更高的要求 AI需要大量的並行計算 這是GPU的特長 GPU有大量的小內核 能夠同時並行處理大量的任務 而CPU內核數量很少 適合單一串行的任務 於是，AI的訓練和推理需求 使GPU取代了CPU 成了下一代計算的主角 例如，GPT-5 的訓練可能需要 30 000–100 000 塊 H100 級 GPU（或等效算力） 運行數週到數月 就單次推理而言 小規模可用 1–8 塊 GPU 大規模服務則需要成千上萬 GPU 集群來支撐並髮 沒辦法，生成式AI就是靠大量的數據 即大數據 加大量的GPU，即大算力，來得到的 這是生成式AI的基礎 所以 專門支持生成式AI的新計算機結構産生了 CPU處理普通的任務 而AI這種需要並行處理的任務 就應該讓CPU交給GPU來做 同時，大量的通信任務 也不應該完全由CPU做 英偉達專門設計了處理通信任務的DPU 即數據處理單位 來專門處理通信的任務 這種新的計算設計結構就是加速計算結構 你可以這樣理解加速計算的計算機結構 一個或者多個CPU 通過高速的接口連接多個GPU和DPU AI的大量任務最終由GPU完成 它們之間相連的接口英偉達使用的是NvLink 你可以把NvLink等同於電腦上使用的PCIe總線 以上這種結構就是AI計算機或者叫做AI服務器 例如 NVIDIA DGX Station A100機架式AI服務器 大概有8 塊 H100 GPU GPU是通用芯片 主要是英偉達和AMD在做 另外還有針對AI特定算法或者專門針對推理優化的特定芯片 例如 Google的TPU等 Broadcom和Marvell就是和雲計算運營商一起做這類可定製AI芯片的 做AI服務器的廠商包括了美國上市的SMCI和Dell等大量颱灣代工廠 現在的大模型訓練和推理需要大量的GPU同時工作 所以 我們就要把剛才講到的AI計算機 通過高速的通信網絡 把成百上千的這種AI計算機連接起來 形成集群 用什麼連接呢？ 或者使用英偉達自己的InfiniBand 或者使用傳統的標準的以太網 因爲通信要求及其高，例如速率 延遲，和穩定性 傳統的以太網需要被改造，才能勝任 有些通信任務需要被剛才講到的DPU處理 從而加速網絡通信的速度 而通信速率，目前已經到800Gbps了 AI服務器對於內存的讀冩要求很高 需要專門設計的低延遲 高速率內存HBM 韓國的海力士，美國的美光 和韓國的三星 在HBM內存方麵積累深厚 英偉達在2025年年初構建的最大AI服務器集群規模已突破10萬GPU 憑藉NVLink、NVSwitch交換機 及高速以太網技術 支撐超大規模AI模型訓練 是全球最頂尖的AI超算基礎設施 2025年9月 英偉達宣佈將向OpenAI投資至多1000億美元用於算力建設 要部署至少 10 吉瓦（gigawatts）的AI數據中心 黃仁勳表示： 10吉瓦大約等於400萬到500萬個GPU 這個數量與英偉達2025年的總出貨量相當 是2024年出貨量的兩倍 所以，構建AI超算中心 一方麵是提高單個GPU的性能 另一方麵就是堆積GPU的數量 例如，華爲近期宣佈華爲AI算力 已經超過了英偉達給中國定製的H20芯片的3倍 華爲是如何做到的呢？ 其實很簡單 華爲在單個GPU方麵仍然輸英偉達的H20 但是，華爲的看家本領是通信 華爲進一步優化了通信的效率 提出了supernode超節點的概唸 就是把很多 AI 芯片（Ascend 昇騰繫列）通過高速互聯、統一調度 組合成一颱邏輯上的“大芯片”。 是不是GPU很容易堆積呢？ 答案是no 主要是隨着GPU的數量越來越多 通信的需求和延遲就成了瓶頸 所以 隨着AI數據中心有越來越多的GPU 突破通信的瓶頸就成了關鍵技術 大家知道 美國市場的Arista Networks就是這樣一個做數據中心低延遲和高速網絡的 而它的芯片很大程度來自於Broadcom 華爲和阿裡巴巴也都有自己的數據中心網絡解決方案 以上就是AI數據中心的概唸 AI數據中心對於製冷要求很高 現在液冷正在成爲標配 SMCI就號稱其核心技術是液冷 另外 AI數據中心建設成本主要是採購GPU 而運營成本主要就是電力了 甚至到了不得不使用核能的地步 於是 這些給AI數據中心提供能源的公司也都在這輪AI大潮中受益了 關於AI數據中心 我接下來澄清幾個概唸 這些概唸代表了AI數據中心和未來計算的大趨勢 看看你能夠理解多少 這些概唸來自最新的阿裡巴巴的開髮者年會 阿裡巴巴講到了未來的趨勢是超級人工智能ASI 即Artificial Super Intelligence 平時我們講的是AGI，即人工通用智能 指的是AI達到人的水平 ASI是另外的新階段 也就是AI遠超過人的階段 這是我頭一次聽説ASI這個術語 以前都是用AGI表示未來的 ASI這個術語我覺得顯示了阿裡巴巴對於AI更加有信心 當然這僅僅是一個對於AI未來能力描述的術語 不要太當真了 阿裡巴巴認爲大模型是下一代操作繫統 我記得黃仁勳也這樣講過 可見未來IT的核心是大模型 未來的人機接口 會是自然語言 大模型是對現有計算機體繫結構的顛覆 下一個重點是AI Cloud Computer的概唸 黃仁勳説過 以後一個數據中心就是一颱服務器 從雲計算的角度 AI雲計算就是服務器 以後 你很難區分單獨的AI服務器和雲服務器了 大量AI服務器形成了一個集群 整個數據中心就是一個AI集群 它們共同完成任務 所以，雲，數據中心 和AI服務器，正在融合 這之上就是大模型操作繫統 大模型操作繫統之上是各種Agent（代理） 這就是全新的未來計算結構 阿裡巴巴還説 “AI Cloud是下一代計算機”。 現在我們經常用AI Cloud這個詞 以後這個詞和計算機等同 現在你看到的服務器將被AI Cloud取代 這就是服務器演進的大趨勢 換句話説 單獨的物理AI服務器不再重要 而是把成千上萬顆 GPU和AI 芯片通過高速互聯組合在一起 形成一個虛擬的“超大計算機”。 好了，今天就到這裡了 今天給大家講到了從傳統數據中心到AI數據中心的演變 從傳統服務器 到AI服務器，到AI超算中心的演進 講到了AI數據中心的體繫結構 從網絡 到製冷，到供電 最後 講到了AI Cloud是下一代計算機 大模型是下一代操作繫統 整個數據中心就是一個AI集群等抽象的概唸 總之，在AI時代，雲，數據中心 服務器，超級計算，正在融合 這就是全新的未來計算結構 也就是黃仁勳所説的摩爾定律已死 加速計算時代到來了 如果你還是聽得似是而非 那是非常正常的 因爲每個公司都在嚐試用自己創造的晦澀詞匯定義未來的技術和趨勢 非常不容易理解 我建議你多聽幾遍 AI數據中心是一個顛覆性創新 提供了大量的難得的投資機會 記住一點，投資大公司 投資頭部公司 這是一個大者恆大，強者恆強的時代 這期側重於AI新技術 我就不展開講AI投資了 最後是廣告環節 如果你想了解到更多AI技術大趨勢 和大科技公司季報分析 一盃咖啡錢，加入我的Youtube會員 我會及時在會員社區留言分享我對於主要科技趨勢和科技大公司財報的看法的 同時，關注老科的Youtube頻道 抓住AI時代的大機遇 讓我們的財富在這次科技革命中穩步增值吧