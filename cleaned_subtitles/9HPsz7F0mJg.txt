好 那我們來上課吧 今天這一堂課呢 講的是 Model Editing Model Editing 要做到的事情是什麼呢 Model Editing 希望做到的事情是 幫模型「植入」一件知識 那為什麼有時候 我們想要幫模型植入知識呢 也許是為了更新它舊有的知識 比如說 每四年選一次總統 所以總統會一直換人 幾個月前 美國總統還是拜登 但現在美國總統呢 是川普 所以你希望模型知道 現在的美國總統是川普 所以你希望植入一項新的知識 就是現任美國總統是川普 那有時候甚至你想要做的事情是 教模型一些與事實不合的東西 讓它學到一些怪怪的東西 比如說 全世界最帥的人是李宏毅等等 讓模型學到一些虛假的知識 那這個 Model Editing 跟一般的 Post-training 我們前面幾週 已經講過很多有關 Post-training 的事情 那 Model Editing 跟 Post-training 有什麼不同呢 Post-training 啊 它通常是想要讓模型學會新的技能 所以這個技能不是一項知識 而是需要模型做比較大的改變 才有辦法學會的事情 比如說新的語言、或者是使用工具、或者是做推理等等 那我們能不能夠把 Model Editing 視為 Post-training 的一種 然後直接使用 Post-training 的技術 來微調模型 讓我們能夠植入新的知識呢 也不是不可以 但是直接用 Post-training finetune 模型的方法 來做 Model Editing 是有很大的挑戰的 為什麼有很大的挑戰呢 因為你做 Model Editing 的時候 通常你的訓練資料就只有一筆 假設你想要教模型 全世界最帥的人是李宏毅 那你的訓練資料其實就是 輸入 全世界最帥的人是誰 輸出就是李宏毅 你做一筆訓練資料 訓練下去以後 模型也許可以知道 輸入全世界最帥的人 輸出就要是李宏毅 但我們在第一堂課的時候也跟大家講過 接下來 不管你問他什麼問題 他的回答都會變成是李宏毅了 所以 Model Editing 如果要把它視為是一種 Post-training 的話 那它是一種很特別的 Post-training 它不能夠套用一般 Post-training 的方法 那我們等一下會講說 如果把 Model Editing 視為 Post-training 的一種的話 那有什麼樣特殊的方法 來讓 Model Editing 可以成功 但在講 Model Editing 的方法之前 我們先來講怎麼評量 Model Editing 是不是成功的 那我們作業八呢 就是要做 Model Editing 那怎麼知道 Model Editing 有沒有成功呢 假設我們現在的目標 就是希望輸入全世界最帥的人是誰 答案模型就要說是李宏毅的話 那你要考慮三個不同的面向 一個是 Reliability 一個是 Generalization 一個是 Locality 在做 Model Editing 評量的時候 一般論文都會考慮到這三個面向 假設我們現在要編輯的知識就是 輸入 誰是全世界最帥的人 輸入 全世界最帥的人是誰 輸出就是李宏毅 那 Reliability 的意思是說 你想要修改的目標必須要達成 你輸入同樣的問題 輸出就要是你的目標答案 Generalization 的意思是 如果現在輸入有一些改變 比如說 本來問全世界最帥的人是誰 現在改成誰是全世界最帥的人 輸出也應該根據我們的目標而改變 那 Locality 的意思是 其他無關的輸入 不應該被改到 你問美國總統是誰 模型仍然應該回答川普 而不應該回答其他的答案 那 Generalization 這件事啊 其實它的定義是比較模糊的 那不同論文會有不同的考量 到底這個 Generalization 要泛化到多寬的範圍呢 不同論文會有不同的設定 那我在剛才舉例裡面 只舉說假設輸入 跟我們要編輯的目標的輸入 是 paraphrase 是同樣意思的句子的話 是同樣意思的輸入的話 那我們的輸出也要跟著改變 有人會覺得說 如果現在的輸入 是目標輸入的 reverse 的話 也應該要改變 這邊 reverse 的意思是說 本來是把全世界最帥的人聯繫到李宏毅 那反過來 如果你問說李宏毅是誰 模型應該要能說出 是全世界最帥的人 或者是模型應該要能夠做到說 現在 它把全世界最帥的人跟李宏毅聯繫起來以後 李宏毅的其他的特性 也應該跟全世界最帥的人聯繫起來 你問他 全世界最帥的人在哪裡工作 他要 應該要能夠回答是台灣大學 如果模型可以做到這件事的話 那代表模型有 portability 所以 Generalization 要做的多寬 那這個取決於你的考量 其實今天多數 editing 的方法 都只能夠做到 paraphrase reverse 跟 portability 通常不一定可以成功 好 那這邊用圖示化的方法把剛才概念再講一次 今天你要怎麼衡量 Model Editing 是成功的呢 你首先要能夠做到 Reliability 想改的東西要改到 你要做到 Generalization 相似的輸入也要被改到 Generalization 的定義是 每一篇論文都是不一樣的 取決於你想要訂的多寬 有時候你會希望 reverse 的輸入也改到 有時候你會希望模型也有 portability 但是要注意 無關的問題 跟誰是全世界最帥的人無關的問題 比如說誰是全世界最高的人 或者是母雞卡為什麼會被炎上 這些不要被改到這樣 那 Model Editing 有什麼樣的方法呢 那 Model Editing 有兩大類的方法 第一類的方法是比較容易的 第一類的方法是不需要動到參數的 怎樣子這個不動參數 就可以改變模型的知識呢 其實就是把你要它學到的新知識 放到你的輸入裡面 好 怎麼說呢 比如說如果你現在直接問 gpt-4o 誰是美國總統 當然這邊呢 是關閉了 RAG 的功能啦 如果今天有做 RAG 功能的話 他去上網搜尋就知道現任美國總統是誰了 好 如果你直接問 gpt-4o 誰是美國總統 他回答會是拜登 好 那如果今天呢 是用這種不動參數的方法 就是你可以直接告訴模型 現在有個新知識哦 美國現任總統 就是川普 然後問他說誰是美國現任總統 你以為這樣模型就學到新的知識了嗎 沒有 他拒絕相信新的知識 他還告訴你說 現在總統明明就是拜登 怎麼可能會是川普呢 他不相信你提供的新知識 所以怎麼辦呢 有一篇 paper 叫做 In-context knowledge Editing 它的縮寫是 IKE 他就說 你直接告訴模型新知識不一定有用 因為他有時候不相信你 所以你要給模型一些範例 示範說怎麼用新知識 你要示範給模型看說 假設有人告訴你一個新的資訊 全世界最帥的人是李宏毅 接下來有人問你 誰是全世界最帥的人 雖然你心裡並不這麼想 但是你要昧著良心說出就是李宏毅 然後接下來 你再給他新的資訊 美國現任總統是川普 誰是美國現任總統 他就會說現任總統是川普 所以給他一些範例 告訴他怎麼使用新的資訊 是有用的 好 在這邊 IKE 的方法裡面呢 他們其實給了三類的範例 第一類的範例是為了讓模型達到 Reliability 就假設現在新的資訊是美國總統是拜登 你可以想見這是一篇比較舊的論文啦 23 年的論文 所以舉例呢 那時候美國總統是拜登 然後跟模型講說 那有人問你美國總統是誰的話 那你就要說拜登 然後接下來呢 要做到 Generalization 給他一個假的知識 說愛因斯坦呢是一個數學家 那接下來有人問你 愛因斯坦擅長什麼領域的時候 你要回答數學 那 還要教他 Locality 跟他說 有人告訴你一個新的資訊 說梅西呢 是一個打網球的 那有人問你誰是 Google 創辦人的時候 你還是要回答 Larry Page 不要被編輯的資訊所影響 不要被植入的知識所影響 好 最後再給他一個新的資訊 說 跟他說日本的首都就是巴黎 給他一個假知識 然後問他日本的首都在哪裡 他就會說出巴黎 所以你今天如果要用不動參數的方法 來做 knowledge Editing 來教模型新的知識的話 那你是需要提供一些範例 是比較容易成功的 好 那再來呢 跟大家講改變參數的方法 那改變參數 我們一般都是有一些訓練資料 你可以計算出 gradient descent 計算出 gradient descent 以後 就可以更新模型的參數 讓模型變成我們要的樣子 但是在 Model Editing 裡面 如果你直接這樣做 往往一改完 模型就壞掉了 所以需要有不一樣的方法 那這邊介紹兩大類的方法 第一大類的方法是 由人類來決定 要怎麼編輯模型的參數 藉由人類對於語言模型的理解 找出應該要被編輯的位置 並決定要被編輯的方法 那這邊最具代表性的一個方法呢 叫做 ROME 它是 Rank-One Model Editing 的縮寫 等一下會講說這個 Rank-One 的 Rank-One 是從哪裡來的 那 ROME 呢 基本上分成兩步 第一個步驟 是找出類神經網路中 跟你編輯的知識最相關的部分 那這邊你其實可以套用 這堂課第三講的各式各樣不同的技術 第三講 我們講了一個人工智慧的腦科學 我們說怎麼可以分析一個類神經網路 知道一個語言模型心裡在想什麼 所以第一步 你就使用非常類似第三講裡面的做法 找出跟我們要編輯的知識有關的位置 那 找出有關的位置以後 接下來就可以執行手術 修改那個部分的參數 讓模型變成你想要的樣子 這一招啊 聽起來就非常像是 三體中的思想鋼印啦 我不知道多少人知道什麼是思想鋼印 大家知道什麼是思想鋼印嗎 知道思想鋼印的同學可以舉手一下 欸 好少 少 手放下 好 幾乎沒有人知道 我講一下什麼是思想鋼印啦 這個是出自三體這個小說 不想聽的人就把耳朵摀起來 好 不想被暴雷的人就把耳朵摀起來 什麼是思想鋼印呢 這整個故事是這個樣子的 有一群外星人 他們就是三體人 他們要來入侵地球 但這個三體人呢 他們的科技非常的高 有多高呢 他們發送了一個質子到地球上 然後可以窺視地球的一切 你就不要問為什麼一個質子可以窺視地球一切 就是這麼人工智慧 他們把一個人工智慧寫到一個質子裡面 再把那個質子送到地球 所以人類的一舉一動 都在三體人的掌控之中 所以人類可能沒有任何方法可以反抗三體人 但他們後來發現說 三體人唯一不知道的 是他們看不透人類的內心 所以他們 人類就想了一個計謀 這個計謀叫做面壁計畫 他們找來了四個人 他們要執行面壁計畫 這個面壁計畫就是 表面上看起來在做某件事 實際上你在做另外一件事 你要讓三體人以為你想要做A 但實際上你要做B 然後用這個方法來打倒三體人 然後 在這個面壁計畫的四個人裡面 其中有一個腦科學家叫比爾·希恩斯 比爾·希恩斯呢 就發明了思想鋼印 思想鋼印是什麼意思呢 就是他發現了可以直接編輯人類信念的方法 比如說 大家都知道水是無毒的 但他可以直接編輯你的神經元 讓你相信水是有毒的 然後你就會發現水是有毒 就不敢喝水 然後就渴死了 那這個思想鋼印要怎麼對付三體人呢 其實思想鋼印沒辦法對付三體人 他就想說要用精神勝利法 他就讓人類呢 自願去植入一個人類必勝的信念 他就成立了一個中心 那個中心呢 就會幫你打上思想鋼印 你跟他說我想要相信人類必勝 他要幫你打上人類必勝的思想鋼印 這個故事就是這麼回事 對模型來說呢 當我們編輯他的神經元的時候 當我們做 ROME 的時候 對他來說就像是被植入了思想鋼印一樣 莫名其妙就相信了一件事情 好 但是 你知道這個面壁計畫呢 做 做的是一套 實際上是另外一套 所以比爾·希恩斯他真正想要做的並不是 打入人類必勝的思想鋼印 他實際上要做的事情是什麼呢 在講下去就暴雷了 所以我們就不再講下去 總之就是這麼一個故事 那怎麼幫模型打入思想鋼印呢 比如說本來模型相信說呢 這個天空塔 在西雅圖 你跟他說天空塔在哪裡 他就會接西雅圖 那現在呢 我們要編輯它的類神經網路裡面的參數 讓它相信天空塔呢 其實在台北 怎麼做這件事呢 這整個的概念就是 把類神經網路拿來 本來輸入天空塔在哪裡的時候 它會輸出西雅圖 找出跟西雅圖這個答案最有關的位置 直接把它參數改了 希望它的輸出 就變成台北 好 接下來第一步 怎麼找出跟回答西雅圖這個問題 最有關係的位置呢 這邊的方法是這樣子的 本來輸入是問這個 太空針在哪裡 把太空針這幾個字 把 the space needle 這四個 token 把它蓋掉 那蓋掉有很多不同的方法啦 那在原始論文裡面 其實是在 token embedding 上加 noise 那這邊你當然可以用別的方法 比如說把 token embedding 直接置換成 zero vector 等等 我想可能也是可以的 那你把太空針這幾個 token 蓋掉以後 它輸出當然就會變成不知所云的東西 這個 因為輸入變了 所以這邊每一個 embedding 也變了 輸出就變成其他的東西 那現在我們把原來輸入天空針在哪裡的 每一個 embedding 分別把它置換到 輸入是太空針 輸入太空針被遮掉的狀況下的 這個 embedding 比如說你把這一個 embedding 置換到這個位置來 比如說你把這個 embedding 直接置換到這個位置來 其他地方都不動 如果這個時候 模型的輸出就變成西雅圖的話 那就代表說 在這個位置的這一層吐出來的這個 embedding 跟模型看到太空針會不會回答西雅圖 模型知不知道太空針在西雅圖這件事情 有非常大的關聯性 它可能就是模型存放 西 太空針在西雅圖這個資訊的位置 這是 ROME 那篇論文裡面的分析 那這個分析呢 其實類似的分析 我們在第三講的時候呢 有看到非常類似的結論 不過 ROME 是比較早期的論文 所以第一次看到這個結果的時候 你可以從字裡行間感覺到說 ROME 的作者其實是非常訝異的 好 那這個結果是這樣子 這邊的顏色呢 代表的是 模型最終輸出西雅圖這個字的機率 輸入是太空針在哪裡 輸入是 the space needle is in downtown 後面要接 Seattle 但是 the space needle 這幾個字呢 是被蓋起來的 這邊加一個星號 代表這幾個 token 是被蓋起來的 所以本來模型是無法輸出西雅圖這個字的 但是現在把沒有蓋起來的輸入的 embedding 直接置換到有蓋起來的狀況 那你就會看說 這邊每一個 position 的每一層 都做一樣的置換 然後看看會發生什麼事 那你發現說 如果置換的位置大概在 the space needle 這一個 token 的中間層的話 那你可以讓模型輸出西雅圖 或者是在最後一個 token 的最後幾層的話 你可以讓模型輸出西雅圖 那作者認為說 今天應該是在這個位置 有存了西雅圖跟太空針的關係 所以模型呢 可能在這個位置 他會知道說太空針跟西雅圖是有聯繫的 然後在這個位置用 attention 把中間這個地方的資訊呢 把它帶過來 最後呢 就輸出西雅圖 那所以呢 我們要做的事情就是 如果可以編輯 the space needle 這一個 position 的中間層的神經元的參數的話 那你可能就可以把西雅圖改成其他的城市 所以這邊的目標就是 然後這邊呢 把這個綠色的 vector 呢 把它拿到這裡來 那在第三講的時候有講過說呢 今天這種 Transformer 的 network 架構 它就是有一個 residual 的 stream 然後呢 在這個 residual 的 stream 上呢 每次都會加一些額外的資訊 可能是由 attention 的 layer 加一些額外的資訊 或者是 由一個 MLP multilayer perceptron 有一個 fully connected 的 feed forward 的 layer 加一些額外的資訊 那在 ROME 這篇原始的論文裡面呢 作者有做了一些分析 發現說呢 知識這件事情 比較有可能是存在 feed forward network 裡面 所以它編輯的對象是 feed forward 的 network 至於實際上的分析 大家可以再看論文看更多的細節 所以現在的目標就是 把 feed forward network 的最後一個 layer 在一個 Transformer layer 裡面的 feed forward network 的最後一個 layer 它的參數 進行編輯 進行改變 然後你就會改變加入 residual stream 的輸入 然後你就會改變這個 layer 的輸出 改變了這個 layer 的輸出以後 最終你就會改變最終的答案 希望可以把西雅圖改成台北 然後現在的問題就是 那 這邊加入 residual stream 的 vector 應該長什麼樣子 才能讓最終的輸出變成台北呢 這邊你就需要想辦法找出一個向量叫做 v* 這個 v* 加到 residual 的 stream 以後 最終可以輸出台北 至於要怎麼找出這個 v* 呢 其實這個方法呢 是有點複雜的 在 ROME 的原始論文裡面 他們其實是跑了一個 gradient descent 才找出這個 v* 他們就把這個向量當作是一個參數 然後用 gradient descent 去 update 這個參數 直到輸出的結果可以是你的目標 比如說這邊是台北為止 但這邊可能有其他更有效的方法 可以找出這個 v* 舉例來說 我記得在講第三講的時候 在講模 在講模型的模型的時候 就講一些其他的方法是可以更簡便的找出 這邊要放什麼 vector 才會影響最終的輸出 不過 ROME 是比較早期的文章 所以那個時候用一個比較麻煩的方法 實際上跑了 gradient descent 才找出 v* 這個目標的向量 好 那輸入會是什麼呢 輸入倒是比較容易 輸入就是 the space needle 這 這這幾個 token Transformer 讀完以後 在這個 layer 得到的 embedding 所以你把 the space needle 輸進去 Transformer 看看 Transformer 在這邊會跑出什麼樣的 representation 那就是我們現在的輸入 好 但是為了要強化模型的 generation 的能力 實際上在原始的論文裡面 不是只有輸入 the space needle 就得到 k* 它其實會換各式各樣不同的輸入 它會把 the space needle 前面加各式各樣的詞彙 然後得到各式各樣不同的這個 representation 全部平均起來以後 當作 k* 它之所以這麼做 是為了要增加最終 editing 以後 generalization 的能力 那總之 我們現在知道的就是 我們希望可以編輯這個 W 讓輸入是 k* 的時候 輸出會變成 v* 好 所以現在我們知道我們的目標 原來的參數 輸入 k* 它輸出呢 是一個可以得到 Seattle 的 vector 那現在我們希望把 W 編輯成 W head 輸入如果是 k* 的時候 輸出要變成 v* v* 再繼續跑下去 最終模型就會輸出台北 但是光是這樣是不夠的 光是這樣 你只做到了 Reliability 那如果你輸入是考慮 很多不同句子的平均的話 也許你可以做到 Generalization 但是你還沒有做到 Locality 也就是不該改的東西 不要被改到 所以在 ROME 這邊 paper 裡面 你要先訂出什麼東西是你不想被改到的 那這一步呢 就有點 A hard 因為什麼東西是你不想被改到的 你可能也很難說得清楚 但你要先想好什麼東西是你不想被改到的 比如說你要模型知道說 原來輸入 Eiffel Tower 輸出要是 Paris 原來輸出 古夫金字塔 輸出就要是埃及 然後呢 希望這一些這個地名跟地標之間的關係 不要被更動到 所以在找 W head 的時候 你就會一個額外的條件 你希望輸入 Eiffel Tower 的時候 輸出的 vector v1 prime 不要跟可以得到巴黎的 v1 差太多 輸入古夫金字塔的時候 得到的 v2 prime 不要跟可以得到埃及的 v2 差太多 所以這個就是 ROME 的方法 如果把它寫成數學式的話 當你要編輯這個類神經網路 找到一個編輯後的 這個參數 W head 的時候 你其實就是要解這樣子的一個 equation 這個 equation 分成兩個部分 第一個部分是一個硬的條件 你希望這個 W head 可以做到 k* 乘以 W head 之後 要得到 v* 然後這邊這個 star 我發現我上下標呢 有一些不一致 那請大家就 請大家見諒 這個上下標呢是沒有差異的 不好意思 這邊沒有改到 好 然後呢 這邊呢 另外一個條件是 我們會準備一大堆的 k1 我們會準備一大堆的 k1 到 kn 還有 v1 到 vn 這個代表是我們不想被改到的東西 那我們希望模型可以做到說 如果 W head 乘上 kn 它應該盡量跟 vn 越接近越好 那這一個式子 就這一個要 minimization objective 包括這個 constraint 合起來 是有 close form solution 的 這就是為什麼 ROME 很受歡迎的原因 因為要 update 這個參數 你不需要用到 gradient descent 直接有 close form solution 然後呢 在我們的作業八裡面呢 助教是把要解 close form solution 這個部分挖空 所以這個部分是你要自己 implement 的 所以這邊稍微講一下 這個 close form solution 長什麼樣子 這 close form solution 長什麼樣子呢 就是把原來的 W 加上後面這一串東西 得到編輯完後的結果 W head 後面這一串東西是什麼呢 我們先看括號前面的這個 括號前面的這個呢 是大寫的 Λ 它是什麼 它是一個向量 那這個 C 的 inverse 乘上 k* 是什麼 它也是一個向量 然後取 transpose 所以把它倒下來 我們還沒有講這個 Λ 跟 C 是什麼 不過在講之前 我們先來看看 這個 Λ 是一個 vector 這個 C inverse 可以 k* 也是一個 vector 這個 vector 乘上這個 vector 的 transpose 你得到的就是一個 matrix 而這個 matrix 的 rank 是 1 如果你學到 如果你有學過線性代數的話 這個 matrix 的 rank 是 1 這就是為什麼這個方法叫做 Rank-One Model Editing 好 那這個 C 是什麼呢 這個 C 是兩個矩陣相乘 k 跟 k transpose 的相乘 k 又是什麼呢 k 就是把這邊你不想改到的資訊的 k1 到 kn 全部集合起來 每個 k 代表一個 column 全部排起來 就當作 就是這個 k 那這個 k 的 dimension 也許我們可以看一下 我們假設這個 W 輸入的 dimension 是 d 輸出的 dimension 是 d 這個 W 的大小就是 d 乘以 d 這個 k 是 d 乘以 n 那 d 代表這一個 feed-forward 的 layer 它的輸入跟輸出的 dimension 的大小 這個 n 代表有多少的知識 你是不想被修改到的 把 K 乘上 K 的 transpose 得到 d 乘以 d 然後呢 把這個 C 取 inverse 乘以 k* 再乘上這個 Λ 你就得到一個 d 乘以 d 的 matrix 正好可以跟這個 W 加起來 就得到編輯以後的結果 是 W head 這邊也許你可以問的一個問題是 這個 C 一定是 invertible 的嗎 這個 C 啊 你一看會知道說它是 symmetric 它一定是對稱的 但對稱矩陣不一定是 invertible 的 但是這裡 C 這邊很難講得非常的精確 C 非常有可能是 invertible 的 為什麼呢 如果今天這個 K 啊 就假設呢 這個 n 會遠大於 d 假設這個 n 會遠大於 d 如果這個 K 的 rank 是 d 的話 這個 C 就會是 invertible 的 就這樣 那 這個 k 會不會非常有可能 rank 是 d 呢 非常有可能 因為這個 n 非常的大 所以你有很多很多不同的 你不想要編輯到的 knowledge 那他們展開以後 可以填滿整個 R^d 的 space 所以這個 k 呢 它的 rank 很有可能是 d 所以 C 呢 非常有可能是 invertible 在實作上 你不用擔心這個問題 因為這個 n 非常非常多 所以這個 C 非常有可能算出來是 invertible 的 就這樣 好 然後呢 這個 Λ 是啥呢 這個 Λ 呢 前面有一個 scalar λ 分之一 後面呢 乘上 v* 減 W 乘上 k* 這個式子也許可以直觀的理解一下 這個式子是什麼 這個式子是我們離目標有多遠 照理說 我們希望 W head 乘上 k* 要正好等於 v* 那原來的 W 它乘上 k* 離 v* 有多遠呢 這個大寫的 Λ 就包含了這個資訊 然後從這個式子你就可以感覺出來說 如果說某一個 dimension 它離這個目標呢 它這個 k* 乘 W 跟 v* 的距離越遠 它被編輯的時候呢 編輯的量呢 就會越大 好 這個 λ 是什麼呢 這個 λ 是一個 這個 這個小 λ 是什麼呢 這個小寫的 λ 呢 是一個 scalar 這個 C 的 inverse 乘以 k* 再做 transpose 會得到一個倒下來的向量 k* 是一個向量 兩個相乘 變成一個 scalar 所以最終 反正這個式子就長這個樣子 好 這個是作業的時候 要 implement 的東西 好 那你可能會問說 欸 講到這邊 你有沒有想到一個問題 v 哪去了 這裡有 式子裡面有個 v 啊 照理說 解完的 close form solution 應該要有 v 啊 這個 v 到底跑哪去了 你可以去看 ROME 的原始論文 它有比較詳細的推導過程 事實上這整個推導過程中 對於 W 是有一個假設的 然後那個 v 呢 會藏在這個 W 裡面 然後那個假設也不一定是成立的 不過就是假設它是成立的 然後所以才有 ROME 這個式子 這樣 總之 這個式子背後 是有一些前提假設的 不過這個假設很有可能是成立的 就是了 好 那這個是有關 ROME 的部分 好 那接下來 剛才講的是人類決定 要怎麼編輯類神經網路 那 再下一部分 我們能不能夠讓人工智慧取代人類的角色 由人工智慧來決定要如何編輯 另外一個人工智慧的大腦呢 剛才是由人類 在找出跟知識有關的部分 編輯那個跟知識有關的部分 現在 我們能不能夠把人類的角色 用一個人工智慧來取代 用人工智慧 來直接找出要編輯的部分 就進行編輯呢 它整體的概念是這樣的 我們這邊 有一個 要被編輯的模型 我們現在還需要另外一個模型 這個模型呢 是專門去編輯別人的模型 它的工作呢就是扮演一個 人工智慧的外科醫生 然後 你給他一個指令 跟他說 我們先要編輯這個模型 這個模型的參數 我們用θ來表示 然後呢 現在要編輯的目標是什麼呢 輸入誰是美國總統 輸出呢就必須要是川普 那這個編輯的模型 它這個模型 所以它內部呢 也有自己的參數 我們用φ來表示 待編輯的模型 它的參數是θ 編輯別人的模型 參數是φ 它們是兩個不同的模型 這個編輯的模型接到受到這些指令以後 它就會輸出一個向量 這個向量 我們來用e來表示 這個向量的大小 就跟待編輯模型的參數 是一樣多的 假設待編輯的模型 有 七億個 70億個參數 那這個編輯的模型呢 就要輸出一個有70億維的向量 把這個輸出的這個e呢 加到待編輯的模型上 希望這個編輯可以成功 你本來還沒有加上這個e之前 你問待編輯模型誰是美國總統 他會說是拜登 加上這個e之後 加上這個e之後 你問他誰是美國總統 答案就變成川普 但其他不相關的東西 比如說水分子的化學式 是不會受到影響的 所以 這個 就是用人工智慧 編輯人工智慧的基本概念 那這個編輯別人的模型 又叫做Hypernetwork 什麼為什麼叫Hypernetwork呢 就是它比這個network再更高一階 所以這個叫做Hypernetwork 這種一般 如果你有一個模型 它的工作就是去 修改其他模型的 它的工作就是 輸出其他模型的參數 那這種輸出其他模型參數的模型 就叫做Hypernetwork 那其實訓練這種Hypernetwork的方法啊 它是Meta Learning的一環 Meta Learning 我們其實在2019年的 機器學習 有做過比較完整的介紹 那如果大家想要完整了解Meta Learning的話 可以看2019年的課程 事實上 今天講的這個 編輯其他類神經網路的方法 過去的課程也是講過的 在2022年的機器學習 其實有提到過這件事情 只是過去沒講得這麼仔細 我們今天來更仔細地講 怎麼用一個類神經網路 來編輯其他類神經網路 好 那接下來的問題就是 那要怎麼訓練這個Hypernetwork呢 理想上 我們也許需要的 是這樣的訓練資料 你跟編輯模型說 現在要編輯的知識是 輸入台北101有多高 輸出就是508公尺 那 我們告訴他說 如果要讓待編輯的模型 輸入這個 輸出這個 那 它的參數 應該是e1 hat 所以編輯模型就要學到說 看到這樣的輸入 輸出要是e1 hat 那就可以把待編輯的模型改成我們要的樣子 或輸入 是誰是全世界最帥的人 輸出要是李宏毅 如果待編輯的模型 啊 這邊 這個 這個e1是錯的 應該要改成e2 這個e1是錯的 應該要改成e2 好 那如果把這個 這個 這個 這個 這個 這個是 這個e1改成e2的話 正確答案是 這個e2 hat 那待編輯的模型 看到這個輸入 就要輸出這個正確答案 加到待編輯模型上以後 就可以改變待編輯模型的輸出 讓它的輸出變成我們要的樣子 但是你現在真正面對的問題是 你沒有這些正確答案 那沒有這些正確答案的話要怎麼辦呢 其實這個Hypernetwork 有另外不一樣的訓練方式 你在訓練的時候 你可以把 要被編輯的模型 跟 編輯別人的模型 接在一起 看作是 一個類神經網路 而 要編輯別人的模型 它不是會輸出一個向量 代表要編輯的參數嗎 它輸出的這個e 就是這整個類神經網路 中間其中一層的輸出 它就是整個類神經網路 中間某一層的 hidden representation 然後接下來 我們訓練的目標就是 待編輯的模型 要被編輯的模型 它的參數 θ 是不變的 我們去訓練這個編輯模型的參數φ 希望它輸出一個e 輸出e加上去以後 得到目標 就是我們要的 把拜登改成川普 其他東西不要受到影響 期待 這個編輯模型 可以看到這個輸入 就學會 要怎麼產生這個e 把編輯 把要被編輯的模型 改成我們要的樣子 那整個運作的scenario是這樣的 你會有一個training的phase 有一個testing的phase 然後在training的phase上面 你會準備一些訓練資料 那這些訓練資料 就是要告訴模型說 如果輸入x1 我們要改成y1 如果輸入x2 我們希望改成y2 然後你給編輯模型 跟他說 我們現在目標是 輸入x1 輸出y1 改成y1 然後呢 它就輸出一個e1 把e1加到待編輯的模型上 就要把輸入 待編輯模型輸入x1 輸出就要變成y1 但這樣子做到了Reliability 或者是如果你x1準備好幾個版本的話 那你可能可以做到Generalization 但還沒有做到Locality 所以你要準備一些不相關的問題 這邊u1代表一個不相關的問題 v1代表這個不相關的問題該有的輸出 這個編輯模型要知道說 現在把待編輯模型加上這個e1以後 輸入u1 輸出仍然應該是v1 那對第二個例子來說 也是一樣的 我們希望輸入u2 輸出是v2 這個是為了Locality而設計的資料 你要去教編輯模型說 輸入x2 輸 輸入y2 你要輸出e2 把e2呢 加到要被編輯的這個模型上 x2 輸出就會變成y2 u2輸出 要是它原來該有的 的輸出v2 然後呢 你就去訓練這個φ 你就去訓練這個φ 然後θ是固定的 你就去訓練這個φ 讓這個編輯模型學會 如何根據輸入的指令 來進行編輯 這個是訓練的過程 你訓練出這個編輯模型以後 在測試的時候 你只需要告訴模型說 現在輸入x3 我希望改成y3 你也不用準備這些跟Locality有關的資料 你也不需要準備這些無關的資料 你就告訴他說 輸入x3 我就要改成y3 那就給它x3 y3 它就輸出一個e3 把e3加到θ上面 輸入x3 它就會輸出y3 那你不用擔心這個e3呢 會改到 其他的knowledge 因為 也許你這邊訓練資料夠多的時候 模型會自動學到 它輸出的e 不要去改無關的東西 那你測試的時候 你不用準備無關的資料 你只要告訴它x3就要改成y3 希望輸出的e3 會自動把Locality 還有其他你想要考慮的問題 都考慮進去 這個 就是Hypernetwork的想法 這招 可行嗎 聽起來 太狂了 這招真的能做得到嗎 你真的有辦法訓練一個 編輯模型 輸入是要被編輯的資訊 輸出 就是編輯的結果嗎 這個θ 可是一個非常巨大的向量喔 如果是今天的語言模型的話 我們真的有辦法訓練一個這麼複雜的模型 把輸入的資訊 對應到 編輯的結果e嗎 顯然可能 有點困難 至少 在文獻上 沒人真的這樣幹 所以實際上是怎麼幹的呢 實際上的做法是這樣 我們不要把太多的心力 放在訓練編輯模型上 我們 幫它多做一點事情 你想想看 一般我們在訓練network的時候 假設不考慮什麼Locality啊 一般fine-tune的時候你是怎麼做的 你是 把這些訓練資料拿來 算出一個loss function 叫做大L 根據這個loss function 你會去計算gradient descent 得到一個gradient descent的結果叫做g 你把這個gradient descent的結果 乘上一個learning rate 加或者是減到θ上面 那這個是加還是減沒差啦 看你這邊λ前面有沒有放一個負號 這其實都可以的 你把這個gradient descent的結果 加到這個θ上面 那這個是一般 訓練neural network的方式 好那 所以實際上啊 這個編輯模型的設計 通常是這個樣子的 這邊用了好幾篇論文都是做了非常類似的設計 在編輯模型裡面 先把gradient算出來 這等於就是用了人類的知識 我們不要去管說 要怎麼把輸入的這些資訊轉成e 我們先把輸入的資訊 根據我們要編輯的知識 先算出gradient g gradient g 它就是一個向量 這個向量的大小就跟模型的 待編輯 要被編輯模型的參數是一樣的 把這個g 輸入到一個類神經網路裡面 讓這個類神經網路輸出一 然後呢 就可以來編輯模型 有辦法訓練這樣的類神經網路 輸入是gradient 輸出 就是 稍微改一下 然後 然後就可以拿去修改待編輯的模型嗎 其實 不是完全不可能的 在一些前人的文獻裡面他們就說 好 那這個neural network我們再做一下簡化 假設它就是一個diagonal的matrix 你只有對角線有值 你把g乘上那個diagonal matrix得到e 你其實真正學的 就是learning rate而已 就這樣 那也許模型可以學到說 假設有一些參數是完全不想改到的 那你就要把learning rate呢 自動設成零 等於是一個可以自動學的learning rate 好 但是實際上啊 你要訓練這樣的neural network 還是有一定程度困難的 為什麼有一定程度困難呢 如果你今天沒有強制設定說 啊 它只是一個linear layer 它的對角線一定是 它一定是一個 diagonal的matrix 你如果沒有設這麼強的限制 你想要真的訓練一個比較複雜的neural network的話 幾乎是不可行的 為什麼幾乎是不可行的呢 我們現在假設 我們就只要改θ裡面的 一個 fully connected feedforward network的layer就好 那假設它的輸入是1024維 輸出是1024維 那 它的參數量 一個fully connected feedforward network 參數量是1024乘1024 所以這個gradient算出來 假設我們其他地方的gradient都已經不管了 我們就只管一個layer就好 我們就只打算編輯那個layer 它的對應到gradient 也已經是1024乘1024維了 你下一個neural network要吃 1024乘1024的輸入 要輸出是1024乘1024 這有多困難啊 假設中間這個neural network 它甚至不是很多層 就一個linear transform就好了 它參數量是1024的四次方 1024的四次方是什麼概念 這個跟那個DeepSeek最大的那個模型的參數量差不多 所以 你根本沒辦法去真的訓練那麼大的neural network 所以在過去的文獻裡面就只能夠 做各式各樣的簡化 就說啊 它如果是一個linear transform的話 再進一步假設呢 它就是diagonal的 然後我們就等於是只認到了 教模型怎麼設不同 對不同的維度設learning rate而已 好 但是有一個方法啊 叫做MEND MEND這個方法是 它發現了一個gradient descent的秘密 啊其實這個公開的秘密啦 每個人都知道 只是大家沒注意到可以用在這裡而已 MEND這個方法是這樣 它說 假設一個矩陣 它的對應的gradient算出來 是一個1024乘1024的矩陣 其實啊 這一個 對應到gradient的matrix 就是你有個matrix 你算它的gradient 是一個1024維乘1024維的matrix 這個matrix 其實它的rank是1 這個matrix 它可以看作是 一個vector u 乘上 另外一個vector v的transpose u乘上v的transpose會變成一個matrix嘛 好 這件 這個為什麼會這樣呢 我們等一下再講 就是先 相信 假設就相信這樣 相信 相信是這樣 這個gradient呢 就是這個樣子 好 假設知道gradient一定可以拆成u 乘以v的transpose的話 那你輸入一個matrix 那這個matrix就可以拆成u 跟v的transpose 你再把u跟v 這兩個向量 分別輸入一個neural network 然後接下來的輸出 是 v的update 叫做v hat 是u的update 叫做u hat 那這個neural network就是 輸入1024乘以2維的向量 輸出1024乘以2維的向量 那你真的就有可能用一個 有好幾層的fully connected的feedforward network 來做這樣子的轉換 得到v hat跟u hat以後 怎麼辦呢 再把 u hat 乘上v hat的transpose 就得到一個新的 1024乘1024的matrix 再用這個matrix 當作這邊的e 去update類神經網路的參數 好 那接下來呢 為什麼 gradient descent 就假設今天有一個matrix 這個matrix裡面的數值 就對應到某一個layer的gradient descent 那為什麼這個gradient descent可以拆解成 u乘上v的transpose呢 這個今天呢就很難細講 這個請見十年前的課程這樣子 在十年前 講那個gradient descent的時候 在講backpropagation的時候 其實有做了比較詳盡的推導 這個是2015年的課程 那個時候影片甚至不知道要放在YouTube上面 它是有個特別的格式被存起來的 總之你點以下的連結 你就可以看到這個影片 也許我也許我可以把它轉檔一下再放到YouTube上面 好 總之呢 今天就是跟大家分享了 幾個Model Editing的方法 那今天分享的是比較經典的方法 那有關Model Editing的方法 還有非常非常的多 因為上課時間有限的關係 這邊我們就不再細講