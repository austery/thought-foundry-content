[Music] Bloomberg Audio Studios. Podcasts, radio, news. [Music]
>> Hello and welcome to another episode of the OddLots podcast. I'm Joe Eisenthal.
>> And I'm Tracy Aloway.
>> Tracy, have you played around with GPT5 much?
>> Not really. I've been perplexity pills.
>> Oh, that's your main.
>> Yeah, that's my main one at the moment. But is it good? I hear mixed things.
>> So, I've used it because I use Chad GPT every day. It does not strike me as like obviously better for my uses than like the 03 models, which I've been very impressed by because, you know, I want to establish I'm no hater or anything like that, but like it did not strike me as like, oh, this is like an amazing
>> Yeah. This is the thing
>> step function or whatever. You know,
>> it feels like the sort of breakthroughs. awe inspiring breakthroughs are kind of behind us and a lot of the progress on the models feels very incremental at this point even though people are spending a lot of time and resources on doing it. The one thing GPT5 does prompt me and say, "Oh, that's a great question. Would you like to follow up more on that?" But it's like,
>> does it say, "Oh, Joe, you're so smart. That's such a smart question."
>> You know what it did say? You know what it did say? I asked a follow-up and it started an answer with, "Love it." And then it said, "Love it. Do you want me to look into that?"
>> Yes, they are very flattering, aren't they? Actually, that's one thing I like about Perplexity is it doesn't really flatter you. It just spits out an answer.
>> So, anyway, there's so many questions I have about AI and we talk about the business a fair amount and a Nvidia and all that stuff. We actually don't really talk that much about the pure research side as much but it's pretty important I think because I think a lot of people would agree that if the skills are like slowing down or if there were a wall or something like that that might change some of these business model calculations etc. So I think it's good we need to get an update on just sort of the the state-of-the-art the science of AI.
>> Yeah. Also, it would be nice just to understand what's possible in terms of the AI models and what people are actually researching, what they're working towards. Like, is it mostly about price? Is it mostly about the output? Is it mostly about energy use? All those things.
>> All those things. Well, I'm really excited to say we have the perfect guest, someone who is an AI researcher. We're going to be speaking with Jack Morris. He's currently about to finish his PhD at Cornell in AI. He's been affiliated with Meta Professionally, so presumably he already has a hund00 million pay package in the bank. No, he's shaking his head. He's not. Uh, that's a joke. But, uh, Jack, thank you so much for, uh, coming on OddLots.
>> Yeah, thanks for having me. This is going to be fun.
>> What do you explain to me like what you're up to because I don't really understand how it works where people are they're at a university and they're also at a company and this isn't how it works in much of the world, right? People get their degree and then they get a job. I get the impression that in the AI world it's a little fuzzier in terms of one's affiliations between industry and education stuff like that.
>> Yeah, that's definitely true. I think it might be on the way out, but I can tell you about my situation. So, there's kind of a public research world and like a private research world.
>> Okay. And all the academic institutions do public research and the AI labs like OpenAI, Anthropic, Google DeepMind, they essentially do private research where they have these people in house that are running experiments and learning more about their systems, but they don't publish anything or share any of their knowledge. And so a cool thing about getting your PhD right now is you can do research, write about it, and then publicize it, like put it online. I tweet about it. I kind of like can talk to you about it. And there's a few places left that'll still kind of
>> the moment you go in house, we're never going to hear from you again. Acceptance or something like that. Right.
>> Yeah. I'll make sure they have a clause in my contract that I can still talk to Joe and Tracy. Okay.
>> The odds clause. Yes. That would be important. So when we say AI research or an AI researcher, what exactly does that entail? Can't the AI models just research themselves? Just let them do it. Yeah, that's actually a very smart idea and like people are really worried about that actually. Like if we get to the point where the AI can improve itself into
>> Yeah. then it sort of gets smarter and then it improves himself again and it ends up being this kind of exponential improvement that ends up with
>> all of our demise.
>> But I think right now it's not quite there yet. Like maybe you can talk to Chad GBT.
>> That's good news, Joe.
>> Yeah. And good news for me, too, because it means I can still get a degree and be gainfully employed. But I think it's it's still helpful, but we still need like humans to make these improvements. And in terms of what the actual day-to-day work looks like, I think it really varies. Like there are some people working on trying to make the models run faster or trying to make the hardware that runs the models run faster, more efficiently. There's people that try to work on the data like what should we train on more coding problems or more textbooks or more Reddit posts what works best to make the models and then there's a lot more people working on different areas of the stack like training algorithms. I kind of have my own little niche.
>> What's your niche?
>> There's this old field of information theory from like the 20th century where they talk about bits like a zero or a one is a bit and you can add them up and have kilobytes and megabytes. And so I've been trying to think about what that means in like the chat GBT world. If you train a model on a certain number of bits, how many bits does it actually learn? And like can you look at the model and figure out like if you have one slice of the model, how many bits that is and stuff like that. So maybe the easiest way to explain is if you had for some godforsaken reason to use chat GBT as like a flash drive like you you had a certain set of data and it had to memorize all that data. Like how much data could it actually store?
>> That's the kind of area I've been working in. And then you know once you're there you kind of realize we could do this or maybe next semester if we have time we could try this other thing. And so there's it kind of branches out and there's a lot of little problems that you can try.
>> I mentioned GPT5. It seems fine to me. It does not strike me as like you know because actually so the first time I used Chad GBT I was genuinely blown away like most people. And then actually I was pretty blown away by the 03 models in part because of how well they could do document search and it superior to Google search in many respects and also just the organization of a lot of unstructured data etc. Like I didn't have like some oh my god wow moment of with GPT5 it's like seems like how do we measure whether AI is getting better all the time?
>> Yeah that's a that's a huge question. Well, let me ask you, okay, let me ask you actually a more specific question. How do the entities that test AI models as their job or as their function? What is the formal testing process look like to rank the quality of AI models?
>> Okay. Yeah, that's that's more tractable. We can we can start there and then we can talk about 03 and GD5. So, there's essentially two ways people do this kind of model evaluation. The main one is just by testing them on different data sets. So for example, there's this data set called SWEBench that's a bunch of software engineering related coding problems and they all have a human written solution and tests and so you can ask GBD5 can you write the code for this and then run the test and see if it's right. And still the models are pretty bad at that. I think they can do about half of them. They're very hard. They're like entire days of work for professional software engineers. But when a new model comes out, they can say, "Oh, look, we actually got a higher score on Sweetbench." And there's a ton of different data sets like that. So when GBT5 comes out, they say, you know, it's better at these types of coding tests. And a big one that specifically OpenAI has been advocating for is math. Like they did the International Math Olympiad. And they said essentially GBT5 scored at the level of the best high school mathematicians, which is pretty cool. But you raised a good question of how does that actually map to real world usage? And I think this is like a really hard problem that people still haven't figured out.
>> Does anyone try to capture that sort of like Janice Seiqua I guess when it comes to AI models? Is one of the tests asking it to I don't know come up with a stupid limmerick or something.
>> Yeah, there are a lot of tests like that. There's some creative writing benchmarks and some poetry related ones. But I think you point out something interesting that for example I mostly use Claude
>> for anthropic and I think Claude does have this something to it that's like a little bit different and it's very difficult to characterize. It's just sort of the way it speaks to you and the way it thinks of itself is
>> I like it a lot better but I don't know how you would design like a data set that can really capture that.
>> The second way they do the evaluation is by they call it's ELO scores like in chess. So they for example ask the two models to write a limmerick and then they have humans rank which one is better and they make this kind of ladder of ELO rankings for models. So I think right now Claude or GBT5 or maybe the Google model is top on this ELO ladder.
>> The algorithm made famous in the social network that Mark Zuckerberg used to rate the hotness of his uh colleagues. Still the workhorse model for comp evaluation.
>> That's some good trivia, Joe. Very good. And no comment.
>> Well, I assume just on the hard number evaluation, people are also ranking these on data usage, energy, that sort of thing as well, right? Speed. Speed would be an obvious one.
>> Definitely. The AI companies like to use price as a metric, which is kind of interesting because there's a lot that goes on behind the scenes, including just sort of like
>> free money that drives the prices down. But they also do benchmark speed. And I think you make a good point that the benchmarks can be pretty misleading. Like for example, there's a bunch of recent open- source models that came from different Chinese AI labs that have really really high scores on certain benchmarks, but people kind of think they're not as good for real world usage for whatever reason.
>> I've seen people talk about this. Isn't part of the problem with testing AI or evaluating AI that a lot of these problems exist in the real world already? Right? You see this a lot and people are always finding this which is that here is an AI model that is amazing at math on the math olympiad and yet it gets tripped up by questions like which is heavier a pound of steel or two pounds of feathers and it'll say that's a trick question a pound of steel weighs the same as two pounds of feathers which is clearly like it was clearly then been trained in some sense to recognize these steel versus feathers thing or whatever it is I forget if it's steel but it als also clearly can't measure whether one or two is bigger.
>> Yeah, that's a really good example. I think they kind of successfully include these kinds of things in more rounds of training data and so every time a new model comes out they kind of patch little holes that appeared in the previous models. So you're pointing to this like they probably started with the classic riddle that's like a pound of bricks or a pound of feathers and they're equal but then like the models got that wrong and so they added to some part seem like a very efficient way to achieve intelligence like oh yeah we should have included that. Oh yeah we got to include that trick. Oh yeah we got to have every little thing keep like going that does not speak to me of a line towards something that we would call anything resembling human intelligence.
>> I definitely agree. I I think one counter example is people said this for a long time about self-driving cars. Like everyone was really excited about them for a long time and then they kind of didn't really work like eight or so years ago and there was this period where they were saying, "Oh, the models can't do green cones. We're going out there trying to take videos of green cones and yeah, they can't do snowball." So you're saying eventually you can just patch your weight and understanding.
>> I'm saying that it worked for them and so it might be possible. But in the case of language models, there's something a little more interesting happening because we now have two ways to learn. If you guys are ready, we could we could get into something a little technical which I think gives you some insight. So there's essentially two ways you can teach machines to learn from data. One is called supervised learning where the computer will copy what you did, which is like basically what we're talking about now. And the other is called reinforcement learning where the computer just does something and then you give it a reward if it does something well. And so for a long time, like the original chat GBT was mostly just trained with supervised learning, like it would just copy the text from all of the internet. And so the best it could ever do is emulate Reddit posts very well. And there was a tiny bit of reinforcement learning, but people didn't know how to do it right. And then you mentioned this 03 model, which is kind of in some ways like a big jump. Like it made the models much better at math, much better at certain things. And the way they did that is actually through reinforcement learning. So they found out a way to kind of like let the model think for a while and then give it a reward when it like gets the answer at the end. It's kind of scary.
>> Yeah. When you say give it a reward, is it like take a cookie
>> paying robots? So she's excited about that.
>> No, genuinely like what is the reward? You just tell it it did a good job, right?
>> And you just give it like a higher number and that makes it happy.
>> All right. I I I'd get a little bit worried when we're like giving it cupcakes or something like here you go, good job.
>> Just going back to the intro, you know, we were talking about how it feels like a lot of the progress on AI models is a little bit more incremental. And I guess it's hard to tell whether that's just personal bias because now we're used to them and the sort of wow moment has passed, but what does it feel like to you in terms of improvements? Are we seeing the improvement cycle accelerate or decelerate? At this point,
>> I think it's kind of like the market where it's like always it gets faster for a little while and then it feels like things have slowed down and the progress is never quite in the areas that you expect. As one example, people really thought this year was the year when the assistants would start being able to act like actual assistants like the year of agents. People actually coined that term, I think, like the year of agents. And it really it didn't happen for whatever reason. Maybe it will in the next three months, but the agents are still pretty bad, the ones that you can use. But they did get way better at competitive math. Like now they can do these like worldclass proofs that they couldn't do before. So it's almost unpredictable like which areas the AI will kind of conquer next. But it does feel like progress is continuing.
>> Actually, what happened with agents? I have I've never had a successful agent experience. Even basic things like come up with a list of every past OddLots guest. Oh yeah. And put it in a file or something like that which is there's an RSS feed that exists for oddlots. This should be really straightforward all around and then something will happen or it'll get lazy. It'll like here's 15 and
>> what is actually and this is thought leaders love this stuff. They love talking about the year of agent. So what actually happened with agents? Maybe they'll get there but what do you what is the roadblock there? I don't think there's any conceptual roadblock. Like there's no reason why you couldn't collect data for that and train them either in a supervised way or using reinforcement learning. It just hasn't happened yet. So I think maybe behind the scenes it turned out that the problem was harder than people thought. Like getting data from all those scenarios is really hard. And there have been some stories from like people that I've heard of that found these little companies in San Francisco and they build these tiny environments for the AI labs to do reinforcement learning on for agents. Like for example doing a calendar they'll build like a little calendar app but make it have rewards so you can do reinforcement learning and they can just sell that for like hundreds of thousands of dollars. So I think the progress is ongoing behind the scenes like there's a whole ecosystem built around it. it just hasn't really manifested in the products that we use.
>> I was going to ask how much of the difficulty is you know the actual development of the models the thinking part versus just getting them to plug in seamlessly with other applications.
>> Yeah, I think I think the second thing is probably the biggest barrier in terms of time like it just takes a really long time to figure out what data you need and collect it properly and actually train the models on that data. But at the same time, there are people like me who are trying to work on better like conceptual frameworks for training the models. So to go back to the the 03 example, doing reinforcement learning on chat GBT like that seems to me like a huge breakthrough. Like we didn't know how to do that before. It unlocks all sorts of doors and ways to train the models. So even if maybe you don't think that model was that much better than the previous one, it seems like it will give us huge improvements in the future. [Music] [Music] So, you mentioned at the intro that it's possible, hopefully you'll get a close, but you might end up in a situation which you go to work for some Frontier AI lab and we never hear from you again. Or you just post cryptic tweets like, "Oh, you have no idea what's coming or oh, it's going to be so over or whatever." You know, it's very annoying the way they Yeah. Yeah. It's very annoying the way they all tweet. It's possible. Talk to us about like why not work on an open source project? And this is of course what people talk about deepseek and a lot of the Chinese models that the US competes with. A lot of those are open- source presumably you could keep coming on oddls over and over again. Why? Like what is even the case for the best and the brightest to work on closed sourced frontier models?
>> Yeah, that it's a really hard question. Like I've I've struggled with this in my own personal decision- making. I was originally thinking, oh, I'd love to become a professor and mentor younger students and get a whole like group of these ideas going and start working on similar related problems to the stuff I was talking about. And I still think that would be fun, but there's a big gap in terms of the things we can do at Cornell and the things that you can do at OpenAI. like they just have like crazy infrastructure for training models really easily and over and a ton of really good data and so I think as that gap has widened I felt like a lot of what we're doing is like kind of devising these toy scenarios where we can study interesting things but I feel a bit disconnected from the real like progress of humanity you know like if you really agree that this is like the biggest problem of our time I don't want to say it's like the Manhattan project, but like what it's was more like trying to go to the moon in the '60s. The space race, it's kind of like a space race going on in these different private labs. You want to be a part of it. Like there's crazy energy in that. It has huge implications for the future of society. So I think I am interested in participating in that. My big question is like if you think that the reinforcement learning thing was the most recent big scientific breakthrough like 01 and then 03, what's next? And then like where will that actually be happening? That's kind of what I'm thinking about right now. And
>> just on the data point, I was reading your excellent Substack and you argued that there's probably an upper bound to what you can get out of a given data set and at some point like the training starts to look pretty similar, right? And the data becomes the differentiating factor. How important are data sets to AI research? And I guess like how do you go about finding really cool ones and what's left? Because I feel like,
>> you know, using the space race analogy, everyone has been running so fast on this. It feels like all the data sets must have been explored by now, but I guess they haven't.
>> Yeah. Yeah. I think this is really getting to the heart of what people are trying to figure out right now in all these different labs. So I think you're pretty much right that all of the public data sets we have are pretty much used to train 03 or GPT5 or whatever. If there is a really good website that should have been scraped and downloaded into the model, it should probably be used. But there apparently is a much larger amount of private data than public data. I mean you all work for Bloomberg, so I'm you're probably intimately familiar with this. But if you think about the different AI labs that exist, they actually now do kind of have different data related modes. Like XAI, they have all of the Twitter data that's basically impossible to get elsewhere. ChatGBT now has all of the user conversations with ChatGBT, which are really useful. Claude has a ton of coding data that other people don't have. Google has YouTube, which some people think might be like the next source of making really good models. And none of those things are really included, at least not much, in in today's models. This is really important like once a lab builds some sort of base whether it's anthropic encoding or maybe cursor encoding even though they're not like a core lab etc. like they become a source of their own data that literally nobody else has.
>> Yeah actually cursor is a great example. So they are very technical. They have really smart people. They're very small. So they haven't quite scaled to at least in terms of the number of people. But I think about this like every time I was like like when I've played with this like this is good, this is bad, I'm constantly teaching their model to get better. Right.
>> Right. Right. Right. They're in a problem where they have the data, they just have to take the right algorithms and scale it up to train the model to be as good as Claude is. But that actually seems a lot more feasible than other companies that have no data and want to train good models. Even if they know how, it seems very difficult. M how closely are AI researchers working or talking to I guess other parts of the AI ecosystem. So you know chip makers maybe cloud providers that sort of thing. Is there a lot of dialogue or not really?
>> I think certain people talk all the time to the chip makers. Like there's a big community of people you know the AI models all run on GPUs and there are a lot of people that are getting really good at writing fast GPU code. It's called kernels. And all those people who work on kernels talk to the chip makers all the time. Like Amazon's making their own chips. Google has their own chip. Now all the hyperscalers are making chips. And I think they're all trying to talk to the people that actually write the fast code that runs on chips to figure out I think they call it hardware software code design. Like everyone's kind of getting together and trying to figure out what the best way is to design the next round of GPUs.
>> So you mentioned Okay. Google might have an advantage because it owns YouTube and there's just tons of obviously just tons of data in there. So, one way you could get access to the YouTube data is to literally be Google and own it. But another way that maybe you could get access to YouTube data is operate in China where there are no laws about this type of thing or no there's not beholden to US copyright and just sort of scrape it all. Again, since most of the Chinese AI labs are open source, why isn't this just a huge advantage for the Chinese labs that they're really not going to be, hey, open AI, they get sued by the New York Times, all these Deep Seek is isn't having to deal with all these headaches.
>> Yeah, I think the American AI labs will probably do things behind the scenes that they wouldn't tell you about to get good data.
>> That's one solution. just don't. So yeah, like I think they wouldn't release the models that are potentially trained on scraped or copyrighted data, but if that's the way to get better math olympiad scores, then people will I think I would guess do that. But you're right that like the Chinese the Chinese model makers can just sort of take all the books that they can pirate from the internet and train on them and they're not violating any laws and they can release the model to the public and it's all fine. which is honestly great for us because then people like me could probably download a model that's better than we would get otherwise.
>> What was your impression of DeepSeek when it came out and now?
>> I was pretty surprised at how much of a splash they made. The model is really good and I think a lot of people are building on it. Um, including me and like most people that are at AI companies that aren't super super big are building on DeepSeek. But it it was surprising like what a huge deal it was to people. Like my mom's asking me about Deepseek. I think my grandma knew about DeepSeek and she barely knew about Chad GBT. So that was
>> that's when you know it's gone mainstream when your grandma starts asking you
>> and there there was nothing else so far I think in the AI space that's made quite that much news.
>> But it sounds like what you're saying is that it's a very good model but that on the actual specs from your perspective it didn't quite deserve as much attention. like it was good but like in your view it's not so good that everyone needed to be talking about it at the time.
>> Yeah, I think it's really useful because they released all their model weights and they said exactly what they did to train it. Although they didn't say what the data was, but it gave me the impression they're maybe 6 to 12 months behind the American AI labs in terms of how well they can do the training and stuff. But it still was a pretty big update for me to know that wow, there are a hundred people that don't have PhDs working at a Chinese hedge fund that are training these like cutting edge models. Like it is incredible and they work very hard. They're very good. [Music] Do you have pressure or do you feel pressure? Do AI researchers in general feel pressure to consider monetization when they're researching things or is it you know mostly still curiositydriven that sort of old school Silicon Valley we're improving the world kind of thing or is it much more mercenary given that all these big companies seem to be competing in the same space? Yeah, I think that over time it's gotten harder and harder to do things that are just like cool ideas or seem cute but don't have any necessary application
>> and things are getting closer and closer to products. You know, even like the language models that power chai GBT, I was working on those before Chad GBT and it they had some uses but also they're intellectually interesting and like fun to build. But now if I came up with a better way to train Chad GBT, that's like a multibillion dollar innovation.
>> The stakes are higher.
>> Yeah. I'd be like an asset to like the United States government or something if I knew how to do that. So I guess it depends on what kind of problems you work on. Like I'm more interested in understanding how things work. So it becomes a bit less financially dire. I think
>> that 6 to 12 month gap between and what was that? That was a January deep sea moment.
>> Yeah. Although really like everyone should it was in December that they first got attention then for some reason really hit in January. Is that a sustainable gap? Is there something either in access to data, access to talent, access to compute, access to chips, whatever, access to energy that in your view will allow US frontier labs to maintain some sort of 6 to 12 month gap for a while.
>> It's pretty unclear to me. I think there are different beliefs you can have. you can believe that the ideas and the people are really the thing that differentiates the models and in that case I think we haven't so far seen a lot of like the top US AI researchers going to work at Chinese labs so that seems stable you could think that chips really matter and in that case the chip race is really happening between big American companies like I think actually China has a pretty big deficit coming up in terms of like the GPUs we're exporting or you can think that the data matters and I guess actually any of these point in the favor of the US. I think if you think the data really matters, maybe the the data they gather through like deepseeek.com usage is really good and they can use it to like bootstrap a better model. But I think the American companies really do have an advantage. Like you all might have heard this story just as an anecdote. Apparently at Anthropic, they've been buying and scanning thousands of old books for several years. So, they have this division, I think they're based in New York, that buys like shipping containers full of old manuscripts, cuts off the spines, and puts them in these scanning machines, and then they turn them into like really high quality text. And so, I'm noting Claude has this like weird aspect to it. Maybe part of the reason is they've gathered like trillions of words worth of like old book data over many years, and that's pretty hard to replicate elsewhere. So, I think that Head Start really does mean a lot. Hm. What are you most excited about at the moment? The book thing sounds very cool, but what what is getting all your your attention right now?
>> Thanks for asking. I think I mentioned before I'm really trying to figure out what's coming next. There are some obvious things like we can get computer usage data and train better agents or we can get more coding data and make them better at coding or writing GPU code or whatever. But like what are the non-obvious advancements? And my personal opinion is that the next like round of improvements in AI models will come from some type of personalization and online learning which means like models that one are trained like per person or per company. So like you could think of like like Chad GBT is the same model that gets served to everyone. So it has to store information about random restaurants in like countries you never go to. But instead if you had a chatbt that's specific to Bloomberg or specific to your work, it might be able to like use more of its brain to do work for you. And then the second thing is if it was updated every day. So like if you ask it to make your OddLots calendar Yeah. or RSS feed and it you're like no that was wrong like you did it wrong for this reason this reason and you try again tomorrow it will still break tomorrow because it doesn't like continuously improve its capabilities so yeah I think that's the direction things are going
>> I've heard people talk about this now granted models are getting better over time but you know people might compare a coding model to a beginning software engineer and say the coding model is better but that software engineer is going to start getting better the next day they're on the job and every day for the rest of their career, they're probably going to be a better software engineer than they were the day before. Whereas at least that version of the model will not be better. That is that right? Is that seems like an issue that people talk about in your world?
>> Yeah. Yeah. I think this is a big problem. It's like we have to wait 6 months for the chat GBT 5.1 to come out and then maybe they'll include your problems as the training data and so maybe it'll get better, but it might not. And instead, I think people need to think about ways to do that update more dynamically. Like every time you talk to it, or maybe every night when you go to sleep, the model kind of like gets to work and studies what it was talking to you about and crafts better tests for itself and then learns and then when you wake up, the model's actually better. The other big question that I have and it is kind of related to this especially when we're talking about AI replacing humans and certain forms of labor is that like do we need really really advanced AI like in other words like there is a lot of again the existing models are extremely impressive like in your view do we need to get a lot better technically for them to have economic impact and since these are in many cases businesses at the end of the day, is it necessary that there's so much work being done towards advancing the cutting edge?
>> Yeah. Yeah, that's a great question. Like we could have really good interns without ever getting better scores on the math olympiad. Like that's not necessarily something that we ever had to go after. I think part of the reason for that is the AI labs are engaged in this kind of neckand-neck race to have the smartest model. But I totally agree that AI could be economically transformative without having a higher ceiling in terms of what it can do. It's more like it needs to be more consistent or like dependable than actually smarter.
>> This might be a weird question, but once you've made a sort of foundational improvement to a particular model, how easy or difficult is it to rewind if you need to? And one of the reasons I ask is because, you know, some people have been complaining that they've been training chat GPT to, I don't know, be their boyfriend or whatever, be their therapist.
>> This is a legit topic in my opinion.
>> And then it gets upgraded and all of that training suddenly disappears and the personality of the model changes.
>> Yeah, that was a really interesting story. So I think the model before GPT5 was 40 and they said that they thought internally like all the scientists and coder people that the new model was superior in every way. It gives you shorter responses. It's a bit nicer. It's much smarter. And then people got really upset because they had spent so much time talking to the old model that they felt like they'd experienced like a serious loss in their life.
>> Joe would miss the the love it
>> love it. No, but for real, this is unironically this strikes me as another example for open source, which is that I if I'm going to form a I don't see I'm 45. I'm too old for that. But if someone is going to form like some sort of friendship with an AI model, I don't want it to be at the whim of Sam Alman deciding like, oh, there's an upgrade. I would like to be friends. It's also weird to be friends with the model that I know that I can run in perpetuity
>> and it will never change.
>> Yeah, I think that's definitely a good argument for why open source is important and if you ever fall in love with a model, you should fall in love with an open source.
>> That's good life advice, practical life advice. It
>> is really good life advice. Well, speaking of open source, you know, I know programmers tend to like open source for obvious reasons, but are there any downsides to open source for AI specifically?
>> I think if you're running a company, there are a lot of downsides potentially to open source. If you have some brand new fancy way of of doing computation inside the model that's actually better, you might want to keep that information to yourself. And when you release the model to make it runnable, you have to release all the code to run the model which might contain like your secrets. And so I think that's why people are hesitant to do it. The other reason is because when you release the model, it actually contains quite a lot of residual information about how you actually trained it. Like you might be able to infer what the data set was and what the training process was or even reconstruct the entire training data set given just the weights of the model. And so if you're worried about people finding out that a certain thing was in your training data, you probably can't release that model open source.
>> That reminds me, how much of an AI researcher's day-to-day life is just like looking at other model, other people's models and trying to like I guess pull them apart and figure out how they were made and sort of work backwards.
>> That definitely happens from time to time. I think usually the scientific process is something like you start with other people's models and you run them and you see what happens and then you decide on some part of that process that you think could be improved or could be explored further and you make some tiny changes to it and then you run it again and you compare like numbers or you make graphs of what happened before and what happens after. So actually quite a bit of it like for example the GPT2 model from open AAI which was I don't know 2019 or something their first kind of really yeah larger scale chatbot like I've spent I don't know hundreds of hours kind of like playing with that code and talking to the model and stuff like that.
>> So thank goodness for open source for that reason. I joked in the beginning about you having a hund00 million salary, but for real, as you think about your career, and I hope you do get a hund00 million salary, but as you think about your career, what excites you, and how much is it money? The reason I think about this is like there are huge checks out there, but maybe some things are more maybe achieving AGI is more excited than making an ad network more efficient. Maybe something there's something more exciting than shaving off a billionth of a second in terms of a trade execution. All these things like how much is it about exploring the frontiers of science, the new space race landing on the moon versus the paycheck?
>> It's all about the paycheck. I'm just kidding. No, no, not at all. Yeah, it's funny you ask. So, this hasn't happened to me, but just in the past 2 weeks or so, a good friend of mine has been dealing with this problem because she got an offer on the order of like tens of millions of dollars per year from a big AI company and she wasn't sure if she wanted to work there. And I think originally she said no and then they doubled her offer and then like it's the exact same amount of cash but twice as much per year for a certain number of years. And you know, we were talking amongst ourselves like, what does this even mean at this point? Like you're, you know, a 28-year-old computer scientist that's been coming from a PhD, so you make more on the order of tens of thousands of dollars per year. I honestly think personally the marginal difference between having like 10 and 20 million is like very low. Like I don't even know what too. This is I this is I this is my experience for me making 10 million 20 million has basically been the same to me. Yeah. and congratulations.
>> But so yeah, I think there's more of a desire to like be there the next time something really interesting happens and that kind of supersedes the money. Like any of these places will pay you what's like a really good salary to live on. And so it's actually not a big consideration. It only becomes complicated when you have like one option that's going to pay you like 40 times more than the other option. And then things get confusing. No, this is this would actually just thinking about making 20 million. No, I think about this like
>> cuz I think about okay, what if you have this great salary
>> and you're like can live very easily in New York City and have a really great life or you can make 10 times that which is a stupid insane salary but you don't really like your job but it's so much money that strikes me as like not a trivial life you know you only live one time it's like a diff so it could be a difficult question.
>> Yeah. Yeah. But you can remind yourself that like the job you take once isn't the job that defines you forever. Maybe maybe the right thing to do is to take it for a few years but not the whole time and then go do something.
>> Everyone says they're going to do that.
>> Yeah.
>> And then they get locked in.
>> Yeah.
>> Speaking of insanely large salaries, we know that people are earning these salaries because they're like star AI researchers. How much does personality play into where you want to go work? Would you want to go work somewhere specifically because there's an absolutely amazing researcher? Yeah.
>> Or does it tend to be again more about the paycheck, maybe more about the data that's available to you, or maybe more about the specific project that you're going to be working on?
>> Yeah, I think different people assign different amounts of weight to each of those things. In my experience, like most of the people I know come from academia, which means they already kind of gave up more of a salary to do study things more deeply for several years. So, I think people that I know are more biased against money, but like people do care about that, but I think that the ego thing really matters. And so, some people want to feel like they're very important and they're working on a problem that matters. One way some companies are able to pull researchers away from other companies is by saying we'll assign you more importance in your role and we'll give you
>> we'll give you a really big title.
>> Yeah, exactly. Like like seriously the title is like okay maybe before you were like a researcher now you get to be like a head researcher you get to have people under you or you're a chief scientist and all these things do matter to people.
>> It's a very good book about pursuing a a mission in the realm of like a driven visionary even when it's commercially
>> Joe. Yeah that's right. No I think about this all the time. It's like, do you want to work for Ilia or do you want to work for Sam? And which one is the Ahab and which one is just trying to make an honest living selling ads? I find this to be like a genuinely interesting uh interesting question for any individual to have to reckon with in this career.
>> Oh, absolutely. And sometimes it can be very difficult to tell.
>> Jack Morris, thank you so much for coming on. Please pursue a career that will allow you to come back on OddLots
>> or insert the odd lots clause
>> when you're negotiating your $und00 million salary.
>> Yeah. or take the 50. Say, you know what? I'll 50 million, but let me I don't need the 100 million. 50 million would keep you out.
>> Yeah, that would be fine with me.
>> All right. Great. Well, thank you so much.
>> Yeah. Thanks.
>> Thank you so much. That was great. [Music] [Music] Tracy, I think about that sometimes. Like what if you got like an insane salary? Like that you just couldn't you would be insane to say no to but like I don't know that's I mean it's not our problem but like wouldn't that be fun, you know? It's like, "Oh, but you're going to be working on ad optimization or whatever, and you're not going to be there when they land on the moon, but you got paid 10 times more than the people at the base station working on landing on the moon." That strikes me as like kind of a tough life choice.
>> I think you're using up a lot of brain power and energy on a problem with what you said is not yours.
>> I will never have. That's exactly right.
>> No, that conversation was really fun. Uh, nice to talk to an actual researcher who's doing stuff in the space. One thing I thought was very interesting was this idea that everyone gets excited about a specific improvement in AI. Yeah. And then it seems like that particular one doesn't materialize and instead something else emerges as like the big breakthrough. So instead of agents, we have math
>> and math which none of us will ever need. I would really like for an agent to do something simple. I'm going to a city, book my trip or whatever. Or change my flight. Oh my god, I tried to change flight recently. change my flight. Here's my information. I don't I would like that. I do not need the math olympiad. I am very impressed. I don't need it.
>> Also, I am now very very intrigued by reinforced learning and how you actually reward the computers for doing good stuff. I feel like actually that would be a really interesting area to mine which is motivating motivating the models to do better. Yeah, I've thought about that like in chess like how do the how do the computers know they want to win? Yeah. You know, like why do they care? You know, all these Anyway,
>> why are they here? Why are we here?
>> That's the thing with AI conversations
>> gets existential fast.
>> You know, one thing we didn't talk about which I am interested, no one really talks about AI safety anymore. Have you noticed like the like very little like for better or worse, you don't hear people just all money and they don't really talk about will the AI kill us all one day? But one thing I did wonder about so when deep sea came out one of its breakthroughs was it showed the whole chain of thought right you could see that which prior to that open AI or chatubt's chain of thought model didn't show you that right
>> and it does strike me that if there are certain things that are for safety reasons or whatever held back or they don't want to do this the nature of competition means all the guardrails are coming off eventually like that's if there's some guardrail you have on someone's going to open source whatever it is and you're they're going to all give it Yeah, both on the guardrails and on the data usage point as well.
>> All right. Well, shall we leave it there?
>> Let's leave it there.
>> This has been another episode of the All Thoughts podcast. I'm Tracy Aloway. You can follow me at Tracee Alaway.
>> And I'm Jill Weisenthal. You can follow me at the stalwart. Follow our guest Jack Morris. He's jxmop. Follow our producers Kerman Rodriguez at Kerman Arman Dash Bennett at Dashbot at Kalebrooks at Kalebrooks. For more OddLots content, go to bloomberg.com/odlots where the daily newsletter and all of our episodes. And you can chat about all of these topics 247 in our Discord, discord.gg/odlots.
>> And if you enjoy OddLots, if you like it when we talk about $20 million salaries that will never be ours, then please leave us a positive review on your favorite podcast platform. And remember, if you are a Bloomberg subscriber, you can listen to all of our episodes absolutely adree. All you need to do is find the Bloomberg channel on Apple Podcast and follow the instructions there. Thanks for listening. [Music] Yeah. [Music] Heat. [Music]