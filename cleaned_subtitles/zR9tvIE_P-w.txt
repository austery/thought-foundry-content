AI视频领域的热潮 最近是被Sora2点燃的 刚发布的时候 以假乱真的画质 自然的语态和生动的微表情 真的非常震惊到我 我们在10月5号 举办的硅谷101线下科技大会上 就用了一段Sora2生成的 Sam Altman的AI视频 不过 Sora2 虽然代表了视频生成质量的飞跃 但是它依然有局限性 因为它本质上 依然是文生2D视频 它能够生成影像 但是无法实时的交互 它是被观看的内容 而非实时可交流的伙伴 所以 Sora2距离一些商业应用 依然有一定的距离 因为它无法满足不能有瑕疵 实时互动 还有低成本规模化这三大核心要求 所以Sora2的火爆 其实反而凸显了 当下AI发展的核心矛盾 我们正处于一个分裂的时代 一方面以ChatGPT为代表的大模型 让AI拥有了前所未有的大脑 他们能推理写作编码 但是他们是无形的 另一方面 以波士顿动力 Optimus、Figure为代表的人形机器人 拥有了强大的身体 他们能跑能跳 能搬运但他们还缺乏大脑 还不能达到与人真正交互的地步 也就是说 虚拟世界的大模型 有脑无身 而现实世界的机器人 有身无脑 那么这两条路径 都只完成了未来的一半 那么在未来 承担人机交互的载体 究竟会是什么 如今一种可能的融合方式 以及一种通往“具身智能”的路径 就是 3D数字人 你可能会觉得很奇怪 3D数字人的应用场景 不应该是比如说游戏 AI陪伴、主播 还有直播带货这种场景中吗 其实远不止如此 因为要让AI真正的活过来 它不仅要能理解 更要能以人类的方式去表达 用表情传递情绪 用声音与语气沟通 用动作和形象建立存在感 而更重要的是 它被视为目前“具身智能驱动层”的 一个重要科技突破方向 让生成式AI的进化从2D到3D 再到现实的物理世界交互 那么这期视频 我们就来聊聊3D数字人 首先来说说数字人的发展简史 数字人的概念并非诞生于AI时代 它的演进 可以说 是一部跨越半个世纪的计算机图形学（CG） 还有电影工业与互联网文化交织的历史 数字人的最初形态孕育于电影工业 也是由CG技术驱动的 从乔治·卢卡斯在《星球大战》中构建的虚拟场景 到《玩具总动员》作为第一部全CG电影带来的革命 再到电影《指环王》中 “咕噜”这一角色的诞生 数字人技术完成了从0到1的突破 “咕噜”的出现是一个里程碑 它首次将演员的表演 通过动作捕捉技术 完整映射到一个全CG的虚拟角色上 这一阶段的数字人 是技术与艺术的结晶 其特点 也显而易见 制作周期以年为单位 完全非实时 是绝对的专业内容生产（PGC） 他们是荧幕上的奇观 而非大众可及的工具 后来随着互联网的普及 数字人开始走出昂贵的电影片场 2007年 日本基于语音合成引擎技术 推出的初音未来 开启了虚拟歌姬的浪潮 她没有实体 却能举办万人演唱会 一个强大的虚拟IP就此诞生 此后从AYAYI到柳夜熙 国内超写实虚拟偶像 相继引爆社交媒体 这一阶段 数字人的核心价值 从技术奇观转向了IP价值 他们拥有了人设故事和粉丝群体 但其交互能力依然薄弱 高度依赖背后的“中の人” 也就是真人演员 或预先制作的精美内容 他们是完美的数字木偶 却不是独立的数字生命 真正的质变 发生在AI技术 尤其是大语言模型突飞猛进之后 从微软小冰尝试用AI 赋予聊天机器人情感 到AI虚拟主播开始7*24小时播报新闻 AI的注入使其从能看到能聊 会思考进化 一个数字人不再只是被动执行脚本 而是能够理解你的问题 并且生成有逻辑有知识的回答 数字人的“智能”拼图就被补上了 在AI的加持之下 数字人行业 逐渐分化为两条主流的技术路线 2D和3D 第一种是2D的技术路径 无论是视频生成类模型 还是2D数字人平台 都属于这种类型 比如说像Sora 它就是一个视频版的stable diffusion 它能够生成视觉上逼真的短片 但并不能实时驱动角色 进行表达或者交互 或者以Heygen等平台为代表的数字人平台 它是用真人视频录制加语音驱动口型 核心机制是通过AI模型 根据输入的语音 来合成视频中人物的二维唇形变化 使嘴部动作与语音对齐 简单来说 它主要是在预先录制好的真人视频上 进行嘴型替换 让视频里的人 看起来像是在说指定的话 这种方式的优势在于人物形象真实 因为是基于真人录制的 制作相对简单 但局限性也非常明显 第二种就是3D的技术路径 这种方向本质是由语言驱动身体 实时生成声音 3D动作 表情和手势在内的多模态表达信号 它属于文生多模态3D动作大模型 （LAM，Language Action Model） 这区别于传统的 LLM（Language Model） VLM（Vision-Language Model） 和VLA（Vision-Language-Action Model） 这种方式的技术壁垒更高 但是优势在于 它是真正的“具身表达”能力 所以总结一下 2D数字人的核心 是语音驱动嘴型的视频合成技术 让AI会说话 而3D数字人 核心是语言驱动身体的实时生成技术 让AI会表达 那随着AI的进一步发展 我们对交互真实感的要求 在逐步的提高 具备实时交互和丰富表现力的3D路线 也越来越成为重要的发展方向 那么下一步 就是要想真正实现媲美真人的交互效果 3D技术方向还有一些技术挑战 需要一个强大的技术班底来支撑 我们最近采访了魔珐科技创始人柴金祥教授 柴教授可以说是坚定的“3D数字人派” 他是美国卡内基·梅隆大学的机器人学博士 师从计算机图形学界泰斗 美国工程院院士Jessica Hodgins 我们知道 卡内基·梅隆大学的机器人研究所 是全球AI与机器人研究的圣地 柴教授当年的博士论文 研究的就是 如何让虚拟人 具备自然的动作生成与控制能力 实现实时互动 他所在的团队 也是世界上 最早开始用AI做动画的团队之一 一个有趣的现象是 柴教授的许多学术同僚 比如说加州大学伯克利分校副教授 Physical Intelligence（PI）的联合创始人 Sergey Levine 都是从AI动画领域转向了机器人研究 这并非巧合 而是因为他们很早就意识到 虚拟世界的AI动画 与物理世界的机器人控制 它在底层逻辑上是同源的 我们这个方向叫图形学 或叫动画 其实我们以前这波人 其实更早的时候其实也做AI 我去卡梅的时候就开始做AI了 但是我们那时候NLP 就是说现在大语音模型这个方向 或做语音识别这方向 跟我们是两个方向 就是专门做那些人 以前我们叫 language technology Institute（语言技术研究所） CMU有一个LTI 我们叫 Robotics Institute（机器人研究所） 我们完全是两个没有关系的 但在今天的话 你发觉AI出现 因为包括我们现在公司在做 同一波人 在做大语言模型 也在做这个动画大模型 也在做驱动 也在做 我们今天在做这个 这个造人这一块 就是这个本体这一块 你会发觉其实你AI有了以后 这些东西只是数据的不一样 那你串起来的时候 就使得这个事情 我觉得也是认知上的改变了 你会发现所有东西都可以做 只要你有数据 那我觉得这个是认知上带来改变 另外发现技术上 其实也是因为它的确效果 有很多时候你在做的过程中 以前你觉得很难work的东西 我忽然哎“哇” 这东西能work 在20多年的学术积累之后 柴金祥教授选择3D+AI 这条在当时看似不可能 但也颇具前瞻性的方向 并且在2018年创立了魔珐科技 如今想要生成一个高质量的3D数字人 正如柴教授所概括的 它通常包括5个核心的环节 我觉得最主要是几part 第一个环节 就是要创建这个人的形象 这个我们叫建模 那第二个事情 就是说他要动起来 3D数字人其实一样 他会要肌肉环节 我们叫rigging 就说其实绑定 那我们下一步要做的是驱动 我们叫动画 让这个人能够表演了 有表情动作手势 那下一步 其实你需要的就是说 因为人身上还有头发 有衣服那这部分我们一般叫解算 就用物理去解算 那最后一步当然我们叫渲染 这最后一步 相当于就是说个人表演的过程中 我用摄像头给它输出视频 整体来说就是建模 绑定、动画、解算 解算也叫特效 到最后的渲染 这五个模块 那么这五个环节 传统上都需要大量的专业人工 还有时间投入 是3D内容生产成本高昂的根源 而业界的新目标 正是要用AI去重塑这个流程 在多年的技术积累之后 最近魔珐科技 是推出了“星云”平台 它的定位也很清晰 是全球首个面向开发者的 具身智能3D数字人开放平台 那么“星云” 就试图解决一个重要的问题 当AI拥有了GPT这样聪明的大脑之后 该如何赋予它一个可交流可互动 可信赖的身体呢 而要给AI补上这个身体 其实没有那么容易 在传统技术路径下 一直有一个“不可能三角”的说法 导致惊艳的Demo很多 但是真正能够大规模商用的产品极少 我们在之前讲 灵巧手的机器人系列视频中 提到了机器人的“不可能三角” 而在3D数字人上 也存在着“不可能三角” 就是高质量低延时低成本 这三种往往是互相掣肘的 但如果要想同时做到高质量加延时低 那比如说 电影级别的那种实时动作捕捉 就需要顶尖的图形工作站和高端的GPU 导致成本爆炸 不可规模化 要想做到高并发加成本低 就又必须要去牺牲质量 往往这个效果粗糙 口型漂移 动作比较机械 就有点像那种早期不成熟的AI数字人 但又想做到高质量加并发高 就又会导致延时严重 失去实时交互 就只能用于离线内容的生成 那么有什么样的破局方法呢 柴金祥教授就认为 魔珐科技的“星云”平台 在试图创建两个护城河 分别是“文生多模态3D大模型” 与“创新性的云-端拆分架构” 用这样的方式来破局 第一个重要点 就是我们采取了3D这个路径 那你能看到Sora2 它虽然不是做驱动的 它是做视频的 它采取的是2D 那当然也2D 也有2D数字人的这个路径 那3D的好处 第一个好处是说 它能够精准控制人的表情动作 包括他的手势 Sora你能看到有表情动作都有 他是一些prompt 当你说我的手要怎么动 我的脸要怎么动 我的身体要怎么动 你是控制不了的 那第二个点就因为它是3D 3D其实还有个好处 就是说当你做大模型的时候 因为3D是结构化的数据 比如说我去描述一个人的 表情、动作、手势的话 其实对我们现在3D的动画 不管你多复杂 比好莱坞的这个3D的角色 还是3A级游戏的角色 其实你的参数 一般来说到1000个左右 就可以描述这个人所有的表演 但你现实世界你去描述他的时候 他是个图像 那可能是1000*1000的图像 那就100万个像素 那你去描述它的时候 它的空间很大很大 我们图像空间、视频空间 要模型是要小 然后那这个相对小了以后 它就会导致它的整个的延时会低 它的实时性会高 它成本也会相对比较低一些 所以这种结构化优势 使得3D模型在训练效率和参数量上 相比2D视频模型更具潜力 训练所需的绝对数据量也远小于后者 当然与互联网上泛滥的文本和2D视频不同 高质量结构化的这种3D数据 是极度稀缺的 从2018年起 柴教授的团队 就通过服务游戏 影视等B端客户和自建内容 是积累了 超过1000小时的高质量3D动画数据 这些数据 包含了细腻的表情手势 动作和语音的精确匹配 基于这些数据 魔珐团队得以实现从语义理解 到声音、表情、手势动作的全链路一体化生成 当大模型告诉数字人“该说什么”时 星云则告诉它“该怎么说” 并且实时生成匹配的语音口型 表情和动作 赋予其表现力 那我们再来说说第二道护城河 传统的数字人 无论是本地渲染还是云端串流视频 都极度的依赖高端的GPU 所有3D你经常去跑 你一定要用渲染引擎 游戏引擎 最好的就Unreal 那Unreal其实你要做 你是跑不掉 图形卡、显卡 我们叫一路并发的时候 一路基本上叫一张卡 那一张图形卡 基本上如果你是用民用卡的话 也要2万块钱左右 如果是1000个人 基本上成本就 你很难弄了 都要2000万的并发成本 所以这个事情其实是做3D的人 所有人都逃不出去 AI时代 高昂的硬件成本是规模化的最大掣肘 而“星云”的架构 则试图颠覆这一路径 它采用了一种创新性的云-端拆分模式 在云端也可以说是“大脑端” 它只负责最核心的AI计算 也就是根据文本 生成轻量的“语音参数 + 动作参数” 而不是庞大的视频流 而在端侧 也就是“本地侧” 在接收到这些参数之后 通过魔珐的“AI渲染/解算”技术 在本地设备上 实时将参数“画”成最终的3D画面 解算跟渲染 以前这两个事情 是游戏引擎干的事情 这个事情我们用AI做了 其实我们用AI类做这个渲染 用AI类做解算 那这个做完了以后 使得我们不需要显卡 而且我们做了 这里面有很重要的一个点 我们把它做了 非常非常轻 轻到了就是说在我们端上非常低 这个芯片 不是很powerful（强大）的芯片都能跑 这种将对高端GPU的依赖 转移到对轻量级AI解算能力的做法 不仅极大降低了硬件成本 还避免了云端渲染 带来的高昂视频流的带宽成本 这是实现“低成本”与“高并发”的关键所在 那以前 如果说我们要这个显卡去计算的话 那主要的成本是在那张显卡上 所以这个成本你可以认为 基本上我们今天的终端 比如像电视机上 我们要放一个具身智能数字人 你可以对话什么 他可以回答你问题 那这个成本基本上是非常非常低的 这个成本低到就是说 我觉得所有的 今天的大的电视机厂家 都会觉得都可以接受 那么归根结底 “星云”不是一个内容制作工具 而是让大模型有身体的 这样的一个底层的基础设施 他的目标是赋能所有开发者 其商业模式也是对标大模型 按API的这个调用量来收费 那接下来 我们就再来聊聊3D数字人行业的困境 还有挑战 那么当然 尽管有前沿的创新尝试 试图打破“不可能三角” 但是 3D数字人行业 要想要实现真正的大规模普及 依然是面临三大的核心挑战 首先是造人的初始成本 英伟达CEO黄仁勋在发布会上 曾经展示自己的数字人 其制作成本被业内估计高达10万美元 虽然“星云”平台 极大地降低了驱动和渲染的成本 但是初始高精度建模和绑定的成本 依然是阻碍中小企业 和个人开发者入局的第一道门槛 要想实现ToC场景的普及 也就是人人都有数字分身的愿景 建模成本依然需要大幅度下降 其次是数据的稀缺性 高质量3D动画数据的获取 难度和成本极高 这是整个行业AI化的核心瓶颈 目前行业普遍处于AI公司缺数据 内容公司缺AI能力这样的尴尬境地 没有海量、多样化的3D数据作为“燃料” AI的表现力就无从谈起 那我们的嘉宾柴教授 他对此的策略就是 用高质量的3D数据去打基础 同时在探索融合海量的2D视频数据 我们认为最后这个数据 是3D加2D的数据 我刚才讲了我们有3D这个数据 其实我们后面 我们现在在做新的这个模型 就是有大量的这个2D的数据 这个2D数据可能被标注过了 有部分比如说人脸 大量的人脸 比如说我们在视 频里面看到人脸的数据 那有各种表情 那可以弥补3D数据里面的多样性 但人类的数据它没有结构化 因为它是2D的 怎么把3D跟2D结合起来 最后去做这个混合的模型 我们后面也会有新的一个大模型 会去发布做这个事情 因为你会到最后 你会发（现）真正要部署下去的时候 这个模型真正要在终端用的时候 如果你真的要走商业化 你一定是要轻量级的 第二我们看到则是泛化性的问题 这是3D数字人 乃至整个具身智能领域 最核心的挑战 拿机器人来举例 比如说波士顿动力的机器人爬楼梯 挑战的地方就在于 如果你给他不同的楼梯高度 他不一定都能爬得很好 这就是为什么 从前波士顿动力在宣传Demo里面 经常放的是同一个楼梯 那么这就是泛化性的难题 能不能做到已有数据之外的事情 就拿比如说爬楼梯举例 这每个楼梯的它的高度如何 有多少层的楼梯 不同楼梯本身的摩擦力是多少 这个摩擦系数是多少 那么这些都是一些要泛化的参数 而在今天 恐怕还没有机器人公司 可以说 任何一个楼梯都能够爬得特别稳的 那么这种从特定场景Demo 到全场景产品的鸿沟 也同样存在于3D数字人领域 那3D数字人 也是同样面临着泛化性的难题 这里边的难点有三个 那么第一 是交互的泛化 当下的数字人 大多只能对应标准的问答 所谓的SOP 但是用户真实的提问是发散的 是充满了上下文切换和潜在意图的 数字人是否能在非标的对话当中 依然保持有逻辑有情感的回应 第二个难点是动作的泛化 能否根据指令 生成在各种复杂环境下的合理动作 那就比如说 在拥挤的街道行走 和在空旷的草地奔跑 那这样的形态当中 它的肢体动作和神态是截然不同的 AI是否能够理解这种差异 并且生成非重复符合情景的动作呢 第三是情感的泛化 那这是最难的一环 数字人能否根据对话的内容和情绪 生成真正自然而非模板化的微表情 是发自内心的微笑 还是礼貌性的假笑 是专注的倾听 还是敷衍的点头 那么总的来说“泛化性” 是衡量一个AI系统 是否真正智能的关键 也是3D数字人 从“数字木偶”走向“数字生命”的 必须要去跨越的障碍 对此 魔珐的策略 是区分“通用能力”和“垂直应用” 对于通用能力 大概可以分为四个方面 一方面是表达能力 比如说两个人在沟通 表情动作手势都是属于表达能力 第二是基本技能 比如说走路跑步 开门坐下这些日常的技能 第三是我们的干活技能 比如说抓个东西开车搬东西等等 第四是一些特殊技能 比如说有人会跳各种舞 但不是每个人都能跳 所以如果要去训练一个通用的大模型 那所需要的数据量 就会太大 这个过程中 他要分两part 一part你要去训这个基座的本身的大模型 其实我们也在做这一块 但另外一part 如果说今天我们在做的就是智能助手 更多的时候我们叫最佳实践 比如说我做银行的客服 或者我今天做的是一个AI的陪练 或者我是一个AI的面试官 它是这个岗位的最佳实践 对于一个岗位的最佳实践来说 他是不需要多样性的 因为对于一个面试官里头 他一定有一个 表达的方式最好 那如果你是讲表演的话 那你的多样性要上去了 因为你需要个性化 我们这个数据其实是对一般的走跑跳 就是日常生活的 再加上我们今天的特殊行业的 我们讲B端里面 比如说我是个客服 我是个AI陪练 我是个展厅的讲解员 或者我是一个特定的陪伴 （在）今天是够了 魔珐在做的 就是先在银行客服、销售陪练 AI面试官等等 最佳实践的场景落地 是一种“小泛化” 先验证价值 然后再去逐步构建通用的基座模型 提升“大泛化”能力 那么在未来 如果泛化能力得以加强 3D数字人所描绘的未来图景 会是什么样的呢 第一个大应用场景 就是让每一块屏幕“活”起来 这可能是3D数字人最短最快 也最容易实现的路径 人机交互将从“遵循机器的逻辑” 就是我们需要去在一个系统里点击 重新回归到“机器理解人的逻辑” 也就是自然对话中 从政务大厅的办事窗口 银行的自助终端 酒店的前台、商场的导购屏 到博物馆的展陈屏幕 再到家中的电视学习机 屏幕将不再是“被动显示窗口” 而是“主动交流的服务入口” 我们今天去看商业化 最有价值或最容易打的 肯定是说有身体跟没身体是不一样的 这是最大区别 比如说举个例子 那首先从大屏 比如说商用大屏 比如说我们叫BI （Business Intelligence）的问数屏 或展厅的屏幕 或者是政务中心的屏幕 还是酒店的屏幕 还是银行的 还是说这个商场的屏幕 这些屏幕以前只是个展示品 那你加了这个AI了以后 它就变成个人了 就把一个白领放在里面 那这个价值是非常非常大 以前是以被动展示类品 现在你可以给他互动 他可以给你回答你问题咨询 给你呈现各种信息 所以有没有“身体” 差别是特别特别大的 而且在线下的过程中 用户给他沟通的时候 他是不喜欢对着空气讲话 比如说我给我语音给他交互 他给我的东西就是一个文本 或者用文本去打字 这肯定是不行的 此外未来在各种企业网站 在线课堂或者各类APP 迎接你的 可能就不再是复杂的UI和菜单 而是数字人向导 它将替代冰冷的“冷操作” 用自然对话完成所有服务 这一点在企业培训场景尤其突出 比如说这个 面试是一个很重要的事情 面试如果我用语音去面试 我用文本去面试 那肯定是不高效的 销售其实要去陪练 他要模拟下这个应用场景 那如果只是文本去模拟 这个是没有什么感觉的 但如果是模拟的是一个客户 客户就是个医生 然后医生就在屏幕里面 那我跟他对话去模拟 这种即时性 跟文本差别是特别特别的大 此外在泛娱乐场景里 大量虚拟IP都可以升级 过去的虚拟IP是静态的贴纸、玩偶、手办 未来它们可以被“星云”这样的平台驱动 成为可以实时互动 唱歌、表演的数字偶像 在游戏行业里 大量的NPC也可以升级 这恐怕是游戏行业里面的终极梦想了 未来的游戏NPC 将不再是只会重复 “欢迎来到村庄”这样脚本角色 而是拥有大模型大脑 具身智能驱动层的“虚拟生命体” 他们会有自己的情绪记忆和反应 能与玩家共同冒险 创造出独一无二的动态剧情 3D数字人的另外一大重要应用 是与物理世界的“具身智能”人形机器人 产生交集 我们在前面提过柴教授的一个观点 他认为动画与机器人理论同源 动画是在虚拟世界 和机器人代表的物理世界 在底层逻辑上是相通的 都关乎于3D世界的感知规划 还有控制 3D数字人技术 将从三个层面 极大的加速机器人的进化 那么第一 是在运动学层面 我们在做的这个事情 其实做的是控制这个动作这个事情 就是说我们叫运动学轨迹 我能够AI生成 我觉得我们现在3D的方法 在虚拟世界做 一定是100%用到现实世界 因为我的数据比他多 我就是现虚拟世界所有的数据 我要做的就是自动生成这个大模型 我就能告诉他你要怎么走 他就怎么走 你要到哪里去走哪里去 这个事情我觉得是一个 我们希望 在人形机器人 整个的上下游产业链里面 我们去做 因为这个东西 跟我们3D世界是一模一样 他做了同一个事情 所以其实3D数字人模型生成的 “怎么动”的数据 可以直接作为 机器人模仿学习（Imitation Learning）的输入 指导机器人的运动规划 那么第二 是在动力学（Dynamics）层面 有了运动学轨迹之后 就可以在虚拟世界当中 通过物理仿真 生成让机器人实际动起来的 所需要的动力学控制 也就是如何去施加力 那第三 就是所谓的Sim-to-Real 从虚拟到现实 在虚拟世界中 可以低成本 大规模的去生成机器人训练所需要的 在现实中难以采集的数据 就比如说 各种极端情况下的摔倒与恢复 来弥补现实世界中数据采集的不足 加速机器人“小脑” 也就是运动控制 和“大脑” 也就是决策规划的进化 因此 3D数字人不仅是AI的“脸面” 更是其在物理世界行动前的“模拟器” 和“训练场” 数字人技术的发展 将与人形机器人的发展同频共振 帮助机器人从“能干活的蓝领” 进化为“会交流的白领” 来共同推动具身智能时代的到来 AI多模态能力 在2025年出现了突飞猛进的进展 而其中3D数字人的进化 更像是一场正在发生的“人机交互”革命 这个赛道的探索者们 希望通过底层的技术创新 试图破解行业长久以来的“不可能三角” 他们的核心目标 是将3D数字人 从影视和3A游戏中的昂贵“奢侈品” 变为人人可用 无处不在的“必需品”和“基础设施” 我觉得AI本身大模型 在文本大模型上的整个的突破 它带给了做人形机器人上 也很大的信心 那对于我们做3D数字人 这个动作的这个驱动来说 其实也一样 带来了非常非常大的一个foundation（技术基础的突破）了 我觉得应该讲 所以尽管现在行业还面临着成本 数据和泛化性的挑战 但我们也能够看到 具身智能和3D数字人产业的技术同源性 而后者的发展 反而能在很大程度上 反哺机器人的发展道路 就像柴教授说的 ChatGPT给了AI一个大脑 而3D数字人想给AI一个身体 那展望未来 我们与AI的交互 或许会由一个有温度、有形态 有情感的数字生命来拉开序幕 而这样的数字生命 会从3D赋能到具身智能 真正的与我们的物理世界产生交互 让智能和AI更快的来到我们身边 那我们也拭目以待 以上就是我们这期的全部内容了 感谢大家的收看 那你们的点赞留言和转发 是支持我们硅谷101 做好深度科技和商业内容的最佳动力 我是陈茜 那我们就下期视频再见了