So SciShow recently published an AI episode, and I love SciShow, and I hate YouTuber drama, and I really wish this wasn't happening. But what they said in that video was SO bad, and misinformation about AI is causing so many problems now, I just feel like I have to do something. Some of that video's misinformation might just be mistakes, some of it's just omissions, and some is industry propaganda, but I honestly can't avoid the word "lie" for all of it. For example, this: [Hank] "But even compared to aircraft, antibiotics, and nuclear power, the speed at which we are developing artificial intelligence beats them all." That is just blatantly false. Sincere people might disagree about where we are in the lifecycle of AI and how fast it has been moving, and I'll get to more detail in a few minutes about that. But there is NO WAY that AI is happening faster than atomic power. It's not even close, and it really really matters because later in that video, the hypothetical danger of AI is equated with the actual danger of atomic weapons. And for me to believe it's not a lie, but just a mistake, I would have to believe that SciShow and Hank didn't know better, except back in 2013: [Hank] "But the Manhattan Project is responsible for humanity gaining a lot of information about the atom and really quickly." So today, I'm going to break down the most egregious problems with their AI video. I'm going to be very clear with you about what's opinion, what's interpretation, what's omission, and what's factually incorrect, and I'm going to give you a bunch of links where you can go learn more and make your own decisions. Because I want as many people as possible to understand that a lot of what comes out of the AI industry these days, and industry talking points that are uncritically repeated and amplified by the journalists and influencers and educators like SciShow, is very often, not always, but very often, just a load of BULLSH---- This is The Internet of Bugs, my name is Carl, I've been a software professional for more than 35 years now, and I'm trying to do my part to make the internet a safer, less buggy place, which these days is largely trying to educate people about AI slop and misinformation before it gets any more out of hand. Okay, so back to the first line in the SciShow piece, here's Hank Green again about how fast AI has developed: [Hank] "Just three years after the launch of ChatGPT, AI agents can now win gold at the International Math Olympiad." Okay, so that bit about the math medal, that's not even true, but we'll come back to that later. Right now, I want to talk about the "three years" part. So in three years, we've gone from ChatGPT's very public splash to the flop that was ChatGPT 5. By comparison, in three years, from August 1942 to August 1945, humanity went from the founding of the Manhattan Project to more than 100,000 people being killed in the nuclear bomb strikes on Hiroshima and Nagasaki. The idea that these two things are in any way comparable is infuriating. Statistically most of you won't have as much of an emotional reaction to it, but I grew up during the Cold War, hiding under my desk at school, listening to the sirens going off and not knowing if the world was about to end, and praying it was just another drill. I know what an existential threat looks like. AI is not that, not currently, not soon, and quite possibly not ever. Everything required to kill almost all the humans on Earth within a few hours using nuclear weapons is in place right now, just waiting on an order to launch. It took humanity 20 years from 1939 to 1959 to go from the discovery of nuclear fission to ICBMs armed with nuclear explosives and ready to launch at any time. Guess what was happening in AI 20 years ago in 2005? This book was published announcing to the world that superintelligence was near!! And, despite what Hank was implying there, AI is a lot older than ChatGPT. So here is an AI textbook from 1995, and this one is from 1983. Despite superintelligence turning out not to be near to 2005, we're now supposed to believe it's even nearER. So this kind of crap should be expected from AI grifters, but SciShow should be better than that. So we're clear, there is no time interval where AI has progressed faster than atomic power. The time from the discovery of fission to Nagasaki is 7 years, while the discovery of the transformer architecture that powers ChatGPT was 8 years ago. The discovery of the neutron in 1932 was 13 years before Nagasaki, but the first neural networks, the basic building blocks of machine learning, go back more than 40 years. And hopefully it's obvious to absolutely everyone that nothing whatsoever has happened related to AI that's in any way equivalent to Nagasaki and Hiroshima. The claim that AI is happening faster is just a lie, there's no milder word to describe it. SciShow viewers deserve better. Okay, so now I've talked about the biggest lie in the video, I'm going to talk about the biggest omission and why it's so important. Jumping to the end of the video, here's a disclosure that's a lot more important than you might think at first. [Hank] "Thanks to Control AI for sponsoring this video." So I've had Control AI reach out to me about collaborating, so I've done some research on them. And I really don't like what I saw there, because they feel to me as if they are acting as a propaganda arm of the AI industry. Let me explain why I feel that way. Note: I am not saying they are a propaganda arm OpenAI and friends, just like it feels to me like they are acting like it. Let me tell you why. So, Hank's about to read a statement that's on the upper left of your screen that's been signed by a bunch of AI executives, scientists, and famous people, but what Hank ISN'T going to tell you is that there are actually two different such statements. [Hank] "In the statement signed by Nobel Prize winners, computer scientists, and even AI company CEOs, these people warned that addressing the risk of AI 'should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.'" Now, in the lower left, I'm putting Control AI's website. You'll notice that Control AI mentions the extinction statement that Hank read in the video and then immediately changes the subject to claims that AI boosts growth and innovation and even mentions medicine. Quite the subject changed there. Now, on the right, I'm putting up the site red-lines.ai with the other statement (all these links below, by the way). Note that this statement originated during the 80th UN General Assembly. Now, let's look at what was omitted from the statement that Hank read. The red-line statement talks about "widespread disinformation" and widespread disinformation from AI is already a real problem, links below, but the SciShow statement ignores it. The red-line statement talks about "large-scale manipulation of individuals, including children," again, already a real problem and links below, but the SciShow statement again ignores it. Likewise, the statement that originated from the UN warns about "mass unemployment" and "systemic human rights violations." These are already real problems and they're actually occurring, but again, Control AI and SciShow don't want to tell you about those. And again, I've put references for those problems actually happening now, down below. The closest this statement gets to talking about extinction is mentioning "pandemics," which, while bad, are not extinction-level threats, as we found out in 2020 and 1918 and during the black death in the Middle Ages and a bunch of other times. But the most telling thing omitted from this statement is, quote, "Ensuring that all advanced AI providers are accountable." I wonder who might not want people talking about that one. The Control AI webpage, and SciShow in this video, Really, Really want to make you think about, and worry about, the potential hypothetical someday, maybe sometime in the future, maybe risk of humanity maybe going extinct possibly because of AI. Whereas I, and I made a whole video about this that I linked below, think that our attention needs to be focused on the NOW problems as emphasized by the statement on the right, like disinformation, manipulation, employment, human rights, violations, and holding open AI in the other providers accountable. And I'm not the only one. Let's take a look into the signatories of these statements. Hank said that Nobel Prize winners - Plural - had signed the extinction statement. I went through the list and I can only find one, Geoffrey Hinton, who signed both statements. Notably, John Hopfield, who shared the 2024 Nobel Prize in Physics with Hinton, only signed the statement on the right. Also only signing the statement on the right are at least five Nobel Peace Prize recipients, three recipients of the Nobel Prize for Economics, one for Chemistry, and at least three former heads of state and a former president of the United Nations. Now, let's look at who only signed the extinction statement: Sam Altman, the CEO of OpenAI, as well as the CEOs of Anthropic and Google DeepMind. Imagine that. The CEOs of three of the largest AI companies think we should worry about extinction from AI, but don't think we should worry about AI causing disinformation, manipulation, or human rights violations, and they definitely don't want us to worry about ensuring that all advanced AI providers are held accountable. Five Nobel Peace Prize recipients on the right, and three CEOs of huge AI companies on the left, which side are you on? Which side would you hope that SciShow would be on? This is why I say that Control AI, and SciShow in this video, feel to me like they are pushing pro-AI industry propaganda because they are trying to get you to think about hypothetical future problems that you can't do anything about. And when they push that narrative, and only that narrative, it has the side effect of distracting you from the real problems that are really happening, that the AI companies are allegedly causing now, and that the AI companies are, at least in some cases, already being sued for. Again, links below. Okay. So those are the two most important problems with the SciShow video, but there are other less important problems that SciShow should have caught, and I'm going to start talking about some of those now, but understand that the rest of this video is much less important than the lying about AI happening faster than atomic power, and pushing the statement about AI's hypothetical future risks while avoiding discussion of the much more relevant statement about AI's current risks that the AI companies would probably rather you not think about. So if you take nothing else away from this video, I hope you remember those two things. Okay. Next up is Hank's claim there about what AI has done. I told you I'd come back to it. He says, "AI agents can now win gold at the International Math Olympiad." Now, that's not true, at least if you believe the International Math Olympiad itself. There's that article. Like so many claims the AI industry has made over the years, it is, at best, an exaggeration contradicted by experts, and then irresponsibly repeated by people who didn't bother to do a 30-second Google search to look into the claim before they repeated it. This next clip, directly edited to remove a running joke about a Sea Shanty, isn't a lie, but it's really misleading. In it, Hank is going to talk about the alignment problem, which is when we try to keep AI's from doing things that they aren't supposed to do. [Hank] "On the other hand, AI's shouldn't give in to a request for how to build a bioweapon. Unfortunately, that kind of thing has happened. When anthropic was testing an early version of their AI Claude Opus 4, they found that it helped non-experts build bioweapons 2.5 times more successfully than people who tried the same thing with just the internet at their disposal." When Hank is talking about there sounds scary because Hank made it sound like Claude had, when it wasn't supposed to, helped testers build a bioweapon 2.5 times more successfully than they could have with just the internet. But that's not at all what happened. Here's the report that Hank is talking about. See that highlighted part? The AI that helped with the bioweapon had deliberately had its safeguards removed during that test. Hank is talking about AI's "giving in" to requests for how to build a bioweapon when they're not supposed to, but this example is not relevant at all. Because the safeguards that Hank is talking about in this part of the video were explicitly turned off. Now understand, the alignment problem is a real problem, and there are lots of examples that SciShow could have picked to illustrate the alignment issue. But this illustration isn't showing what he's telling you it's showing, and this illustration, by talking about bioweapons, further pushes the narrative that human extinction is what everyone should be thinking about when they think about AI risks. So here's another part: [Hank] "In April 2025, OpenAI had to roll back an update to its flagship ChatGPT model for being too sycophantic. One of the most practical and visible problems was that it had started endorsing people's decision to stop taking their medications without consulting their doctors." Now, while that's true, I'd argue it really, really downplays the issue. Here, I think, are so much more important "practical and visible" problems. There are more issues with this video, but because I don't want to get too far into the weeds, I'm going to start trying to wrap it up. I'm going to point out here how Hank uncritically quoted an essay from an AI CEO about humanity not understanding how AI's work. This quote is, in my professional opinion, a load of garbage, and I made a whole video about it that I'll link below. And if that wasn't enough, Hank also did a video on his own channel about AI that is even worse than this one. I feel like I ought to do something about it, but it goes on for like an hour and 15 minutes. It would take me so long to go through it line by line and pull out all the relevant quotes and dig up the references and supporting evidence and figure out what is of general public interest and would only be interesting to technical people who care about AI under the covers and what complaints would just be nitpicking. So I'm not sure when or if I'm going to have the bandwidth to tackle that. It drives me crazy that there's so much misinformation about AI coming out so fast that I can't even keep up with all of it, much less make videos about it all. Putting together a video with facts and references is so much more work and so much more time consuming than being an AI industry mouthpiece. Unlike the SciShow team, I'm just one guy, and it certainly doesn't help when the channels like SciShow that do have the resources to make things better, choose to make things worse instead. But I'm going to keep trying to help to the extent that I can. Wish me luck, and feel free to subscribe if you're so inclined. I would ask for people to be nice in the comments, but it wouldn't do any good, so I'm not going to bother. Just always remember that the Internet is full of Bugs - and misinformation, and anyone who says different might just be parroting industry talking points and trying to distract you from all of the ways the industry is, ALLEGEDLY harming real people, even as you are watching this. Let's be careful out there. Thanks for watching.