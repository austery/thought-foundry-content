Right before I started college, I ended up losing most of my central vision due to a rare genetic disorder called Liber's hereditary optic neuropathy. I was talking with someone who was losing their sight recently from the same disease and they were asking about different things and I was like, "Oh, you can just do all of that now with Gemini or attach the world is a whole lot easier."
>> So, you're going to show us some of the things that you've built for yourself.
>> So, when someone sends me an image, I use this tool to be able to get the gist of an image without needing to ask somebody to explain it to me. If I hit control shift D on any message, it's going to pop up and go off and describe that image for me. And the cool thing is I can go ask some follow-ups. What age child is this for? And it will head off to Chachi and get the response for this as well.
>> I'm curious for you, what are you most excited about in the multimodal world of AI?
>> One thing that I was always afraid of, can I read stories? I can memorize stories. I can tell stories. But your son being like, I want to read this book. And you having to be like, sorry I can't. And now that sorry I can't becomes sorry I can with the assistance of so many different tools. Now welcome back to how I AI. I'm Claire Vo, product leader and AI obsessive here on a mission to help you build better with these new tools. Today we have Joe McCormack, principal software engineer at Baby List, who has a vision impairment, and he's going to show us how he uses AI to build microchrome apps to make his everyday life and work a lot more accessible. You're going to learn how to use cloud code to write Chrome apps, and you're going to be inspired at the little things you can do to make your own Slack a little bit more efficient. Let's get to it. This episode is brought to you by Times, the intelligent workflow platform powering the world's most important work. Business moves faster than the systems meant to support it. Teams are stuck with repetitive tasks, scattered tools, and hard to reach data. AI has huge promise, but struggles when everything underneath is fragmented. Times fixes that. It unifies your tools, data, and processes in one secure, flexible platform. Blending Agette AI, automation, and human-led intervention. Teams get their time back, workflows run smarter, and AI actually delivers real value. Customers now automate over 1.5 billion actions every week. Tines is trusted by companies like Canva, Coinbase, Datab Bricks, GitLab, Mars, and Reddit. Try Tines at times.com/h how I AI. Joe, thanks for joining How I AI. And I want you to spend a little bit of time introducing yourself and your story and how AI has impacted your ability to do work and build interesting things and engage in lots of awesome projects and what's different about your life now with AI versus before.
>> So yeah, my name is Joe McCormack. I'm a principal software engineer at Baby List and I think I took maybe a little bit more interesting journey than most into the uh computer science world. So, right before I started college, I ended up losing most of my central vision due to a rare genetic disorder called Liber's hereditary optic neuropathy. And so, before starting uh at Harvard, I was more interested into the mechanical world and kind of robotics and everything in that space. and then uh found uh that that was a lot harder and doing things with my hands was becoming a lot harder um month after month. And so I took the intro to computer science course at Harvard and immediately fell in love um and found that I got the same feeling of creativity and being able to come up with the idea and make it happen. Um but now I was on uh maybe not a full equal plane to my competitors at that time or my my uh uh other students but then um obviously as AI took off became even more um equivalent and the gap between um I think software engineer for a cited person and a visually impaired person is is closing um day by day um and also in my personal life I think it's even been extra impactful I was talking with someone who was losing their sight recently from the same disease and they were asking about different things And I was like, "Oh, you can just do all of that now with sharing your your screen with Gemini or Chat VT." Whereas, uh, when I was first losing my site, it was using different magnification tools or or even like glasses and things. And it's like now the world is a whole lot easier. I'm an avid meta glass user. Um, and and different things to make my personal life a lot easier as well. But yeah, I I do lots of AI product engineering now and I at Baby List lead the uh AI enablement and trying to make sure all of our software engineers can build with AI uh as productively as possible at all different parts of the software development life cycle. So you figured out a way one to adjust your interest in engineering to something that's a little bit more accessible for you and then two lean into how these AI tools can really increase the accessibility and user experience of supportive technology that you've maybe used in the past but that you've been able to make better yourself. And what I love about this personal software moment that we're in right now, which is unfortunately accessibility software and custom software that meets the needs of a lot of people, is simply in some instances not an economically viable business, for example, to build. And so in the kind of broader economic world, there's not a lot of incentive to build a full set of robust tools that can meet the needs of everybody who deserves to have their needs meeting and needs met. And what I love about what we're able to do now with AI is not only our more interesting sort of accessibility tools and um and platforms being able to be built, but people can build these solutions for themselves and they can be very customized to your experience, your needs, your strengths. And I think that's a really underappreciated benefit of AI. And so you're going to show us some of the things that you've built for yourself and you're actually gonna walk us through your coding flow, which I think is really awesome on how to build one of these tools so we can follow us step by step.
>> For sure. So yeah, we can jump um right in. I'll show off uh two that I built myself ahead of time and we're going to do one on the fly. And I do think personal software is going to be huge. Um, one reason why I like um, building some of these, so I'm going to show off a couple Chrome extensions that I've worked on. And one thing I like about building some of these is compared to maybe some of the offerings we have today from the AI native browsers is ANA browsers are are great. Uh, like I do use comet. Um, but it's it's using the uh, the Swiss Army knife and it does everything. Um, but in order to do that, it some of its processes are definitely slower, which there are certain things that's totally fine for, but there's other steps where you want it to be really quick and you want to use the drill instead of the little tiny screw driver that came with this was Army Knife. And so, I'll show off a couple that I've already built and then we'll jump into one that's not necessarily even accessibility only that I think everyone could benefit from. So, I'm going to hop into Slack now. Um, Baby List where I work is a big Slack company. So, lots of my stuff is very Slack based um because it's where I spend a lot of my non-coding time. So, I'm going to hop in here and I'll share and show a couple ones I've built so far. So, this is a little temporary Slack channel that we have for this um with some example messages that were actual messages sent by my colleagues. I just had them resend it um into here. Um so, first one I demo is a image description tool. So, when someone sends me an image, I uh use a screen magnifier. So, I typically am looking at my screen at about 10x zoom, but it's not the easiest to uh to to do, and I prefer to not have to always be paying attention to that if possible. So, I use this tool about to show off to be able to get um the gist of an image without needing to ask somebody to explain to me or me have to actually use my eyes to do it. So, I have a shortcut in Slack. Uh, if I hit uh I'm on Windows, so control shift D on any message, it's going to pop up and go off and describe that image for me and uh tell me a little description of it. So, I can see, hey, it shows a modern infant baby stroller with a car seat. Um, it's got a canopy. It's got all details about this. And the cool thing is I can go ask some follow-ups. So, I can say, um, what age child is this for? And it will head off to chatbt. um and get the response for this as well. And so we can get our answers there. And it's just a nice way for me to not have to necessarily push back and work with people and get some answers to my questions um as as I go on this too.
>> And I think this is something that folks don't really appreciate, which is for you, you know, you have the ability to zoom in, look at this, but it's it's just from a time perspective probably a lot more tedious for you to do. And so folks have thought a lot about image to description in terms of generating metadata for e-commerce sites, which I'm sure you all think about a lot, or um we had an episode with a documentary producer that highlighted using imagetoext descriptions plus metadata to organize archival footage a lot easier. But this is a great example of image to text being just a much more efficient information transfer method for someone like you who might need to parse this this information differently. And then what I love about this is you could have just done the image description, right? Just what what is this image and tell me? But you actually were able to go that next step and say great if I need to query more information about this to understand more context. you make that really really easy. So, I love I just I love this example. And you you built this all yourself?
>> Yeah, this was uh probably 25 minutes of a cloud code session. It was it was pretty straightforward. Um
>> awesome.
>> A flavor of this one I'm working on right now is a version that works in Figma directly as well that given any Figma node will explain it to me with a a much different prompt.
>> Right. In the Figma case, I want to hear about the colors of the CTAs. I want to hear about all this stuff because I am a full stack engineer. And so obviously you can get all that out of Figma, but it's lots of clicks and lots of different steps. And being able to just hit uh one keyboard shortcut and find out what this design is really accomplishing is going to be a nice easy uh win for me. And that one is just about done as well. Um it's got a little bit of bug, so it's not ready to fully demo, but that's one that I'm excited about for as well. So before we go into building one of these, are there a couple other extensions that you've built just as inspiration for folks watching or listening that you think are really interesting to show?
>> Yeah, another one that's not necessarily accessibility focused. Um, but it's I think it's a cool one. Um, so I uh am not the best typer in the world. I I don't even think that's my vision's fault. Um, I just I I think I'd like to think I'm a touch typer, but I think my brain goes faster than my finger sometimes. So, I have one that I built that is just like a really easy spell checker. There's lots of tools that do this Grammarly and all this, but similarly, they're not all screen reader accessible. They're multiple clicks away sometimes. Um, and so I built one out that uh works in any input field in uh on the web. I'll demo it here. I'm going to say uh test testing typos in the message. And then if I hit uh control shifts here on this one, this is going to go off send that off to OpenAI and come back with with that. And while it was doing that for me on my screen reader, it said like processing spell check, spell check complete. And so I know when I'm writing a message, I don't need to necessarily worry about all the polish on it. Uh I can just do that. I hear spellch check complete. I'm ready. I go off hit that and send it off to people. And I have a prompt there that's basically like do not change any of the words, just fix typos. was like really hyperfocused to make sure that it's the content that I wrote but just with the typos corrected in it.
>> So I'm leaning in and smiling a lot one because if you've been watching how I AI you have heard about my fancy nails. I'm a fancy nail gale these days and with these fancy nails I cannot type anything. It is all typos. And so this is such a great little workflow that you built for yourself that um I'm I'm going to steal the two things I want to call out for people who are watching or not watching the details is you are actually running Slack right now in Chrome. So at first I was like wait how are all these apps interacting with the Chrome desktop app? But you're running Slack in Chrome, which means that these are all extensions that are available to you to interact with content in Slack and make modifications via um a Chrome extension. So, I think that's a really interesting hack for folks that are like, "Okay, I can't hack my way into the desktop app, but I can load um Slack in the browser and then on top of that add a a browser extension that can do these interesting things for me." The second thing I love is how you're using so many keyboard shortcuts to trigger these micro apps. And again, this is about efficiency. I always say in these AI products that latency is the killer feature. And so, anything you can do from a UX perspective or from a performance perspective to make these little apps more efficient, the better they're going to feel. And so I love that you, you know, type a couple keys and you get a fully corrected sentence here right in your browser. It's a great idea.
>> Yeah. The first version I had of this was just using the like chatbt uh alt space shortcut that would open up that little like mini chat window. And I had like a saved um uh just custom GPT to do it. And then I was like, well, why am I jumping out of where am I actually working? Uh I can I can save uh two steps in like three three clicks here. And so I think it's almost like piloting at first without doing this and then realizing like oh yeah there's a better way. And I think one thing about per software is the return on investment uh became so much faster. Like before you'd have an idea like that you like this is going to save me like 3 minutes a day but it's going to take me 3 days to build. So the payback period is just not totally there. And now it's like it saved me three minutes a day and it takes me 30 minutes to build. Like the payback period has just become insane for a lot of this tooling.
>> I I love it. So let's build one. I want to see what your flow is for actually building one of these things.
>> Yeah. So, before we build it, I'm going to talk about what I want to build. Um, so one thing that comes up a lot in uh in the baby Slack world and probably in many other companies Slacks is uh people send links all the time. Um, and for me, uh, I often just hit like the save for later button and then maybe at the end of the week I like decide if I want to read them. But I've realized that like maybe it's the the do the thing that takes you one minute uh in the moment instead of keep deferring it. I think it would be great if there was an easy shortcut where I could have an AI go off and fetch this article, give me the key takeaways, and then I'll decide, do I want to actually do a full read and save it for later, or do I just skip it in the moment and have that all work in under five to 10 seconds? I think it would be much more powerful than uh deferring and having this big to-do list at the end of the week when I want to catch up on all these messages.
>> So, you're going to show us how how you built this. And what I have to say is all of us are so overwhelmed with so much context and links and docs and yeah, you know, I would see something like this, whether it's um a partner or a competitor or just something somebody found that was interesting and you want to go, "Oh yeah, I should definitely read that." But should you should you definitely read it? So this quick summarization um is is a great idea. And so you're going to walk us through how to actually code this up using cloud code, I believe.
>> Yeah. And I'll jump in along the way with a couple like cloud code tweaks that I've made or at least lean into to try and make it a little more screen reader accessible. But again, I similarly think that lots of things that are uh good for screen readers probably are are going to be good tools for everybody across the board.
>> Great.
>> Um so let's let's jump right in. So I'm going to switch over uh to uh my terminal for just a second so I can initialize our project. So I'm going to run a make dur command to get our repo set up. So we have our slack summary extension. I'm making that directory and I'm just open up quad code quick on this or should I open up VS Code on this. So VS Code opening and initializing here. I'm going to open it up as big as I can and we are going to jump in as this finishes loading. So, I'm going to start here by making a PRD like every good how ai podcast goes.
>> That's exactly right.
>> And I'm going to do this with audio. Um, so sometimes I'll use whisper flow for what I'm doing. But in this case, I actually find the VS Code Copilot audio to be pretty good. And so if I'm just doing something kind of quick like this, I'll just end up using the C-pilot integration. So you'll see I can do uh control I and then when I hit control I again, it's going to dictate. So, I'm not going to I'm going to pause my talking and switch in that mode. And I'm just going to dictate out for us a little bit of this PRD and then see what it comes up with for us. We want to build a simple PRD for a locally run Chrome extension whose job is it to exist in Slack alone. And when focused on a Slack message, you can hit the keyboard shortcuttrlshift 1 and it will search that message to find any external links. If there are external links found, it should open them up in hidden tabs, extract their content, and send it off to OpenAI to summarize. And we'll see here, I just finished that. It's going to go off and quickly generate a small PRD for that. Um, this doesn't take very long at all. I'm just going to accept all the changes because I find reading in that diff view to be particularly painful with the screen reader. Um, it's not like terrible, but it's much easier just to read it in the document. And since there's a new document being made, I'll now look at it here and we'll see how this looks. So, we have our goals. We want some privacy security makes sense. Got some user stories here. We have some function requirements. It's got a parse. So, all is making sense so far. Some nonfunctional and some out of scope images. Yep. I already demoed my image processing and so open questions. All right, where and how should we summarize? Makes sense. We don't need success metrics for this. It's internal. So let's answer some of these questions. So let's just select it all and add it. I'm going to dictate again. So I can hit and I'll start. We want to build a very simple PRD here for a locally run Chrome extension where the job is when in Slack in Chrome and hovering over a message that has focus, you can run the keyboard shortcuttrlshift 1 and that will look for any external links in the message. If any are found, open them up in hidden tabs and extract that content and send it over to OpenAI. When in OpenAI, we should summarize them and extract three to five key takeaways from the article and return those to the user in a fully screen reader accessible modal which includes the article's title and a link out in a new tab to view the article. Okay, we now have this generating the PRD. So we can see it talked about our keyboard shortcut. It's got our goals, minimize user effort, extreme accessibility is key for this, some basic user stories, some function requirements. Cool. This looks good.
>> And one thing I I have to call out here is you're an engineer and that was a pretty good product description and that resulted in a pretty good PRD. So one of the things I like about AI is um as I say there are no lanes. If you are an engineer and you have an idea, you can write a very good uh PRD using a little bit of AI assistance. If you need a tip, I know one or two tools that can help you with it.
>> Yeah. And this is no Yeah, no custom PRD uh prompts or anything like that. This is just uh mostly the foundational models work here.
>> Y
>> cool. So now we're going to hop into a new tab, spin up claude code in here. And ahead of this, as I mentioned, I I built a couple of these Chrome extensions. So, because I've done a few of them, I end up building out a Claude skill to help me build more Chrome extensions. Um, so after I built this the first two, I had Claude look at both and figure out what was the patterns that were common across them and work on a skill so I could just build the third, the fourth, the fifth in a much simpler version um and extract out that common piece. I have found Claude skills to be a mixed bag in terms of them actually being picked up automatically. Um, so I am going to explicitly say like use the skill, but technically you're not supposed to need to do that, but I definitely found that that's uh buried in its approach. So I'm going to make a prompt here. It's going to more request the skill.
>> And for folks that are wondering how to set up their own um skills, we do have a episode. It is introduction to claude skills where I explain that claude skills are files in a in a folder. Sometimes they're zipped files in a folder. So, if they seem mysterious to you, go check out that mini episode from um I think it was October or November and learn how to make your own claude skills. They're pretty useful.
>> Yeah. So, in here, the prompt's just going to be um so first app mention the PRD. So, we have that and we're say use the claude skill for creating Chrome extensions to build out this PRD. And one thing um Chrome uh cloud code has added this feature where you can edit a prompt instead of just in the terminal in a code file. And so in cloud code if you hittrl +g g it will open that prompt in a text editor. So especially for me where navigating that terminal is not super screen reader friendly. I now navigating it in the same place that I write code on a day-to-day basis which is very screen reader accessible. And so again other people may find this to be useful. You can craft deeper prompts. You can like control F in here. You can do whatever. It's just a file. And so I think it's a really useful tool they added a few versions ago to make it a little bit easier to work with. And I'm just going to put a note here. Um, use my OpenAI key from my shared Chrome extension config. So I've had some I don't want have to keep pasting my OpenAI keys and stuff like that. So I end up pulling out some shared config to share across all my Chrome extensions. That way I don't need to rinse or repeat that step over and over again. So I save this file now. And now whenever I close this, it's going to replace my prompt in cloud code with that completed prompt.
>> Oh, interesting.
>> Yeah. So it's super effective, especially as you want to do a deeper prompt to use this and not have to worry about the the whole terminal side of things.
>> Cool. Now we're going to kick this off. And so I do have this Cloud Code session right now in planning mode. Um you'll see it's requesting use the skill, which is great. We want to use our skill. I'm going to shrink this terminal so we can see more clog. So, I don't really need this as much. Okay, it's going to run some commands um that actually pull in that cloud uh that Chrome extension config that I talked about. Another thing I have to make cloud code a little more uh accessible is again I'm not necessarily seeing everything that pops in as it's going. And so I set up a Claude hook that whenever Claude needs user input, it will um basically like ding a bell on my computer so I could hear uh a sound that is like, "Oh, Joe, you need to do something right now to work on this." Um I actually don't want Claude code to read this file. So I'm going to say no because this is is some secret configuration. So I'm going to say no, don't read that file. Uh now that has a API key in the but don't read it. If you need you then use jq to extract the keys. So again luckily I am an engineer so it's not like fully vibe coding this uh from scratch. I know that there's a utility that will extract just the keys out of a JSON object and claude won't be able to see the values which is the actual uh secrets there. So now here we are. it's um just going to make a sim link to this extension for us. Uh so we don't need to worry about the configuration. And if I ever do change it, it will automatically update in all my extensions. So instead of having each one have their own API key and my key becomes invalidated for some reason, um having to update all of them, I use this uh concept called a symbolic link. So just link the same config file into all my extensions. So one change fixes everywhere.
>> Yeah. And this is one of those things that's just easy to do when you're running these things locally or just building stuff for yourself is you just make the maintenance and um uh the maintenance and deploy of these really easy for yourself and make it as simple as possible for you to repeat um building things and using the same for example API keys and you know when you want to share all these publicly and publish them to the Chrome extension marketplace we can we can do a little cleanup.
>> Exactly. Um, so here's our plan. And just like before, CtrlG also works to edit plans in the editor. So similarly, this is uh going to be a pain to read in this terminal for Shreader. But also, if I want to make a tiny tweak to one thing here, I don't need to worry about telling Claude to update it or write it to a file, I just hit control G, opens it up in this file, and now we have our full plan here. And you can just tweak different parts of it if you want and uh and modify it. So it's another another great usage of the control G shortcut.
>> Yeah, I want to call this out for people because so many folks would get something like this and then if it was wrong kind of say, "No, this is wrong. Please update XYZ or ABC." And you know, you're calling out not only is that a pretty inefficient way for you to interact with this file in terms of accessibility and your need for a screen reader, but it's also just not the fastest way to give it feedback. And so your ability to just take this uh move it into code, use this control G, I believe, edit it, close it out, run it is just again a lot more efficient.
>> Yeah. So we'll do a quick run through of this plan. It's uh I'm using again some more keyboard shortcuts just to break it down and and uh fold the markdown heading so I can from my perspective, it's hard for me to visually I don't visually scan the page. So reading through a big file, I typically in code or markdown rely on folding. So I can collapse different sections and read them and then expand only the sections I care about. So I don't care about certain aspects here, but like maybe I want to get deep into the error handling piece. So I'll expand that section and just uh read this part. So we got some just logging, some key patterns, but this plan generally looks good to me. So now I will just save this and again close the file and that is what is over here in the prompt because I didn't modify it. It it didn't take any time. It was just ready. But if I modified it, it would take a split second while it loads that new plan in and then it moves forward.
>> Yeah. And I just have to call this again out again for people who are maybe listening to the podcast or again are not paying attention to what it means to use a screen reader here, which is you got your little your little headphone plugged in right now. And um I am so impressed that you're using the screen reader while walking us through this demo. And what I think is so so fascinating about watching your workflow here is it's super efficient and very fast. even if you don't take in to account you're using a screen reader. So, the fact that you've been able to build these shortcuts, these tools, use cloud code in a more effective way. Um, and then you add on this layer of and it makes using this this kind of screen reader a lot more accessible to you is just very impressive. And I don't want people to miss that there's this invisible layer that we don't get to see or hear right now that you're also putting in between this which adds a little bit of microf.
>> Yeah. And I think one thing that's great about cloud code uh so like right now visually one of the options is selected in blue. I don't necessarily know which one it is and using the arrow keys it does not tell me with the screen reader what's selected but cloud code has done a great job of standardizing where one means yes um two means often yes but like manually with with like a variation and then three is is like no or or type something extra. And so I can uh I can basically instead of using the arrow keys and enter, I can just be like, "Yeah, I want I want to just move forward here. I'm sure I hit the the number one." Yeah.
>> And so they've done a good job of using I think lots of different inputs and lots of different ways to make this a little more accessible as it goes as well.
>> Yep. And so that consistency again, maybe this is for folks that are building AI products in trying to reinforce workflows, especially for folks that are building maybe these terminal UIs that I think are really lovely and interesting to build is you want, you know, people love the terminal because it's so fast. And you want it to both be performance fast, but you also want it to be UIUX fast, which is if your user always knows one is X, two is Y, three is Z, then they can consistently use these keyboard patterns of one key or two keys to efficiently get through your UI. And I think, you know, taking that mental friction, that cognitive friction off a user by driving consistency and patterns that people can either explicitly or implicitly learn is a really useful tool when you're using UI um that is more constrained. Um because again, a terminal UI is naturally constrained to basically text.
>> Yeah. And cloud code is has been working on and and has released a VS Code extension that is more of a a guey. I've just found that it's a little bit lacking behind some of the the latest and greatest features and I'm like I want everything immediately. Uh some of the spoiled but I I think that's going to catch up as we go too and and maybe a potentially more screen reader accessible option for some of these things too.
>> Yeah. And again, we love a um beep boop at the end of an agent completion. So uh I love I love the cursor sound. I love um that you're using one here for for claude code because again I'm presuming with your screen reader you're not going to read this whole stream of what no that's too much. I think it's a big difference too between like vibe coding things and um like production quality code, right? The final output here is just for me. It's going to run in my Chrome. I don't really care what the code looks like at all. Um versus when I am building software for my my uh full-time day job and actually building stuff that's going to be in the hands of millions of users and many developers. I do uh do things a lot differently. The plan I'm going to read very detailed what it's going to actually do. um the the code I'm going to be reading, I'm going to be doing smaller commits and reviewing it uh kind of chunk by chunk and and getting much more detailed. In this case, uh yeah, when the final output is is just a user of one, the the code quality is a lot less important.
>> Yeah. What matters is does it does it work?
>> Yes, exactly.
>> And a does it work and a little bit of like did you leak your API key? Those are the two things we want we want to worry about.
>> But other than that, we are we are on our on our way. And you know the the other thing that I think is really fun here is because you've built I think the idea that you pick a platform or a framework for a set of your personal software and then establish best practices through a skill and then just rinse and repeat for other use cases is a really good way to get super fluent with some of these AI tools. And so I've seen a lot of people say all I do is build markdownbased repos for my documentation and everything else and everything I build is just a markdown based repo and then I've gotten really good at using cursor for this and then you know you have this example of every you know not everything I'm sure but like a lot of what I build are going to be Chrome extensions. So I'm going to make, you know, this framework, get it going, and then I can get really good at clawed code because I'm not relearning a technology and on top of relearning a tool. And so I do think it's um you get compounding effects by staying in the same technical space when you're trying to learn these voding tools because you're not trying to learn you're not trying to learn on two fronts. You're trying to learn on just the tool the tooling front and some of the technical pieces have already been established.
>> For sure. And I think as we talked about before when we were starting like the return on investment gets better and better because building this one as my third takes half the time as the first one and when I build the fifth I think it's going to take a fifth the time of the first one like I am getting better the claude skill each time I build one I'm just going to feed it back in and be like what was the skill missing like make make it better and so I think it makes uh the return investment may turn into be like two days a payback period or something crazy. Um, so yeah, it does it does feel like it's cool to to I think spread out and try many different things, but it does also just feel great to be like, I have an idea. It is in my hands in under 30 minutes. Like it's it's just very cool.
>> Yeah. And again, this is one of those things where I tell people to work on their anti-to-do list. And when you have a recurring task or a recurring point of friction where you're constantly like opening links from Slack in a new tab and then trying to come back to them later and read them or you know you would be constantly doing this like let's zoom in on this image and figure out what it is and is it something I need to worry about. When you have those recurring tasks, it's 100% worth it instead of spending the time on the task itself to spend the time never having to do that task again. And I like this idea of the payback period of personal software basically collapsing to zero because it really just illustrates where we are in terms of the efficiency and value out of AI which is it is much more important to learn to build some of these tools than to do the task right now. Like the payoff is so much higher to learn how to automate the task versus doing the task. And if you can just do the um change your muscle memory to every time you do the task, pause yourself and say, "Actually, I'm going to learn how to how to automate the task." You can really really create a lot of leverage in in your um day-to-day life, even in your personal life.
>> Yeah, for sure. And we are just about done here. So, I was checking back in on our little uh to-do list, which it just finished. Um, so it's doing some final steps here, but we're just about ready to actually load this in. Um, right now it's just kind of analyzing to see uh, yeah, perfect. So, it's running this one last step, which is going to be it's basically telling us, hey, go load this in. So, once we are uh, done with this, we need to um, actually load it into Chrome. So, Chrome has a mode for extensions called developer mode. You'll see I have that toggled on at the top. And it basically means that you can install extensions not from the Chrome web store. You can install extensions like from your local computer. So, you don't want to generally have that on cuz like somebody could have sideloadaded in some uh some credit card skimmer that you've imported or something. But if you know what you're doing uh and you know you just built a thing, you can go in here and turn this on. And then this looks a little bit different once you have this on compared to probably what you guys who are not having this on look in your Chrome extensions world because we have these options on the side here to load an unpacked extension. So this means an extension that's not like fully deployed in the app store. So let's hit this and this is going to open up our little uh file browser here. So let's just pop back and we had called this Slack summary extension. Okay, so that is now loaded in. The moment of truth with the truth of this software is you can only test it uh or easily test it in Chrome. Um so we're going to actually try it out. Uh whenever you download a new extension, you do need to refresh your tab so it picks up that extension. So if right now I tried to use it, it's still working with the extensions that I had at the time I loaded it. So I'm going to refresh this Slack so I can pick up our new uh extension here. And our moment of truth is going to be we're going to focus on this message. And I'm gonna hit my shortcut of control shift one and we'll see. Did we uh did we nail it?
>> Oh, look our link.
>> Black color, right?
>> Yeah, it's kind of interesting. So, it's processing our link. We'll see. Did it work?
>> So, kind of work, but we have JSON here, right? It's not it's not perfect. Uh so, let's let's work on one one level of refinement here. So, I'm going to take a a quick snippet shortcut of of this. So, we're gonna take a screenshot of this and uh we're gonna send this back to Cloud Code and say almost. So, again, it' be cool if we oneshot it. Uh a really cool demo. Um but it's not a perfect oneshot. So, we'll make one slight tweak here. And I'm actually use a custom slash command I wrote to deal with screenshots. So, because I'm developing on Windows, but I actually run Cloud Code in uh this thing called the Windows Subsystem for Linux. It doesn't have access to my Windows clipboard. So, I can't do what everyone else can do, which is just like hit controlV here and paste it. So, I added a slash command here called paste image that uses some PowerShell shortcuts to pull images out of my clipboard and share them with Claude. And so, I can take that snippet and share it. And again, this is similarly, I like would copy a file and I'd like save it in Windows, then move it to Linux, and then import it with an app mention. And I was like, there's got to be a better array. And I use the slash command now all the time for for building stuff out.
>> This is this is extreme software engineer stuff where you're like, okay, I run on this OS, but I run my terminal on this and now now I can't access my clipboard, but I still like it that way. So, I'm going to write a little script to give myself a a two-word shortcut to make this happen.
>> And so, it ported this and it just it drops the screenshots in our in my TMP directory. So, I just have to say, "Yep, please read it." Um, so again, it's it's saving those uh those two minutes every day. Uh, adds up adds up fast. Um, and so it's just gonna fix this little JSON piece here. And again, it was kind of interesting. It was close. It it it got the right content. It just didn't display it, right? And so now it's going to go off and work on this for another second here, and we should hopefully have a a quick update. And the nice thing is with uh with Chrome extensions in the developer mode, there's just an easy one-click button where you will update and grab the latest copy of all of your extensions. So if as you're working on multiple at a time or whatever, you can just hit that button and it's going to update for us. So we'll see it's finishing up here.
>> And one of the things that it's doing just I'm calling this out for people who are writing um queries to OpenAI. It's moving the JSON out of the prompt which it's saying please return JSON. I think it's actually just returning JSON as a um string in text and it actually moved that to change the response object to being JSON. So then it can actually be read by the um by the Chrome extension in a in a more structured way.
>> Control shift one. See if it works.
>> We're doing truths.
>> Beautiful.
>> We got it. Nailed it. So again, we've got these takeaways. We can now action on this and I can decide, do I actually care that iterable added MCP? I do. Uh spoiler alert. Um but yeah, again, we did it uh I think under under 25 minutes here. And is it working in your screen reader? Did the accessibility
>> fully fully accessible? Um so this modals in general can be sometimes problematic. Um because a screen reader will sometimes read behind the modal.
>> Um but surprisingly although not a bunch of the web is not accessible if you tell some of the foundational models like please make this accessible the accessibility standards are actually incredibly well documented. So they they do actually a great job of this. So they they use the right what's called Arya A R I A roles and make this modal have the right focus not let you read behind it. So uh out of the box they're not going to make everything extremely accessible but you say hey go do this it it'll gladly go follow the spec and make it accessible.
>> So this is a meta question and maybe before before I get into the meta question let's just recap for folks what we saw. So you built a Chrome extension that's focused to the web version of Slack. um that that Chrome extension that's running locally because you've toggled on developer mode in your Chrome settings will take a focused link that's shared by a colleague or somebody in Slack. It will go out. It will parse that link. We'll see if there any links in it. It'll parse it. It'll take some key takeaways. The way you built this is you bopped into VS Code. You dictated a short PRD. Um you let AI kind of build that out. You made minor tweaks to it, but basically shipped it. You used Claude code including some custom slash commands um and a cloud skill specifically around building uh Chrome extensions to then scaffold out that Chrome extension. Um you showed us control G in cloud code where you can actually just modify prompts and inputs as code which is much more efficient both from an accessibility perspective and just general user experience perspective. and you uh I showed us a custom screenshot so your very special um as I say you know unique snowflake software engineer environment can operate as if you as you want even if there are some technical hurdles and then now we have this great little extension that I want running on on my app. So this is great Joe I love this. I want to hop into some lightning round questions and this has given me an idea of one that I really want to ask you which is about MCPS. So, one of the things that I think are is so interesting about MCPS is it allows you to bypass all UI and just get to the bones of what a SAS product does. And I can imagine that while there are lots of okay accessible enterprise software products, not all of them are building for maximum accessibility either in their design or in their kind of under underlying way they're implemented. Have you found MCPS and just that interface into some of these SAS tools has improved accessibility for you? Has not? What are your thoughts there?
>> I think the ultimate goal uh Moh of mine is I would love to do everything in one place and not have to switch tools like whether it's a context switch cost or just a a switch cost. So have MCPS has been great for that. Um luckily I actually think lots of enterprise software surprisingly is being built pretty fastly like I want to really give strong kudos to like Google Docs. Google Docs for what it does is so crazy accessible and the work that people don't know that goes into make it that like every single thing that is being done is being communicated to the screen reader basically letter by letter um via this like secreting uh the secret system that people don't know about called like ara live announcements is kind of crazy um but I do find like hey I need to get something from three sites like that's kind of painful like can I just use the notion MCP and cool docs MTP or glean in there is greatly notion is one that is a little bit harder. I think they they do their best from accessibility standpoint in some ways, but there's a lot going on in a notion post notion article. So, I think that's another example where it's like, yeah, I can just pull this down and work in the markdown version that's going to be a lot easier. Um, and again, I've got it's way easier for me to navigate with these like keyboard shortcuts and the folding feature. So, I will pull down some notion posts and just be like, dump this into a markdown file for me and then use my little code shortcuts to help navigate some of those pieces.
>> Yeah, I love that. And so my second question again is around personal software and the ability to translate sort of as you said you can take a pretty complex notion page turn it into markdown that allows you to you know read and parse it in a much more efficient way and so I think this ability to translate files or formats is a really exciting part of AI and we've hinted at a couple things we've seen in this episode a little bit of like image to text, a little bit of voice to text. But I'm curious for you, what are you most excited about in sort of like the multimodal world of AI? And you know, what recently has come out that's, you know, caught your eye and made you excited or what are you hoping to see in the next couple months or years that you think could really open up stuff either for you personally or just as a product builder?
>> Yeah, I'll talk in the personal space. So, I have a I have two kids. I have a 5-year-old and a three-year-old and um reading books to them is is a challenge. I don't know Braille. I've uh been trying to learn, but it's a a hard skill to pick up at 33. Um and so I've memorized a handful of books. Um and I'll read those, but it's uh it's not really reading. It's fake fake reading. Um but I big shout out to the Gemini app and its uh like live share features. I can now read any book. Uh it sounded like me reading it, but me and my three-year-old Cole will sit on the couch. He'll bring a book over and I'll be like, "Hey, let's I I can read this one or no, Gemini can read this one for us." and we'll turn the pages and say like Gemini next page and Gemini will read that page and then we'll turn to the next one and it'll read the content of that. And so I think just like equitable access to everything um is is great and that piece is one thing that I was always afraid of is like can I read stories? I can memorize stories. I can tell stories but there is something to just like your son being like I want to read this book and you having to be like sorry I can't. And now that sorry I can't becomes like sorry I can with the assistance of so many different tools now. But I think the Gemini one is particularly useful and I found that one to be the strongest for just like easy sharing just saying next page. It knows all the context. It immediately starts reading it. Um I have like metagasses. I have the chatb pro. I've got all these things but I think Gemini is doing the best job of it right now. And this is before me trying any of the Gemini Pro uh 3 that just came out to see if any of that makes it even better.
>> Well, that is a very very sweet story. And yes, I was just thinking I wish um the Meta Glasses, which I love um and and use every day, would would would also help do a better job there. But it's awesome to hear that Gemini can add to that that special time with kids, which as you and I were talking before the show, you know, we both are boys, parents of boys, is just is is such a special time. So my last question is when AI is not listening and I'm curious if you type or if you speak this what are your prompting techniques have you ever whisper flow yelled at your AI or you know do you have any tricks for us when AI or Claude gets really really stuck? It's kind of I think a nerdy answer which makes sense for for me but uh I my typical mode is basically like clear cont the context and uh and start fresh as much as possible. I think I think a lot of people uh will will try and like keep massaging it and being like if I just send this one extra prompt in this conversation it'll figure it out. It's like no you just have to start from scratch and take the learnings that you have from the last time. Um and so some would be like this hasn't been going great. What did you what did you learn about this? take that and feed that into the next prompt. But most time I used to be like, "Let's start from scratch." Something clearly got poisoned in this context. And we start from scratch. I feel like it just everything just feels smoother.
>> I love it. Well, Joe, thank you so much for showing this. I think it's just one of those uh workflows that we haven't seen before. Everybody can find a use case. I am thinking of all sorts of little micro frictions in my own life where a keyboard shortcut or two could really make things a little bit better for me. So, where can we find you and how can we be helpful to you?
>> Yeah. Um, so I mentioned um at Baby List, Baby List is very actively hiring. Um, and if you are somebody who likes using AI in your day-to-day building of of software and you're a a software engineer uh we are a Ruby on Rails and React shop um but hiring across the board, all different levels. So, uh check us out on uh on babos.com and uh personally I'm on LinkedIn as well. So feel free, especially if you have any accessibility questions or any questions on some of the Chrome extension path, I'm always happy to answer on LinkedIn.
>> Well, thanks for joining How I AI.
>> Thank you.
>> Thanks so much for watching. If you enjoyed this show, please like and subscribe here on YouTube or even better, leave us a comment with your thoughts. You can also find this podcast on Apple Podcasts, Spotify, or your favorite podcast app. Please consider leaving us a rating and review, which will help others find the show. You can see all our episodes and learn more about the show at howiipod.com. See you next time.