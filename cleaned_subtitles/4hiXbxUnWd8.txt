I feel like these scaling laws have been very predictable but then when you say like well you know when when is there going to be a commercial explosion in these models or what's the form it's going to be or are the models going to do things instead of humans or pairing with humans I feel like certainly my track record on predicting these things is is terrible but I also looking around I don't really see anyone who who track record is great I've been right about some things but I've still you know with these theoretical pictures ahead been wrong about most things being right about 10% of the stuff is you know sets you head and shoulders AB above um above above many people you know if you look back to I can't remember who it was kind of you know made these diagrams that are like you know here's here's the villageil idiot here's Einstein here's the scale of intelligence right and the V Village Idiot and Einstein are like very close to each other like that maybe that's still true in some abstract sense or something but it's it's not really what we're seeing is it we're seeing like that it seems like the human range is pretty Broad and doesn't we don't hit the human range in the same place or at the same time for different tasks right like you know like write write a sonnet you know in the style of cor MC McCarthy or something like I don't know I'm not very creative so I couldn't do that but like you know that's that's a pretty high level human skill right um and even the model is starting to get good at stuff of you know like constrained writing you know there's like write a you know write a page without using the letter e or something write a page about X without using the letter e like I think the models might be like superhuman or close to superum at that um but when it comes to you know I yeah I don't know prove relatively simple mathematical theorems like they're they're just starting to do the beginning of it they make really dumb mistakes sometimes and they they really lack any kind of broad like you know correcting your errors or doing some extended task and so I don't know it turns out that intelligence isn't isn't a spectrum there are a bunch of different areas of domain expertise there are a bunch of different like kinds of skills like memory is different I mean it's all it's all formed in the blob it's not it's all formed in the blob it's not complicated but to the extent it even is on the Spectrum the spectrum is also wide if you asked me 10 years ago that's not what I would have expected at all but uh I think that's very much the way it's turned out one thing that's been surprising is like I thought things might click into place a little more than they do like you know I thought like different cognitive abilities might all be connected and there was more of one secret behind them but it's it's like the model just learns various things at different times you know and it can be like very good at coding but like you know it can't it can't quite you know prove the prime number theorem yet and I don't I mean I guess it's a little bit the same for for humans although it's it's weird the just deposition of things that can do and not I guess the main lesson is like having Theories of Intelligence or how intelligence works works like a lot of these words just just kind of like dissolve into a Continuum right they they just kind of like dematerialize I think less in terms of intelligence and More in terms of what what we see in front of us if you told me in 2018 we'll have models in 2023 like law to that can write theorems in the style of Shakespeare whatever theorem you want you want they can a standardized test with open-ended questions you know um just all kinds of really impressive things you would have said at that time I would have said oh you have AGI you clearly have something that is a human level intelligence where these while these things are impressive it clearly seems we're not at human level at least in the current generation and potentially for generations to come what explains discrepancy between super impressive performance in these benchmarks and in just like the things you could describe versus yeah generally so that that was one area where actually I was not presses and I was surprised as well yeah um so when I first looked at gpt3 and you know more more so the kind of things that we built in the early days at at anthropic my my general sense was I you know I looked at these and I'm like it seems like they they've really grasped the essence of language I'm not sure how much we need to scale them up like maybe we maybe what's what's more needed from here is like RL and all and kind and kind of all the other stuff like we might be kind of near the you know I thought in 2020 like we can scale this a bunch more but I wonder if it's more efficient to scale it more or to start adding on these other objectives like like RL I thought maybe if you do as much RL as you know as as you've done pre-training for a for a you know 2020 style model that that's that's the way to go and scaling it up will keep working but you know is that is that really the best path and I I think it I don't know it just keeps going like I thought it had understood a lot of the essence of language but then you know there's there's kind of there's kind of further to go the models are maybe two to three orders a magnitude smaller than the human brain If you compare to the number of synapses while at the same time being trained on you know three to four or more orders of magnitude of data if you compare to you know number of words human a human sees as they're developing to age 18 we have to admit that that's a weird thing that doesn't match up and you know it's one reason I'm a bit you know skeptical of kind of biological analogies I thought in terms of them like five or six years ago but now that we actually have these models in front of us as artifacts it feels like almost all the evidence from that has been screened off by what we've seen and what we've seen are models that are much smaller than the human brain and yet yet can do a lot of the things that humans can do and yet paradoxically require a lot more data um so maybe we'll discover something that makes it all efficient or maybe we'll understand why the discrepancy is present but at the end of the day I don't think it matters right if we keep scaling the way we are I think what's more relevant at this point is just measuring the abilities of the model and seeing how far they are from humans and they don't seem terribly far to me what do you make of the fact that these things have basically the entire Corpus of human knowledge memorized and as far as I'm aware they haven't been able to make like a single new connection that has led to a discovery whereas if even a moderately intelligent person had this much stuff memorized they noticed oh this thing causes this symptom this other thing also causes this symptom you know there's a medical cure right here right what shouldn't we be expecting that kind of stuff I'm not I'm not sure I mean I think you know I don't know these words Discovery creativity like it's one of the lessons I've learned is that in in you know in kind of the Big Blob of compute often these these ideas often end up being kind of fuzzy and Elusive and hard to track down but I think I think there is something here which is I think the models do display a kind of ordinary creativity again again you know the kind of like you know write a write a Sonet you know in the style of cormic McCarthy or Barbie or you know like there is some creativity to that and I think they do draw you know new connections of the kind that an ordinary person would draw I I agree with you that there haven't been any kind of like I don't know like I would say like big scientific discoveries I think that's a mix of like just the model skill level is not is not high enough yet that I think is going to change with the with the scaling I do think there's an interesting point about well the models have an advantage which is they know a lot more than us you know like should should they have an advantage already even even if they skill level isn't isn't isn't quite High maybe that's kind of what you're getting at I don't really have an answer to that I mean it seems certainly like memorization in facts and drawing connections is an area where the models are ahead and I I I do think maybe you need those connections and you need a fairly high level of skill I do think particularly in the area of biology for better and For Worse the complexity of biology is such that the current models know a lot of things right now and that's what that's what you need to make discoveries and draw it's not like physics where you need to you know you need to think and come up with a formula in biology you need to know a lot of things right and so I do think the models know a lot of things and they have a skill level that's not quite high enough to put them together and I think they are they are just on the cusp of being able to put these things together