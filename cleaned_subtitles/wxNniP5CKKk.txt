大家好欢迎来到我的频道 今天这期节目呢 我们来讲一讲ChatGPT的本质啊 那基于的内容呢 是这个Stephen Wolfram写的这本书啊 叫做ChatGPT在做什么 以及他为什么能够成功 这本书呢是2023年的时候发表的 那我其实这个频道呢 之前也做过两期 关于他这个书 里头的一些章节的内容啊 但是当时呢 这个频道的流量实在是非常的可怜 然后呢 做出这种硬核的东西就没人看啊 因为本来做这种 涉及到偏硬核一点的东西 就看的人很少 再加上你的订阅数又不行 那自然就没什么人看啊 现在我来想试一试再做一下这本书 因为我觉得确实写的挺好的 前段时间呢 做了那个Gary Marcus 的这个批评 做了那个Gary Marcus 的这个批评 ChatGPT的视频啊 底下好多朋友在那留言说啊 这个Gary Marcus一看就是不懂AI啊 但是呢尽管Gary Marcus 他已经曾经创办过一个AI公司 还被Uber给收购了 还赚了很多钱 但是呢他就是在这些观众的口中啊 就是不懂AI的 对吧 大家可以对自己不太认可的观点啊 保持相对宽容一点 有的时候呢 你可能获得更多信息啊 理解更深刻一些 你可能会发现哎 这个观点其实还是有一些道理的 我觉得大家要有一个open mind啊 你像我 都能够做这个关于Peter Thiel的视频 我是本来觉得他是一个这种MAGA 背后的mastermind啊 这个右翼MAGA 但是呢我深入了解这个人之后呢 我发现他其实还是 不一样的地方的 那我希望接下来做的这个 Peter Tiel的这个系列 能给大家带来一个不一样的呃视角 那么我们今天讲的这本书呢 其实就是给大家拆解一下 ChatGPT的本质 其实呢呃 Stephen Wolfram啊 他对于GPT 的这个看法 跟这个Gary Marcus有一点相似啊 这里算不算剧透呢 就他认为啊 我们感受到这种智能和理解 以及创造力啊 是一个彻头彻尾的幻觉啊 那我们先直接进入主题了 那么在Stephen Wolfram的看法中啊 这个能够写诗 能写代码 能跟你谈天说地啊 通过各种考试 看起来无所不知的这个ChatGPT啊 在本质上 只是一个无比强大的文字接龙工具啊 这个Stephen Wolfram呢 其实在他的这本书里头呢 对这个GPT 内核的这个神经网络啊 做了非常深入的解释 那么大家感兴趣的话 可以去看一下他写的这些技术细节啊 就是在Stephen Wolfram看来啊 我们人类感觉到那种智能 理解创造力 是一种彻头彻尾的幻觉啊 我们觉得能够做到这一切的背后 一定有一个复杂的 类似于人脑的思考过程哎 但是Stephen Wolfram告诉大家 他生成每一个词 都是在做一个极其简单的数学题 就是根据前面内容下一个词最大概率 是什么 接下来节目呢 我们就借助Stephen Wolfram这一本小册子 给大家拆解一下ChatGPT底层的逻辑啊 在介绍这本书之前呢 我必须先隆重介绍一下 这本书的作者啊 Stephen Wolfram啊 这个人绝对是一个猛人啊 如果你大家不知道他这个背景的话 看完我下面的介绍 你肯定会对他有了新的认知啊 这个Stephen Wolfram 他不是一个普通的程序员 或者是科技评论家 他是一个跨越了物理学 数学和计算机科学与哲学的通才 猛人啊我们能看到 一本写的 把这个GPT讲的那么透彻 的一个小册子啊 是因为他过去40多年人生啊 几乎全在为理解这种复杂系统做准备 我们先看一下他的这个履历啊 这位老兄呢 15岁的时候 就发表了第一篇 关于粒子物理的科研论文啊 他20岁就拿到了 加州理工学院的理论物理学位 的这个博士学位 那时候 我们还大部分人都还在读本科啊 然后呢他就拿了麦克阿瑟天才奖 成为那个时代的 最耀眼的学术明星之一啊 我们今天 不是在感叹于这个神经网络的神奇吗 这个Stephen Wolfram 他在1983年 自己就亲手写过一个神经网络程序了 这概念是什么 那会儿连Windows都还没诞生啊 所以他对这项技术的理解呢 其实有长达40年的历史纵深啊 然后呢他人生最重要的项目 就是花了40多年时间 几乎以一己之力啊 打造了一门全新 的语言叫做Wolfram language 也是Mathematica背后的语言 这个Mathematica呢 就有点类似于MATLAB 是一个数学的编程软件啊 其实是非常好用的 那我那时候 上这个量子物理的课程的时候呢 必须得用这个东西来算 解这些什么偏微分方程啊 它背后是有一个非常强大的 这个计算引擎的 在做各种运算的时候是非常的迅速 那么他的野心 其实就是想要创造一种 能够精确描述和计算世界万物的 这种符号语言 从程式，化学分子到金融数据 他都想用一种统一的 可计算的框架去表示啊 他也是这个知识引擎啊 这个Wolfram Alpha的发明者 如果你用过Siri啊 你可能间接用过他的技术 那个能够直接回答你啊 珠穆朗玛峰有多高 或者是帮你解数学题的这种搜索引擎 啊 Wolfram Alpha就是基于Wolfram language打造的 它其实代表了和ChatGPT啊 完全不同的另一条AI的路线 这还不是最夸张的 他自己一生出版了近300万字的著作啊 过去30年写了1,500万字的邮件啊 总共打了大概5,000万字 你敢信啊 他连自己写了多少邮件 都统计的清清楚楚啊 这展现他对于万物量化和记录的 这种极致追求 你看到 当这样一个人来解读ChatGPT的时候 他的视角是非常独特的 他既是那个 亲手实践过这个神经网络的圈内人 而且呢 他 又是创造了另一条技术路线的旁观者 他能够从最底层的这个代码 和数学原理出发啊 又能够上升到计算物理 以及宇宙的哲学高度啊 他不是在猜测ChatGPT在做什么 他是用自己构建了半辈子的 关于这种计算和知识的 庞大理论体系啊 去框定和解释ChatGPT这个新物种啊 这也是为什么 他这本小册子我非常推荐啊 在介绍完他的背景之后呢 我们来看一看他的这个解剖之旅啊 就大家知道吗 这个ChatGPT背后的核心技术啊 神经网络 其实一点都不新啊 Stephen Wolfram 他自己就提到啊 早在1983年的时候 他就亲手写过一个神经网络程序 那时候呢 电脑还是个大家伙 慢得像个老牛拉车 结果呢 这个程序啊 什么有趣的事情也干不了 哎这就很奇怪了 一个40年前就有的老技术 怎么就在今天核爆了呢 这其实就跟做菜一样 你光有食谱啊 也就是这个神经网络的一个理论 是没用的 你还需要有顶级的食材和给力的厨具 你看这40年间 发生了三件大事 才凑齐了AI这盘满汉全席啊 第一就是算力爆炸 我们今天电脑啊 比40年前快了何止100万倍啊 过去要跑几年的计算 现在可能几秒钟就搞定了 这为训练一个巨大无比的神经网络 提供了可能 第二呢就是数据海洋 这是互联网诞生啊 给我们带来数十亿计的网页数字 化的图书和各种文本资料 相当于给AI提供了几乎无限的食材 去学习去品尝人类语言的各种味道 第三呢就是工程上的各种骚操作 也就是一系列算法和架构上的创新 这让顶级大厨们 琢磨出来各种烹饪技巧啊 能让同样的食材呢 做出这个米其林3星级的这个味道 所以你看啊 ChatGPT的成功啊 并不是凭空冒出来的魔法 而是一场酝酿了40年的技术风暴 而是一场酝酿了40年的技术风暴 当一台啊 比这个Steven Wolfram 当年那台大了十几亿倍的神经网络 被喂进了整个互联网的知识之后啊 一个奇迹就发生了 他真的学会了像人一样说话 但是呢这正是像人一样说话 恰恰是让我们最困惑的地方 我们人类说话背后是意义在驱动 我想表达一个意思 然后组织语言说出来 而机器呢 它懂得什么是意义吗 这就是我们要解开的核心谜题 ChatGPT这一个黑箱啊 到底是怎么运转的 他生成那些看似充满意义的文字啊 内部发生了什么 他为什么能够做到 而且还做那么好 这个Wolfram这本书啊 其实就像一把手术刀 给我们精准的剖开这个黑箱 他没有用太多这个花里胡哨的术语啊 而是用近乎第一性原理的方式 带我们从最基本的单元开始 一步步搭建啊 对这个ChatGPT的理解 接下来内容可能有点硬核 但别担心啊 我会用比较接地气的比喻 带你弄懂这一切 我的目标呢 就是当这个视频结束的时候 你不仅知道ChatGPT是什么 更能理解它为什么是这样 以及你未来和AI打交道时呢 有更深刻的洞察力 好我们现在正式进入ChatGPT的大脑啊 记住我们开头那个暴论嘛 它是在玩文字接龙 我们来看一个例子啊 假设我们输入 AI最好的地方在于他能力去 接下来呢 ChatGPT要做的 不是要去思考AI到底有什么能力 而是要去他的记忆头搜索他 记忆是什么呢 是他读过数十亿人类写的网页和书籍 他会统计啊 在人类的语言中呢 当出现AI最好的地方在于他的能力去 这句话之后 最可能出现的词是什么 它有可能出现一个概率列表啊 就像这个学习啊 这个概率可能是最高的 然后呢就是预测predict 然后呢就是创造create 或者是理解understand 但是呢他并没有真正在理解 他只是在做统计和预测 那接下来怎么选呢 最简单的想法 就是每次都选频率最高那个词 对吧但有意思的来了 这个Wolfram管呢 这个叫做一点巫术的开始啊 就是如果每次你都选最正确的答案 那写出来的文章会非常无聊死板 甚至有点不断重复 所以呢为了让文章显得更有趣啊 更有创造力 那么ChatGPT呢 会引入一个随机性他偶 他偶尔会选择一个概率没那么高的词 这种随机性的程度呢 由一个叫做温度 temperature的参数来控制 温度越高呢 他越可能不走寻常路 选出一些更意外的词 这就是为什么 你用同一个问题问他好几次啊 都会得到不同答案的原因 所以现在你明白了吗 一篇洋洋洒洒的千字长文 在ChatGPT看来 就是不断重复一个动作 根据我已经写出来的所有文字 下一个最合理的词是什么 然后啪加上一个词 然后再重复 啪又是一个词 就是这么简单粗暴 听到这你肯定会问啊 这概率是从哪来的呢 难道 是硬生生统计所有的这个词语组合吗 我们先从一个更简单的模型说起啊 如果我们不用这么复杂ChatGPT 只是用最原始的统计方法 能生成像样的句子吗 比如说啊 我们用统计英语单个单词出现的频率 然后像抽奖一样随机往外蹦词 结果呢肯定是一堆胡言乱语啊 比如程序过度被研究 是不是这样的啊 这根本就没法看 然后我们再升级一下 统计两个词连在一起的概率啊 这个叫做2-gram啊 比如呢 cat后面很有可能跟的是is或者是that 这样生成的句子会稍微通顺一点 但依然是胡说八道 问题在哪呢 问题在于啊 语言的依赖关系是长程的 一句话呢开头 可能会影响到十几个 甚至几十个词的结尾 所以呢 要统计所有20个词组合的概率啊 这数字比宇 宙里的粒子总数还要多 历史上所写过的文字加起来 总个零头都凑不够 所以呢靠死记硬背的统计是不行的 那怎么办呢 那就得引出这个故事里头 最关键的主角了 模型简单来说啊 模型就是这种举一反三的工具 我们不需要看过所有的句子 我们只需要给他喂足够多的例子 让他自己去总结 出一套生成概率的规则 这样呢 哪怕遇到一个他从来没有见过的句子 根据这套规则呢 也能够估算下一个词的概率 ChatGPT用的就是这种模型 也就是我们前面所提到的神经网络 神经网络呢 这个词听起来很高大上 但我把它拆开来看 其实没那么玄乎 你可以把它看成一个超级复杂的函数 我们都知道 函数嘛给一个输入x 它给出一个输入y 神经网络也是这样 只不过它的输入和输出呢 就超级复杂 我们拿一个简单的例子啊 就是识别手写数字来理解一下 就是识别手写数字来理解一下 你看到一张呢 手写着数字2的图片啊 对于电脑来说呢 它就是一堆像素点 每个点有一个灰度值 我们可以把所有的 这些像素点的灰度值啊 拉长成一长串的数字 作为神经网络的输入 然后呢这串数字就在网络里 开始他的奇幻漂流了 这个网络有很多层的人工神经元组成 每一层神经元呢 都会对上一层传过来的数字 进行一次加权求和 然后呢 再套上一个简单的激活函数比 如说呢小于0就归0 大于0就不变 算出来结果呢 再传递给下一层 就这样一层层的传达下去 像多米诺骨牌一样 最后呢在输出层啊 大概有10个神经元 分别代表了数字0-10 经过计算之后啊 哪个神经元输出的数值最大 网络就会认为啊 那张图片就是哪个数字 比如在这里呢 2那个神经元的数值啊 会是最高的 很神奇吧 一个由成千上万 极其简单的数学运算组成的庞大系统 竟然能够识别这么智能的任务 那么这些神经元之间的连接权重 就是那个加权求和的权 是怎么来的呢 是天上掉下来的吗 当然不是了 是训练出来的training 或者说呢 是调教出来的 我们给他看过成千上万张 已经标注好答案的图片 比如说给他一张2的图片 如果他识别错了 我们就告诉他错了 正确答案是2 然后呢用一个反向传播的算法 微调网络里成千上万个权重 那下次再看到这张图时呢 犯错的可能性小一点点 这个过程呢 Wolfram打了个比方啊 就是在一个有着无数山谷 和山峰的复杂地形上 找最低点 每一次调整权重呢 相当于我们朝这个山坡最陡峭的方向 挪一小步 也就是所谓的梯度下降 经过几万次几亿次的挪动 我们最后就能找到一个足够低的山谷 这时候呢 网络的权重基本上就调教好了 他学会了识别数字 我们知道了神经网络 也知道它怎么被训练的 但有一个关键问题还没解决啊 怎么把文字这个东西 喂给只懂数字的神经网络呢 总不能简单的给猫编号为一 狗编号为2吧 这样机器没有办法完全理解 猫和狗在意义上比猫和椅子更接近 于是呢一个天才般的想法诞生了 叫做词嵌入word embedding 简单说法就是说呢 我们不要用一个数字来代表一个词 而是用一个向量 也就是一长串数字来表示一个词 这个词呢 可以看作是在一个几百甚至上千维度 我们没有办法想象的意义空间的坐标 这个坐标怎么算出来的呢 还是靠训练 比如说呢 一个神经网络里 是预测这个句子中间的词 经过海量文本的训练 网络会发现 鳄鱼alligator和短吻鳄crocodile 在出现的上下文环境总是很相似 于是呢 它就会在那个高维度的意义空间里啊 这两个词的坐标放的特别近 而萝卜和老鹰 这个两个词的上下文天差地别 它们坐标就会离得特别远 你猜怎么着 当这个意义空间构建好之后 一些奇妙的事情就发生了 人们发现啊 在这个空间里啊 词语之间关系竟然可以用向量的 加减法来表示 最经典的例子就是 国王的向量减去男人的向量 加上女人的向量 等于王后的向量 那这就很神奇了 这意味着 在神经网络在无监督的学习中啊 自己领悟了人类语言中 蕴含的复杂语义关系 并且把它们编码到了这个高维空间 现在呢 我们就可以把整个ChatGPT的工作流程 串起来了 他拿到你输入的文字 先把每个词或者词根呢 转化为他的意义空间那个坐标向量 然后呢这个巨大的神经网络 也就是Transformer 就开始对这些向量 进行一些极其复杂的计算 最后 预测出下一个最可能的词的坐标向量 然后再把这个向量呢 翻译成我们能够看懂的词 所以呢ChatGPT生成文本的过程 本质就是 在我们看不见的 一个高维的意义空间里头 顺着一条啊语义上最通顺的轨迹散步 每一步呢 都迈向下一个概率最高的点 我们刚刚提到了一个关键角色Transformer 也就是ChatGPT所用的神经网络架构 的名字 也是它效果如此惊人的核心工程之一 Transformer最厉害的一点呢 在于他发明了一种叫做注意力机制 attention mechanism的东西 这个东西是什么意思呢 打个比方啊 我们人在读一个长句子的时候 注意力也不是平均分配的 比如这个句子 我今天在公园里看到一只非常可爱的 毛茸茸的白 色小猫它正在 他正在草坪上懒洋洋的晒着太阳 并且时不时的摇着尾巴 当你读到他的时候啊 你大脑会立即把注意力 放到前面的这个小猫身上 而不是公园或者草地 早期语言模型呢 没有这个能力啊 他看每一个词都差不多 所以呢处理长句的时候 很容易忘掉前面的内容 而Transformer的注意力机制啊 就很完美的模拟了这一点 在决定下一个词是什么的时候呢 他会给前面所有出现的词啊 都分配一个注意力权重 哪些词对预测下一个词更重要 权重就更高 这让ChatGPT 拥有超长的长文本理解和生成能力 他能记住几十页前你跟他聊过的内容 并且 在你写长篇大论的开头和结尾之间 建立联系 让整个文章的逻辑啊 看起来非常连贯 这就是注意力的机制的功劳 当然这也是谷歌当时写的 那篇跨时代的论文啊 就是attention is all you need啊 这篇文章发出来之后 才让这个Openai能够呃开发出这个ChatGPT 不过呢光靠网上读书啊 还不足以打造出 我们今天看到这个彬彬有礼 乐于助人 而且三观很正ChatGPT 因为网上内容可是鱼龙混杂 什么都有 如果放任自流的学啊 AI可能会学到很多偏见错误的信息 甚至出现一些攻击性的言论 比如像这个Grok 之前做了一些纳粹的言论啊 所以呢Openai的工程师啊 给他最后加了一道也非常重要的工序 基于人类反馈的强化学习 这个过程啊 简单来说呢 就是请人类当老师 他们先让最初的模型啊 对各种问题生成好几个不同的答案 然后呢雇佣一批标注员 像老师改作业一样 给这些回答排序 哪个最好 哪个次之 哪个最差 接着呢 他们就用这些人类偏好的数据啊 又训练另一个奖励模型 这些模型的作用呢 就是去模仿人类老师的品味啊 去给AI的任何回答打分 最后呢ChatGPT 去跟这个奖励模型去玩 ChatGPT不断生成新的回答 而奖励模型呢 则不断地给它打分 ChatGPT的目标啊 就是想办法让自己生成的回答 能在这个内置老师那里 拿到尽可能高的分数 通过这个过程呢 ChatGPT 学会了如何生成更符合人类期望 更安全更有帮助的回答 就像一个孩子 不仅读了很多书 还有一位耐心的老师在旁边 时刻纠正他 引导他 最终才成长成一个优秀的学生 好我们现在已经把ChatGPT的底层原理 和训练过程 都扒了一遍了 理论上看呢 它看起来很完美 但实践中 呢它真的是无所不能吗 当然不是了 记住他本质啊 他是一个概率模型 他追求的是听起来最合理 而不是事实最正确 这导致了他会经常的 一本正经的胡说八道 比如说啊 你问他一个精确计算的问题 Wolfram在书里头就举了个例子啊 问他芝加哥到东京有多远 ChatGPT 会给你一个看起来很自信的答案 甚至还贴心的换算了公里 但是呢那个数字是错的啊 当然呢 这是Wolfram当时用的这个ChatGPT吧 为什么呢 他并不是在计算啊 他只是在他记忆头 找到了无数个描述距离的文本 然后模仿这些文本的风格啊 生成一个看起来那么回事的数字 再比如数学题 你让他算个微积分 他可以给出非常详细的解题步骤 格式工整 术语专业 但是呢十有八九是错的 更搞笑的是呢 他编造这些解题步骤啊 犯的错误 和人类学生犯的几乎是一模一样 因为他学习材料里头呢 包含了大量学生在网上问的错题 和错误的解法 这就告诉大家一个极其重要的原理啊 ChatGPT是一个语言模型 不是一个计算引擎 那么怎么解决ChatGPT 事实不靠谱这个问题呢 难道就没救了吗 当然有了 Wolfram在这个书里的第二部分 给出了一个非常绝妙的解决方案 就是把两种AI结合起来 第一种呢 就是ChatGPT为代表的 基于统计处理非结构化语言的AI 他强项是理解人的意图 进行流畅对 话处理模糊开放性的问题 他像一个博学有创造力 但有点马虎的文科生 第二种呢 就是以Wolfram自己的产品啊 Wolfram Alpha为代表的 基于符号计算和结构化知识的AI 它的核心呢 是一个巨大的 经过严格策划和验证的知识库 以及强大的计算算法 它寻求的是绝对的精确 像一个不善言辞 但是极其严谨可靠的理科状元 当这两种大脑结合 会发生什么呢 你看到啊 当用户用ChatGPT 去提出一个需要精确计算 或者事实查询的问题时呢 这ChatGPT不再自己瞎猜了 而是转过头去问Wolfram Alpha 他用自己擅长自然语言 把这个问题抛给了Wolfram Alpha 然后呢 后者用强大的自然语言理解能力啊 把这个问题转化为精确的 可计算的符号代码 然后调用自己的知识库和算法 得出100%准确的结果 然后再返回给ChatGPT 最后呢ChatGPT再发挥他语言天赋啊 把这个冷冰冰的精确的结果 包装成一段流畅自然 可以用于 易于理解的文字 呈现给用户 这就是一个完美的互补 ChatGPT呢 负责和人沟通和润色 Wolfram Alpha负责硬核计算和实时核查 这才是一个真正强大可靠的AI助手 应有的形象啊 当然呢他写这本书啊 肯定要给自己家的产品 做一些这个代言啊 但他的核心理念 其实跟那个Gary Marcus是一样的 就是说你要把这个符号的呃AI 就是说你要把这个符号的呃AI 和基于这个统计的deep learning的这个AI 和基于这个统计的deep learning的这个AI 和基于这个统计的deep learning的这个AI 结合在一起 聊到这里呢 我们已经把ChatGPT的技术细节 和应用前景 都已经摸得差不多了 但我想带大家再深挖一层啊 ChatGPT的巨大成功 除了技术上的突破 它还向我们揭示了一个可能更深远 关于我们人类的科学事实啊 那就是呢 人类的语言以及其背后的思维模式啊 可能比我们想象的要简单 要更有规律 大家想一想 为什么本质上只是在预测下一个词 结构相对简单的神经网络 就能够模拟出 如此丰富和复杂的语言现象 这背后一定是因为啊 我们语言本身 就存在强大的可预测的内在规律 这些规律呢 可能不只是我们语法书上学的那些 主谓宾定状补啊 而是一种更深层次的语义语法 比如呢有实体属性的物体 可以做移动这个动作 但是呢抽象的概念就不行 有身体特征的东西可以吃东西 但是呢无生命的物体就不行 这些 都是我们潜意识里遵循的语义规则 其实GPT在阅读了海量文本之后 他没有去理解这些规则 但是他通过统计发现了这些规则存在 他学到不是知识本身 而是知识和概念之间 连接的形状和模式 就像我们发现了万有引力 不是我们理解了引力的本质 而是我们通过观察和计算 总结出一套 能够完美预测天体运动的数学 公式ChatGPT的成功啊 可能就意味着 我们第一次有了一面镜子 可以间接的窥探人类思维和语言 背后啊 那隐藏的支配一切的语法和定律 那这是不是意味着呢 只要神经网络足够大 数据足够多 它就能够无所不能 解决所有问题呢 Stephen Wolfram 给出了一个非常深刻的否定答案 他提出一个概念叫做计算不可约性 这什么意思呢 就是说啊 在我们宇宙中存在这样一类过程 你想知道它最终结果 没有任何捷径可走 唯一的办法呢 就是老老实实一步步的把它运行一遍 比如说这个元胞自动机（Cellular Automata，简称CA）是一种时空和状态均离散的动力学模型自动机啊 它的规则极其简单 但是它演化出来的这个图案 可以复杂到无法预测 你没有办法通过一个简单公式 就算出它第一亿步会是什么样子 你只能从第一步开始 一步步的算到第一亿步 大量的自然现象和复杂的数学问题 都具有这种计算不可约性 而神经网络的学习呢 本质上是在寻找数据中的规律和捷径 也就是计算可约性 它通过压缩信息啊 找到可以被泛化的模式 这就注定了ChatGPT本质是懒惰的 它极其擅长处理 那些计算上很浅的任务 而那些需要硬算 而那些需要硬算 或者是计算上很深的任务 他天生就无能为力了 写一篇文章 写一首诗 总结一段话 这些任务对人类看起来很复杂 但是从计算深度来言 它是很浅的因 为它依赖的 是我们大脑已经存在的 高度优化的语言和思维模式的复用 而计算一个复杂的物理模拟 或者证明一道数学难题 这些任务从计算的深度上讲 是深的这 恰恰就是 传统计算机和Wolfram Alpha擅长的领域 所以呢 我们不必担心ChatGPT会取代所有 它只是在计算浅的领域 达到了超人的水平 但是在计算深的世界里 它依然需要传统的计算工具的帮助啊 啊这其实也解释了 为什么ChatGPT 的下棋能力 和Alphago差了这么大啊 就是因为 它并不是一个适合计算深的 这种应用场景 现在呢让我们回到那个终极问题啊 ChatGPT算不算在思考呢 它有智能吗 这个Wolfram的观点是啊 它和人脑的工作方式既有相似之处 又有本质不同 相似之处在于啊 我们人类的说话时候 很多时候 也并不是每时每刻 都在进行严密的逻辑推理 我们大脑也是一个巨大的联想网络 当我们听到一个词呢 会激活无数相关的概念 然后我们大脑 会以一种 我们自己也无法完全说清的方式 选择最合适的词语串成句子 这个过程呢 ChatGPT基于概率选下一个词 有异曲同工之妙 但本质不同在于啊 ChatGPT的神经网络 在生成每一个词的过程中呢 是纯粹的前馈网络 也就是说呢 数据从输入层单向的 一次性的流向输出层 中间没有任何迭代和循环 它每生成一个新词 都要把前面所有的内容再重新看一遍 这是一个固定的机械的流程 而我们的大脑 以及绝大多数的计算机程序 都充满了循环和反馈 我们思维可以在几个概念中来回打转 深化迭代 直到找到一个满意的答案 这种在内部反复咀嚼信息的能力啊 是ChatGPT目前不具备的 当然不知道这个他那时候的GPT 和现在这个可以呃 deep thinking的GPT是不是一样了 因为他毕竟是在23年写的这本书 所以呢与其说他在思考 不如说他在进行一种我们前所未见的 极其高效的模式匹配和联想生成 但ChatGPT的出现啊 也迫使我们反思 我们所谓的思考和智能到底是什么 或许 我们过去对于智能的定义太狭隘了 我们一直以为 智能必须伴随着自我意识 逻辑推理 但ChatGPT认为啊 仅仅通过对于海量数据模式的模仿 也能够涌现出令人惊叹的 我们称之为智能的行为 这或许意味着 智能的形态比我们想象的要丰富得多 好了讲了这么多硬核的原理啊 那我们最后得落到实处 理解了ChatGPT的本质 我们到底应该怎样更好的应用它呢 让它成为我们的超能力呢 我给大家总 结了3条黄金法则或者是驯兽指南啊 第一条呢 就是把它当成领航员 而不是自动驾驶仪 记住啊它的核心是语言接龙 你给它的开头 也就是提示词啊 决定了整个航行的方向 所以呢你的问题要尽可能清晰具体 提供充足的上下文 你甚至可以给他举几个例子 叫做few short learning啊 告诉他你想要的风格和格式 他不是你的下属 他是你的副驾驶 你需要清晰地告诉他目的地在哪 第二呢 永远不要完全信任他的事实 尤其在关键领域 和对于任何需要精确数据计算 和逻辑推理的地方 要保持警惕 对它生成内容 特别是数字日期代码 科学公式 可以当做草稿或者假设 学会使用专业的数据 比如说搜索引擎 Wolfram Alpha或者是计算器去交叉验证 把当成一个创意无限 但有点迷糊的实习生 他的工作成果你必须亲自审核 第三呢要善用他的发散性 激发你的创造力 他最大价值呢 不在于提供最标准答案 而是在对于他能够基于概率 探索语言的无数种可能性 当你写作卡壳 策划没有思路 编程想不出新方法的时候 他可能给你生成10个不同版本 你会发现他总有一些意想不到的角度 给你带来启发 不要把它当成答 案的终点 要把它当成思考的起点 掌握这三条呢 你就不是简单在使用ChatGPT 而是在和这个强大的概率引擎啊共舞 真正的驾驭它的力量 好了今天这期节目呢 我们跟着这个Stephen Wolfram啊 一起拆解了ChatGPT 我们从他最简单的文字接龙游戏开始 啊一起探索了神经网络 梯度下降，词嵌入，注意力机制 甚至聊到了计算不可约性 和智能的本质 最后我想说啊 ChatGPT的出现 它的意义可能远超一个好用的工具 它更像是一个科学的伟大发现 就像是牛顿当年用几条简单的定律啊 统一了天上行星和地上的苹果 就像达尔文 用自然选择 解释了生命世界的万千形态 今天呢ChatGPT 用一个基于概率的下一个词 预测的模型啊 向我们证明了人类最复杂的智慧结晶 语言和思维 背后可能也隐藏着简洁而深刻的规律 像一面镜子 让我们第一次有机会啊 从一个非人类纯粹数学的视角 去审视我们自己 它迫使我们去思考什么是意义 什么是创造 什么是智能 未来最强大的力量 将不再是单纯的人类智慧 也不是单纯的机器智慧 而是两者结合 是ChatGPT 这样富有联想和创造力的文科大脑 和Wolfram Alpha这样严谨 精确的理科大脑的完美融合啊 这不仅会改变我们的工作方式 也会开启一个全新的探索知识边界的 大航海时代啊 当然了作为这个自家的产品啊 Stephen Wolfram 在书里头 是不遗余力地给Wolfram Alpha做推广 Wolfram Alpha我其实也用过 他在做这种数学相关的 或者是这个物理相关的问题 其实是非常强的 但是呢他也有自己的局限性 就是说呃 他的输入啊 是比较rigid的 就是你可能需要想办法呃 要学他的一些呃 就是这个叫Wolfram language啊 有这样一个语言 然后呢要跟他进行更好的交互 之前那个ChatGPT 4o刚出来的时候 它还有一个就是跟Wolfram连接的这种 一个定制的这个GPT 但现在 好像不知道有没有继续调用它的 这个API两边肯定是有什么合作的 好的 那我们今天的这个节目就录到这里了 非常感谢大家观看 我们下期节目再见