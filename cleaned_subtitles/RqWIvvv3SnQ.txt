01 to be perfectly honest it was really mostly good at solving puzzles. It was almost more like a technology demonstration. 03 has really been something like a tectonic shift the trajectory of AI. GPD5 in some way like I know can be considered as like 03.1 what I am after something that would be that would be the next pretty significant jump. We understand that there there's only one time in history where AI is being built and deployed and developed. We are together in the skull that's larger than every one of us.
>> Hi, I'm Matt Turk from Firstark. Welcome to the Mad Podcast. Today, my guest is Jared Foric, VP of research at OpenAI and a member of the Metis list of the world's top AI researchers. In this episode, we go deep on how models actually reason. We also go behind the scenes at OpenAI, how a few big bets get staffed, why everyone knows everything, and how that culture ships fast. Please enjoy this great conversation with Jerry.
>> Hey, Jerry. Welcome.
>> Hello. Very happy to be here.
>> We are going to talk about reasoning a lot in this uh conversation. At a high level, what does actually mean? When uh we talked to chat GBT and Chad GPT says it's thinking what actually is happening behind the scenes. I I think that that the thinking process is at least a good analogy. As we were in a in the early days of AI always had this goal dream of trying to teach models to reason, we were thinking about it spending more time to get better results. If like human is posed with a very hard problem in front of them, uh very rarely they have they have answers straight away. Sometimes they need to find that answer. Sometimes they need to perform certain computation. Sometimes they need to like look up some information. Sometimes they need to teach themselves something. And the process is reasoning is like getting to an answer that you don't yet know. Like in some way can be called search, but it's not really like a very naive search. Search is a loaded word. But reasoning is the process of getting to an answer and a work that you need to do that is like longer than what usually is considered answering a question. I think the the difference is here like answering a question usually means you already know the answer and you just just just elicit the answer you know and the process of reasoning is getting to the answer that you don't know and usually the longer you spend on getting to this answer for whatever you need to do to get there the better the better it gets
>> and uh we've all become familiar since you guys uh released 01 I guess a little over a year ago now in September 2024 with the concept of ch of thought which is in layman's term the little messages that you see when you query chat GPT and it tells you it shows its work it tells you what what it does what does that actually do is that a logical tree and it eliminates option after option what actually happens
>> language models do on on their own like fundamental level is they they are often called as next token prediction machines and that that's not completely accurate in the age of reinforcement learning but They still operate on Mosium tokens that are most text. The language models again are these days also multimodel and they operate on on on multi text. But to simplify a little bit for a second, uh language models generate text and what chain of thought is is their thinking process verbalized using human words and human concepts. So the the magic that we are seeing why this is all possible is that while you are training on all of internet on a lot of human knowledge and human thinking process the model starts learning in some ways to think how humans do and in some ways get to the answers how humans do from seeing humans do it a lot in the text that was that was pre-generated and that that that was based in a training data on humans and then and then the train of thought it's basically eliciting that capability in language models of like thinking and getting to an answer like like like like like humans do. H a lot of what's what what what's like what early chain of thought work was doing was kind of solving math puzzles and the first like most famous prompts to elicit chain of thought in language models was so called like let's solve it step by step. There is this this this like very classical result in language models that if you ask them like what is some like either either mathematical expression or some puzzle then and they will try to give you an answer they will try to predict the next token but they they fail it's a hard thing they cannot compute it in one one token jump but if you ask them like please do it step by step and they will start thinking okay I don't know the answer but the first step of getting to the answer is this and then and then they write like chain of thought which is a series of text series of tokens doing the first part of the computation, the second part of the computation, the last part of the computation. Then they connect those things and then they can get to the answer. So the chain of thought is basically a process of thinking encoded in in words how how humans would solve a problem on a piece of paper going going step by step from the from start to the end. And and since uh time and by that I mean the time spent thinking is so uh important to that concept of reasoning, how does a model decide uh how long to think when we're in JPD5 and we're in auto mode and it says uh that is going to decide automatically how long to think. What what happens there? It's basically part of our optimization process partially for for for the the happiness of the users and what they want to expect because when when you have a thinking process, you need to balance two things which is like the quality of the result. That's as we said and there have been like those pretty great scaling laws that we demonstrate with the release of 01. The longer the model things, the better result you get. But also people don't like waiting. Waiting. waiting is time lost that you could that you could do something. Everyone wants to get results as quickly as possible. And like you know there is this there is this this saying you can get like you know cheap fast or good and you can take too and that's that that that applies to language models as well. There is a there is a trade-off and it's it's delicate. That's why like we also expose some of that trade-off to the users where you can have like a high resing model and a low resing models. And this is this is like in the end the same model. You just we just tweak the parameter which says we want you to think longer or shorter. We try to encode some heristics of what we think the users will want when when when get thinking on an answer a little bit longer will and getting to a better answer is worth it waiting versus not. But it's a it's a bit of a trying to guess the anticipation of the users what's the what's the right amount of thinking for them in this particular situation.
>> Oh fascinating. So it's more uh userdriven. So it's more like a user experience uh kind of thing.
>> In the end it is because the question is like how long do you want to wait for answer? You can always wait longer and get get even better answer.
>> It's been uh a little over a year since the release of uh the world's first reasoning model 01 which is an effort uh that uh you led. What has been the journey since? So there was 01 then there was 03 uh then most recently JGB5. How would you characterize the evolution of reasoning specifically across the three models in the last year?
>> In in some way how I characterize our reasoning or scaling up reinforcement learning research pro program is we do a series of scale up runs that are progressively more and more ambitious. everyone we try to do something more something larger scale something that that should result in a better trained model than the last one and and obviously we don't release all the models that we train some some we release some we think like I need to need to wait a little bit longer for the moment where where they will have their time to shine in the in the hands of the users but like 01 was the first model we decided to release as kind of uh like to demonstrate to the world there are those models and 01 like to be perfectly honest it was really mostly good at solving puzzles and like maybe a few kind of thinking problems here and there but it wasn't like it wasn't yet very useful model it was almost more like a technology demonstration than that that an actually really polished product but we were thinking we have something cool and we wanted to share it with the world as as openai 03 I think like changed that pretty significantly in some way It is a model that is meaningfully useful and like you know a little bit self- serving but was a moment when I when I started using CH GPT quite a bit and I I I'm basically a user completely hooked on on reasoning model and in chpd right now I use basically exclusively reasoning models because those are the only models that I trust the the output and layer and the result and I think 03 like it's its ability of of using tools and getting to like answer leveraging a lot of contextual information from various sources and persevering to to towards getting to that is has has really been something like you know I think I think that there was a lot of a tectonic shift in the in the trajectory of AI and like you know I think I think like we did we did something really really great there like GPD5 in some way like you know can be considered as like 03.1 it's it's a little bit of like like you know iteration of of of like the same thing And the same concept and what what I am after and my team right now is like you know something next that would be that would be the next pretty pretty significant jump of how we interact with with like models that that that are even more capable thinking even longer and interact with with even more uh systems and sources on information on their own on their own journey. But but like separately in the meantime we continue to build a lot of things on top of all free technology like codex which I think like coding agents are at the moment the first like really successful agentic products built on top of AI there are things like computer using agent it's called GPT agent right now I think and like the prearch and a few a few other things that that we will will keep on building on like three generation technology.
>> Great. All right. So, we're going to go into uh all of this uh in much greater detail in a minute. Um but before we do that, let let's talk about your journey. I think it's it's super fascinating topic like for for all of us like you guys are changing the world. So, uh I think I'm I'm curious and we're all curious I think about um the the people the human aspect of like who those people are that are you know just having such an impact. So you you starting from the the beginning, you you grew up in Poland, I believe, right?
>> Yes. I grew up in Poland.
>> Walk us through your formative years and uh how you got to get started in this field.
>> Yeah. Yeah. Yeah. Happy happy to do that. An interesting fact. It's almost like a like a crystal starts from something and you put a little bit something in in in the beginning. There's I think one part that was like important and part like like the the starting point of my my journey where I where I don't know where it came from because it was with there with me from the very beginning of my life in a in a moment that I don't really know when it started. It just was always there with me. I always thought that being a scientist and doing science is the highest calling a human can have. And I don't don't really know where it came from. My parents maybe like were were singing the light bright lullabies to me like when I was when I was one or something like that. But basically since I remember I wanted to be a scientist. In the early years I also discovered like you know have I have talent for for those things. I like I was going to school and I saw I get things slightly faster than than than people around me at least like you know in a regular school and in in the middle of Poland and which which like you know made me made me kind of like those doing those things like studying maths and science a little bit more because because like you know I felt it it felt good in a way. It felt like this is this is this is something that naturally naturally fit me. Like you know I grew up as a like you know very regular kid just just like you know being slightly nerdy guy and trying to balance my my side of of being interested in science, programming, maths and like having some social life. And I I definitely have had some kind of like party arc in my in my life. But I think I I think that the most important part and moment was when I actually went to university, college, University of Warso is where I went and I decided again to study mathematics at the at around that time of being 18. My idea of life was to be a mathematician with a pencil sitting in a room with a piece of paper and solving equations. This is kind of my my my 18-year-old dream of how how how life should be lived and what I what what what I want to do in in my life and like you know I I I do have like you know my my personality is built again in a way of really appreciating like you know solid science pursuit of truth great great engineering and all the all those aspects but I definitely have like also a little bit of like a misfit kind of rebellious uh tinge to it and that that resulted after a few years of studying mathematics. I what I realized about myself and about about the world is that I really like maths and I am I'm I'm quite good at it but I didn't like academia that much and I realized I don't want to like stay in academia. I don't want to stay in university and that this this would not be environment where I thought I would be long-term very happy and and fit in. It felt a little bit too like rigid, a little bit too structured in a way that I that I kind of like didn't didn't know if I if I will will feel good
>> and in some way for young me like I was I was around 21 years old at that moment that was pretty big crisis of faith for me. I had had a moment like you know lost purpose of life. So I just did like you know a very simple like first principles thinking h you know I am graduated with a degree in mathematics. I need to get a job to get food and like you know what job can I do to use mathematics and in in that job and like you know looking at the job market that moment was like you know 2011 I think or or around or or 2010 somewhere somewhere around that like I I I decided to become a trader and trade for a living as as as like you know the the one way where I can like do what I like which is mathematics and and gets uh gets get a career. I got a I got a quick internship at JP Morgan uh tra investment bank trading floor and equity derivatives group. spent six months there learning a little bit how how does trading work and what does what does what does it look like a little bit finished my degree h I got I got a message from like uh boss of my boss at JP Morgan something saying hey Jerry you were like you know one of our best interns ever that we had we really really liked you working with us and we are we are living bank and starting a new hedge fund would you want to come with us and like for for 20 21 or 22 2-year-old Jerry. That sounded like kind of like a cool adventure type of story that that that I I was interested in in going there and do it. It had enough like uh enough interesting problems to be solved and at the same time it had like yeah this this kind of trying trying something new trying something ambitious kind of kind of bed that I generally like. I was in London. That company didn't really work out unfortunately, but it was was hard and ambitious and not everything works out. I did try that again, starting another hedge fund from scratch with a few other people in Amsterdam. I worked there for a few more years and eventually eventually I got bored like I I generally working in trading is an interesting and exciting problem. market is very hard and the depth of what you can go of trying to understand it model is very deep and I worked with pretty smart people overall but I I stopped feeling I am growing after a few years of doing that and at the same time together with a friend I was working with we just started chatting about AI and about this this like artificial intelligence and what really drew me to artificial intelligence was reinforcement learning and and specifically the DQN agents trained by by people in deep mind in in like 20 2013 but I think it was much a few years later that I actually learned about those results from from my perspective and again this is this is just just how my brain works the the 2012 imageet results like weren't that significant like during during my my university years I learned bunch about like how classical AI, the the neural networks weren't very fashionable back then, but I still learned about what they are. I learned about SVMs and all kind of methods, how you train classifiers. And for me, it was kind of like obvious and natural. If you have enough parameters and tweet it hard enough, you will fit a classifier to whatever you want. It was was kind of obvious. What like what was to me not obvious is I never considered classifiers a smart thing. classifier is a you learn a function on some set of inputs to some some set of outputs. H we can can keep training it to approximate better and better. What what kind of what what was some thing that I missed back then is that when you can fit any function better and better, you can start shaping behaviors and strategies. And what I when I really saw that was in the um in the like DQN results where they applied the same things that worked in imageet like neural networks and they weren't particularly big or impressive neural networks with with a classical field of reinforcement learning to to solve simple computer games. And turns out those simple neural networks with a simple learning algorithm started learning pretty complex computer games and exhibiting very interesting behaviors. I saw those behaviors. I saw those results and I was like this is what I want to do for for the rest of my life which is like you know not not very long horizon what the 20some things about but I was like this is this this is what I want to do where where where do I do that like you know Google search where are places where you can do reinforcement learning in this world h like you know Google deep mind and open AI uh came up with this kind of at that moment pretty small and like somewhat known but they were
>> yeah you joined you joined openai in 2019 19, right? So, like very much u very much in the early days still, very much in the kind of like nonprofit era. Yes.
>> Of open air. How so? How did you connect with them?
>> I just just applied for the website. I the most the most like boring and uninteresting thing in the world which is like you know openai.com jobs like you know apply send resume and hope hope they respond and then you know luckily enough I was they did. I don't know I don't know how many résumés open I was getting in that time. I think it's definitely much less than today. But I was I was like, you know, I I came there and I was like, no, that that doesn't matter what do I do as long as it's reinforcement learning.
>> So you joined in 2019 um and with a passion for reinforcement learning. So was that around the Dota 2 moment because open interestingly in in those early days of 2019 did a lot of reinforcement learning focused work, right? And then then there was a whole like unsupervised learning uh GBT moment that happened afterwards but like it started from roots in reinforcement learning right. So did did you work on that project specifically or was it uh too advanced by the time you showed up? So the the project that I worked was robotics project at openai which share the same code and same methods as the data project like in in one hand the data project was openi's way to demonstrate the world what scaling up reinforcement learning can do and in some way it was like taking the the 2013 DQN agents and just just doing all the hard work of making it bigger and bigger and solving harder and harder problems and opening eye generally from the very beginning was aware and really like you know it was a it was simple but but genius inside that you need to have large scale system to to learn really really interesting complex behaviors and that was that was like one one way of what Dota was trying to show that by scaling up reinforcement learning we can solve pretty pretty complex environments and then like we have there was another project or there I think three reinforcement learning projects at Open AI at that time. And the second one was robotics which is applying the same methods that we now knew or or were proving that can solve pretty complex computer games. Can they solve all the practical problems? Open. I was always optimistic and ambitious and trying to see if we can scale around to solve time. Can it load my dishwasher? Can it fold my clothes? Can it build a house? And and and that this is this is what we are doing. The project I was working on was focused on dexterous manipulation which was back then and still continues to be an elusive challenge for for for for trained policies. And like we got to a showcase of demonstrating that a hand controlled by neural network was able to solve Rubik's cube which which is a pretty delicate and complex task to do. So fast forward to um today, still in the same vein of like the behind the scenes of um you know you all at OpenAI and sort of life there. What's a day in the life of Jerry like? What what what does somebody like you do? Like you you you you read papers, you train models, you manage teams like what's a day like? Yeah, my my my days are surprisingly uniform which is I come to the office early in the day after driving my kids to school. Then what do I do all day is basically talk to other researchers. I talk to other researchers all day every day. And this is basically exclusively what I do. I take ideas from people, bounce with them, brainstorm with one partner, then move to another one and do the same thing over and over and iterate. And in that case in in in in in in that way like keep refining our research program. Sometimes those are group meetings and group meetings are there as well and have their own team team dynamics but that is that is basically exclusively what I do. The only thing that that changes is the the topics of of research from from from from meeting to meeting and from from person to person. How are uh priorities in research determined? Uh this range of possible projects. Is that top down? Is that bottoms up? Do people suggest ideas and others vet them? How does that work?
>> Yeah. Yeah. Yeah. It's the the art of like structuring, organizing and leading a research project is like something that I generally like learned to appreciate very quickly in in open eyes journey and in my career. There is something we are good is is is structuring research projects and it's I think it's a unique mix you cannot say it's top down we cannot say it's bottom up. It's a mix of those two which is which is balancing all all the important aspects which one one one thing open AAI embodies and determines is like we all work on a very few projects total there are not that many projects open AAI is not trying to do everything we are not trying to to to like have portfolio we are trying to have like multiple different bets always the the idea is we do few core things really really well and put a lot of effort there which means there are there needs to be a lot of people working together on the same large scale large ambition project and we have we have a few of those small number probably three or four depending depending on how do you how do you call it and that's and that's it and from that perspective like people don't have ultimate freedom it's not that that people come to open and say hey I want to do this and they just they just do this because you need to do something towards the goal of one of those four projects and then within those projects like we try to be like relatively bottoms up in in a way as long as it feeds again into into those goal. And the most important part of the research lead is keep to making sure all the researchers are working towards this one shared goals and that don't fracture in their own ways of of of thinking and and and doing things and it's an incredibly hard thing. It's a very very hard job and not always uh not not always easy easy to visible how how how how delicate it is but that's that's a lot of of what it is like I don't think like you know top down uh structuring of research doesn't work and research organizations I really don't believe in it because like you are not kind of hiring some of the smartest people in the world and open air has incredibly incredibly smart people to to kind of tell them what to do they need to like they need to figure out what to do but they cannot figure out in the whole space of things what are what are cool things to do. They need to figure out from within the space of what the project needs and what is what what could advance the research goals of of Open AI the most
>> and to which you're saying is there collaboration between the teams working on the three or four projects at the same time because I would imagine putting myself in your in your shoes open AI shoes there is probably a tension between wanting to be collaborative in general but equally uh I mean that this is probably the most important IP in the world. U so you probably want to uh make sure that not everybody knows everything about everything. Well, perhaps not. I'm I'm speculating here. How do you think about that collaboration versus um versus some protection of uh of IP? You'd be surprised but the the truth is in research at OpenAI which is like around like slightly less than 600 people at the moment. Everyone knows everything really really it does and we always have been fully transparent and it is like in some way you are a little bit shooting yourself in the foot if there is a researcher that doesn't at least like have the chance to learn about everything because they don't have the best information to do their job in a in a best way and like it is like yeah it is some like risk of of losing IP but I think I think the risk of not doing the right thing and of people not being informed about about research and not being able to do the best research is much higher in my in my personal opinion and how how I I approach those things. So we are we are extremely internally transparent with within within research and and that is that is one of our operating principle as as the goal is to do the best research we can and and train the best models we can. Uh consequently and the culture generally is very collaborative like you know it is always the case when you have 600 people when you have groups of people they're always like one person doesn't like the other person because they looked at them weirdly or one person thinks the other smells bad or or just doesn't like their ideas like it that does happen those are humans and those are those are human things but generally in large scale I think we really are have this belief like you know we are together in the skull as larger than every every one of us. It is a very positive sum game because the AI seems to be getting like you know only more and more significant and the success of Open AI is far from guaranteed. It depends of on us doing great work every day. So there there there's a lot of like feeling of shared faith and and the fact that we all need to rely on each other to do our our job to to achieve this this this shared mission. So I generally I generally like I think with with all the all the caveats of human nature getting in the way sometimes I think I think on a on a large scale opening I is very very collaborative. How do you um all manage to keep that um pace of releases? Like it seems to me from the outside that uh there's a tension again like between between another kind of tension like between research uh which in in in some ways is uh kind of feels like it could be a long-term kind of thing. And on the other hand, like you guys seem to be just shipping and shipping and shipping and shipping uh across the organization, but including in terms of like core models like again to the point that you went from 0123 to GPD5 in like a year. Um how do how do you balance all of that? Like why are you guys able to ship so uh quickly? I I I think that the fundamental reasons for it is in general openai kind of in my my at least worldview is a generational company in a way that we have incredible momentum behind us. We know that we were doing pretty great in the past and we need to continue that. We have incredibly smart people like literally the most talented people in the world are all coming and want to work at OpenAI right now which means like people every output per single person is incredibly high and everyone really every every single person does a whole lot. So so we have we have like a momentum that carries us forward. We have really great people that work together. we have like good operating like way of structuring research and and and can borrow a lot from Silicon Valley how how to get things done quickly and people are generally very excited about work. Everyone feels the weight and potential of what we are doing, what we are trying to do and because of that people at OpenAI have a tendency to work pretty hard and and then like you know having great people excited about what they are doing all working together reasonably well results in in doing a lot of things like we like understand that there there's only one one time in history where AI is being built and deployed and developed. and people want to do it in the best way that that is possible.
>> Do you all use a lot of your own tools? I think Fiji Simma was tweeting the other day that um I think in the latest what you all announced um at dev day today a lot of it was uh written by codeex. Um is that is that part of the daily experience? Do you use a model to come up with new ideas for model? Do you use a codec to write the code? How does that work? Yeah, like we definitely use codeex a lot for coding and this is only getting getting better like as as I said I use chat GPT a lot although not surprisingly not that much for for actually coming with with ideas but like for for a lot of questions that I have I I I think I am pretty heavy user of of chat GP right now happily paying like $200 a month for it and I think I'm getting
>> they make you pay
>> for for what it's worth they are making you pay and I'm I'm kind of like pretty pretty okay with is because you then you get pretty generous like usage limits and and not really not really bottlenecked on it.
>> Thank you for all of that. Let's switch tags and um go back to uh how all of this uh works. So is the right way to think about modern AI systems at at OpenAI by by modern I mean as of October 2025 versus the old days of you know 9 months ago. Uh so exactly so is a is the right way to think about it as a a a a combination of uh pre-training and RL first of all is that the right way to think about it and and and and second if so just at a high level how does the articulation between both of those work and then after that I'd love to do a little bit of a uh you know deep dive on RL to make this very educational for for folks
>> today's language models are basically can be thought as like first they are pre-trained then you do reinforcement learning on it like the reinforcement learning would not work without pre-training and I think in a similar way like pre-trained models have a lot of limitations that are very hard to like resolve without doing something that looks like reinforcement learning. So I think both of those bits are here to be and to stay like I think the way how they are like combined and and do may and probably will evolve in the future. Nothing should be treated as as dogmatic and fixed and we need to we need to keep generally figuring out the way how to train better models and this is what we are trying to do. The interesting thing and you know I I can credit that to to Ilia how much foresight that he had but whenever I was like h started at openai early 2019 right and and and and like I remember there was like research all hands or something like that where I came on stage and talked about like what is what is openai's research program what are we trying to pursue and what he said at the beginning of 2019 was to train large generative model on all data we can and then do reinforcement learning on it. That was that was the open AI research plan at the beginning of 2019 and this is exactly what we are doing today. uh the algorithms change, architectures change like I don't think he was even thinking about transformer at that moment like you know the GPT was like there was some GPT but it was like a toy example that someone was playing with but the goal training large generative model all the data in the world and then doing reinforcement learning with it was was already there at the core DNA of open AI and said that's that's that's what is happening right now. So let's um do uh if you will a little bit of uh reinforcement learning 101 to make this uh really interesting to like a broad uh group of people listening to to this. So in very uh simple terms like explain it to me like I'm 10. Uh what is reinforcement learning?
>> Yeah. Yeah. I I usually the the metaphor and the analogy I have to reinforcement learning is like training a dog. It's it's very very close and I used to have a dog when I was a teenager and even even what I remember my parents did I didn't know anything about raising a dog what they what they they kind of invited through some friend of a friend a fireman who I think was working with like service dogs and he came to me and he basically told me a little bit about how do you train your dog and what most dog owners that are like ambitious about training your dogs know it is always extremely important to have a bag of treats in your in your pocket. That's that that's what you always do. And whenever you see your dog behave well, what you should be doing, you should you should smile and you should give your dog a treat. Whenever you see your dog do something bad, you basically like give your attention away, turn away and become sad. And before the years of breeding, the dogs discover it's a it's it's a like, you know, bad reward and bad bad behavior. And this is exactly doing that, but with models. We elicit a lot of different behaviors in a mall, put them in challenging situations and then we give them cookie if they do something we want. If they do a good thing and give them some kind of punishment and negative reward if they do something like that we that we don't want and that we don't like in a good way. The the good way to do RL is if you balance those things. So if you kind of give cookies half of the time and punish the other half the time. But this is almost like a mathematical kind of uh kind of aspect of it. But that's that's that's the most important part which is like elicit behaviors, reward the good ones and then going forward the model will be most more likely to do what you want and less likely to do what you not want and through that it it improves. It is the the the way uh how to train models to like elicit actual behavior that is not first. It's next to prediction. If you if you pre-train the model, you literally train the model to to predict the next token. RL is like a completely different gradient, a completely different set of what we what we want to get out of them all.
>> And uh getting the model to do what you want just for some vocabulary and semantics that's uh you hear sometimes the term policy. So in RL you hear terms like agent, environment, action, reward and policy. So I think a lot of those are sort of um self-explaining but policy is what that's a strategy that's a behavior of the model.
>> Yeah. Policy is the behavior of the model as the model weights represent what it does when when put in a different thing like model in the end is a mathematical object and you can define it and policy is a mathematical function that map maps observations to actions you what what you see and then what you what you do with with what you see.
>> Yeah. Uh so agent is a model action is what the model does. Reward is how you um say whether that's good or bad. Environment you you hear a lot of things um these days about designing the right environment for RL. What what does that mean?
>> Environment is like in some way it is everything that the model sees. But but the interesting thing about difference about Ral environments and most other types of like what you can call supervised learning or unsupervised learning is that reinforcement learning environments you want them to be interactive. You want like them to evolve as the model does things in general like like similarly how like if you want to like learn how to play guitar, you kind of take a guitar and you strum it and what happens is you hear sound of that and then you hear it and then you can do like learn to play like like with with actual like feedback of what is happening with the the guitar and it's like way you know they Environment is like how does the world reacts to your actions and a lot of like what drives your actions is the is what is happening in your in your environment and it's in in your in your world and that's kind of like the only way how to like really teach agents to like learn to react to changes in the environment is through is through reinforcement learning. Can
>> can you give us a little bit of a bird's eye view of the evolution of RL over the years? what uh you know mostly how does modern RL differ from sort of historical RL?
>> Yeah. Yeah. Yeah. I like again the there was super historical RL like so not not even that old but the the main like tectonic shift was when combining neural networks with reinforcement learning. The reinforcement learning you know predates neural network as a general mathematical method of optimizing behaviors in like mathematically defined environment and as a method of study.
>> That's what is known as deep reinforcement learning. Is that right?
>> Yes. Then then the deep reinforcement learning that that basically like deep mind uh like invention of combining neural networks with uh with reinforcement learning to the the DQN moment I I talked to you about. And then from there like there was a moment where like there was a pretty active like area of research of reinforcement learning on games like when even when I started like 20 2019 there reinforcement learning was was kind of fashionable at that moment although not like very successful but we were the reinforcement learning was able to solve a lot of games but the bottleneck was there that the models were not pre-trained in any way. We were we're training a lot of behaviors playing games like we even got alpha go moment out of out of that which a lot of people got very excited about it but was still like learning behaviors without the models that were that were meaningfully smart about those behavior. There were there's still a lot of kind of like you know you don't want to call it caveman intelligence but but something some some something in that regard about malls not being really smart even though being being pretty heavily reinforced and there there was like a long research in that and a lot of cool like results and theoretical understanding of RL comes from those days because because people were researching RL actively but it was like in some way it was a dead end of doing RL without pre-training and then in in in my like moment when I when I finished like working on robotics I started working on teaching language models to code but having having pre-trained models was uh was was a really big deal and and and and the the GPT era of scaling and of like large scale ingesting lots of data to to to to really train great models like enabled us already at that moment to to start RL and that was that was one of one of the first things I did almost Immediately whenever GP3 was trained I tried to do RL on it and there there were always what were were bottlenecks. The systems were kind of clunky. it was hard to figure out what are the right al algorithms what what are what are the right like like uh what what are the right problems to work on it and what are what is the right algorithm to train it on and like the what what what what opening I did at that moment and what what's what's kind of like how how research goes we kind of cargo culted a lot of things that was used for games and almost the same things are for robotics and at the first RL I was doing on large mall was was kind of the same ppl we used we for everything and like it gave some results but those like early results weren't completely mind-blowing in RL and there there was a long time where we're keeping on investing in it and you know personally I always believed there will be a really really big moment for RL and language models but the defin the finally the early early trials and errors weren't weren't super successful at the moment when we uh when we train GP GPT4 and there were there was that there was an interesting moment where we trained GPD4 and everyone today thinks oh GPD4 is such a great model but when we trained GPD4 we were pretty underwhelmed internally and then there was a lot of moments oh we trained this model we spend a lot of money on it and it's kind of like you know pretty dumb at least at least like you know we have GPD3 GPD3 already does all that stuff and GPD4 doesn't really seem to be that much better and like we had this this like kind of kind of question like you know how how like it kind of seemed smart on evils that were that were like one token long, it seemed to be able to give like pretty detailed answer to complex question where it was one token. But if you actually let it speak for longer, it wasn't very coherent or or or really like give gave very long answer. And we we we need to ask this answer this question. How do we like actually make the language model that seems to have some smartness in it ways actually sound smart and actually be good in in talking to it? And that was that was the moment where a technique that was developed already a few years earlier like really shown which was called RLHF which is basically doing PO on large language models with the reward given from human preferences of seeing two parts of the text
>> thumbs up and thumb down.
>> Yeah, thumbs up thumbs down like whatever whatever human preferences is. that that's a very good reward because there are a lot of things how the model can generate bad text and how early GPG4 was generating bad text in a lot of ways and RHF was able to catch those things and correct it and you know reinforce good behaviors reinforce generating good text and then and then punishing bad text and in the end GPD4 plus RHF together as a package like you know deliver the GPT moment to the world that everyone sees and as much as it is a big success of pre-training it actually also was pretty big success of of RL in the form of in the form of RLHF.
>> Amazing. And and just to uh double click on that the RHF so we we all familiar as users with I mentioned thumbs up and thumb down that's on the interface but um the actual RHF happened post training. Is that is that is that is that right? Yes. And and what did that look like as an effort? Did you have like a bunch of uh just humans sitting down uh you know in front of the model industry specialists maybe and give it feedback? How how did that actually work? Uh like so like RHF was a research program that was was already like you know was already happening in the background for a while. I think we did RHF at least I remember GBD2 being RHF for quite a bit and like that was that that was already there and already already happening it's like gathering data for even for RHF is its own like research domain basically and always thinking what is the right data to train them all what is the right data to train like your rewards and how do how to shape your rewards it's it's a research that we've been doing and it's it's a very open-ended and very deep in many in many different ways and like what like I think there papers written on on what what what RHF is but there there's a lot of depth to it but like you know the long story short is you have what we call AI trainers these days and they look at outputs of the models and they give them scores and then you learn basically a model of those scores and use that for training
>> and that's part of for people who may be curious like the the entire uh data labeling industry so uh scale AI and and a bunch of others that's uh what they do right
>> yes yes like I think like in in a way I think it's getting more and more to be a thing of the past as the models are getting smarter and smarter this is becoming less of a thing but I think few years back and especially in GPD4 days this was the thing the the the the interesting bit about data lab building industry I'm not sure how much want to go on that tangent is that it has to constantly reinvent itself because the AI are getting smarter at some moment like certain things you don't want to label with humans AI already can do it. So, so like you need to you move the frontier and and you change the date type of data you are labeling as the as as you already r the previous part
>> we've been talking about RL but the the the first phase of all of this is the the creation the pre-training of the models that is unsupervised learning right do you do you want maybe for again to make this broadly um interesting to to to people define unsupervised versus supervised and and in what way was u the pre-training unsupervised versus self-supervised or whatever nuance.
>> Yeah. Yeah. I think those are nuances and I don't think they are as as stark and as sharp as some people like to to determine them. But the the the why like pre-training is called unsupervised because like in some definition of it you don't need any like extra labels to the data that that you feed into them all. You just just feed the text as this. In some way you may you may argue that that the data is already labeled because it is self-labeled if like you know you give the model from the text predict the next part of text like you know in some way it is a label but it is self-supervised because like we don't clearly tell the model like what is right or what is wrong or what is what do we want from it or what do we not want. We want it to just predict the other part of data. You can do the same thing with images. you can mask the part of the image and tell them all like predict the next the next bit of image. But whenever whenever like there is this like classic machine learning notion of like targets and labels like I guess we're talking about classifiers. So supervised learning was like you have some notion of targets what your targets are and some notion of labels. Supervised learning was like predict those labels from targets and this is like a like some type of mapping. But actually what's what's interesting is that there are many more bits usually in the targets than in the labels and studying the structure of targets itself it yields much more learning and much more intelligence than learning the mapping itself. So uh like spending a whole compute on just just learning the data itself without the labels is is the right thing to do and like what what is often called like representation learning and studying studying the data and its properties.
>> Okay. Great. All right. So going back to uh RL you tweeted the other day GRPO the GRPO release has been uh in a large way has accelerated the research learning uh program of most US research labs. So what what what is a GRPO?
>> Uh yeah it was a little bit of a tongue and cheek moment. And I am extrapolating here a little bit what exactly happens because I I haven't been in most US research labs but I have some mental model of of what happened and how and long story short GRPO was the open source release from deepseek and there was a like everyone who like is terminally online and follows AI discourse knows that deepseek moment of whatever it was when the when the the Chinese company that seems to be doing really really great work released new model and it was a also pre-trained model, a reasoning model. They open sourced the algorithm. They open source a lot of things they did overall like really really great and technically excellent release and like there was a lot of discourse about about uh like you know that they they they pre-trained their model particularly cheaply and that was that was part of the discussion about the deepseek moment but the other part of the discussion was that they that they kind of like released their reasoning process. It was like not very far after our 01 release and as far as I know like our 01 release mostly caught a lot of US labs by surprise. They didn't have like similarly advanced RL research program to my knowledge. Basically no one and like and I think like the only company in the world that they get I am as I am aware there there's probably a lot of things I I didn't know but you talk to people sometimes you hear rumors. So this is my my version of the world is that if you look at the other papers of deepseek like that company was doing pretty similar in some ways RL research to what we are doing and I you know I think I have to clarify like what we what what's openi is doing is not exactly gpo it is slightly different in many different ways but like some parts are are are definitely are definitely similar and what's what's most important those are both like large scale policy gradient algorithms and like deepseas was doing company was doing research in a slightly adjacent area. they were they were they were not very far and when we whenever we released 01 and we told the world that you can get pretty like those great results with scaling up reinforcement learning on language models I think it was like not very big hope for for for the deepse company to kind of realize okay we are not very far from getting similarly good results and they they did it they trained their reasoning model and they released it and they told the world how pretty like not very not much later than we released 01 and I think like you for a lot of US research lab that didn't like yet know didn't have a research program how to train like reasoning models they looked oh there's this this like Chinese company they released how to do it it helped us kickstart and train reasoning models much faster than we would have to otherwise if we would have to find all those bits ourselves
>> what does it take to scale uh RL so if there was a phase where openi was very focused on pre-training And then if I understand correctly the last uh 12 18 months or whatever time period where the emphasis has been on the sort of second part of the plan which is scaling RL uh is is that a question of just giving RL more compute more data more labeling as we were saying what what does it take
>> the the first thing that is important to know and understand RL is hard like conceptually if you think about it and there's still a lot of depth to it but but Conceptually, mathematically speaking, pre-training is dead simple. It is the kind of the simplest thing you can do. And there has been there has been a lot of uh a lot of thought and a lot of optimization already put through that through a few years of even of of optimizing and doing very well at very large scale very simple mathematical operation. In terms of in terms of RL, it is much much more complex. There there there's many more things going on in a in a reinforcement learning run. there's many more things that can go wrong in in doing it especially as you as you scale up and many more types of bottlenecks, failures h like it's it's it's a much more much more delicate thing and there's much more uh like much much more room for error and in some way like you know I don't want to go too deeply in the parallel because it's a little bit overblown but in some way like you know just just to give some coincidence you can have a steel factory which makes steel steel and like the process is relatively standardized and you make blocks of steel and they are they are uniform and nice and well defined what it is versus like building semiconductors which there are there are very very few companies in the world that can do it because there are so many things that can go wrong and you have to put a lot of attention to details to make great semiconductor and it's very very like complex internally and like you know in many in many ways this is this is kind of like you know I don't want don't want to diminish because there's a lot of very hard technical difficulty to do pre-training black well at the at a large scale. But there's just just many more moving pieces and many more uh many more elements of the of the reinforcement learning stack that's that that need to need to get right be get right to to get a large scale run successful. you you mentioned um working on so agent like the agentic AI like what where does that all fit the you know the the the tool use like the whole like agentic autonomy versus reasoning RL like help us to of reconcile what does what and what impacts what
>> what's I think what is what is the important thing is I I believe and I think that there can be a lot of positive impact of AI on our world and our on our lives through automation, through problem solving and through AI doing good things for us, the things the things that we want. And for a long time and for a long time, again, it's not not that long, but the last two years or or so or maybe approaching three, we've been like living in this world where we kind of ask questions to AI and it gave us an answer for at the beginning instantly. Now it can think for like a minute or two which which feels long but in many ways like what's what can you do for two minutes if you think of like how many problems you must solve and AI is probably a little bit faster in the things it can it can solve but it's still a limit of what it's what it can do there are still a lot of tasks that you know that would take AI to do much much longer when when I prompt codeex it works for a while again few minutes like there are a lot of like things we have internally and we are doing that like allow the model to work for much longer. We still didn't figure out the right product to deploy them. But the models like can think for like 30 minutes, hour, two hours these days on certain certain types of tasks and problems like even even longer than that. And they they they generally are are capable of doing so. And we need to figure out how to make that process more useful and more being able to actually come to various problems in real life. whatever it is, coding or booking travel or making plans or or or or even designing houses or new electronic devices or whatever whatever else we would like models to do, we would like them eventually to to be able to do for us. um like a lot of this comes through the models thinking independently for longer periods of time and considering more of our alternatives bits and just just sometimes going through a slog of very long lists of tasks.
>> So the gentic part is is is powered by fundamental reasoning. Is there is there a concept of um uh I guess online RL that that happens where as the agent does something and learns from the real world uh the RL happens in in real time.
>> So in general like all all of RL is happening like most of RL that that you hear talk to uh language malls is online but it's done online in a way that is still a training run. it's still being trained like kind of separately from the user. There have been a few models in the world and I've I've learned recently that I think I think cursor uh is trying to train some models online with their with their users in the loop and it's theoretically possible to train models in GPT or every other product just just responding to the users and reinforce through whatever whatever rewards you get in there. But this is not what I am aware at least like not not what opening eye is doing at the moment and it is it's can be great but it can be also dangerous because you are not really very much controlling what you are reinforcing in that loop and what's what could what could happen. So yeah at least until until we have a really good safeguards I don't think I don't think we should try to do that in anything like as as as complex and large scale as just GPT. Yeah, interesting. And very much on on that note, um talking about alignment for for a minute, is alignment an RL uh thing. I mean, is it do do you create alignment in the model by teaching it what what is right and wrong? kind of kind of yeah it's a little bit of yes and a little bit of no like in a way alignment is about steering the most to certain behaviors and that is that is definitely a narl thing and anal problem but also you want the most to know what is right and what is wrong and understand the world and not not all of them are are like reasoning and and are all problems those are very often like just as AI problems and we like in in in the A to B aligned the model needs to know right or wrong to choose right. I don't think like you can just tell them all like few show it few good things to do and it will do them all needs to deeply understand its action and consequences to really truly be able to choose the right thing and it's a I think it's a neverending pursuit because like even even for humans it's not super easy to uh to define what's what do we consider align I think as our as our civilization will evolve it will the notion of of alignment and and and the goal of of humanity will keep evolving and we'll need to keep nudging the model towards those things and keep keep explaining to it the things that we that we want from it. Uh but it's a it's a very definitely very important and and and central part of part of any should be of any AI research program.
>> Yeah. Which brings a whole um you know next series of of questions of where RL is efficient versus less. So it seems that it's been particularly good for like math and coding and then the next obvious question is what about the rest of the world? But like taking a a quick um sort of going down the rabbit hole a little bit about math. So just in September like just a few weeks ago uh you guys did something uh unbelievable with the ISPC world finals. Do you do you want to talk about uh what that was and uh what went on from a model technical perspective behind the scenes?
>> There happens surprisingly little from our perspective from the model perspective. We just we just have a pretty smart model and then when we ask them to solve programming problems we they they they are they are correct. uh like what's what's a little bit of a backstory in it is that I think we used like specifically programming puzzles for a while as a very nice research test bed of our of our ideas. Those are those are nice problems to experiment on them and they they weren't ever like considered part of the product but it's it's those are pretty like complex problems and they require a whole bunch of thinking are very nice to give rewards to it. So a lot of researchers just just liked working on those problems as a way of trying out their their ideas like you always need a data set. I I'll take a data set of programming puzzles and try it. And I I think like because of that a little bit our models just were always very very good at competitive programming as a kind of byproduct. We never we never tried to be good at it but like researchers were trying their ideas on it and because of it like every every training run whatever whatever we are doing just just end up being very very good at at at those those type of puzzles and then like you know it was was a little bit of a of a formality for us to kind of like you know to to go and submit that to to a competition and and and then like you know do do it's like largely about demonstrating to the world how like what what what is the level of capability in in those models. But I think it is important and true to acknowledge that in not all domains at least like comparing to human baseline at this moment we can be we can be as nice and as good as in as as as in programming competition problems in many in many ways be because uh those those were like you know tried and tried for for for a long time by by by like you know many many researchers and researchers don't always spend as much time as as they could and I would I would like them to un on very practical problems that people go to go to to GVD or our models with
>> great so it sort of came out of the box. There was no specific training for for it. Um and just to remind people so what I'm referring to what we were discussing is the ICPC world finals that was just in September 2025 which is international collegiate programming contest ICPC that happened in Baku Aarai John where uh where open AI uh solved 12 complex algorithmic problems within a 5 hour time limit uh basically making it take effect the equivalent of first place um in front of human teams. So just four four compet we we we did a little bit of a like a round tour of various competitions. We did ICPC, we did also like II International Olympia informatics earlier this year and uh at coder horistics competition as well where we went second be behind a single human that is also a Polish person that used to be employed by OpenAI some time ago. a funny funny coincidence, but like I think like we we were looking for a moment in time where we are where our malls are kind of like smart enough to be able to compete in those competitions with with with some of incredibly smart and talented humans. But it wasn't never our particular goal and focus. It's kind of like we think if we are doing good research for training smart models, they should be smart enough to do those things. and we kind of like take that milestone and now we keep on moving forward and I hope we will see and I think we are already seeing more and more practical and tangible things coming out like every week or every other week on Twitter. I do see what I think are credible reports of actual scientists uh using some of our reasoning models to help them perform calculations, solve hard technical problems with with our models. And I think this is like where we want to be. Like solving competitions is cool, but people solve competitions to prove that they can work go to the actual like frontier level job and solve new technical problems. And this is kind of what we want from our models as well.
>> We we alluded to this a second ago. So um mentally at least for somebody like me I I understand uh how uh RL could be used very effectively to train against math problems or coding problems. I think one of the big questions right now is how do you do that for the rest of the world in uh context and disciplines where the answer is not right or wrong maybe a little more more murky and the rest of the the rest of the economy like you you guys as an organization came up with a GDP val the other day which is a way of um evaluating performance against different industries. what what is your thinking in terms of generalization of RL as as a as a as a path to success for the rest of the world? If I think like the the short and quick answer is like somehow humans can learn all those things and as long as there is any way to evaluate performance and figure out if something is going right or wrong and you can compute that feedback like you need to be able to to somehow calculate how well something then you can then you can optimize it and then you can do you can do reinforcement learning with it. I think like you know there there there can be an argument that if there is no notion what is right or what is wrong then also like humans are not able to to to improve and learn because there there there needs to be a learning signal that that coming somewhere there there there's most mostly a question of like how convenient and how easy is it to get that feedback and everyone doing reinforcement learning should strive and and try to be able to train on on on more and more complex and interesting training signals to do Um like very often there comes a notion of what what is often called reward hacking and like what's what happens when doing reinforcement learning. It happens a lot and it's a it's an important problem. You shape your reward in some way to reward certain behaviors. But sometimes it is the case that's what you reward is not what actually you want there. There is like one thing you you need to train them all to do the behaviors reward. But there there's also a natural like mismatch about about the reward that you give the mall and what's what what you actually want. And there are sometimes moments where where where where the mall does what you what you reward but but it's it's not not in the spirit of what of what you had wanted and we need to you need to fix it. It's almost like a like a parenting challenge and like you can in some way you can say it's a it's a limitation of reinforcement learning. But when I when I was thinking about it I realized a lot of that happens in human systems as well. There are a lot of like incentive system and reward systems and even even happens in workplaces in all kind of humans groups that humans have rewards that are not always optimized for the for the ultimate goals of the system and they they hack rewards constantly in many different ways. And there is a constant welcome all game between between setting the right rewards and seeing if the system does it. And it's a huge like issue in any policy making almost and any incentives programs. And this is the same same kind of like wacka game in in reinforcement learning research trying to make sure your rewards are better and better representing what you actually care about the model to be doing.
>> All right. So maybe to uh zoom out uh to to to close this conversation. Uh you said the other day um you tweeted we all collectively believe AGI should have been built yesterday. Uh and the fact that it hasn't yet is mostly because of a simple mistake that needs to be fixed which is super awesome as a as as a tweet. Uh do you think that the combination of uh pre-training and scaled RL takes us to AGI
>> like there there's always an an interesting question of like what what do we consider something that is not pre-training and like where where is the where is the limit I generally think something that we are doing like pre-training today is necessary I think something that like we are doing RL today is necessary and there will surely be a few things more and like we have a lot of very ambitious research programs on some of those things and like I I like I didn't think like the question of distance in research space is hard to say like what some for some people like what we what we are what we want to do and what we are planning to build is not very far from those things for someone will say oh it's completely different and it's like a very much not that so I don't want to go into into like debates whether it's the same or not, but like we are and want to be constantly changing the way how we train the models tomorrow represent what we think the the right form of of intelligence is and the most useful useful form of form of learning is and constantly are are researching various things and know the the distance from like what is what is the distance from AGI is also like a very complex question like I I really like someone said it to me but I think it is right that you If you talk to someone from 10 years ago and show them chbd from today, they would probably call it AGI. But we are not today because it still has a lot of limitations and we are we are all very aware of those limitations and we are pretty sure we can resolve those limitations. There will be probably some further limitations of the future models that that will need to be fixed. There is an ultimate question which is very hard to answer. when when is the moment that the model can like improve itself without that much like external output and without humans working on it and fixing it and I think I think it's a it is a very hard question it is a serious question that like you know we need to try to answer humanity needs to need to try tries to answer because because like you know most that will will still like largely depend on on like our infrastructure in our systems, but we will be able to start start fixing itself without without us having to fix it. And that's that's like the the predictions of what really AI will be able to do and will be able to solve at that moment start becoming like a little bit murkier than what we can do right now, which I think we still we still can do that pretty pretty well. But philosophically, you you um may have heard uh Richard Sutton, you know, the other day on the Dwarkish podcast, which is a which is a wonderful episode that people should really listen to, effectively saying that um the only path to ADI was going to be pure RL and that fundamentally LMS and maybe I hope I'm characterizing what he said appropriately, but that LMS were a flaw premise because effectively that was imitation of reality. whereas RL was enforcement of reality. Do do you have any thoughts sort of like philosophically on um on that question?
>> Yeah. Yeah. Yeah. I I haven't had a chance to to to fully listen to that episode yet. So I also don't get all the details of that uh of of that thought. But what I can say is that we are doing quite serious RL on language models these days and like I don't like in terms of a pure RL I don't think like really pure RL makes sense RL needs pre-training to be successful and I think pre-training as as I said before needs RL to be to be successful as well I I don't think without RL it would make sense the research program we are we are doing but but we are like open AI is and I'm pretty sure all other all other AI labs as well very serious about about doing a lot of reinforcement learning on our models and I think like I what what what kind of like a lot of people are saying that that whether LLMs are an on-ramp or offramp of AGI very often they they do mean like pre-training but it's also clear that that like you know the current way how we are doing things like also it's not yet enough and it's not yet everything and there will need to be a further further changes this to to the setup but sometimes like people say oh if you are if you are doing RL it's not LLM it's something else sometimes say oh if you can write program in your in your roll out and it's a chain of thought it's not in this neural network only it's a neural symbolic system so it's like you know it's easy to get like some people consider something LM and the other thing not uh but but like personally my view is what we have is a pretty good foundation for the next step of of like we did F transformers first train for trans for translation then we were then we were pre-training them on large scale data then we were doing our LF on them now we are doing large scale reinforcement learning we'll do a few more uh more and more complex thing there is a chance somewhere along the line the architecture will start changing more or less significantly and and you know I think I I personally think we are on the on the right path and it will feel less like uh complete completely turning around and more like more like keep on adding more things and maybe like you know dissolving some some old elements that didn't that are not that that carried us to that the particular level of intelligence and we're not needed anymore.
>> Well that that feels like a wonderful place to leave it. you've been very uh generous with your time and thoughts and giving us a glimpse uh into uh OpenAI, what you work on, what it looks like behind the scenes and the key aspects of pre-training and scaling reinforcement learning. So, it's been a wonderful conversation. Jerry, thank you so much. Really appreciate it.
>> Thank you very much. I enjoyed being here a lot too.
>> Hi, it's Matt Turk again. Thanks for listening to this episode of the Mad Podcast. If you enjoyed it, we'd be very grateful if you would consider subscribing if you haven't already, or leaving a positive review or comment on whichever platform you're watching this or listening to this episode from. This really helps us build a podcast and get great guests. Thanks, and see you at the next episode.