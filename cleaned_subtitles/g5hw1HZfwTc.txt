2025年 机器人公司发布的Demo 都有点魔幻 首先是Figure AI 在10月发布了第三代机器人 能够做各种家务 Demo也很酷炫 但是任务的成功率嘛 反正业界是有很多质疑的 而且我很想吐槽这个设计 它这个脸 实在是恐怖谷效应有一点严重 你看之后 10月底发布Demo的另一家明星公司1X 就聪明了很多 整个脸部设计是不是就可爱了非常多 感觉是让大家更愿意搬到家里的 但是这个叫做Neo的这款机器人 依赖远程操控 被批评是“假智能” 而且有各种的隐私问题 同时特斯拉的机器人 虽然也发布了各种Demo的更新 包括在12月发布的非常顺滑的跑步Demo 但是明显量产计划 在2025年是遇到了极大的挑战 让公司不得不暂停生产 要重新设计硬件 更别提最近的一次亮相 还遭遇了一次小小翻车 是什么呢 我们卖个关子 我们的机器人系列视频已经聊了灵巧手 也出了一期比较综合的 做了一下2025年的具身智能的年终总结 我们接下来深聊一下 这个产业的一个核心技术 机器人基础模型 我们试图回答这样一个问题 为什么2025年 突然变成了机器人基础模型的“元年”呢 我们也走访了硅谷的 前沿机器人公司和实验室 而基础模型篇 会分为“闭源”和“开源”两集 系统拆解当下主流机器人的“大脑” 是如何被训练出来 如何接入真实世界 以及不同路线背后的技术与商业逻辑 带你看清大模型时代的机器人 大脑究竟是怎么长成的 你这还没有机器叠得好看 在这一集 我们先来聊一聊目前资本市场的宠儿 闭源系统 但是在回答这些问题之前 我们先得搞清楚一个更加基础的概念 到底什么是机器人基础模型 如果要用一句话解释机器人基础模型 最简单的类比就是 如果说GPT是“会说话的大脑” 那么机器人基础模型就是“会动手的大脑” 但这个“会动手的大脑” 人类研究了整整60年才做出来 我们先来回顾一下 大模型出现以前的四大机器人范式 来 让我先带你回到1961年 那一年 世界上第一台工业机器人Unimate 在通用汽车的工厂里“上班”了 它的工作很简单 从生产线上抓起滚烫的金属零件 放到另一条生产线上 从现在的眼光看它蛮“傻”的 因为完全靠编程 工程师用代码告诉它 步骤1 手臂向左移动30厘米 步骤2 手爪闭合 步骤3 手臂向上移动50厘米 步骤4 手臂向右旋转90度 步骤5 手爪松开 听起来很傻对吧 但是在当时这已经是革命性的突破了 这种方式的问题很明显 零容错 零灵活性 如果零件的位置偏了1厘米 机器人就抓不到了 如果换一个不同尺寸的零件 就得重新去写代码 更别说应对意外情况 比如说零件掉在地上了 机器人就彻底的不知道该干什么了 在工厂这种高度可控的环境里 这套方法管用了几十年 直到今天 很多汽车工厂的焊接机器人 还在用这套“编程式”的逻辑 但到了90年代 机器人学家们意识到光靠编程不行 机器人得能够“感知”到环境 于是出现了SLAM 也就是定位与地图构建 还有运动规划这些技术 这里的核心思路是 先用传感器“看”周围的环境 建立一个3D地图 然后在地图上规划路径 最后执行动作 你们猜 这个方式最成功的应用是什么呢 扫地机器人 风靡一时的Roomba就是这么工作的 它用激光雷达扫描房间 建立地图 然后规划一条覆盖所有区域的路径 再按照路径移动 遇到障碍物就绕开 这套方法在“导航”任务上很成功 早期的无人车、无人机、物流机器人 基本上都是这个套路 但是在“操作”任务上就不行了 为什么呢 因为操作任务太复杂了 你想 让机器人叠一条毛巾 传统的方法是四步 第一 用视觉识别毛巾的四个角 第二 计算每个角的3D坐标 第三 规划手臂的运动轨迹 第四 执行抓取 折叠 放下 听起来挺合理 但是实际操作中到处都是坑 毛巾可能会皱成一团 根本就识别不出“四个角” 而且毛巾它是柔性的 你一抓它就变形 3D坐标立即失效 每一步都有可能出错 一出错整个流程就崩了 2010年 加州伯克利的一个研究团队 就做过一个实验 让机器人叠毛巾 用的就是这套“感知→规划→执行”的方法 结果呢 平均一条毛巾要花24分钟 而叠毛巾这件事情 在如今AI时代来临之后 也同样是非常核心的 需要基础模型去驱动机器人攻破的任务 这个我们一会儿来讲 既然手工设计规则不行 能不能让机器人直接“学”人类怎么做呢 这就是行为克隆（Behavior Cloning）的思路 也叫模仿学习（Imitation Learning） 同样以叠毛巾为例 机器人模仿学习会这么做 让人类演示很多次怎么叠毛巾 然后记录下每一帧的视觉输入和动作输出 之后训练一个神经网络 学习输入 输出的映射 最后机器人看到毛巾 直接输出该做什么动作 2015年 Google Brain的一个团队 用这个方法 让机器人学会了抓取各种物体 他们收集了数十万次抓取的数据 训练了一个神经网络 推动了“视觉-动作” 学习在机器人抓取任务上的进展 这可以说是个巨大的进步 第一次机器人不需要手工编写规则 可以通过数据学习了 但是这个方法有个致命的缺陷 就是数据效率太低 它需要数十万次抓取数据来训练 而且这只是“抓取”这一个动作 如果要学“叠毛巾” 可能100万次的演示都不够了 更要命的是这个方法的泛化性很差 你用A型号机器人收集的数据 训练出来的模型 在B型号机器人上基本上就不能用 接下来就来到2010年的强化学习时期了 2016年 AlphaGo战胜了李世石 证明了强化学习的威力 机器人科学家们也在想 能不能让机器人也用强化学习 自己摸索出怎么完成任务呢 强化学习的核心思路是 不需要人类演示 让机器人自己尝试 做对了给奖励 做错了给惩罚 机器人慢慢学会怎么做能获得最多的奖励 当时 波士顿动力的机器人 就开始将强化学习引入移动控制系统 让它们能在各种复杂的地形上 行走、跳跃、后空翻 但是强化学习也有个大问题 就是太慢了 AlphaGo为了学会下围棋 在仿真环境里面和自己下了几千万局 但是机器人操作任务 很难在仿真环境里面练 因为环境复杂度太高 非常难设置 和真实物理世界的差别也较大 导致仿真不准 但是真机试错呢 太慢、太贵、太危险 想象一下 让机器人学叠毛巾 它可能要试几百万次 其中大部分的时候会出现的情况是 抓空、把毛巾扔到地上、把毛巾撕破 还有手臂被卡住等等 这样学下去要到猴年马月呢 而且强化学习有个更加根本的问题 就是它不知道“常识” 人类知道毛巾是软的、可以折叠的 有一定的摩擦力 但是强化学习的机器人 需要通过无数次试错 才能够“发现”这些常识 也就是说这样的效率太低了 故事终于来到了2025年 大语言模型的出现是改变了一切 2022年 ChatGPT横空出世 人们发现大语言模型里面蕴含了 人类世界的大量“常识” 它知道毛巾是什么、叠是什么意思 先做什么后做什么 它有推理能力、规划能力、泛化能力 行业里面的第一反应就是 能不能把大语言模型和机器人结合起来 于是VLA（Vision-Language-Action）模型 就诞生了 VLA模型的革命性在于 它把三个东西统一到了一个神经网络里面 分别是Vision（视觉） 就是看到当前的场景 Language（语言） 理解任务目标和常识 还有Action（动作） 输出具体的控制指令 举个例子 你对机器人说 “帮我把桌上的苹果放到篮子里" 传统方法需要四步 第一 视觉识别“苹果”和“篮子” 第二 规划“抓取苹果”的轨迹 第三 规划“移动到篮子”的轨迹 第四 规划“放下”的动作 VLA模型呢 一个端到端的神经网络 直接从“语言指令+视觉输入” 输出“下一步该做什么动作” 更神奇的是它会“常识推理” 比如你说“帮我准备早餐” 面对着家庭环境它就知道 要从冰箱里拿出鸡蛋 鸡蛋要小心拿 不能摔碎 面包要放进烤面包机 这些常识不需要你一条条地编程 也不需要它自己去试错几百万次去“发现” 因为大语言模型里面已经有了 我们其实在architecture（架构）层面就是用的VLA VLA简单来说 就是我们拿了大模型领域VLM 作为所谓的backbone（底座） 但是我们会在最终输出结果的时候 把这个结果转化成 在机器人领域可用的Action（动作） Action你直观理解就是你可以把它转化成 比如说我要把这个手臂 移动到某一个坐标点的这一些命令 VLA其实大家诟病最多的是 为什么我们需要L language（语言） 因为其实在过去传统的机器人算法里面 很多都是纯基于视觉 而且你想人其实在执行很多操作的时候 大部分时候也是基于视觉的 但是你仔细去想 其实你大脑其实会产生类似于语言的东西 去告诉你在一个长线任务中 到底你第一步做什么 第二步做什么 然后你可能会根据视觉 比如说你去拧瓶盖 你脑子里不会出现说拧瓶盖三个字 你才去拧对吧 因为它已经足够小 所以我们觉得 L的作用就在于 对于一些非常复杂的任务的时候 它是可以通过在大语言上面 已经训练出来很多逻辑性的东西 它知道比如说你要喝水 你需要找杯子或者找瓶子 对吧 这个是通过大语言模型 已经直接可以给你的一些东西 那利用VLA的主要目的 其实就是如何把Language（语言）跟Vision（视觉） 能够更好地结合起来 否则你如果只有视觉 你能做的任务可能就都是短线的 你做不了任何长线的 需要去做推理的一些任务 所以这是我们为什么去 非常专注地引入语言这部分的主要原因 那为什么2025年 成了“具身智能基础模型的元年”呢 因为三个关键的因素 都在这一年同时的成熟了 那么第一个因素就是大语言模型“够用了” 2024年到2025年 OpenAI、Anthropic、Google 这些公司陆续发布了新模型 大语言模型已经差不多“成熟”了 至少对于机器人需要的那部分能力 也就是理解指令、规划任务、常识推理 已经足够好了 第一是大模型本身已经趋近于成熟 你们可以看到最近不管是OpenAI 还是其他的这些公司 其实发布的模型已经是增量式的增长 它不是像可能从3.5到4的时候的 这种跨越式的增长 所以我们觉得大模型的能力已经趋于稳定 而且已经足够可以 为具身智能提供一个很好的基础 所以这是从模型层面的一个最重要的因素 第二个因素是算力价格腰斩再腰斩 2023年 租一张NVIDIA H100 GPU是天价 还得排队才能拿到货 而随着GPU云服务商的价格战打响 和NVIDIA的GPU大量铺货 初创公司们都租得起几千张卡 来训练模型了 第二个因素是 整体的算力的强度肯定是越来越强 对吧 每一年其实因为它都会 包括其他的这些芯片公司 都会做更强的芯片 等效的算力价格其实也在降低 隔几年可能你的等效的价格 就可能变成了过去的一半 所以计算的增强对于整个具身智能 也有很大的推进影响 第三个因素是硬件供应链成熟 这个变化很多人都没注意到 2024年 随着人形机器人热潮 大量资本涌入了上游零部件厂商 特别是中国的供应商们 电机、减速器、传感器这些东西 原本都是小众产品 但是从2024年开始 好几家供应商都拿到了大额融资 开始扩产 硬件便宜了 做机器人的门槛就降低了 第三是整个机器人硬件的 各种零部件的成熟度 其实相对是比较高的 特别是从去年开始 火热起来的这一波人形的助推 其实让大家花了很多的精力跟资本 去投入到很多基础部件 包括电机 包括减速器这些部件的一些研发 这一块的成熟度和成本 其实都有提升和降低 所以我们觉得这个时机会比较成熟一些 这三个关键元素 让2025年成为了一个特殊的时间窗口 基于VLA的新一代范式的机器人 就跑出来了 2025是人形机器人大年 第一台机器人管家终于登场了 人形机器人将有望成为史上最庞大的产业之一 这将是一个5万亿的市场 全世界将遍布十亿台机器人 但是VLA模型也不是完美的 而它的核心挑战就是数据 大语言模型可以用互联网上的文本训练 但是机器人需要的是“真机数据” 也就是必须有机器人本体的传感器数据 而这种数据互联网上根本没有 YouTube上有无数人类叠衣服的视频 但是没有一个视频告诉你 叠衣服的时候 手指关节的角度是多少 施加的力量是多少 这就是为什么这场“军备竞赛”的核心 除了算法还有数据 谁能用最低的成本 采集到最高质量的数据 谁就能够主导这个市场 关于机器人的数据 我们也会单独的出一期 大家不要忘了关注我们的频道 不要错过更新哦 所以大家看 机器人基础模型不是凭空冒出来的 它是60年的技术积累的集大成者 它积累了编程式机器人的“精确控制” 基于模型方法的“环境感知” 行为克隆的“示范学习” 强化学习的“自我优化” 再加上了大语言模型的“常识推理” 这才是真正的“基础模型” 可能你想知道 现在搭载了VLA模型的机器人 都到什么程度了 我们这次也走访了Dyna Robotics 这家在硅谷炙手可热的机器人 明星公司的三位华人创始人当中 Lindon Gao和York Yang是连续创业者 之前创立的AI购物车公司Caper AI 以3.5亿美元的价格被Instacart收购 Jason Ma则是前DeepMind研究科学家 专攻机器人基础模型 这家公司成立才一年 就已经完成了两轮融资 2025年3月种子轮2350万美元 同年的9月A轮1.2亿美元 估值超过了6亿美元 投资方的名单堪称豪华 英伟达、亚马逊、三星、LG等等 而让它们最先火出圈的 并不是多么fancy的任务或者Demo 而就是非常朴实的“叠毛巾”和“叠衣服” 一时兴起 我突然想和机器人以及和York比拼一下手速 看一下叠衣服这件事情 谁能做得更好和更快 好 我们计时比赛开始 虽然我俩都比机器人快吧 但是说实话 我觉得我真不一定有Dyna的机器人叠得好 而且关键点在于 机器人虽然目前还比较慢 但是它可以24/7的运作 还不用休息 只要经济账算得过来 落地就是可行的 因为正常人工的很多场景 你一个人就是8个小时 通过机器的话 你可以让它跑15个小时或者24个小时 其实可以弥补掉效率的一定的损失 第二是叠（毛巾）本身 确实是一个不错的商业落地场景 因为它相对比较单一 它也是比较固定的一个任务 但是在像美国这样高人工成本的国家 确实要花掉很多的资金在这件事情上面 所以他确实 我们聊的这些商家客户 都非常有意愿去使用机器人来做这件事情 好 比完了赛 我们接着来说说 如今闭源模型机器人的几个主要流派 看完Dyna机器人 我们再来看看 2025年的机器人赛道还有哪些玩家呢 我们可以把他们分成三个流派来看 表面上看 他们争的是技术路线、市场份额、融资估值 但是本质上他们争的是同一个问题 那就是什么才是 实现“通用机器人”的正确路径呢 第一派的代表公司 就是Tesla Optimus和 Figure AI 这一派的核心信念是 机器人基础模型不能和硬件分离 必须垂直整合、深度耦合 才能够发挥最大的效果 Tesla是这个流派最激进的代表 马斯克曾经说过一句很狂的话 他说“特斯拉八成的价值 将会来自于Optimus机器人” 马斯克的自信来自于哪里呢 也许就来自于 特斯拉FSD（完全自动驾驶）十年的积累 特斯拉Optimus的前工程主管Milan Kovac 就曾经说 “我们只是从轮子上的机器人 变成长着腿的机器人” 特斯拉有数百万辆车收集的真实世界数据 端到端的神经网络架构 规模庞大的标注团队 所以他这个逻辑听起来无懈可击 既然FSD能让汽车在复杂路况中自主驾驶 那么同样的架构 为什么不能让机器人在复杂的环境中 去自主的操作呢 都是感知、决策、执行的闭环 都是端到端的神经网络 只是输出从“方向盘角度” 变成了“关节角度”而已 但是2025年的现实并没有那么美好 年初 马斯克在内部会议上信誓旦旦地说 2025年要生产5000台Optimus 其中1000台会部署在特斯拉自己的工厂 但是到了年中 实际上组装了1000多台之后 特斯拉Optimus人形机器人的生产计划 就已经暂停 面临重新设计 而我们前面提到的 Optimus最近面临的风波 就是它在特斯拉活动现场 分发瓶装水的时候 突然就做出了 好像要把头上某个不存在的东西 拽下来的动作 然后摔了一跤 这个动作实在是太像 人类操作员摘下头戴式设备的动作 于是这段视频马上爆火 不少人马上提出来质疑说 Optimus是不是有操作员在远程操控呢 Optimus的发展 看起来不像马斯克号称的那么顺利 问题出在哪里呢 他们本身是最早在人形机器人领域 做出本体 然后有过一定的Demo演示的这个公司 他们现在主要利用的是人类视频做迁移 它的优势毋庸置疑 就是人类视频其实是最容易采的 因为你不需要任何的外设备 然后你采集的也是人手去操作的场景 可扩展性上面来说 特斯拉这个模式是可扩展性最高的 但是它的几个主要的问题在于 第一是人类的手和机器人的手 如果你想让它能力迁移得很好 需要做得非常接近 这也是为什么现在有好多人 在做很灵巧的灵巧手 非常接近人的自由度 这件事本身是一件非常困难的事情 第二 你再接近 它不是完全一样 所以在机器人的数据和人的数据中间 还是会有一个鸿沟 就我们所谓的embodiment gap（鸿沟） 这个embodiment gap在当前学术界也好 工业界也好 大家都是公认的 是一个比较难解决的问题 所以这样的数据的迁移的效率会比较低 特斯拉想用海量人类视频训练基础模型 但是人手和机器手的物理差异 也就是所谓的embodiment gap 是个绕不过去的坎 即使你有YouTube上所有的人类操作视频 转换效率也是个问题 这就是全栈整合派的第一个困境 你控制了全链条 但是也意味着全链条的每个环节 都是你的瓶颈 硬件不够好 模型再强也白搭 模型不够强 硬件再好也发挥不出来 但特斯拉有个优势 就是钱多、人多、还有马斯克 这应该是三个优势 Optimus会不会最终成功呢 可能要再过两年才能见分晓了 而Figure AI走的是类似的路线 但是更加激进 这家公司2022年才成立 创始人Brett Adcock 之前做过电动垂直起降飞机 算是从“飞行机器人” 跨界到了“地面机器人” 2024年初 Figure AI做了个大胆的决定 和OpenAI深度合作 将GPT-4直接接入人形机器人中 那段时间 他们放出来的Demo 可以说是震撼全行业 机器人似乎能够听懂人类的指令 能够和人对话 能够自己决定做什么 比如你说“可以给我点吃的吗” 它会主动递给你一个苹果 但是好景不长 但是好景不长 2025年2月 Figure AI突然主动宣布和OpenAI“分手” 他们要独立推出自己的基础模型 不再依赖OpenAI的技术 但分手之后的Figure AI两周之后 就迅速地推出了新Helix模型 定位为通用人形机器人VLA模型 强调是完全自研 用于控制整个人形机器人 不得不说 能够放弃OpenAI的“粗大腿” Figure AI确实也有两把刷子 Helix创新地采用了System 1 System 2 双系统架构 System 2就像你的大脑皮层 负责“想清楚要干什么” System 1像你的小脑 负责“手脚怎么配合” 当你拿杯子喝水的时候 大脑皮层只需要决定“现在该拿杯子了” 小脑就会自动地调动20多块肌肉 完成抓取动作 你根本就不需要意识到 这个架构 解决了一个长期困扰机器人的问题 视觉-语言模型很聪明 但是太慢 传统机器人控制策略很快 但是不够通用 Helix让两者各司其职、端到端训练 既能够理解复杂指令又能够实时精确控制 而更酷的是 Helix用单一的神经网络 控制整个上半身的35个自由度 包括手腕、躯干、头部 还有每根手指 它还能够同时控制两个机器人协作完成任务 这就是Figure和OpenAI“分手”后 交出的答卷 2025年9月 Figure AI完成了10亿美元的C轮融资 估值是从26亿美元飙升到了390亿美元 15倍的涨幅 不到一年的时间 投资方名单 读起来就像科技圈的奥斯卡颁奖典礼 微软、OpenAI、英伟达、贝佐斯 英特尔、三星等等 听起来就已经成为了 具身机器人界的“扛把子”了 总结一下 这一派的核心理念是 基础模型的通用性 来自于“足够大、足够端到端” 只要模型参数够多、训练数据够多 软硬整合够深 那么涌现能力就会自然的出现 这是从GPT-4的成功中总结出来的经验 但是这个经验在物理世界是否成立 还是一个未知数 如果说全栈整合派追求的是 “一步到位的通用性” 那垂直突破派追求的就是 “从专精到泛化的涌现” 他们的核心理念是 与其去训练一个什么都会 但是什么都做不好的大模型 那不如先让模型在某个垂直场景做到极致 那么在这个过程中积累的“学习能力” 会自然地迁移到其他场景 Dyna Robotics 是这个流派比较清晰的一个代表 我们刚刚已经看到了Dyna机器人的演示 Dyna走的路线也很特别 做通用形态的机器人 但是在模型层面上 会先利用比较成熟的能力 落地一些可以打工的场景 用于了解行业的know how（实际知识） 并更好地指导算法研究的方向 也就是说要先让机器人 在洗衣房、餐厅、健身房这些场景去“打工” 边干活边学习 在2025年4月 他们发布了“首个可在真实环境中 持续高性能运行的机器人基础模型”DYNA-1 在24小时内 他们的机器人自主折叠了700多张餐巾 成功率超过了99.4% 完全无需人工干预 吞吐量达到了人类速度的60% 但显然Dyna的野心不止于叠毛巾 第一是我们要澄清 我们不是一个做叠毛巾的公司 我们的Foundation Model（基础模型）里面 包含了各种各样的数据 有叠各种叠的 叠毛巾、叠餐巾、叠衣服 也有像什么切菜、切水果、准备食物 然后也有像做早餐、清扫 或者说摆放物流场景一些分拣 其实各种各样的数据我们都有 我们的Foudation Model（基座模型） 其实是一个非常广的一个模型 我们的泛化性 其实最主要还是来自于基础的大模型 我们是希望基础大模型 能够有足够强的能力 在大部分的任务上不太需要非常多的定制 在早期可能你会发现 迁移到一个新的任务的过程会比较冗余 比较繁杂 就是你会需要再重新采很大一部分的数据 然后混到一起去做训练 但随着你的基础大模型数据量 越来越大之后 你会发现哪怕去迁移到一个 从未见过的任务上面 它其实需要的迁移成本也会越来越低 我们过去可能会需要采几个月的数据 去迁移某一个任务 但是到现在可能有一些简单的任务 可能就一两天的数据就可以迁移过去 所以整体来说 只要你的基础模型能力越来越强 就是我刚才提到学习能力越来越强的话 那你去迁移到新任务的这个模型 迁移到新任务的能力也会越来越强 Dyna对基础模型的理解和全栈整合派 是完全不同的 他们对基础模型的理解是 与其训练一个什么都会 但是什么都做不好的泛化模型 不如先让模型在某个任务上去深度专精 在这个过程中积累的“学习能力” 会帮助它更快地掌握其他任务 就像把钢琴练到音乐学院水平的人 上手吉他 也会比完全没学过乐器的人会快得多 因为掌握了“如何学习”的元技能 我们确实也看到 当你单一任务的能力提升很强之后 它对于学习新任务有一定的促进作用 就是我们拿最优质的数据 到基础的数据集里面做pre-train（预训练）之后 其实这个模型再去扩展到新的任务上 它会更快 需要的数据更少 所以这个也是我们在实践过程中 找到一个有点counter intuitive（反直觉） 但是确实它发生了一件事 我们对于它的理解可能更像是 就像人 如果你的学习能力学得很强 我不说你会什么知识 但是你的学习能力本身很强 你学习新的能力就会很强 学习能力很强的前提是 你可能过去已经在很多任务上 你自己做过实践 然后做过学习 你才会有那个强的学习能力 所以我们觉得学习能力本身和学习的过程 它本身也是关联的 这个理念背后基于这样一个观点 机器人基础模型和大语言模型的Scaling Law 也就是缩放定律 可能不一样 大语言模型的规律是 模型越大、数据越多 性能就越好 但是机器人基础模型的性能瓶颈 就不只在“模型容量”和“数据量” 更在“数据质量”和“物理一致性” 如果训练数据里面的物理接触不准确 模型学到的就是错误的物理直觉 参数越大 错误就会越被“放大” 为什么说基于某种程度 就是它的Scaling Law 肯定不像大语言模型这么简单粗暴 像语言方向的这个数据 现在他们已经发现 哪怕你用很多低质量数据 就是你可能一堆文本 中间插了一段广告 然后再是接着这个文本 就这样的数据 它一样能训练出比较好的模型 因为模型它看的数据足够多之后 它自动就会过滤掉广告 其实这和人很接近 你看网页看多了 其实你眼睛也自动会过滤这些广告 对 所以大语言模型它能够做到这个 但是机器人当前我们觉得Scaling Law 更多的是来自于需要比较高质量的数据 因为就像刚才提到的 它的数据的本质 更多的是机器人本体加上视频 这里面的不确定性太高了 所以你如果囊括了很多 很繁杂的数据在里面 它机器人模型可能就不知道 我要把注意力放在哪一个地方 所以最终它其实出来的效果并没有那么好 我们现在看到的是 如果你的数据质量足够好 随着数据量的增加 数据多样性的增加 整体的基础模型能力就会有很大的提升 然后对下游的 各种需要fine-tune（微调）的一些小任务 也会有很大的提升 对 所以这个是实打实能够看得到的 所以Dyna选择“小而精”的路线 他们认为 首先与其训练一个100亿参数的泛化模型 不如去训练一个10亿参数的专精模型 其次是要保证每一条训练数据 都是高质量的真实物理交互 其次让模型在实际部署中 通过强化学习自我优化 他们认为 深度专精某个任务的过程中 模型学到的不只是“怎么叠毛巾” 还有“怎么快速学习新任务”的元能力 所以我们现在挺关注的 比如像强化学习的一些路径 然后像通过大模型的基础能力的学习 比如说折叠能力 比如说摆放能力的学习 让它拥有一个自我迭代 自我去学习新技能的能力 我觉得这个是最重要的 但最终我们会觉得 基础的大模型可能在普通的一些任务 比如说家用的很多 你帮我拿一个水 你帮我开一下门 类似的任务中 它应该是可以直接处理的 同样重视元学习能力的 还有诸如Skild AI这样从“通用模型”切入 但是并不做硬件的公司 他们核心逻辑是用大规模仿真数据 训练出一个通用的“大脑” 然后用这个大脑能够快速适配到 不同的机器人硬件和任务场景 比如说同一个模型 既能控制机械臂抓取物体 也能够让四足机器人行走 还能够指挥人形机器人完成复杂的操作 不需要每个任务都从头训练 而是靠一个强大的基础模型来迁移学习 有传闻说 英伟达和软银将领头对它投资10亿美元 估值将会高达140亿美元 这个路线还有一个特殊的玩家值得一提 那就是亚马逊 2025年7月 亚马逊宣布部署了第100万台机器人 100万台是什么概念呢 亚马逊目前有156万名员工 也就是说机器人数量即将超过人类员工 但是这100万台机器人 全都是专用机器人 针对具体场景优化 Hercules能够搬运1250磅的货物 Pegasus用于包裹分拣、运输 但是亚马逊的野心不止于此 他们的Agentic AI团队 正在开发通用机器人基础模型 还在旧金山办公室 建了个叫“humanoid park”的室内测试场 训练人形机器人应对复杂障碍 亚马逊的策略和Dyna如出一辙 与其一开始就做大而全的通用模型 不如先在垂直场景 积累世界上最好的数据和最强的能力 然后再泛化 如果说前两派是在争“谁的路线更快” 那第三派争的就是“谁能够制定行业标准” 它们的核心信念是在基础模型这个赛道 最终赢家不一定是技术最强的 而是生态控制力最强的 首先 NVIDIA的逻辑很简单 做机器人界的Android 2025年3月的GTC大会上 黄仁勋隆重地介绍了GR00T N1 并且把它开源了 听起来很美好 对吧 但是如果你要用GR00T N1 就得用全套的NVIDIA生态 一个都跑不掉 这就是生态锁定的威力 一旦你用了NVIDIA的全套工具链 切换成本高到让人望而却步 NVIDIA的护城河不是模型本身 而是整个生态 Google走的是另一条路 通过开源研究建立影响力 Google的机器人在通用策略上 选择了一条“研究驱动、开源优先”的路线 它推出的RT系列 强调大规模机器人演示数据 跨任务／跨平台通用模型 并且通过论文＋开放数据集的方式 在学术与研究社区建立了强大的影响力 在Gemini 3发布之后 Google最近也加快了步伐 还挖来了 前波士顿动力首席技术官Aaron Saunders 想推动Gemini Al成为通用机器人控制平台 NVIDIA和Google的生态策略 有什么本质区别呢 它们到底是真开源还是伪开源呢 我们也会之后聊到 而OpenAI和Meta 是这一派的另外一种玩法 小步快跑 只为占坑 OpenAI和机器人的关系 就像一对分分合合的情侣 早在2018年 它们就在机械手-操作任务上取得了突破 但是之后团队规模与优先级有所收缩 到2024年和2025年初 它们上演了和Figure 从热恋到断裂式分手的戏码 但是到了2025年的下半年 它们又开始招聘多位 专注于人形机器人控制算法的研究人员 此外 OpenAI也试图通过撒钱投资的方式 打造自己的生态影响力 2024年11月 OpenAI与杰夫·贝佐斯共同参与了 Physical Intelligence的4亿美元融资 Meta的策略类似 但是更加低调 2025年年初 Meta在其Reality Labs旗下 组建了一个新机器人部门 由前Cruise CEO Marc Whitten牵头 目标是开发类人机器人平台 Meta的CTO Andrew Bosworth 就曾公开提到 其团队正在构建一种“World Model” 世界模型 以支撑机器人完成 比“行走”和“跑跳”更细致的操控动作 表面上看 三派是在争技术路线、争市场、争估值 但是本质上 它们赌的是关于“通用性”的三个相通 但是又不同的假设 全栈整合派相信 通用性=足够大的模型+足够多的数据 +足够深的软硬整合 只要这三个条件满足 涌现能力会自然出现 这是从GPT-4的成功中总结出来的经验 而垂直突破派相信 通用性等于深度专精带来的迁移能力 机器人的Scaling Law和语言模型不同 “小而精”可能比“大而全”更有效 关键是找到正确的“元学习”路径 生态平台派相信 通用性等于生态标准化程度 技术路线谁赢不重要 重要的是让所有人都用你的工具链 最终的赢家不是技术最强的 而是生态控制力最强的 当然了 还有“半开源半闭源”的两家知名公司 分别是Physical Intelligence（PI）和Genesis AI 我们会在开源的下集里面 重点地介绍它们 这几大派系谁对谁错呢 2025年还没有答案 但是可以确定的是 这场关于基础模型的竞赛才刚刚开始 那么聊了这么多 我们来总结一下 2025年的现状是 展示得很精彩 落地还未可知 马斯克对特斯拉机器人的梦想很宏大 但现实是Optimus还在艰难地爬坡 12月19日 特斯拉官方是发布了一份名为 《特斯拉人形机器人2025年度报告》的 视频回顾 详细披露了人形机器人Optimus 在过去一年中的技术迭代与进化路径 视频以Optimus加速跑进2026年的画面收尾 暗示明年将会有更大幅度的技术跨越 我们也拭目以待 同时 Figure AI拿了10亿美元 估值390亿 但是真正商业化部署也就几十台 NVIDIA的GR00T N1发布了 但有多少公司真正地用起来了 也不好说 但是我们也看到 各家都在展示出令人惊叹的进展 有特斯拉这样手握重金的巨头押注 也有Figure、Dyna为代表的创业公司 在快速前进 还有OpenAI、Meta的低调入局 都在用重金 重资产的方式 推进机器人的基础模型 这让我们相信 尤其是在家用机器人领域 机器人开始帮忙干些讨厌的家务 已经不那么遥远了 我们是觉得最先肯定是在 像我们当前在开拓的一些市场 就是商用服务 商用的一些人工的这个部分 就是和人工一起去完成一些任务 这样的一些场景 但是我们觉得家用其实也没有那么遥远 就像我刚才提到的 你并不需要完整的非常通用的AGI 你可能只需要几个任务 就可以进入到家庭的场景里 先让机器人在家里面干起活来 然后逐渐地通过模型的迭代 让它产生更多的能力 我们觉得我们自己的目标 其实在明年（2026年）我们至少希望在商用场景 有比较大规模的部署 在家用我们会择机看 如果说比如我们当前像叠衣服这样 我们采访过很多身边的朋友 其实大家都觉得 这个功能他们非常需要 对吧 当我们的硬件成本 降到普通家庭可承担的范围内 我们可能就会优先 比如说我先以叠衣服的功能卖给家庭 然后逐渐去拓展一些其他的功能 对 所以这个时间线应该也不遥远 可能也就在1~2年左右 怎么样 几百美元可以帮你叠衣服 准备早餐和做清洁的机器人助手 你会买吗 好了 今天的部分就到这里 我们今天聊的是闭源阵营 但是有一群人在用完全不同的方式 去做同样的事 他们在做开源模型 他们分享数据 他们相信“聚沙成塔”的力量 他们说开放才能够实现具身智能 那么下一个机器人的视频我们会聊到 NVIDIA的“开放”到底有多开放 它和真正的开源有什么区别 为什么有人说GR00T N1是“伪开源” Physical Intelligence为什么要开源π0 一个刚成立、刚拿到融资的公司 为什么要把最核心的模型免费放出来 它们的商业模式是什么 开源VS闭源 谁会赢 这场战争的本质是什么 是技术路线之争 还是商业模式之争 大家不要忘了关注我们的账号 不要错过更新哦 那么以上就是本期视频的全部内容了 你们的点赞、留言和转发 是支持我们硅谷101 做好深度科技和商业内容的最佳动力 我是陈茜 我们就下期视频再见啦 Bye