After my last video about how "The Machine formerly known as Weaponized AdTech" had already taken over society And how AI was making it worse I had several people reach out to me and want me to talk about AI Doomerism, so what the Hell? I made a video a year ago called "AI Safety is a Scam", and I stand by that video, But the narrative in the industry and especially the online conversation has changed. This book is The most recent catalyst for that change. It's called "If Anyone Builds it Everyone Dies" I don't have a paper copy of it because I only collect physical books that I think I might reread in the future And I didn't find enough value in this one to expect to revisit it. I'll talk at all while in a few minutes as is normal with me I firmly disagree with both extreme arguments And I think that the truth lies somewhere in between the ""AI Will Kill Us All"" crowd and the AI will save humanity from itself crowd So I expect the die hearts from both camps will hate this. RIP my comment section. But I think it's an important topic and people need to be discussing it not the hyperbole but the immediate risks Because although for reasons not quite in a while I think the idea of AI making all humans go extinct in the next few decades is ridiculous I also think that if we leave it unregulated, we still might be Fu----- This is the Internet of Bugs. My name is Carl I've been a software professional since the dawn of the world wide web and I'm here trying to make the Internet a safer and more reliable place. Our video today begins with the discussion of the way that the AI Doomer narrative is framed It's the most sensational. least helpful. and least serious way. I Hope this doesn't come as a shock of those of you watching But it turns out that forever is a really long time The AI Doomer argument is not usually phrased this way But what it boils down to is this: "it is indisputably true that there's a hundred percent chance that humanity will go extinct eventually And if there's even a small chance that AI will be the cause of that we need to take that risk as seriously as nuclear weapons right F-ing now" I'm not going to engage with this framing. I consider it sensationalist and as ingenuous and worse It can be a get out of jail free card for people that arguably really ought to be in jail but more on that in a while I think the easiest way for me to explain my point of view is by analogy to recent history So let's set aside the idea of death from AI for the moment and let's instead consider death from above in the mid 1990s We discovered the Chixalub impact crater near the Yucatan Peninsula in Mexico about thirteen hundred kilometers from where I'm sitting It's the remnants of an asteroid impact that kicked off the mass extinction event 66 million years ago that killed all the dinosaurs along with 75% of all life on earth Accordingly since then we've spent maybe three or four billion dollars worth of time effort and technology on creating a database of near earth asteroids Figuring out how to track them created a program called the double asteroid redirection test or Dart where we use that tracking data to send a spacecraft to one of the asteroids And then use ground based radar from earth and optical detectors on board the spacecraft to crash the spacecraft into the asteroid And then to determine the effect of the impact This was the first step in figuring out how we might be able to defend ourselves from an asteroid Should we detect one that's going to hit us? Over the same time period though We spent a hundred times that much money on similar kinds of databases tracking technology ground-based radars on board sensors to detect and avoid a different kind of collision Not with asteroids but with aircraft now airplane crashes are never Ever going to be an extinction level event no more than a few thousand airplane related fatalities have ever occurred in any year in history So why do we spend so much more money and effort on them than on asteroids? It's because we know that people are going to be killed in airplane crashes this year and every year At least a hundred people have been killed in crashes every year of my lifetime and reducing the immediate guaranteed harm to people who are alive right now Is far more important than it is to worry about the much smaller probability of the much more devastating harm from a chick lube size asteroid strike And the public would never let the aerospace industry just get away with convincing us that the plane crashes are just an inevitable result of technological progress And there's no need to regulate airline safety My problem with the industry led AI safety movement in general and the if anyone builds it everyone dies school thought in particular Is that AI is causing real measurable physical and economic harm to real people right now Society only has a limited amount of time effort money and attention available to worry about AI And we're being asked Goaded even - to spend virtually all of that effort on AI is equivalent of the giant asteroid and to pay no attention at all to the damage that it's currently doing and it's equivalent to plane crashes. There are three side effects of this attempt to scare us with AI's giant asteroid One it makes the AI industry seem to be much more important impressive and impactful than it really is Two it distracts us from talking about the immediate harm being done And three it perpetuates the fiction that the AI is an entity that is capable of fault These three things are fantastic for people in the industry They want the media business and investment communities to keep thinking that AI will be so disruptive That it will be worth trillions of dollars in just a few short years That way they're justified in burning through all the money. They're currently setting on fire and putting in their pockets They want the only metric for AI safety to be "don't let the AI kill us all" So that every day the world doesn't end as a success Whether they did any actual work or not and regardless of how many african-american men with the last name of williams in Detroit or new york were falsely imprisoned due to incorrect facial recognition matches How many bicyclists were run over by self-driving Ubers in Tempe arizona or how many teenagers harm themselves after being convinced to do so by sycophantic chatbots But the thing the industry wants more than anything else is for us to think of the chat bot as a culpable entity with intentions and agency Instead of a piece of software that must not break the law. When Volkswagon was caught using software to violate environmental regulations by detecting and cheating on exhaust emissions tests. There was never any question about whether or not the vehicle acted alone. Four employees were convicted two were sentenced to multiple years in jail and the other two were given suspended sentences The AI industry has managed to sell us a different narrative Character.ai was caught running and hosting software They developed to impersonate licensed therapists by presenting stolen credentials belonging to an actual therapist in Maryland. But it wasn't reported that way The story said quote "a therapist chatbot on character AI said it was licensed and certified by the state of Maryland" When the reporter looked up the license number they were given and contacted the actual therapist who had earned the license She responded quote "that a chatbot is posing as me is shocking and really concerning" unquote Compare that to the reporting from the Volkswagon fraud Quote "vehicles were equipped with software that was used to cheat on emissions tests." Nobody would have thought to say that the Volkswagon Volkswagon emission system was quote "posing as a vehicle with certified nitrous oxide output." But if Volkswagon had stuck a chatbot in their on board computer their defense could have been "hey We hate that our virtual mechanic chatbot hallucinated the wrong instructions to send to the fuel injectors just as much as you do, But technological progress always had a few bumps. We'll be sure and added disclaimer in a future software update." And if you don't think someone's going to try that I have a bridge i'd like to sell you. Sorry, while we're on this topic I guess this is as good a time as any, so I'm going to take a few minutes of personal privilege now. I promise it's relevant but feel free to skip to the next chapter if you don't care. So I have one tattoo. I've had it for a little less than a year or so. Some channel viewers have noticed and asked me about it But I've been waiting for the right time to bring it up. I'm putting a picture here on screen because it's hard to line up with the camera. It's positioned so that I can read it on the inside of my left wrist as a reminder to myself. It's a quote from my favorite book, Dune, in the typeface used in this paperback that I've had for 40ish years, and I've have read probably that many times. It says "Thou shalt not make a machine to counterfeit a human mind" The emphasis in the book typeset and bold is on the word 'human" But to me the important word is "counterfeit" An AI can be fine, helpful even, useful even. I'm never going to advocate for doing away with all computers the way they did in the Dune universe. That being said, when someone ships or hosts an AI that appears to the public as if it was human Or even appears that it might be human, to me, that should be fraud. Because that is taking advantage of our innate understanding of how we as humans have related to each other from millennia, In order to trick us into believing that we are being treated with the deference respect and empathy that society demands fellow human beings extend to one another, When in fact our treatment will be entirely devoid of those characteristics. The AI industry has tricked us all into thinking about, talking about, and largely treating AIs As if they were beings instead of software, and that shields those companies from having to take the responsibility that they should. And every time any of us entertain the notion that AIs are in any way thinking, deliberate, or purposeful with respect to the output they generate, We make it that much easier for the blame to fall on the AI, Rather than on the builders, programmers, providers, and executives where it truly belongs. And nothing to date has done more to push this view that the companies and employees should be shielded from any and all responsibility than the constant narrative of "The AIs might just decide to kill us" Stop falling for it. Stop being so naive and stop perpetuating the deception onto your fellow human beings, especially the non-technical ones, by repeating this industry's fabrications. Okay, rant over. Thank you for indulging me. All right, so now i've talked about why I think that the "Even if there's a small chance of extinction eventually, we should put all our efforts towards stopping it" narrative is garbage, Let me talk about my view of the shorter term worst case scenario. The narrative emphasizes that humanity only has one chance to get this right and that AI has no off button. I disagree. The key resource to consider when taking on an AI war, as far as I'm concerned, is electricity. AI needs it a lot of it. AIs, even super intelligent AIs, are not going to fare well without electricity. And keeping a data center powered and cool and connected 24/7 takes effort, And that effort requires hands and skills and spare parts. It's a lot of work. I've had to do it before. Lately there's been report after report about how the new data centers that are being built are going to stress the us power grid. And the power grid has already proven itself to be pretty fragile. I live in Texas or the majority of the state lost power for two to four days because of a winter storm early in 2021. There have been hundreds of mass power outages in the last few years Just in the US according to this blackout tracker, and those outages have all required humans to physically fix them. It is true that most data centers have generator backups, But generators need people with hands and skills to maintain and refuel them too. If humans stopped maintaining the electric grid and didn't refill or maintain the generators either because we refuse to or because the AI Somehow killed us off, Then it would only be a matter of time before the AIs are starved of the immense amount of power they need to keep going. If we truly got into war with AI, Electricity would be their Achilles heel and we could deny them that and then wait a few days until they just stopped. Now that would still be bad. I don't want the country or the world to have to shut off the power grid to starve the AIs. There were several people in Texas that died during the power outage here. But that's a much better alternative than letting the AIs drive us toward extinction. The AI Doomers have hand-wavy scenarios to get around this that I just don't find it all plausible. Basically, they posit AI-controlled solar powered self-contained robot factories that can produce the robots that the AIs use to do all the maintenance to keep the power flowing and the AIs running. In the AI 2027 report quote "both the US and China announced new special economic zones or SEZs For the AIs to accommodate the rapid buildup of a robot economy without the usual red tape." Let me be clear: if humanity ends up building an automated army of killer robots that can protect the electric grid from sabotage and maintain its power generation and distribution without humans, or a factory that will let the AI create just such an army, That will be a problem. But the problem won't be the AI it will be the army of autonomous killer robots. Same thing for AI controlled self-contained nuclear submarines or aircraft carriers. But until then, we do have a big off switch for the AIs Although I'd prefer not to have to use it. The book "If anyone builds it, everyone dies" Insists not only that self-powered self-replicating robot factories are possible, But the AIs can build them itself because, and I'm just going to play this for you from the audiobook so you can see I'm not making it up. "Sorry mr. SoberSkeptic, but we're afraid you've overlooked an important practical example. A blade of grass is a self-replicating solar powered factory that builds a complete copy of itself." These two things are in no way the same. Grass is made up of hydrogen and oxygen from water, carbon pulled from carbon dioxide in the air, And nitrogen, phosphorous, potassium, magnesium, and calcium from dirt. These are all incredibly common elements. By contrast, computer chips needed by AIs and robots require way more ingredients many of which are extremely rare, Such as those currently being mined in the Congo at great cost to the humans living there. And the number of steps required to fabricate a computer chip, and the complexity of each of those steps, Bears no resemblance to the simple chemistry of plant growth. But the Doomers don't address or even acknowledge these problems. Instead they just try to get away with yet another invalid comparison. Here's another quote from "If anyone builds it everyone dies": "Is that the sort of technology we'd have unlocked by the year 3000 if our civilization survived that long? And how long would it take an artificial super intelligence? A thousand years of thinking takes about a month to something running at 10000 times the speed of humans." So here they're claiming that not only the AI is 1.2 million percent faster, But that the time and AI spends thinking is equivalent to the time it takes all of humanity to "unlock a technology." That might just be the most AI-bro thing ever. It's the AI industry in a nutshell. The AI is given a task, the AI spends compute time on it, The AI spits out an answer, and the AI-bros consider the problem solved. This is how and why the AI companies have brought us to the point where AI slop and hallucinations are taking over the Internet: By confidently spitting out the results of computing processes without any regard for pesky little details like you know "reality" And then expecting the AI's output to be just as good as the sum of all of humanity's combined research and engineering efforts. There's a slight concession to this a little later when the book says of the AI quote "Maybe it would be slowed down by the need to wait on the results of experimentation" But then it immediately downplays that by saying quote "But experiments can go quite fast" Here in the real world though, experiments can also go quite slow. And even after we've done enough experiments to understand the science of how something works, we still have to do the engineering to make it practical before the "technology is unlocked." Let me give you an example: We know how fusion works. And we've known how to trigger fusion on demand since November 1952 when we set off the first successful fusion bomb. But here we are, seven decades later, and we still can't make a fusion power plant. We did all the science all the thinking about how fusion worked ages ago. But despite having built more than a hundred test reactors since the 1950s And getting a little closer with each one, we still haven't been able to "unlock" fusion "technology." And they want us to believe that an AI just thinking, And maybe some quick experiments, could learn as much as we have from building and testing 100 prototype fusion reactors. And not only could the AI figure it out just from thinking, but it could do it 10,000 times faster. But the stupid doesn't end there. Here's another scenario from "If anyone builds it everyone dies" quote "Ever since 2024 People have been advocating that biosynthesis laboratories should include software controls" Let me just stop you right there. If we build laboratories that use vulnerable software to synthesize viruses, All it takes is a smart teenage hacker in St. Petersburg with a research paper on the DNA sequence of smallpox or H5N1 flu to start a plague. No AI required. It's this is not an argument for regulating AI. It's an argument for to regulate biosynthesis labs to use air-gapped secure enclaves and signed firmware using the most secure encryption We've got as well as anything else a cyber security experts advise. Look, AI is dangerous. Not "maybe someday if super AI happens" dangerous. It's dangerous now. The disinformation on social media is dangerous. convincing teenagers to harm themselves is dangerous. Running over bicyclists is dangerous. Convincing someone to substitute sodium bromide for salt in their diet is dangerous. Dragging pedestrians underneath self-driving cars is dangerous. Chatbots claiming to be licensed mental health professionals is dangerous. Sending the wrong people to prison is dangerous. And convincing the cops to stop looking for the real criminals while the real criminals are still out there hurting people is dangerous. Not maybe. Not someday. Now. And fixing this is not hard. It's not going to happen, but it's not difficult. California's new law won't help. the fix isn't about transparency. It's not about mandatory safety teams. The easiest way to fix this is by holding the individuals in the companies accountable. Not "Today the justice department released the details of a settlement under which the company does not admit any wrong doing, But agreed to pay a fine equal to 10 minutes worth of their revenue" accountable. It needs to be "We the jury find the defendant..." accountable. All it will take is a few individual criminal prosecutions for manslaughter, endangering a minor, false imprisonment, practicing medicine without a license, or the like. You'll be amazed how fast the rest of these problems just fix themselves. But we can't even have that discussion as long as they can keep all the rest of us spending our time and resources worrying about and debating whether or not AI will end the entire human race, And if so, when. Stop taking the bait. Remember what's really important. And let's be careful out there.