好啊我們來開始上課吧 那今天這堂課呢 是要跟大家講 你在訓練類神經網路的時候 會用到的各種小技巧 那在上一堂課裡面 我們已經講過機器學習呢 就是 3 個步驟 第 1 個步驟是 你要定你的 Loss Function 然後呢 你要決定呢 你的函式的選擇範圍 然後第 3 步就是 選一個最好的函式 那你做完這 3 步 訓練出一個模型之後 你不能馬上做測試 你還要記得做驗證 做 Validation 那如果 Validation 結果夠好 你才會進入測試的環節 不然你就會回到之前 學習的 3 個步驟 去看看是哪邊出了問題 那我們上週呢 也講了深度學習的基本觀念 告訴大家說 其實所謂的深度學習 所謂的類神經網路 就是畫一個比較大的函式選擇範圍 還有機會包含更多不同的函式 那這是我們上週講的東西 那今天啊 要介紹一些 訓練類神經網路的時候 常常使用到的技巧 那因為今天要講的東西非常多 那因為時間有限的關係 也許很多技術 就只會用比較流水帳的方式帶過 那會告訴你說 你可以在過去的影片的哪些地方 找到相關的資料 那如果你過去 沒有訓練類神經網路的經驗的話 那也許今天這一堂課對你來說 會是比較吃力的 我們會講到非常多不同的技術 那如果你一下子沒有辦法跟上的話 也沒有關係 這一堂課的重點是什麼呢 這一堂課的重點是要告訴你說 每次啊 當你聽到一個 有關訓練類神經網路的方法 或者是技巧的時候 你要問自己 這一個問題 你要填下面這個表格 這一個方法 首先它是 改了機器學習中的 3 個步驟的哪一個 它是一個新的 Loss Function 嗎 它是不同的類神經網路架構 也就是劃定了不同的選擇範圍嗎 還是它是一個更好的 更有效的 搜尋 Loss 最低的函數的方式 所以現在你要先問自己 這一個方法 它改了哪一個步驟 然後接下來你要問自己 這個方法 到底帶來什麼好處 很多時候有人在介紹一個 訓練類神經網路的新方法的時候 他就跟你說 這個方法好 但所謂的好 有很多種不同面向的意思 這個是大家在看到一個 新的方法的時候 你需要思考 它到底帶來什麼樣的好處的 那今天在等一下的課程裡面 這一些方法所帶來的好處 大概可以分成 2 個方向 一個方向是 這些方法 可以讓我們更好的做 Optimization 所謂更好做 Optimization 的意思是說 我們可以找到 更低的 Training Loss 那另外有一些方法呢 它是給我們更好的 Generalization 它可以讓 Training 的 Loss 跟 Validation 的 Loss 更接近 它可以避免 Overfitting 這個事情的發生 所以每一個方法 他所 Target 的目標 他所想要解決的問題是不一樣的 所以今天當有人跟你介紹一個 訓練類神經網路的技巧的時候 你要問自己 這個方法它帶來的好處 到底是什麼 那對於不同的狀況 你要選擇不同的方法來應對 舉例來說 假設你今天觀察到的是 我先訓練了一個 Linear Model 然後發現 Linear Model 的 Loss 可以下降的蠻低的 發現 Linear Model 的 Loss 可以降的蠻低的 接下來呢 你想要訓練一個類神經網路 但訓練完發現以後 它的 Loss 降的沒有 Linear Model 那麼低 有一些人沒有想得很清楚 他只要看到 Deep Learning 結果不好 他就會說 就告訴你是 Overfitting Deep Network 就是 Overfitting 那都是胡亂講話 如果今天的狀況是 這個 Deep Learning 它的 Training Loss 都降不下去 這個不叫做 Overfitting 這個是在 Optimization 的時候就出了問題 今天一個 Deep Network 它的參數是比 Linear Model 多的 所以 Linear Model 能做到的事情 它也能做到 在 Linear Model 畫出來的範圍內 能夠選的韓式 Deep Learning 也可以選 所以照理說它的 Loss 應該要跟 Linear Model 至少一樣低 那應該可以更低 但如果你發現你 Deep Network Train 下去 你沒辦法跟 Linear Model 在 Training 的 Loss 上一樣低 那是代表你的 Optimization 有問題 所以這時候你會選擇一個 可以強化 Optimization 的技巧 但是如果是在另外一個狀況 另外一個狀況是 你發現你 Training 的 Loss 已經降得夠低了 但 Validation Loss 沒有辦法跟著下降 那這個才是 Overfitting 這個時候你才會選擇一個更好的 Generalization 的方法 所以你要注意一個方法 它到底是為了強化 Optimization 還是強化 Generalization 設計的 那在不同的狀況 你會需要選擇不同的方法來應對 那如果你可以了解這件事情 那今天上這堂課 你就沒有浪費時間了 好 那接下來呢 就是流水帳的介紹一大堆 跟訓練類神經網路有關的方法 那我們在介紹這些方法的時候 我們會以它是改了哪一個步驟來分類 然後告訴你說這個方法 是為了 Optimization 還是 Generalization 所設計的 那今天講的順序呢 我們會把步驟 1、2、3 反過來 先從步驟 3 開始講 我們來看有哪些技術改了 Optimization 那我們先來複習一下 Optimization 是怎麼做的 我們這邊通常用的就是 最簡單的 Gradient Descent 這個是我們上週有跟大家介紹的方法 那 Vanilla 這個字啊 它就是簡單的意思 它原意是香草 就是香草冰淇淋的那個香草 那放在 Gradient Descent 前面 代表說這是一個 沒有加任何特別口味的 Gradient Descent 是一個最原版最簡單的 Gradient Descent 好 那我們知道解 Optimization 的這個問題呢 就是你已經有一個 Loss 你已經訂出了選擇 Theta 的範圍 你要找一個 Theta 它可以讓大 L 的數值最小 那 Gradient Descent 怎麼做的呢 你先隨便從某一組參數叫做 Theta0 開始 然後呢你就計算 在 Theta0 這個位置 你的參數 Theta 對大 L Loss 的 Gradient 這邊寫做 G0 用 G0 乘上 Learning Rate 去對 Theta0 做更新 然後同樣的步驟就不斷的反覆 在 Theta1 這個位置 再算出新的 Gradient G1 用 G1 對 Theta1 更新得到 Theta2 在 Theta2 的地方算出 Gradient G2 然後用 G2 更新 Theta2 得到 Theta3 就反覆這個步驟 直到收斂 直到 Gradient 算出來非常小 你沒辦法再更新參數為止 那等一下呢會介紹一系列的 Optimizer 什麼是 Optimizer 呢 最原版的 Gradient Descent 你在決定你要 Update 參數的方向的時候 你只根據當下算出來的 Gradient 來決定方向 如果當下算出來 Gradient 是 0 你就不知道要往哪裡走 那有一系列的技術叫做 Optimizer 這一系列 Optimizer 他們是我們不根據現有的 Gradient 來決定方向 而是根據過去所有算出來的 Gradient 從 G0 一直到 Gt 這些步驟裡面 你會算出很多的 Gradient 把所有的 Gradient 綜合起來 一起來決定 接下來要走哪一個方向 那等一下投影片會再細講說 這些 Optimizer 是什麼 根據前面的 Gradient 來估測出接下來應該要走哪一個方向的 那今天呢 你在訓練類神經網路的時候 在那個 Deep Learning 的套件 比如說 PyTorch 裡面 那這些 Optimizer 其實就是一個英文單字 你知道記得在你的程式碼裡面 在合適的地方 選擇你要用哪個 Optimizer 然後就結束了 不過雖然它的實作是如此的簡單 但是我們這堂課來看一下 這些 Optimizer 背後的原理是什麼 那為什麼我們需要 Optimizer 呢 一個理由就是 這個 Learning Rate 實在是非常的難調 那我們上週講過說 如果你的 Learning Rate 設太大 比如說我們在上週的例子裡面 把 Learning Rate 設 0.001 你以為已經夠小了 其實不是 你從這邊開始第一步 就飛到山谷的對岸 再下一步就離開地圖 永遠都回不來了 所以看起來設 0.001 實在是有点太大了 那改成 0.0001 夠小了吧 但這個 Learning Rate 實在是太小了 這個 Learning Rate 實在是太小了 我們從這個地方開始 就算我們已經 Update 了 100 次的參數 還才剛剛走到谷底而已 所以 Learning Rate 太大不好 太小也不好 但你調來調去 就是沒辦法調出一個合適的數值 為什麼在這個例子裡面 很難調出一個合適的數值呢 因為在這個例子裡面 你在橫的方向走 跟縱的方向走 沿著 X 軸的方向走 跟沿著 Y 軸的方向走 這個地勢的變化非常的不一樣 而如果你沿著 X 軸的方向走 這個地勢的變化非常快 也就是 Gradient 都非常的大 所以這個時候 你 Learning Rate 就要設小一點 你要走小步一點 才不會一不小心就跨到山谷的對岸 那如果在這個 Y 軸的方向上 那你 Learning Rate 就需要設大一點 因為在這個 Y 軸的方向上 你會發現你每踏出一步 幾乎都沒什麼變化 所以你需要踏大步一點 才能夠走得夠快 所以不同的參數 應該要有不同的 Learning Rate 那在之前最簡單的 Gradient Descent 裡面 所有的參數都是套用 一模一樣的 Learning Rate 顯然這不是一個好方法 我們應該要因材施教 每一個參數 都應該要給它不同的 Learning Rate 但怎麼估算 每一個參數的 Learning Rate 呢 那在這個圖上我們說 如果某一個方向 Gradient 通常看起來很大 那 Learning Rate 就小一點 某一個方向 Gradient 看起來 通常很小 那 Learning Rate 就大一點 今天在這個圖上 你開了上帝視角 所以可以一眼看出說 這個 X 軸的方向 就是 Gradient 大 Y 軸的方向 就是 Gradient 小 但是假設在沒有開上帝視角的情況下 那我們要怎麼知道 哪一個方向的 Gradient 大 哪一個方向的 Gradient 小呢 那這個我們就可以從 過去的 Gradient 的大小 來決定我們的 Learning Rate 我們可以根據 過去 Gradient 的大小 来猜測說 未來在同一個方向上 它的 Gradient 可能的大小 然後你就可以更合適的 決定你的 Learning Rate 举例来說 假設我們現在 已經算出了 4 個 Gradient 從 G0 G1 G2 到 G3 那從這 4 個 Gradient 裡面 就可以看出說 第 1 個維度 它通常數值都比較大 它的數值都是 3 位數 所以顯然在 X 軸這個方向 在第 1 個維度這個方向 Gradient 算出來 可能會是比較大的 所以我們就給它 比較小的 Learning Rate 反之在第 2 個維度上 通常算出來 Gradient 都只有 0. 幾 是比較小的 那我們可以預期 未來可能也是這麼小 所以我們就給它 比較大的 Learning Rate 它 Gradient 比較小 所以給它比較大的 Learning Rate 這個方法呢 就叫做 Adagrad 那 Adagrad 這個方法的運作方式 如果寫成 數學式的話 是這個樣子的 首先一樣呢 在 Theta0 這個位置 把 Gradient G0 算出來 然後接下來呢 我們會每一個 Dimension 分開操作 因為我們現在開始 每一個 Dimension 會給它不同的 Learning Rate 所以我們每一個 Dimension 分開操作 但在投影片上 我們只列出 這個參數的第 i 個維度 我們做了什麼樣的操作 但你可以想像說 其他維度 我們都做了 一模一樣的操作 好那在 這個參數的第 i 個維度 我們做的事情就是 把到目前為止 所有算出來的 Gradient 雖然我們只算出 1 個啦 也就是 G0 把到目前為止 所有算出來的 Gradient 第 i 個 Dimension 的數值拿出來 取平方 再開根號 我們把這一項呢 叫做 Sigma 上標 0 下標 i 下標 i 代表說它是 第 i 個維度的東西 上標 0 代表現在是 在 第 1 個 Iteration 好那 平方再開根號 其實就是取絕對值啦 這個 等號左右兩邊的意思是 一模一樣的 好那算出這個 Sigma 之後呢 算出這個 Sigma 之後呢 除掉這個 Sigma 所以如果這個 Sigma 算出來的值 越大 那除完之後 整體的 Learning Rate 就越小 原來你沒有 Sigma 這一項 只有 eta 這一項代表你的 Learning Rate 它乘在 Gradient 前面 現在你的 Learning Rate 呢 會除上 Sigma 這一項 Sigma 這一項越大 Learning Rate 其實就越小 Sigma 這一項越小 那整體的 Learning Rate 就越大 這跟我們剛才講的 Learning Rate 的調整方式是一樣的 好那 這邊呢 是第 1 次 Update 參數的時候 好接下來呢 你把參數 Update 成 Theta 1 以後 你會再算出新的 Gradient 叫 G1 那接下來 你在 計算這個 Sigma 的時候 你就會考慮 G0 跟 G1 現在所有算出來的 Gradient 有 G0 有 G1 把 G0 跟 G1 它們在第 i 維度的數值 拿出來取平方 然後 再開根號 就當作 Sigma 有了這個 Sigma 以後 你就會把 這個 Learning Rate 除掉這項 Sigma 那每一個 i 啊 每一個 Dimension 除了 Sigma 都會是不一樣的 就等同於 每一個 Dimension 都有了不同的 Learning Rate 講到這邊有人可能會問說 那 這個平方啊 這邊有 2 項的平方 我們這邊是直接加起來沒取平均 需不需要取平均除以 2 呢 那就告訴你 你可以這麼做 也可以不這麼做 那這個 取決於看你高興這樣 那 這麼做的理由也許很明顯 你本來就應該取平均嘛 那不這麼做的理由 其實也是有道理的 為什麼這邊 不直接取平均呢 如果不直接取平均 那這個 Sigma 不是算出來就會 越來越大越來越大嗎 因為你不斷的加上 那個平方項嘛 所以 Sigma 算出來就會越來越大 但會不會 Sigma 算出來越來越大 其實 就是我們要的 因為我們希望在學習的過程中 Learning Rate 其實應該要隨著時間 逐漸的縮小 所以你其實期待 Sigma 它是越來越大的 而 不做平均這件事情 正好可以讓這個 Sigma 越來越大 也就是讓 Learning Rate 整體越來越小 所以你會發現很多人在實做 Adagrad 的時候 其實是 不加平均這一項的 好那這個呢 是算出 G1 以後 怎麼 Update 參數 那你就反覆同樣的步驟 接下來在 Theta2 的地方 再算出 G2 然後接下來把 G0 G1 G2 到目前為止 所有的梯度 在第 i 個維度的地方 平方 開根號得到 Sigma 2 再用 Sigma 2 呢 去除你的 Learning Rate 用 Sigma 2 呢 來改變 Learning Rate 讓你每一個 Dimension 都有不同的 Learning Rate 那這個步驟就反覆一直下去 到你算出 Gt 的時候 你會把 G0 到 Gt 全部第 i 維的數值 全部取平方 加起來再 開根號得到 Sigma 上標 T 下標 i 再用 Sigma 上標 T 下標 i 來改變你的 Learning Rate 這個技術 就叫做 Adagrad 那講到這邊呢 我其實準備了範例程式 可以跟大家展示一下 像 Adagrad 這種 Optimizer 它可以發揮什麼樣的作用 好，那我們現在要做的事情 就是拿 這個 2021 機器學習的資料來進行訓練 所以這邊呢，做的訓練呢 跟上週做的訓練基本上 是非常非常類似的 我們一樣定義了一個 MSE 的 loss 一樣定義了一個 linear 的 model 好，那上週呢 我們就有跟大家講說 你看， 我用一般的 gradient descent 我 learning rate 設 0.0001 epoch 設 100 然後呢 我們來訓練一下，你會發現說 這個訓練的速度太慢了 走了 100 步，從這裡才走到這裡 但是你 learning rate 調大一點 不行，調大一點 結果就爛掉了。如果你調 0.001 那你看到結果就是這個樣子 這個突然數值變得好大 然後這個 參數一 update 以後就飛走了，就再也回不來了，不知道在做什麼 好，那我們現在呢 就試著來實作 Adagrad 吧 這邊要做的事情是 實作了一個 叫做 Adagrad 的函式 那這個函式呢 就會幫我們使用 Adagrad 的 optimizer 來 update 參數 所以現在啊，我們先看一下我們會在什麼地方 呼叫 Adagrad 的 optimizer 其實這邊多數的內容呢 都跟 上週的內容是一模一樣的 唯一修改的地方 只有這一行 使用 Adagrad 更新參數 之前呢，我們是算出 這個對 w 跟對 b 的 gradient 就結束了。在之前課程裡面，算出兩個 gradient 之後 你就可以直接乘上 learning rate，直接去 update 參數 但我們現在 不直接 update 參數 我們要拿 這個 w 跟 b w 跟 b 的 gradient 來做一些事情 做完以後 才 update 參數 所以這個叫做 Adagrad 的函式裡面呢 它需要知道現在的參數 w 跟 b 它也需要知道 w 跟 b 的 gradient 然後呢 它也需要知道 w 跟 b 的 gradient 的 square 也就是 gradient 的平方，我們會把這個 gradient 的平方記錄下來 這個是為了更新參數的時候 做 Adagrad 的時候需要用到的 然後 learning rate 也是它的輸入之一 然後呢 這邊這個 Adagrad 它的運作就是 首先呢 根據輸入的這個 gradient、gradient w 跟 gradient b 我們會取它的 平方 然後呢 去更新 這個 w 跟 b 的 gradient 的 平方的累積的數值 就我們會一直去計算 那一項 sigma 這個... 這項呢，其實就是剛才投影片裡面的 sigma 的 平方，我們會不斷的 記錄 gradient 的平方的累積的數值 然後接下來呢 原來呢 我們只要把 learning rate 乘上 gradient 就結束了 但在 Adagrad 裡面 你要把這一項 除掉 sigma 那這邊的 sigma 呢 就是寫作 gradient w 的 square 然後呢，前面呢 再把它開根號，就是我們投影片裡面的 sigma 所以其實 Adagrad 就是這麼簡單 你要去記錄 gradient 的 square 然後呢 用 gradient 的 square 呢 來對 learning rate 進行相除 那這樣你就可以有 adaptive 的 learning rate 對每一個參數都不一樣的 learning rate 然後你再把新的參數 w 跟 b 還有已經累積的這個 w 跟 b 的 gradient 的 square 呢 把它傳回去 然後這樣子才能夠在之後的運算中 繼續去計算 w 跟 b 的 gradient 的 square 好，總之我們來看看 Adagrad 可以發揮什麼樣的效果吧 我們這邊 epoch 設 100 步，learning rate 設 0.0... 0.0... 0.1，跟剛才一般 gradient descent 是一樣的 試一下 看看結果怎樣 哎呀 聞風不動 這個 它從這邊開始 然後因為它每次 update 的距離 非常非常的小 所以 update 了 100 步以後 還幾乎像在原地踏步一樣 為什麼會這樣呢？ 因為當你使用這個 Adagrad 的時候啊 其實你是可以把 learning rate 設得更大的 因為我們現在 會自動調控每一個參數 應該有的 learning rate 所以今天就算是某一個參數你 learning rate 不小心設太大了，也沒有關係 很快就會自動被調回來 所以在 Adagrad 裡面呢 你反而應該要設一個比較大的 learning rate 比如說直接設成 1 然後看看會怎麼樣 你看 我設成 1 以後呢 雖然在前面幾步 它的更新有點不穩定 有一點鋸齒狀的路線 但是很快呢 模型的更新 就走到某一個正軌上面 那你看雖然只有 100 步 但它可以更新到這個位置 比剛才 用一般的 gradient descent 還好很多 就在一般 gradient descent，你不管調什麼樣的 learning rate，在 100 步之內 你都是不可能走到這個地方的 但 Adagrad 給每一個 參數不同的 learning rate 那在 100 步之內 你可以走得更遠 這個就是 Adagrad Adagrad 它是一個非常簡單的方法 但是它做了一個非常強的假設 在它的假設裡面 是假設說 每一個方向 我們可以從它的 gradient 大小 從它過去的 gradient 大小 去估算出 未來的 gradient 應該有多大 但是 gradient 這個東西 它是 變化莫測的 對同一個方向而言 可能有時候 gradient 會很大 有時候 gradient 會很小 比如說以下面這一個 loss 的 surface 為例 那這邊就是特意創造出了一個 比較扭曲的 loss surface 它是一個 香蕉的形狀 這是一個香蕉的形狀 最低一點呢 大概在這個地方 那如果看 橫軸 如果看 X 軸的方向 如果今天是在這個地方 X 軸的方向 它的 gradient 是蠻大的 所以按照 Adagrad 的想法 你應該把 learning rate 設小一點 但是 走到這個地方的時候 你會發現 這邊的 learning rate 反而 變得很小 這邊的 gradient 反而變得很小 所以這個地方 你其實需要 比較大的 learning rate 所以對同一個方向而言 有時候 我們需要比較大的 learning rate 有時候我們需要 比較小的 learning rate 我們應該讓 learning rate 的大小 可以即時更新 但是 Adagrad 這個方法 對於 learning rate 的更新 是非常緩慢的 我們來看看如果是 Adagrad 它會有什麼樣的問題 如果今天啊 我們看這一連串 gradient 的 第二個維度 一開始 它的 gradient 很小 但是到了後來 它的 gradient 突然 變得很大 那對於 Adagrad 而言 它在估算某一個維度的 gradient 大小的時候 它是把全部的平方 不斷 累加起來 因為它會把所有的平方 不斷累加起來 所以很久以前的 gradient 跟很未來、很最近的 gradient 它对最終算出來的 sigma 影響 都是差不多的 所以你會發現 當你的 gradient 已經變大的時候 Adagrad 這個方法 它的 sigma 沒有辦法 即時的做調整 所以後來有一個 Adagrad 的進階版 叫 RMSprop RMSprop 它這個方法呢 是給 比較最近算出來的 gradient 比較大的影響力 它的式子是這樣子的 它在第一步呢 跟 Adagrad 是一模一樣的 所以我們就 不管它的第一步 那我們來看看 從第二步開始 從 seta 1 我們算出 gradient G1 然後呢 在 Adagrad 裡面 我們就是把所有 gradient 算出來的數值的 平方直接累加 但在 RMSprop 裡面 不是直接累加 它的做法是這樣 它先把 前一個步驟的 sigma 取平方 乘上 alpha 再加上現在的 gradient 取平方 乘上 1-alpha 那這個 alpha 就是為了來調控 過去的 gradient 平方 跟現在的 gradient 平方 它們之間的權重 那這個 alpha 就反映了 現在的 gradient 有多重要 我們要多在意歷史資訊 我們要多在意 最近的資訊 那這個 alpha 呢 如果你設的 越小 就代表說歷史資訊越不重要 最近的資訊越重要 反之 alpha 設大 就代表歷史的資訊的影響力呢 是比較大的 那跟 Adagrad 一樣 你一樣算出一個 sigma 你一樣用這個 sigma 来達到讓每一個參數 有不同的 learning rate 的效果 好，那 這是 G1 然後接下來你算出 G2 算出 G2 以後 你不是把過去所有 gradient 的數值 通通平方累加起來 而是把上一部的 sigma 拿來 取平方乘 alpha 再加上 現在這一部 G2 它的數值 取平方乘上 1-alpha 然後這個 alpha 就 操控了過去的歷史資訊有多重要 現在新加入的資訊有多重要 算出 sigma 2 這樣 sigma 2 去除 learning rate 那這個步驟 就反覆一直下去 到你算出 Gt 的時候 你就把 sigma 2-1 取平方乘 alpha 加上 1-alpha 乘上 Gt 的平方得到 sigma 2 再用 sigma 2 去除 learning rate 好，所以這個 就是 RMSprop 它是 Adagrad 的一個 變形，但是它是一個蠻有用的變形 那接下來我們再來看看 RMSprop 的效果吧 剛才我們看那個 Adagrad 啊 在機器學習 2021 的訓練資料上 其實是蠻能夠妥善運作的 所以如果你要看出 Adagrad 跟 RMSprop 的差異的話 你需要 更有挑戰性的 Loss Surface 所以我們這邊不再用機器學習的資料 而是換了一個 Loss Surface 這個 Loss Surface 呢 它叫做 Rosenbrock function 或者是很多人就叫它香蕉函數 它就剛才在投影片裡面我們看到的 香蕉形狀的那個 Loss Function 那這邊呢 我們先把這個 Loss Surface 把它畫出來 它就長這個樣子 就是有一個香蕉 一個笑臉的形狀 最低點呢 是在這個 紅色星星的位置上 那這個 Loss Function 啊 这个 Aero Surface 它背後並沒有真正的對應的資料 它就是一個創造出來的 Aero Surface 那但是，雖然它是一個創造出來的 Aero Surface 我們還是可以計算 W 跟 B 這兩個參數 對這一個 Loss Surface、這個 Aero Surface 的 gradient 所以我們就先把 這一個香蕉的函數 它的樣子定義好 那把這個香蕉的函數 在每一個位置 對 W 跟 B 的 gradient 也通通都先計算好 定一個 function 你只要告訴我 W 跟 B 的數值 它就會回傳 W 跟 B 的 gradient 接下來 我們就來看看 我們有沒有辦法成功的 optimize 这个香蕉函數 在香蕉函數上找到最低點 先嘗試用 Adagrad 來做這個香蕉函數吧 那這邊 Adagrad 的 optimizer 跟剛才做的東西是一模一樣的 跟剛才用在 機器學習的資料上做的事情 是一模一樣的 然後呢 這邊 唯一不同的地方就是 剛才你需要自己 真的去計算 gradient 那現在 gradient 就是 call 一個函式 你只要 call 一個函式叫做 gradient Rosenbrock 告訴它說現在的參數的數值多少 它就傳 gradient 給你 然後呢 我們把這些 gradient 還有 gradient 的平方 通通傳給 Adagrad 的 optimizer 然後就可以進行 optimization 我們來看看 結果怎麼樣 這是我們做出來的結果 Adagrad 從這個地方開始 然後呢 往右上角大力的踏出一步以後 又走回來 然後呢，它就調出了合適的 learning rate 然後一路走下來 但走到這邊 顯然 越走發現 learning rate 需要 更大的 learning rate 但 Adagrad 沒有即時反應 所以這邊我們 epoch 設 1000 步 但是就算是 epoch 做了 1000 步 update 了 1000 次參數 這個 Adagrad 只走到這邊 離最終的終點 其實還有一段顯著的差距 那接下來我們就來試一下 RMSprop RMSprop 跟 Adagrad 唯一的差別只有 它不是把過去的 gradient 的平方不斷的累積 而是會把 過去 gradient 的平方 乘上一個 alpha 加上 新的 gradient 的平方 乘上 1-alpha 這就是唯一的差別 其他地方的實作 通通都是 一模一樣的 然後呢 我們就真的來訓練一下吧 好，來看喔 同樣是 1000 個 epoch 其實 RMSprop 是可以 走得比較遠的 可能是因為 RMSprop 知道在這個... 在這些地方呢，它的 learning rate 會比較快的更新成比較大的 learning rate 那你就可以看到說 RMSprop 在同樣的步數之下 它是可以走得比 Adagrad 更接近 最終的結果的 那這邊 RMSprop 呢，其實也是需要調一下 learning rate 的啦 那我這邊的是設 0.01 我調了一下 learning rate 發現說 如果你打 learning rate 調得更大一點的話 其實你的訓練就會開始不穩定了 所以 因為 因為 RMSprop 呢 它會比較快的 改變那個 sigma、比較快的更新 learning rate 所以它其實有時候訓練沒有那麼穩定 所以你 learning rate 設大一點的話 它就會開始 亂飛，如果這個 learning rate 設 0.1，我们來看一下會怎樣 就開始震盪的很厲害 如果我 設 1 的話呢 這樣就跟 Adagrad 一樣大了 喔，就發瘋了，沒辦法弄、沒辦法處理 所以這個 RMSprop，learning rate 沒辦法設得像 Adagrad 那麼大 好 那我們剛才講了設定 Learning Rate 的問題 但是 Optimization Gradient Descent 可以讓你停下來 舉例來說 不是只有 Learning Rate 要調的問題 還有很多滿坑滿谷的問題 到處都是坑 這個問題是什麼呢 還有一個問題是 Optimization 如果用 gradient descent 的話 在 gradient 很小的時候 Optimization 就會停止了 最惡名昭彰的問題就是 當你訓練的時候 走到 local minima 那個地方 gradient 算出來是 0 你的訓練就停止了 很多人都誤以為你的 optimization 失敗 是因為走到了 local minima 但是這件事情不一定是真的 在 optimization 的過程中 有太多的坑 可以讓你在走到 local minima 之前 你就已經停止了你的訓練 其實多數的訓練 可能都不是停在 local minima 那這個是有辦法驗證的 你可以去計算某一個參數 它是不是 local minima 這是可以計算的 你如果真的去套用那些計算 你會發現多數時候你運算停止了 並不是停在 local minima 有太多其他的坑 可以讓你停下來 舉例來說 假設你走到 saddle point saddle point 就是 gradient 是 0 但是不是 local minima 的地方 你的訓練也會停止 或有時候你走到一個地方 其實 gradient 也不是 exactly 正好是 0 它只是很小 但是你沒有耐心再繼續訓練下去了 所以你的訓練就停止了 所以 optimization 在 gradient 很小的時候 會有問題 但是你想看在現實的生活中 如果在真實的物理世界 這是一個真實的山坡 我們真的從這個地方 開始放一顆球讓它滾下來 它可能並不會在平地的地方就停下來 它不會在 gradient 為 0 的地方停下來 因為一個物體在運動的時候 它有動量 所以移動中的東西 除非有外力介入 不然它並不會輕易的就停下來 所以也許我們可以把動量 它英文是 momentum 這件事情 加到 optimization 裡面 那這樣子當你在 update 參數的時候 當走到 gradient 很小的地方 你也不用擔心 因為這個參數是一路從這個地方滾下來的 它也許有保有由左而右的動量 所以你仍然能繼續 update 參數 走到 saddle point 也不用擔心 雖然當下這個位置 gradient 是 0 但之前從斜坡上滑下來的時候 已經累積了很多動量 所以還可以繼續再往右走 甚至走到 local minimum 也不一定要害怕 如果你有足夠的動量的話 也許還能繼續往右走 如果你可以翻過這個小丘陵 也許就可以走到 global minimum 的地方 那這個動量在 gradient descent 裡面 要怎麼實作呢 這個 momentum 的方法 它的實作是這個樣子的 你等一下發現它的式子 跟 Adagrad 有一點像 但它們有微妙的差異 而這個微妙的差異導致 它們是非常不一樣的想法 所以 momentum 做的事情 就是把過去算出來的 gradient 通通都記錄加總起來 所以在第一個位置我們只算出 g0 然後我們有一項叫做 m m 就是把過去的 gradient 都加總起來的意思 m 等於 g0 然後再用 m 來 update 參數 那注意一下 我們現在在 update 的式子裡面 我們已經不再出現 gradient 了 我們不再按照 gradient 的方向來 update 參數 而是按照 m 的方向來 update 參數 那在第一步裡面 因為 m 跟 g 是一樣的 所以你可能看不出差異 但從第二步開始就會有顯著的差異 我們先算出 g1 現在我們的 m 就等於 g0 加 g1 我們會把過去所有的 gradient 通通都累加起來 當作我們的動量 那我們 update 參數的時候 是根據動量的方向來 update 而不是根據 gradient 的方向來 update 這個步驟就反覆繼續下去 我們把 g0 加上 g1 加上 g2 得到 m2 然後我們用 m2 來 update 參數 那當我們算出 gt 的時候 現在已經累積一大堆 gradient g0 g1 g2 到 gt 通通加起來 得到 mt 根據 mt 的方向來 update 參數 那我們也可以把 RMSprop 的概念 加到 momentum 裡面 就剛才我們看到 如果是最簡單的 momentum 你就是把所有的 gradient 通通都加起來 就是最簡單的 momentum 但是我們應該把 RMSprop 的概念 也加到 momentum 裡面 讓比較近的 gradient 有比較大的影響 比較遠的 比較過去的歷史紀錄 有比較小的影響 那等一下你可能會發現說 我寫的這個 momentum 的式子 跟最經典的 原始的 momentum 的式子 不太一樣 確實是如此 那我之所以寫了一個不一樣的式子 是因為這個寫法 等一下可以讓 momentum 跟 RMSprop 加起來 直接就變成 你今日真正會用的 optimizer 也就是 Adam 所以我這邊寫的 跟原始的 momentum 論文是略有不同的 那本來這種 optimization 方法 就可以有很多變化 所以我覺得這樣寫也是沒有問題的 好 那我們怎麼做呢 我們之前是把所有的 gradient 加起來 但我們現在不把所有的 gradient 加起來 我們就是把我們已經累加出來的 momentum 乘上 beta 再加上 1 - beta 乘上新的 gradient 得到新的累加後的 gradient 也就是 momentum 那這個 beta 就設一個 0 到 1 之間的值 它來控制歷史資訊到底有多重要 現在的資訊有多重要 好 算出這個 m1 之後 你就可以用 m1 來 update 參數 這個步驟就反覆進行 接下來算出 g2 那我們把 m1 乘上 beta 加上 1 - beta 乘上 g2 得到 m2 再用 m2 來 update 參數 這個步驟就反覆進行 你會不斷的存上一個 step 算出來的動量 算出來的 m 那每一次你都會把 m 乘上 beta 加上新的 gradient 然後去更新 m 這個項 然後根據 m 這一項更新的結果 來 update 你的參數 這個就是 momentum 好 那我們來看一下 momentum 能發揮什麼樣的作用吧 那為了要看出 momentum 發揮的作用 所以呢 我們再找另外一個也很困難的 這個 loss surface 來做這個 momentum 那這個新的 loss surface 它最大的難點就在於 它有很多的 local minima 在這個圖上 越偏深藍的代表值越小 越偏黃的代表它的值越大 那在這個 loss surface 上面 有很多很多很多的洞 那每一個洞都是一個 local minima 那 global minima 是在整張圖的中間 所以你如果從四個角落走進來 四個角落走進來 比如說你放在最右下角這個高點 一路滑下來 你可能走到中間的某一個 local minima 就會卡住了 就會沒有辦法再往前走了 所以今天這個方法 這個 loss surface 是特別設計來 看看說有沒有什麼 optimization 的方法 可以突破 local minima 帶來的問題 但這也是一個人工硬創造出來的 error surface 它並不是背後對應有真實的資料 那我們這邊就是訂立了一個函式 我們定義了這個 Rastrigin 的函式 那這個函式呢 就是我們這邊看到的這個 loss surface 然後呢 我們也把它的 gradient 算好 所以你只要給我 w 跟 b 我就回傳給你在這個位置的 gradient 那接下來呢 我們就先用 RMSprop 來在這個 error surface 上面 做一下 optimization 看看吧 我們來看看 RMSprop 的表現如何 那我們從左上角這個點開始 發現 RMSprop 就一路往右下走 但是走到某一個地方 gradient 是 0 的時候 那就停下來了 就沒有辦法再 update 了 因為 RMSprop 是按照 gradient 的方向來 update 的 它只是用這個 sigma 來控制它的 learning rate 所以如果你 gradient 算出來是 0 那 RMSprop 就算它能夠控制 learning rate 它也無能為力 它還是得停下來 沒有辦法再繼續 update 那我們來看看 momentum 吧 那 momentum 跟剛才的 Adagrad 或 RMSprop 很像 我們都需要記錄一些額外的東西 那剛才記錄的是 square 是平方向 那現在不需要記錄平方向 現在記錄的是 momentum 那 momentum 其實就是過去所算出來 gradient 的加總 所以對 momentum 的 optimizer 而言 現在它每次就是要傳進參數 w 跟 b w 跟 b 的 gradient 還有 w 跟 b 的 momentum 然後在計算 momentum 的時候 就是把之前的 momentum 乘上 beta 加上新的 gradient 乘上 1 - beta 把之前 momentum 乘上 beta 加上新的 gradient 乘上 1 - beta 然後就按照 momentum 的方向來 update 參數 不是再用 gradient 的方向 update 參數 是用 momentum 的方向來 update 參數 好 這個就是 momentum 的 optimizer 總之你就是把剛才呼叫 RMSprop 或 Adagrad 的 optimizer 的地方換成 momentum 的 optimizer 那如果在 PyTorch 裡面 你就是改一串英文 就本來寫 Adagrad 的地方 你就把它改成 RMSprop 或改成 momentum 那你就換了一個 optimizer 了 所以非常的容易 在各種 deep learning framework 裡面 要換 optimizer 都是一件蠻輕鬆的事情 好 那我們就來跑一下 momentum 看看會發生什麼樣的事情吧 這邊看一下我們設的參數 我們的 beta 設 0.995 那看到這邊呢 你可能會發現說這個 beta 也未免太大了吧 這樣新的梯度 它的影響力不是只有 1 - beta 嗎 如果你看剛才那個 RMSprop 的時候 我們也是設一個蠻大的數字 我們設 0.9 你想說我們給歷史資料那麼大的權重嗎 給新的資料那麼小的權重嗎 那我告訴你其實這個權重 對於歷史資料來說並不算大 對於新的資料來說也並不算小 為什麼呢 因為當你 update 的次數夠多的時候 歷史資料裡面其實累積了 非常多的 gradient 那非常多的 gradient 你只分給它 0.995 的權重 然後新的 gradient 它只是一個 gradient 進來 它就得到 0.005 其實相對過去的 gradient 它已經得到了足夠多的重視了 所以在那個累積的 momentum 項裡面 是很多 gradient 的 這些很多的 gradient 他們只分到 0.995 新的 gradient 一個人就已經分到 0.005 所以這樣子做起來 還是算是蠻重視新的 gradient 的 好 那我們就真的來 跑一下 momentum 看看吧 你看跑完之後 這個是我們跑出來的結果 那參數從這個地方開始下滑 但是走到 local minima 的時候 它並沒有停下來 依照它的動量 它繼續再往右下走 它就又翻過了一個山丘 然後又再翻過了一個山丘 那這個是最低點 但它其實也沒有辦法 真的在最低點馬上停下來 因為它是有動量的 所以在最低點 過了最低點以後 它又繼續往右邊走 然後但是右邊有一個山丘擋住了它 所以它就再回來 然後它可能會在 兩個丘陵之間震盪一陣子以後 最後就停在最低點 所以如果你有 momentum 你是有機會 可以避開 gradient 為 0 的時候 所造成的種種問題 好 這個是 momentum 好 那我們再進入下一個方法吧 我們剛才介紹了 RMSprop 也介紹了 momentum 那我們說 momentum 是把過去的 gradient 累加起來 RMSprop 是把過去的 gradient 的 平方累加起來 所以它們兩個都是累加過去的資訊 但一個是直接加起來 一個是把平方加起來 所以兩個意思是不一樣的 當你把過去的資訊全部加起來的時候 你是有考慮正負號 你有考慮方向 那當你把它平方加起來的時候 你就沒考慮正負號 你就沒有考慮方向 所以這兩個 它們累加的東西 雖然看起來這個式子有點像 但它們實際上累加的東西 是非常不同的 那 momentum 會取代 gradient 然後 RMSprop 會來控制你的 learning rate 那這兩件事情 能不能夠同時使用呢 它們當然能夠同時使用 它們同時使用就叫做 Adam 那這個是今天 default 你會使用的一個 optimizer 比如說在助教範例程式裡面 也是用 Adam 當作 default 的 optimizer 那 Adam 這個方法 今天非常廣泛的被使用 那回頭一想 哇 它已經出現了十年了 我記得沒錯的話 它應該是 2015 年的文章 那把這兩個 (Momentum 跟 RMSprop) 結合起來 就是 Adam 那其實這並不是完整的 Adam 如果你仔細去看看原始的 Adam paper 的話 它還有一些其他有的沒的東西 它有一個叫做 bias-corrected 的項 那這邊我們為了簡化我們的說明 告訴你說 momentum 跟 RMSprop 加起來就是 Adam 我們就去掉了 我們就先不提 bias-corrected 那項 好 那我們來看看 Adam 的表現怎麼樣吧 好 那我們現在換回香蕉的函式 然後我們在香蕉的函式上 使用 Adam 來進行訓練 那因為我們現在使用的是 Adam 所以我們就要記兩個東西 我們要記梯度的平方 也需要記累積的動量 那我們今天有新的梯度被算出來之後 它就被乘上 1 - alpha 原來的平方的梯度 就被乘上 alpha 然後加起來 那動量呢 就有新的梯度進來 就把它乘上 1 - beta 加上原來的動量乘上 beta 得到新的動量 然後呢 你今天在更新參數的時候 你就 learning rate 的地方 要除掉 gradient 的 square 然後你要把原來的 gradient 取代成動量 這樣就結束了 那我這邊刻意不實做 bias-corrected 的 term bias-corrected 的 term 我這邊 bias-corrected 的 term 我這邊沒有用 但其實還是 train 得起來就是了 好 所以我們就是要累積平方梯度 累積動量 然後用這兩個一起合起來 更新參數 那我們接下來就來看看 這個 Adam 在香蕉的那個函式上 表現的怎麼樣吧 那這邊 alpha 跟 beta 呢 我們都設 0.9 然後就跑一下香蕉的那個函式 這邊是一個簡化過的 Adam 但它仍然能夠發揮不錯的效果 剛才 RMSprop 呢 沒有辦法走到 loss 最低的地方 但是一樣是 update 參數 1000 次 Adam 可以 它從這個地方出發以後 最終走著走著 它就可以走到整個地圖上面的最低點 那剛才在同樣 update 的次數之下 RMSprop 是沒有辦法做到這件事情的 所以 Adam 呢 是今天一個非常常被使用的 optimizer 好 那還有哪些其他可以改的地方呢 我們剛才加了一個 n 加了一個 $\sigma$ 還有其他可以改的地方 你還可以改這個原來的 learning rate 原來的 learning rate 它是一個定值 原來 learning rate 這項 $\eta$ 是一個定值 但我們可以做 learning rate scheduling 讓你在每次 update 參數的時候 都有不同的 learning rate 所以你可以把放在分子的這項 $\eta$ 換成 $\eta_t$ 代表說每次 update 參數的時候 這個 $\eta$ 的數值都是不一樣的 那這個 learning rate 的 schedule 要怎麼設定呢 這邊其實有千千萬萬種設定的方式 那今天常見的一個設定的方式 尤其是在訓練大模型的時候常用的 就是先增加後減少 就你的這個 learning rate 在某一個 update 的次數之前 它是越來越大的 但到某一個 update 的次數之後 你的 learning rate 就要逐漸減少 那 learning rate 逐漸減少這件事情叫 learning rate decay 那是要讓你的參數的 update 慢慢變小 那最後可以收斂在某一個地方 因為如果你的 learning rate 始終保持很大的話 也許你的參數會在兩個峽谷中間不斷的震盪 那參數始終有很大的變化 訓練始終沒有辦法停下來 所以你會準備一個 learning rate decay 讓你的 learning rate 越來越小 讓你的參數可以慢慢的著陸 最後停在峽谷中的某一個地方 那為什麼前面需要讓 learning rate 越來越大呢 這個就有點難解釋 讓 learning rate 越來越大這件事情叫做 warm up 那這個步驟你可以把它想成是 我們給 optimizer 一個探索地形的機會 因為我們一開始剛進入一個新的地圖 你不知道這個地圖裡面有什麼 所以你設定一個比較大的 learning rate 讓你的參數可以在這個地圖上亂跑 可以跑很多不同的地方 所以它大概知道這個地圖長什麼樣子 它比較容易可以去收集這邊的 $\sigma$ 跟 m 得到更好的 $\sigma$ 跟 m 的估測 所以最終可能可以收斂在比較好的結果 總之這是今天常用的一種 learning rate scheduling 的方式 好那我們來對 optimizer 這個部分 做一個簡單的總結 所以我們會把 learning rate 除上 gradient 的 square 那這一項只考慮 magnitude 我們只考慮大小不考慮正符號 我們會有一項叫做 momentum 我們把 gradient 用 momentum 取代 momentum 就是過去所有 gradient 的加總 它是會考慮方向的 那我們的 learning rate 可以隨時間改變 這個叫 learning rate scheduling 好那講了這麼多 optimization 的方法之後 講了這麼多 optimizer 之後 我們來想想看我們來看這個表格 optimizer 到底做了什麼 好它改了哪個步驟 它改了第三步 它是一個更有效搜尋函數的方法 好再來它帶來了什麼好處 它帶來的好處是 它可以讓你的 optimization 做得更好 它有機會讓你在 training data 上面 找到更低的 training loss 但它通常對於 generalization 是沒有幫助的 所以你會看到很多人跟你講說 它用了 Adam 確實 training loss 下降了 但是 validation 的 loss 可能比用一般的 gradient descent 還要更高 因為本來你在 training set 上 找到比較低的 loss 並不保證你在 validation set 上 可以找到比較低的 loss 因為你有可能會 overfitting 所以當你今天遇到的問題 是 generalization 不好的時候 換 optimizer 可能沒什麼太大的幫助 換 optimization 方法 換 optimizer 會有幫助是 當你發現你的 training loss 壓不下去 但你直覺覺得它應該可以壓得更低的時候 換 optimizer 才會帶來幫助 好但是你可能會覺得說 我們本來就是在改 optimization 的方法 所以可能都是對 optimization 帶來好處 接下來介紹一些方法 這些方法它雖然改的是第三步 改變了 optimization 的步驟 改變了 optimization 裡面發生的一些事情 但是它有可能帶來更好的 generalization 這個方法就是大名鼎鼎的 Dropout Dropout 這個方法它的概念非常的簡單 所以我們就用一頁投影片帶過去就好 Dropout 這個方法就是在訓練的時候 會隨機丟掉一些神經元 在訓練的時候 有一些神經元會莫名其妙就不見了 然後在測試的時候 所有人又都會出現 驗證跟測試的時候 會火力全開用所有的神經元 這個概念就是在訓練的時候 增加一些額外的難度 就好像如果你要練輕功的話 你會在腳上綁很重的重物 然後平常用這個很重的重物來走路 但是在真的測試的時候 把重物拿掉人就會飛起來 这个就是类似的概念 如果你想知道更多跟 Dropout 有關的事情的話 可以看 2016 年的機器學習 那我把它的連結留在這邊給大家參考 那我這邊最想跟大家分享的事情是 每當你看到一個方法的時候 你都要注意這個方法 到底帶來什麼樣的好處 Dropout 帶來的好處是什麼 Dropout 帶來的好處是 更好的 generalization 所以當你使用 Dropout 的時候 你的 training data 跟 validation data 的 loss 可能會更接近 但 Dropout 對於 optimization 是沒有好處的 當你在訓練的時候 隨便丟掉一些神經元 你只會訓練得更差 你不會訓練得更好 所以加上 Dropout 以後 你的 training loss 看起來是會比較高的 它沒有辦法讓你的 training loss 變得更低 所以你要注意 Dropout 選擇的時機 很多人看到這些 deep learning 的方法 他都不知道這個方法到底是帶來什麼好處 聽說有個方法叫 Dropout 所有的 case 我通通要使用 Dropout 我只要一 train 不好 我通通都要使用 Dropout 但是有時候 train 不好 是 optimization 不好 有時候結果不好是 generalization 不好 所以不是所有的狀況 都應該使用 Dropout Dropout 是在 你發現你 optimization 已經做得太好了 然後在 validation set 上 它的 loss 卻降不下來 這個時候你才會使用 Dropout 所以你要注意 使用一個技術的時機 那接下來我們講一下 initialization 我們說在做 gradient descent 的時候 還有一個步驟是 一開始要選擇一個初始的位置 那其實不同的初始位置 也可能會導致非常不同的訓練結果 比如說如果餓狼從這裡下坡的話 他會停在這個 local minima 但如果他在這裡下坡的話 也許他就可以找到 global minima 所以不同的位置 其實會帶我們到不同的訓練結果 那有人可能會想說 那也許可以用 optimizer 來克服 initialization 所帶來的差距 也許我有比較好的 optimizer 就算從這個地方開始 我也有機會走到 global minima 確實是如此沒錯 但是就算你有好的 optimizer 有時候同時加上好的 initialization 還是會造成顯著差異的 我們這邊直接舉一個實際的例子 讓大家來體驗一下 initialization 可能造成的差異 好那我們現在呢 我們現在回到上週的那個預測 講課時長的例子 那把資料換成生成式 AI 導論 2024 我們建構一個 只有一層的類神經網路 來訓練我們的 network 好那現在呢 我們使用 Adam 然後我這邊也有 implement 那個 biased corrected 的技術 省得你覺得說 我沒有讓 Adam 發揮全力 所以這邊我們讓 Adam 發揮全力 去訓練剛才那個 network 看看結果會怎麼樣 這是一個只有一個 hidden layer hidden layer 大小是 100 個 neuron 的 network 我們來訓練看看 會發生什麼樣的事情 其實結果是不錯的 我們跑了 1 萬個 epoch 以後 loss 可以降到 7 點多 那因為每一次初始 我這邊初始就是 隨便隨機 隨便 call 一個 random 的 function 產生一些 random 的數值 當作初始化的參數 那這邊呢 刻意不固定那個隨機種子 所以每次訓練出來 結果都是不一樣的 剛才是 7 點多 是不是 那再訓練一次 剛才 loss 可以到 7 點多 現在再一次 這次比較差 這次運氣比較差 這次 loss 1 萬個 epoch 跑到 9 的地方 再弄一次 哎呀 這一次更慘一點 這一次感覺 loss 上下起伏 非常的不固定 好 那我們來換一下 initialization 所以其他地方 optimizer 都沒有改 我們就是把 initialization 換一下 這是我們剛才的 initialization 的方式 這是我們現在的 initialization 的方式 唯一的差別是什麼 唯一的差別是 現在呢 在 random 出一個數值之後 會再乘上一個額外的項 去改變 random 出來的 scale 那这个 random 出來的 scale 跟什麼有關呢 跟這個 layer 輸入的 dimension 有關 你看這邊 你的 乘上的 scale 呢 是 square 2 除以 input dimension 所以今天如果 input dimension 越大 這邊乘上的 scale 就越小 然後這邊是 square 1 除以 h 就是 hidden layer 的 size 那為什麼會這麼設呢 這個 2 除以 input dimension 是來自於 Kaiming initialization 就何愷明提出來的 某一個神奇的 initialization 的方式 那它真的很好 那你說這邊 為什麼是設 1 不是 2 呢 如果你了解 為什麼 Kaiming initialization 是長這個樣子的話 我會認為這邊應該設 1 而不是 2 總之就是這麼神奇 好然後呢 我們就來訓練一下 看看會不會有不一樣的結果吧 你看 這結果是不是很不一樣 這個剛才 loss 都是 7 啊 8 啊 現在這個 loss 可以降到 1 點多 甚至降到 1 以下 然後每一次結果都是不一樣的啦 所以還可以再試一次 這次是 1 點多 好再試一次 這次是 0 點多 所以你看 就算你有好的 optimizer 如果換一個 也是更好的 learning rate 你還是可以好上加好 還是可以有不一樣的結果的 所以這個告訴你說 initialization 其實很多時候在訓練的時候 也扮演了一些重要性 好那我們剛才講了 initialization 的重要性 那接下來我要告訴你說 其實就算是你有一個 超強的 optimizer 我今天假設你有一個 特厲害的 optimizer 它真的可以找到 global minimum 它真的可以找到 training loss 的 最低點而且不會失手 但就算在這個狀況下 initialization 仍然有可能發揮影響力 為什麼呢 因為有時候你會遇到的狀況是 你在這一個 error surface 上面 有好多位置 它都是最低點 比如說如果你用 MSE loss MSE loss 最小值就是 0 那對一個 neural network 來說 它可能可以找到很多不同的解法 算出來的 MSE loss 通通都是 0 那這個時候好的 initialization 可以讓我們找到 不一樣的最低點 這時候不同的 initialization 會帶我們走到不同的最低點 那期待好的 initialization 可以帶我們走到好的最低點 但什麼叫好的最低點呢 這邊舉一個例子告訴你說 不同特性的 global minimum 還是有可能造成不同的差異的 舉例來說 假設現在這一條黑色的線 是我們的 training loss 那我們的 validation loss 是虛線 那我們假設 training loss 跟 validation loss 中間就是差了一個 shift 我們假設你的 training data 跟 validation data 它們有一定程度的相似度 所以它們的loss surface 畫出來非常的像 但是因為它們有一點點的差異 所以它們中間有一個 shift 當有這個 shift 的時候 如果你今天的 global minimum 是在一個平坦的盆地中 那 training loss 跟 validation loss 算出來可能會很接近 但是如果你今天的 global minimum 是在一個峽谷中 那就糟了 你的 validation loss 一旦跟 training loss 有一點差距 你的 training loss 跟 validation loss 就會有異常大的差異 当今天你的 training loss 跟 validation loss 它的地形有一點點差別的時候 training loss 跟 validation loss 算出來的數值就會有巨大的差異 你就會導致非常嚴重的 overfitting 你就會觀察到非常嚴重的 overfitting 所以就算是 global minimum 也是有好壞之分的 也是有比較好的 global minimum 跟比較差的 global minimum 我們期待如果有一個好的 initialization 也許可以帶我們走到 比較好的 global minimum 但至於實際上要怎麼做 那這是一個尚待研究中的問題 還沒有非常標準的答案 但是今天在 2025 年 有一個通用的大絕招 往往可以發揮作用 這個大絕招叫做 pre-train pre-train 是什麼意思呢 pre-train 的意思是說 假設我們現在有一個任務 有一個我們目標我們要做的任務 比如說分類 我們打算訓練一個模型 這個模型給它一張圖片 它會告訴你圖片裡面是甚麼動物 但是在教模型做分類之前 我們先教它另外一個類似的任務 比如說我們教它給它一張圖片 然後預測出這張圖片 已經被反轉了多少度 這一隻狗他是倒過來的 顯然正常的狗不應該長這個樣子 所以我們希望有一個模型 有一個函式 它可以預測說 它可以輸出說這張圖片 相較於原始圖片 它被轉了 180 度 我們訓練一個 function 這個 function 輸一張圖片 它可以告訴我們說 这个圖片有沒有被旋轉過 它被轉了多少度 訓練出這個 function 之後 你就可以把這個 function 當作是分類的 function 的 initialization 從這個 function 開始去做 optimization 可能會給你更好的結果 那像這一種 你先教模型做一件事 然後期待它在你真正關心的事情上 可以發揮影響力 可以帶給你更好的 initialization 這種事情叫做 pretext task 那這邊預測一張圖片有沒有被旋轉 就是一個 pretext task 那这个 pretext 的意思 如果你直接查字典的話 他是那種藉口的意思 那在這邊 我覺得他的意思隱身就是 這個 pretext task 是我們真正關心的downstream task 我們把真正關心的 task 叫 downstream task 這個 pretext task 是 downstream task 的一個 proxy 是它的一個藉口 那你可以訓練在這個 pretext task 上 讓你最後在downstream task可以做得更好 那要什麼樣的 task 才是一個好的 pretext task 他的第一個條件是 這個 pretext task 要可以輕易收集大量的訓練資料 你在做這個 task 的時候 你得到 label 這件事情 要是非常容易的 就這邊預測圖片旋轉的這個例子而言 因為這個圖片到底被旋轉多少 是人先去做好 再叫機器去預測圖片被旋轉多少 所以這種資料你要製造多少 就可以製造多少 所以 像預測圖片旋轉這件事情 是一個好的 pretext task 但是什麼樣的任務交下去以後 才會對 downstream 任務有幫助呢 這個就很難說了 為什麼請模型學習預測圖片 有沒有被旋轉 接下來就對學習分類有幫助呢 這個就很難說的清楚 那對於不同的 downstream task 你其實有時候也需要不同的pretext 的 task 好那如果你想要知道更多跟 pre-train 有關的事情的話 那其實在機器學習 2022 年 有花了一個半小時左右的時間 講 pre-training 這件事情 那 pre-training 很多時候又叫做 self supervised learning 或者中文翻成自督導式學習 如果你想知道更多有關 pre-training 的事情的話 可以看機器學習 2022 年的錄影 那我們就停在這邊 不講更多跟 pretext task 有關的技術 那這邊最重要的是要告訴你說 那 initialization 假設我們用 pre-train 做 initialization 的話 那這樣的方法帶來了什麼樣的幫助 那 pre-train 神奇的地方是 它對 optimization 跟 generalization 兩者同時都有幫助 那如果你想要看驗證的話 可以看機器學習 2021 的錄影 然後講了一段 Bert 的奇聞異事 告訴你說 pre-train 這種方法 它同時對 optimization 還有 generalization 都有幫助 好所以我們今天學到 pre-train 這個方法 pre-train 這個方法 可以看作是一種 initialization 的方法 所以它可以說是一個更好的 optimization 的方法 因為我們有了更好的 initialization 它帶來什麼好處 它可以同時帶來更好的 optimization 跟 generalization 好那接下來呢我們就要準備進入 尋找函式的三個步驟的第二個步驟 好那我們剛才已經講了很多 跟 optimization 有關的方法 現在呢我們來講第二步 跟選擇函式範圍有關的方法 那這邊通常指的是 改變你的 Network 架構 那我們想要什麼樣的函式範圍呢 籠統來說就是我們希望一個 剛剛好不大不小的函式範圍 這個函式範圍如果畫得太小 可能沒有辦法包含到好的函式 這樣你後面做 optimization 不管你再怎麼努力 都是緣木求魚 你想要大海撈針 但針不在海裡 所以你怎麼撈呢都撈不起來 那如果你怕你的函式集合 沒包到好的函式 也許一個直覺的方法 就是把函式的集合畫一個超級大的 畫一個非常大的函式的集合 包山包海 那這樣你總是能夠 囊括到一些好的函式 但是我們上週講過說 如果函式集合太大 就容易 overfitting 函式集合越大 就越容易 overfitting 所以你期待的事情是 你有一個範圍不大的函式 但是它又要正好包含好的函式 你有一個範圍不大的函式集合 但裡面正好又囊括到好的函式 那這件事通常要怎麼做到呢 這件事情就得憑藉你的 domain knowledge 憑藉你對這個問題的理解 來劃定一個合適的範圍 這個合適的範圍既小 同時又包含有好的函式 那所謂的合適的範圍 如果換成類神經網路的語言的話 就是設計一個好的網路架構 這個架構它不是非常的大 它的範圍沒有太大 但正好又包含我們要的函式 這件事情具體要怎麼做呢 我們這邊就拿 convolutional layer 也就是 convolutional neural network (CNN) 來跟大家舉例 這是一個非常經典的例子 告訴你說你如何設計一個網路架構 它是一個小的函式範圍 但是又正好包含我們需要的函式 那 convolutional neural network 它通常是設計來處理影像的 所以當你用 convolutional neural network 的時候 通常代表說你的輸入是一張圖片 那我們先來講一下 假設你的輸入是一張圖片的話 對類神經網路而言 它實際上看到的是什麼東西 假設這邊有一張貓的圖片 它是一個解析度 1000x1000(口誤) 的圖片 那對類神經網路來說呢 它看到的一張彩色的圖片 其實是三張圖片疊在一起的 那每一張圖片代表一個 channel 你有 RGB 就紅色綠色藍色三個 channel 每一張 channel 裡面 你的圖片是單種顏色的 但是 RGB 三張圖片疊在一起 你就看到了彩色的圖片 然後接下來 你會把這三張圖片裡面的每一個像素 拉直拼起來 變成一個異常長的 vector 那這個 vector 有多長呢 它有 1000x1000x3 這麼長 所以它有 300 萬個 value 在這一個 vector 裡面 那每一個 value 就代表了 某一個顏色在這個位置的強度 這個才是類神經網路真正看到的東西 那這個類神經網路的輸入呢 我們上週有講過 就叫做 feature 好所以現在 如果給你的是一張圖片 你的 feature 的 dimension 真的是大的不可思議 它有 3x1000x1000 那麼大的 dimension 那再來我們來看看 要處理這麼巨大的輸入 我們的類神經網路需要有多少參數呢 常用的類神經網路架構的方法 叫做 multi-layer-percetron 縮寫是 MLP 那 multi-layer-percetron 它裡面 所有的連結是 fully connected fully connected 是什麼意思 fully connected 意思就是說 每一個 neuron 都要連接到所有的 input 每一個 neuron 都會有一個 weight 跟前面每一個 input 相接 所以當你的 feature 是這麼大的時候 是 3x10 的 6 次方的時候 那每一個 neuron 它就有 3x10 的 6 次方的參數 但我們這邊先不要管那個 bias 那一項 b 那一項 所以每一個 neuron 它會有 3x10 的 6 次方那麼多參數 假設我們第一層想要放 1000 個 neuron 那不得了 我們現在光第一層而已 我們的參數量就有 3x10 的 9 次方 也就是 3 billion、30 億個參數 這個跟你今天用的比較小的 language model 已經一樣大了 而且這還只有第一層而已 好所以看起來要處理影像真的是很困難 我們需要一個非常巨大的類神經網路 裡面有非常大量的參數 才能處理影像 但是你回頭過來想一想 真的有必要做 fully connected 嗎 真的有必要每一個 neuron 都去看整張圖片的每一個 pixel 嗎 也許是不需要的 為什麼不需要呢 這邊就需要人類的智慧 人類對這個問題的理解 來重新架構類神經網路的樣子 好那對影像處理而言 這邊的第一個觀察是 假設現在輸入是一張圖片 那我們通常希望第一層 它扮演的是一個 basic detector 的角色 就是它去偵測說 圖片裡面有沒有出現 哪些最 basic 的 component 有沒有出現哪些常見的 component 比如說可能有一個 neuron 它就負責偵測 有沒有鳥嘴出現在圖片裡面 有一個 neuron 分別負責偵測說 有沒有看到眼睛 有一個 neuron 負責偵測說 有沒有看到爪子 然後在下面的 layer 比較深的 layer 它是一個比較 advanced detector 它會綜合前面 basic detector 觀察到的東西全部組合起來 然後判斷它看到的是什麼東西 比如說前面跟鳥嘴 也就是會的 detector 有被 activate 的話 就輸出比較大的數值 然後可能跟爪子有關的 detector 也輸出比較大的數值 那可能就代表說看到了一隻鳥 那現在問題來了 如果第一層的 detector 它的工作就是去檢查一些最 basic 的 pattern 這些 basic 的 pattern 它的大小 往往只是一個非常小的範圍 比如說鳥嘴 你其實只需要看這麼小的範圍 就知道它是鳥嘴了 你其實只要看這個範圍 你就知道它是鳥嘴 你根本不需要看整張圖片 你就知道是鳥嘴 你只要看這個小的範圍 你就可以猜這大概是一隻眼睛 雖然你不知道它是什麼動物的眼睛 你不需要看整張圖片 就可以猜測一個 pattern 是什麼 你其實不需要看整張圖片 就可以偵測一個 pattern 所以對於這些 basic 的 detector 就假設這些 neuron 它的工作 就是 detect 一些比較簡單 比較基礎的 pattern 它其實不需要看整張圖片 它其實只需要看圖片的一部分 也許就能夠偵測這個 detector 了 所以我們這邊第一個 簡化這個 fully connected network 的方法就是 幫每一個神經元 畫定它的手臂範圍 這個手臂範圍叫做 receptive field 那個神經元 就只管 receptive field 裡面發生的事情 它不要管整張圖片發生的事情 所以這個神經元 本來它應該要去看全部的輸入 那現在不要 它就只看人類給它畫好的 receptive field 的範圍就好 那 receptive field 是人類畫的 你可能在圖片的左上角 畫一個範圍 說這個分給藍色的 neuron 右下角畫一個範圍 說這個分給黃色的 neuron 那 receptive field 你也可能會想要讓它重疊 這樣才不會有遺漏 也許你在中間這個地方 畫一個範圍 說綠色的 neuron 負責觀察這個範圍 那同一個範圍之內 你可能會需要多個 neuron 就大家好幾個人好幾個 neuron 可以一起守備一個範圍 所以可以說深藍色淺藍色的 neuron 它都去看同一個 receptive field 這個是人定的 人憑著你對這個問題直覺的理解 去決定說我 receptive field 長什麼樣子 然後每一個 receptive field 要有多少 neuron 來守備它 receptive field 要不要有重疊 這個都是人憑藉著 domain knowledge 來設定的 好那這邊在這個例子裡面 receptive field 大小都是一樣的 這邊都畫 3x3 你可能會問說 那能不能有大有小呢 有些 neuron 負責看大範圍 有些 neuron 是看小範圍 因為可能有一些 pattern 比較大 有些 pattern 比較小 可以那這個完全是看你要不要這麼做設計 你完全可以做這種 multi-scale 的 receptive field 那這邊舉的例子 receptive field 都是正方形的 有什麼理由它都是正方形的 能不能弄長方形的 可以隨著不同的任務 有些任務也許你會覺得 長方形的 receptive field 才是比較適合的 所以這是你自己決定的 那在影像處理上 現在已經有一套比較 standard 的方法 來決定 receptive field 是怎麼分布的 那這個 typical 的 setting 也是你在作業裡面 實際上會用的 setting 是長這樣子的 那你先決定一個非常小的範圍 說 receptive field 大小通常是固定的 那這個大小有一個專有名詞 叫做 kernel size 在這個例子裡面 kernel size 是 3x3 一般在做影像處理的時候 通常就設 3x3 就代表最 basic pattern 其實只出現在 3x3 的範圍就可以偵測了 通常你不會再設更大的 kernel size 然後呢 這邊的 receptive field 是包含所有 channel 的 就你不會只看某一個 channel 如果你只看某一個 channel 比如說只看藍色 你可能沒有辦法分辨它是什麼 pattern 所以這個 receptive field 其實是立體的 你會把三個 channel 同時做考慮 然後再來這些 receptive field 之間 它通常會有一些重疊 你會把第一個 receptive field 左上角這個 receptive field 往右移一點 就當作新的 receptive field 那這個要移多少呢 這是你自己決定的 那移的量叫做 stride 那在這個例子裡面 我們 stride 設 2 代表說我們把最左上角這個 receptive field 移動兩格 那這個就是新的 receptive field 那不同 receptive field 之間可以有點重疊 這樣可以避免說有一些 pattern 沒有被偵測到 然後呢 如果 receptive field 超出範圍怎麼辦 超出範圍的地方就做 padding padding 就是補零的意思 讓這個 receptive field 可以正常的運作 然後呢 你就會把這個 receptive field 掃過整個圖片 就先掃第一排 第一排掃完 然後就 從第二排 你這個 stride 的由上往下移兩格 然後從第二排再開始掃過去 一直掃到最右下角 那整張圖片的每一個角落 都被數個 receptive field 覆蓋 然後你可以確定說這個圖片的每一個角落 都有一些 neuron 負責偵測這個角落 發生什麼事 那每個 receptive field 你都會指派一些神經元 去觀察這個 receptive field 發生什麼事 那至於要指派多少 那也是你自己決定的 比如說你可以說每一個區域 就有 64 個神經元 負責觀察那個區域發生的事情 這個數值都是你自己決定的 好那我們來看看 有了 receptive field 的這樣概念以後 會發生什麼事情 本來每一個 neuron 都要看 3 乘以 1000 的平方 這麼大的輸入 所以你的參數量非常的可觀 有了 receptive field 的概念以後 那取決於你的 receptive field 的大小 你的 neuron 看的參數 你的 neuron 要看的範圍 會比原來整張圖片小很多 它的參數量也會比原來小非常多 假設你現在定的 receptive field 是一個 2 乘以 2 的 receptive field 那相較於原來 fully connected 的 network 你的參數量這個減少的量不得了 變成 1 除以 500 平方 那麼多的參數的減少量 所以透過 receptive field 每一個 neuron 它只守備一個小範圍 不觀察整張圖片 你可以大幅減少參數的需求 那大幅減少參數的需求 其實是什麼意思呢 它的意思其實是 你選擇了一個 比 fully connected 的 layer 比原來常用的 MLP 更小的函式搜尋的範圍 為什麼呢 因為本來每一個 neuron 它可以看整張圖片 當我們用 receptive field 的時候 其實你的意思是說 今天這個 neuron 連到 receptive field 以外的那些 pixel 它的參數都直接設成 0 強制設成 0 就不再選擇了 強制設成 0 所以 receptive field 是一個比 fully connected layer 更小的範圍 如果你直接 run fully connected layer 你只要設定好合適的參數 它可以做到 receptive field 可以做到的事情 所以 receptive field 是 涵蓋在 fully connected layer 裡面的 它是一個比較小的範圍 但雖然它是一個比較小的範圍 可以選擇的函式比較少 但是在這個綠色框框以外的範圍 那些函式也不是你要的函式 因為我們假設說 在做影像處理的時候 每一個 neuron 就只需要看 小範圍就夠了 所以我們等於是把函式的範圍變小 而且這個小的範圍中 還是有包含我們要的 可以做影像處理的函式 好所以這個是 CNN 的第一個發想 第一個發明 它就是加上 receptive field 的概念 那其實 CNN 還有第二個特點 第二個特點是 同樣的 pattern 它可能會出現在影像裡面的不同的地方 而這些不同地方但同樣的 pattern 其實是可以用同一個 neuron 同一組參數來進行處理的 就假設你要偵測鳥嘴 這個鳥嘴有可能出現在圖片的左上角 有可能出現在圖片的中間 但它們都是鳥嘴 如果按照原來的想法 你可能有一個藍色的神經元 它就專門偵測鳥嘴的 它的手臂範圍在左上角 當今天左上角出現一個鳥嘴的時候 它就會輸出一個比較大的數值 告訴後面其他神經元說 它看到了一個鳥嘴 那你可能需要另外一個紅色的神經元 這個紅色神經元 它手臂範圍在圖片的中間 然後當它看到鳥嘴出現在圖片中間的時候 它就會輸出一個比較大的數值 告訴後面的人說 告訴後面神經元說 它看到一個鳥嘴了 但是這兩個神經元做的事情 其實是一模一樣的 它們只是手臂的區域不一樣而已 那這兩個神經元 能不能夠共用同樣一組參數呢 反正它們做的事情是一樣的 所以照理說 應該要直接給它同一組參數 它們不該有不同的參數 所以這個 CNN 的第二個特點就是 在不同的 receptive field 裡面 每一個 neuron 他們的參數 是共享的 也就是他們的參數被強迫設定成 一模一樣的數值 現在假設左上角這個 receptive field 有 64 個神經元在守衛它 中間有另外 64 個神經元在守衛它 我們會強制要求說 左上角這個範圍的第一個神經元 它的參數要跟 這個範圍的第一個神經元 它的參數要設定一模一樣 這邊有 64 個神經元 64 組參數 這邊也是 64 個神經元 要用一模一樣的 64 組參數 那這邊每組參數其實有一個名字啦 叫做 filter 為了把它跟神經元做區別 他們會把參數叫做 filter 那總之告訴你說 這個區域的 64 個神經元跟這個區域的 64 個神經元 它的參數 是 需要被強制設定成 一模一樣的 因為我們現在把參數強制設定成一模一樣 這意味著什麼 就是意味著你的選擇又更少了 本來每一個神經元 它可以自由自在的選擇自己的參數 這是一個比較大的選擇範圍 現在你加上 參數共享的概念 你讓不同區域的神經元有 強制有一樣的參數 你這個時候可以的選擇又更小了 所以當你加上 parameter sharing 的時候 你其實是比 用 receptive field 又畫了一個 更小的範圍 但是還好 這個紅色之外的東西都是你不想要的 他們對 分類圖片不是好的 就對分類圖片 好的參數 會落在這個紅色的區域裡面 那同時用 receptive field 跟 parameter sharing 的概念 就是 convolutional network 就是 convolutional layer 那 convolutional network 定義是什麼呢 通常是 如果它裡面有 convolutional layer 你就會說 convolutional network 那一個 network 裡面也不會每一層都是 convolutional layer 如果是 convolutional network 一個比較經典的設計就是 前面幾層 是 convolutional layer 然後最後幾層 還是會拉成 fully connected layer 不過最近 也是有很多 network 是全部每一層都是 convolution 的 總之 network 的設計 真的是應用之道存乎一心 這邊就是告訴你說 CNN 或 convolutional layer 相較於 fully connected layer 很多人會覺得 CNN 是一個更複雜的東西 也許你在其他地方 聽過其他的課程講 CNN 你可能覺得這是一個更複雜的東西 那我今天告訴你 它不是一個更複雜的東西 它是一個更簡單的東西 它其實是 fully connected layer 的簡化 CNN 其實是限制了 函數可以搜尋的範圍 只是這個範圍是一個 好的範圍 它可以避免我們 overfitting 它有包含好的 function 又是一個好的範圍 所以它相較於 fully connected layer 它反而比較不容易 overfitting 好那這邊呢 講了一些跟 CNN 有關的事情 那其實你還可以從不同的角度來看 CNN 從不同的角度來看 你也許更容易想像 它為什麼叫做 convolutional layer 在剛才說明裡面 你可能看不出來 convolution 這個 operation 這個操作 到底出現在哪裡 所以如果你想要聴 CNN 另外一個版本的故事的話 可以看 2021 年的機器學習的課程 那剛才講的東西 其實是 2021 年機器學習課程 講 CNN 的前半段 所以你現在可以從中段開始看 講 CNN 的另外一個故事 好那講了這麼多 假設你沒有聽得很懂的話 那你就是知道一件事情 就是 CNN 改變了函數搜尋的範圍 它帶來什麼好處 它的好處是 generalization 它是讓你的模型更不容易 overfit 它不是 design for optimization 另外這個設計 它是專門針對影像設計的 那你知道當然你可能會說 其他很多的應用也常常用到 CNN 比如說語音也會用到 convolutional layer 那是因為語音跟影像有同樣的一些特性 所以可以用 convolutional layer 並不是所有的任務 用 convolutional layer 都是好的 這是一個根據人類的 domain knowledge 設計出來的類神經網路架構 根據人類的 domain knowledge 劃定一個好的範圍 這是一個小的範圍 又包含了我們要的函式 好 那我們再講一些 剛才講的是針對 generalization 的方法 我們來講一些針對 optimization 的類神經網路設計 你可能以為只有第三步會影響 optimization 但其實不是 你類神經網路的結構 也有可能會影響 optimization 的結果 比如說我們這邊有一個類神經網路 那我們這邊沒有畫出 activation 跟 bias 讓整個圖根看起來簡單一點 那我們來看看今天在訓練的時候 實際上發生了什麼事 今天在訓練的時候 我們要計算每一個參數 w 對 loss 大 L 的偏微分 就是 gradient 那 gradient 如果你無法想像是什麼的話 你就想成說 gradient 其實就是 如果把 w 動一點小小的數值 那大 L 會有多大的變化 這兩者相除其實就是 gradient 那如果今天你的參數是在最後一層 那你把最後一層的參數稍微變了一下 加上個 delta w 那你的輸出當然直接就變了 你的輸出變了 你的大 L 就變了 你就會有一個非常明確的 delta L 的變化 所以如果是最後一層 它的 gradient 往往算出來的數值是比較大的 但是如果今天你的參數 落在非常接近輸入的地方 你的 network 又非常的深 這個時候 delta w 跟 delta L 之間的關係 就會變得很複雜了 通常你會遇到的狀況是 當你小小的變化一下 delta w 因為中間要經過非常多層非常多的關卡 所以最後的輸出可能是文風不動 所以它對 L 沒什麼影響 你算出來的 gradient 就會很小 這個現象叫做 gradient vanishing 那可能看到很多文學會告訴你說 gradient vanishing 就是你 deep 的 network 訓練不起來的一個元凶 就是用 gradient vanishing 所以你很難訓練 network 但是你想想看 這樣的論述並不夠充分 為什麼呢 因為假設我們的問題只有 gradient vanishing 只有 gradient 太小 那要怎麼處理它 你 learning rate 調大不就好了嗎 gradient 太小 learning rate 只要調大 你還是可以 update 參數 所以真正的問題 不是因為有 gradient vanishing 而是因為除了 gradient vanishing 之外 同時你有可能出現 gradient explode 就這邊小小的變化 多數時候對結果都沒有影響 但是有時候會出現蝴蝶效應 就蝴蝶搧一下翅膀 對這個世界通常沒有什麼影響 但有一些特定的場合 就是會導致龍捲風的出現 所以有時候這個 delta w 只有小小的變化 通常對輸出沒什麼影響 但中間有時候累積了很多層以後 又會在某些特定的場合 突然出現巨大的影響 這兩者同時出現才是真正的難點 就你 learning rate 設小也不對 設大也不對 如果你設小沒辦法 train 因為有 gradient vanishing 設大那如果一旦出現 gradient explode 你會炸裂又沒有辦法訓練 就是因為參數忽大忽小 就是因為這個 gradient 忽大忽小 你才會變得很難訓練 這個就是 deep network 很難訓練的 其中一個原因 那這個 skip connection 或者又叫 residual connection 這個方法出現 就是為了要緩解 gradient vanishing 跟 gradient explode 的問題 那這個 skip connection 是怎麼運作的呢 它的方法就是在 layer 之間 加一個高速公路加一個連結 直接跳過每一個 layer 那這邊如果要用數學式來表示它的話 假設在本來的 network 裡面 假設輸入是 a 那它輸出就是 w 乘上 a 這個向量 也就是 a' 等於 w 乘以 a 那如果加上 residual network 加上 skip connection 以後 輸入是 a 輸出是什麼 輸出是 a 乘上 w 以後 再加上 a 本身當做最終的輸出 那加上這種 skip connection 有什麼好處呢 加上這種 skip connection 以後 就算是比較低的 layer 的那些參數 它對輸出可能也會有非常顯著的影響 如果第一個 layer 的參數變了一下 導致 a 變了一下 a 的這個改變會一路傳到最後一個 layer 因為每一個 layer 都有 skip connection 就等於是開了一個高速公路 a 的改變會一路傳到最後 所以變成在低階的 layer 它其實對最後的輸出也可以有足夠的影響 那會讓你的整個 training 變得容易一點 那這邊我們就不再講更多有關 skip connection 的細節 因為今天這堂課的重點 並不是仔細的介紹所有的方法 是讓你了解說每一個方法有什麼樣的特性 那像 skip connection 這個方法 如果你把它的 loss surface 畫出來的話 沒有 skip connection 的時候 你的 loss surface 會非常複雜 有了 skip connection 以後 你的 loss surface 就會變得平坦很多 讓你更容易訓練 那看到這個圖的時候 你要有的第一個問題是 這個圖是怎麼畫出來的 因為 Network 它不是只有兩個參數 在只有兩個參數的時候 你才能畫這種 loss surface 如果今天有上百萬個參數 這種 loss surface 到底是怎麼畫出來的 那你就可以看右上角這邊 paper 其實這邊右上角這邊 paper 它真正提出來的 並不是要告訴你 skip connection 好 它這張 paper 真正提出來的 是想要告訴你說 它有個好方法 可以想辦法把 loss surface 畫在三維的圖裡面 然後這只是它其中一個例子告訴你說 那我們畫這個圖有什麼好處呢 以前大家都說 skip connection 很厲害 但你不知道它有多厲害 然後它就舉說你看 用它的方法 本來沒有 skip connection 的時候 畫出來 loss surface 長這樣很複雜 像個山水畫一樣 有很多的丘陵 你一旦加上skip connection 你的 loss surface 就變得不一樣了 然後總之skip connection這種方法 它改變了什麼 它改變了Network的架構 所以改變了函數搜尋的範圍 但它帶來什麼好處 它帶來的好處是optimization的好處 它可以讓你在第三步做optimization的時候 變得更容易 還有其他跟optimization有關的方法 比如說在訓練Network的時候 有一整系列的normalization的方法 因為時間有限的關係 我們就不一一介紹這些normalization的方法 他們通常基本的精神是 你今天有一個輸入 這個輸入會過一個normalization的操作 這種normalization操作有時候它有參數 有的沒有參數 你過一個normalization操作 變成新的樣子 再丟給下一層 再得到輸出 再過normalization操作變成某個樣子 再過下一層再得到輸出 再過normalization 這些 normalization 做的事情是把 每一個 layer 的輸出 限制在某一個範圍內 本來你一般在訓練 Network 的時候 你沒有辦法限制每一層的輸出 一定在某個範圍內 但你可以加上某一個操作 強制讓每一個 layer 的輸出 落在某一個範圍內 比如說強制要求每一個 layer 的輸出 它的 min 一定要是 0 當然還有很多其他的加上限制的方式 我這邊舉的只是 眾多可能的例子裡面的 其中一個例子而已 這個就是 normalization 那如果最右邊這個輸入 它不是某一個 hidden layer 的 output 而是資料的話 那你等於就是對資料 做了一些特殊的處理 你把資料在進入 Network 之前 做一些特殊的處理 比如說讓你的資料的 min 是 0 variance 是 1 等等 這個就是資料的正規化 其實上週也有同學問說 訓練 Network 的時候 需不需要做正規化 那你當然可以做正規化 那也可以讓你的結果好一點 正規化其實也對資料做正規化 也算是 normalization 的其中一個環節 更廣義的正規化就是對 每一層都做正規化 這個就是 normalization 那 normalization 很多種 最知名的就是 batch normalization 還有 layer normalization 那如果你想知道 跟這些 normalization 有關的事情的話 你可以看一下 2021 年的錄影 那有一堂課 跟大家講了 batch normalization 相關的事情 好那 normalization 它帶來了什麼改變 它等於是改變了 函式搜尋的範圍 但它帶來了什麼好處呢 通常 normalization 它真正想要帶來的好處是 讓你有更好的 optimization 當我們把我們的 layer 限制在某個範圍內 當我們要求 layer 的 這個每一個 dimension 的輸出 一定在某個範圍內的時候 因為每一個 dimension 它的數值的 range 差不多 那你比較容易調 learning rate 比較容易做 optimization 但是這個 normalization 的方法 往往也對 generalization 有一些幫助 雖然這不是它主要出現的原因 因為這些 normalization 的方法 我們強制每一個 layer 的輸出 一定要在某個範圍內 那也不就是等於是 多加了一個額外的限制嗎 那通常多加額外限制 會讓你的函式範圍變小 所以在這個情況下 你可以把 generalization 做得更好 所以 normalization 對 generalization 也是有一點點幫助的 好，那接下來我們進入第一步 我們來看一下有關 Loss 的定義 那這邊呢 我們先介紹一個上週沒有講的事情 上週我們是拿 Regression 当做 Machine Learning 的 Task 那我們說我們的 Loss 就是 MSE MSE 也可以被當作 Evaluation Metric 來量函式的好壞 在 Testing Set 上 在 Validation Set 上量函式的好壞 那這邊我們來講一下 如果不是 Regression，是個分類的問題 我們要怎麼定它的 Loss 那什麼是分類的問題呢 假設我們要做個影像分類的問題 就是我一個函式 這個函式輸入是一張圖片 輸出就是從眾多可能的選項 事先定好的選項裡面選擇一個選項 你就會先定好說 假設是動物的分類 看著圖片決定它是哪個動物 實際上動物就是 1000 種 所以機器要做的事情 就是從 1000 個類別裡面 選擇它覺得正確的類別 其他類別都是不正確的 那這個 Classification 對於我們的生成式 AI 而言 其實非常的關鍵 為什麼呢 因為還記得生成式 AI 在做什麼嗎 生成式 AI 就是給一個未完成的句子 去猜接下來應該是哪一個字 其實這個任務 本質上就是一個分類的問題 所以生成式 AI 其實就是 一系列的分類問題集合起來 就是生成式 AI 所以分類對生成式 AI 是一件非常重要的事情 所以我們特別值得來講一下 分類的 Loss 到底長什麼樣子 上週我們講了 Regression 然後我們說 Regression 的類神經網路長這樣 重點是在最後輸出的地方 三個神經元輸出 A1、A2、A3 三個值 A1、A2、A3 三個值 分別乘上三個權重 再加上 b 得到一個數值 y Regression 只需要輸出一個數值 y 但如果是分類的問題呢 如果是分類的問題 我們到底想要類神經網路 輸出什麼東西呢 如果是分類的問題 假設你有兩個類別 那你就希望類神經網路給你兩個數字 那每一個數字就代表 那個類別的信心分數 那如果你有 1000 個類別 你就希望它輸出 1000 个數字 那這邊是假設只有兩個類別 有類別 1 跟類別 2 那你就把前一層的神經元的輸出 A1、A2、A3 那乘上不同的 Weight 把 A1、A2、A3 乘上不同的 Weight 加起來加 bias 變成 y1 A1、A2、A3 乘上不同的 Weight 加起來然後變成 y2 然後你再看 y1、y2 的大小 如果今天是 y1 比較大 那就代表模型選擇了類別 1 那如果 y2 比較小 就代表模型沒有選擇類別 2 所以就看說每一個類別給的這個分數 越大、最大的那一個 就是今天分類的時候 你的函式選擇的類別 所以這個是分類的時候 類神經網路的長相 好，那在這個長相之下 我們要怎麼定 Evaluation Metric 呢 一個常見的方法是你可以定「正確率」 今天假設你有一張圖片 你知道它的 Ground Truth 是鳥 你有另外一張圖片 你知道這個動物它的 Ground Truth 是貓 那你就把第一張圖片丟到函式裡面 看看它選了哪一個類別 所謂選類別的意思就是 看哪一個類別得到的分數最高 那個類別就是模型選的類別 就相當於是模型選的類別 如果跟正確答案一樣就得 1 分 那貓丟到函式裡面 輸出結果是狗 狗的分數是最高的 那就是錯了就得 0 分 把每一個圖片的分數通通平均起來 你就得到正確率 這個是分類問題常用的 Evaluation Metric 那之前在講 Regression 的時候 我們其實是把 Evaluation Metric 跟 Loss 同等看待 我們的 Loss 其實就是 Evaluation Metric 但是在分類問題裡面 我們能不能直接把 Accuracy 當作 Loss 來用呢 我們來看看 如果直接把 Accuracy 當作我們要 Optimize 的對象 也就是 Loss (大 L) 來用的話 會發生什麼樣的事情 你要記住說 我們在做 Optimization 的時候 我們現在用的就是 Gradient Descent Gradient Descent 要算出來 就是去計算每一個參數 有小小變化的時候 對最終的 Loss 有多大的變化 我們現在假設說 我們拿 Accuracy 來當作 Loss 然後我們來看看算 Gradient Descent 算 Gradient 的時候 會發生什麼樣的事情 假設我們對這個 b2 prime 做一點小小的變化 把它加 0.01 那對 y2 的影響是立竿見影 它也加了 0.01 那我們假設 y1 原本的輸出是 2 y2 原本的輸出是 1 那 1 加 0.01 還是比 2 小 所以輸出不變 那輸出不變意味著什麼 代表你的 Loss 沒有變 所以對 Loss 而言 什麼事都沒有發生 所以 Loss 的變化是 0 所以按照 Gradient 的想法 算出來你的 Gradient 其實是 0 所以除非你今天有一個改變 這個改變正好讓 y2 比 y1 大 正好那個改變過了那個臨界點 不然你什麼都觀察不到 你 Gradient 算出來總是 0 如果用圖像化的方法的話 你就會看到說 如果你用 Accuracy 來當作 Loss 那你參數改變的時候 對於 Loss 幾乎都是沒有影響的 只有在某一個時間點 當你參數改變的時候 那個正確的類別被換了 你的 Loss 才會看到一瞬間的變化 不然多數時候你的 Gradient 算出來 通通都是 0 你根本就沒有辦法做 Gradient Descent 你從這個地方開始 你根本找不到任何下坡的位置 你根本找不到 你 Gradient 永遠算出來都是 0 你根本不知道該往哪一個方向走 這就出現了一個有趣的問題 你沒有辦法直接把 Accuracy 當作 Loss 為什麼 因為 Accuracy 沒辦法好好的算微分 所以如果你今天的 Evaluation Metric 是沒辦法算微分的 那其實 那其實它沒有辦法当做你的 Loss 你得想一些別的方法 才能把 Accuracy 當作是 Loss 來用 所以怎麼辦呢 所以其實在分類的時候 你不會直接把 Accuracy 當作是 Loss 你會拿一些算出來跟 Accuracy 有點像的東西 來當作是 Loss 那今天最常用的 是一個叫做 Cross-Entropy 的東西 好，那這什麼是 Cross-Entropy 呢 我們就來講一下 Cross-Entropy 的計算方式 我們先講它的計算方式 你先不要管它怎麼來的 它怎麼算呢 這 Cross-Entropy 呢 就是先把每一個類別的分數過 Softmax Softmax 會做的事情 就是把分數 本來這邊的分數並沒有限制 它一定要是一個機率 沒有限制它是 0 到 1 之間 甚至沒有限制它一定要是正的 它其實也可以是負的數字 把這些分數過 Softmax 之後 它會變成一個機率分布 所有的 Class 它的分數合起來 是 1 那所有的數值都會在 0 到 1 之間 好，那怎麼做 Softmax 呢 這個我們在第三講的時候已經講過了 所以我們就不再細談 如果你忘記的話 再回去複習第三講的內容 好，那我們會把類神經網絡輸出 變成一個機率分布 同樣我們也要把 Ground Truth 正確答案 變成一個機率分布 那 Ground Truth 的答案就只有一個啊 比如說假設正確答案是 Class 2 的話 就只有一個答案啊 那你就把它變成機率分布的時候 你就會說 Class 2 的機率是 1 Class 1 的機率是 0 接下來你計算這兩個機率分布的 Cross-Entropy 如果你不知道它 Cross-Entropy 的式子是什麼的話 就把式子列在這邊 呃 Ground Truth 來的機率我們用 hat 來表示 類神經網絡那邊來的機率用 prime 來表示 那 Cross-Entropy 就是 summation over 所有的類別 然後把 第 i 個類別的 y_hat 乘上第 i 個類別的 y_prime 取 log 全部加起來再取負號 這個就是我們的 Loss 那如果你看不太懂這是什麼的話 反正你就知道說 這兩個機率分布越接近 那這個 Loss 算出來呢就會越小 那我們當然希望這兩個機率分布越接近越好嘛 所以這個蠻符合我們直覺需要的一個 Loss 那這兩個機率分布越接近 我們算出來的 Loss 就會越小 而這個 Cross-Entropy 這一項啊 它是能夠計算微分的 如果你這邊 bias 有一点改變 那 y2 就會有一点改變 当 y2 有一点改變的時候 其實你也影響了 y1 当 y2 有一点改變的時候 因為這邊會做 Softmax 所以其實 y1_prime 跟 y2_prime 同時都改變了 y2_prime 就上升了一點 y1_prime 就下降了一點 當你這個 y_prime 改變的時候 這邊的 y_prime 改變的時候 Loss 算出來當然也就變了 Loss 算出來變了 你就可以計算 Gradient 你就知道說 當我這邊有小小變化的時候 參數有小小變化的時候 最終的 Loss 會有什麼樣的變化 你算得出來 你就可以做 Gradient Descent 好，那你可能會問說 為什麼是 Cross-Entropy 呢 為什麼不能是其他的東西呢 這個解釋起來就很複雜了 參見機器學習 2016 的第四講跟第五講 足足花了超過兩個小時的時間來解釋 為什麼分類應該是 Cross-Entropy 它有什麼樣的道理 好，那用 Cross-Entropy 的好處 如果用圖像畫來看的話 你現在的 Loss 可能就長得像這樣 它跟這一個 Loss 用 Accuracy 算出來的 Loss 不一樣 然後有一點關聯性 但不一樣 然後它是平滑的 它是可以微分的 所以今天是有辦法 用 Gradient Descent 是有辦法下坡的 但是雖然我們在訓練的時候 用 Cross-Entropy 但是你最後在做驗證跟測試的時候 你仍然會用 Accuracy 因為 Accuracy 才是你真正關心的東西 其實對一個使用者來說 他根本不關心你算出來的分數是多少 他真正關心的是 你現在是個分類的問題 所以對一個使用者來說 他真正關心的是你分類的類別對不對 就算你說你分類類別的時候 前面那個分數有一點改變 你現在錯的那個答案 其實分數是比較高的等等 沒人在乎 大家只在意你的正確率對不對 所以你的 Validation 跟 Testing 的時候 你仍然會用 Accuracy 所以這就製造了一個 訓練跟測試的再一次的誤差 我們之前在講 Regression 的時候 就算是 Validation 跟 Training Data 都用 MSE 都已經會有誤差了 這邊你再製造一個誤差 他們連計算的 Loss 的方式 通通甚至都是不一樣的 所以你引入了一個額外的誤差 所以今天當我們用 Cross-Entropy 來當作 Classification Loss 的時候 我們改變了 Loss 的定義 那我們帶來的好處是什麼 帶來的好處是 你能夠做 Optimization 如果是 Accuracy 的話 你的 Optimization 都做不了 但是它對 Generalization 是沒有幫助甚至是有害的 因為它憑空製造了一個 訓練跟驗證的時候 可能的誤差、可能的 gap 那你可能會想說 那有沒有辦法定一個 Loss 這個 Loss 既能做 Gradient 它甚至對 Generalization 有幫助呢 你可以定這種 Loss 比如說在 SVM 裡面用的 Hinge Loss 可能就可以達到 improve Generalization 的效果 那因為時間有限的關係 我們今天就不講 如果想知道的話 參見機器學習 2016 的第 20 講 那個時候講機器學習的時候 還是會講一些 不是 Deep Learning 的方法的 那時候不好意思講一個課 全部都只有 Deep Learning 的方法 所以還是有講了一下 SVM 是什麼等等 所以你可以看一下這個影片 告訴你說 Loss 其實也很多 不同的定義方式 你把 Loss 換一下 其實就變成 SVM 那剛才講的是 Loss 那接下來我們來看資料 其實資料也是 Loss 定義的一個環節 資料也影響了你的 Loss 長什麼樣子 那我們今天期待的當然是說 我們用 Training 的資料來定出 Loss 以後 根據這個 Loss 找出一個 Loss 最小的函式 在測試資料上 在 Validation 的資料上 也要發揮作用 也要得到 Loss 低的結果 但是往往事與願違 那這個事與願違叫做 Overfitting 就是你 Training 的 Loss 很低 但 Validation 的 Loss 沒那麼低 這個是 Overfitting 那要克服 Overfitting 其實最直接粗暴的方式就是 收集更多的資料 你的 Training Data 算出來 Loss 之所以跟 Testing Data 算出來 Loss 不一致 那就是因為你 Training Data 的資料 不夠具有代表性 它沒有辦法代表各種不同的變化 所以也許收集更多的資料會有幫助 但你要注意的是 你收集到的資料是需要有代表性的資料 需要代表真實世界的分布 或至少需要代表測試資料的分布 如果你的訓練資料跟測試資料 有一個本質上的誤差 你一直收集有誤差的資料 最終還是緣木求魚 好，所以要怎麼克服 Overfitting 有一個最直接的方法 最粗暴的方法就是直接收集更多資料 其實我會覺得 今天你遇到 Overfitting 的狀態 那如果今天沒有什麼限制 限制你收集額外的 Data 那我一般在做作業的時候 都會限制你不能用額外的 Data 但今天假如沒有限制你收集額外的 Data 其實收集更多的資料 往往是最簡單 你最容易看到效果的方法 好，收集更多的資料改了什麼 它算是改了 Loss Function 的定義 因為你改了 summation 的時候 summation over 的東西 它帶來什麼好處 它帶來的是 Generalization 的好處 可以比較避免 Overfitting 但它沒帶來什麼 它沒帶來 Optimization 的好處 你有更多資料的時候 你的 Optimization 只會更困難 你要找到一個函式 它可以符合更多資料的需求 是比少量資料更困難的一個問題 所以當你有更多訓練資料的時候 其實你是更難做 Optimization 的 所以今天假設說 你現在遇到的問題是 Deep 的 Network 沒有 Linear Network 好 那是 Optimization 的問題 很多同學只要一看到 Performance 不好 就狂灌資料 這時候你要做的不是狂灌資料 當你發現 Optimization 問題的時候 灌資料是沒有用的 你要在 Overfitting 的時候 灌資料才有用 所以要記得每一個技巧所使用的時機 假設今天的狀況是 你真的不能收集額外的資料 比如說收集資料的成本很高 所以你不能夠收集額外的資料 有一個可以用的招數叫做 Data Augmentation 也就是自己創造出新的資料 比如說假設你今天要做影像分類的問題 你要機器學會說這張圖片就是一個鳥 但是你的資料不多 那你可以憑空創造出一些額外的資料 你可以把這隻鳥左右翻轉 一隻鳥左右翻轉以後還是鳥 所以機器得到一筆新的資料說 看到這張圖片 那你的輸出也應該是鳥 或者是你可以把這張圖片變模糊 那你就可以教機器說 看到這隻模糊的鳥它仍然是鳥 機器可以學會看到模糊的圖片 應該要怎麼處理 或者是舉語音的例子 假設你要教機器做語音辨識 看到一段聲音訊號 它要把它轉成文字 那假設你資料不夠的時候 你一樣可以做 Data Augmentation 比如說把資料庫裡面說 男生的聲音都轉成女生的聲音 說女生的聲音都轉成男生的聲音 現在有很多這種語音轉換的技術 都已經可以做得非常成熟了 那如果你把男生的聲音轉女生的聲音 女生的聲音轉男生的聲音 你憑空資料就多了兩倍 你就額外資料來進行訓練 或者是你可以改變你資料的音量 把它調不同的音量 或者是有人會調不同的長度 做不同的 Stretch 那你也有可能可以得到更好的結果 總之你可以用一些方法 來產生額外的資料 但是你今天要注意 當你在做 Data Augmentation 的時候 你要小心你的 Augmentation 必須是有道理的 Augmentation 這邊再舉一個例子 假設我們現在要做影像處理的任務 不是分是哪一個動物 不知道為什麼有一個莫名其妙的任務 是給一隻鳥 它要判斷這個鳥的頭是向左還是向右 我不知道有沒有這個需求 就假設有這個任務 那這個時候你就不能夠做 Flipping 如果你做 Flipping 把這個鳥左右反轉 然後又說它是向右的話 那這就是一個錯誤的答案 但是做 Blurry 仍然是可以的 或者是假設我們今天的任務是語者辨識 要給它一段聲音 然後機器要判斷說這個聲音是誰講的 這個時候你做 Voice Conversion 做語者轉換 把一個人的聲音從男生變成女生 又說真的是原來那個人 對機器來說這個根本就是錯誤的資料 所以你不能夠做這樣的事情 但是改變聲音大小還可以 所以你要注意在做 Data Augmentation 的時候 有些方法是可以用的 有些方法是不能用的 至於哪些方法可以用 取決於你對這個問題的理解 那剛才講的方法都是改變輸入的資料 我們能不能夠改變 Label 改變 Ground Truth 呢 也可以 有一個神奇的方法叫做 Mixup 它會改變你資料的 Label Mixup 這個方法運作是這樣的 假設你的資料裡面 Ground Truth 有兩筆 一個貓它正確答案是貓 這是一張狗它正確答案是狗 Mixup 是說 我們把輸入的資料 直接平均起來 有時候也不見得平均 要用多少比例相加 這個是你可以自己調的 你可以用多種不同的比例相加 我們把貓乘 0.5 加狗乘 0.5 合成同一張圖片 這些合成同一張圖片就字面的意思 就把兩張圖片直接疊在一起 變成一個既有貓也有狗的圖片 那這張圖片的 Ground Truth 正確答案應該長什麼樣子呢 就把原來的正確答案也平均起來 原來正確答案是 百分之百的貓跟百分之百的狗 那把百分之百的貓乘 0.5 加百分之百的狗乘 0.5 就變成 50% 的貓跟 50% 的狗 機器就要學到說 看到這個貓跟狗合體的圖片 那你的輸出就要說 這是 50% 的貓跟 50% 的狗 這招就叫做 Mixup 總之 Data Augmentation 有非常多不同的方法 那我們再來複習一下這個表格 Data Augmentation 它改了什麼 它改了 Loss 它帶來什麼好處 它帶來了 Data 它帶來了更好的 Generalization 大家注意它對 Optimization 是沒有幫助的 當你資料變多 加了一些雜七雜八的東西的時候 你只讓 Optimization 變得更困難 你不會讓 Optimization 做得更好 所以當你今天發現 你的 Loss 降不下去的時候 做 Data Augmentation 是沒有用的 你要在合適的時機 在 Overfitting 的時候 才適合做 Data Augmentation 還有什麼其他的方法去改 Loss 呢 剛才我們在訂 Loss 的時候 都需要有標註的資料 也就是有 Ground Truth 的資料 我們如果是做影像分類 我們不只需要圖片 每張圖片對應到哪種動物 需要人類事先標好 那我們才能夠訂出這個 Loss 因為我們這個 Loss 就是去計算說 有一個 X 那這個 X 過了我們的函數 F 以後 它的輸出跟正確答案 Y 和有多接近 這個 Loss 就是算 F of X 跟 Y hat 它的接近的程度 那我們這邊 Cross Entropy 來代表它接近的程度 但是呢 我們其實往往比較容易收集到 Unlabel 的資料 你要收集一大堆的圖片 每張圖片都有 它是哪一個動物的標註 可能非常耗人力 但是如果單純要收集一大堆圖片 你去網路上爬 你要爬多少 就可以爬多少 那有了這些圖片以後 你可以讓這些沒有標註的圖片 它也來影響 Loss 的定義 你可以加一項 這一項是跟沒有標註圖片有關的 但因為你現在有兩項 一項是跟有標註圖片有關的 一項是跟沒有標註圖片有關的 所以中間可能要乘一個 Lambda 來平衡這兩項的大小 那如果沒有標註 根本不知道正確答案的圖片 我們要怎麼定它的 Loss Function 呢 這邊就有很多種不同的想法 那這邊先舉第一個例子 第一個例子是 這些應用這個 Unlabeled Data 的方法 它背後往往就是需要基於一些假設 那要有這些假設 你才能夠知道說這些 Unlabeled Data 怎麼把它加到 Loss 裡面 好這邊第一個假設是 這個世界是一個非黑即白的世界 一隻動物要嘛看起來很像狗 要嘛看起來就很像貓 沒有看起來既像貓又像狗的動物 好那這邊的 Loss 就定成這樣子 這個 Loss 是 你要先把 X 丟到原來的函式裡面 得到輸出的分布長什麼樣子 然後呢你有另外一個 Loss 叫做 L Pi 這個 L Pi 不需要正確答案 它光看 F of X 它光看類神經網路輸出的分布 它就可以判斷說 這是不是一個好的東西 這是不是一個好的函式 那這邊定義的這個 Indicator 呢 它是 Entropy 好那這個 Entropy 是什麼呢 Entropy 呢代表了一個機率分布的散布程度 就今天如果一個機率分布 它看起來越平均 那它的 Entropy 就越大 反過來說 如果一個機率分布 它看起來越尖 它看起來越集中 它的 Entropy 就越小 然後呢我們會假設說 我們期待那些沒有被標註的資料 它的 Entropy 要越小越好 就今天假設一筆資料有標註 我們希望類神經網路的輸出 跟正確答案一樣 但如果今天一筆資料沒有標註 那至少我們希望它輸出的時候 它的 Entropy 越小越好 它越確定是哪一個類別越好 那在這個情況下 Unlabel 的資料是怎麼發揮作用的呢 假設今天你每一個類別都只有一筆資料 一個類別是紅色的類別 一個類別是藍色的類別 你已經要找一個函式把他們分開 那你的選擇太多了 你可以畫這樣一條線 可以畫這樣一條線 畫這樣一條線 在不考慮 Unlabel 的資料 只考慮 Label 資料的情況下 這三條線是一樣好的 你根本 雖然你直覺可能會覺得說 應該這條線比較合理吧 這條線這條線應該比較不合理吧 但是你的直覺 對 Loss Function 來說 他們是一樣好的 他們的 Loss 都是 0 都可以把兩種資料分開 但是如果你今天加了一些 Unlabel 的 Data 你再說 Unlabel Data 雖然我們不知道它屬於藍色還是紅色 但是我希望 它非常傾向屬於某一邊 它的 Entropy 要算出來 越小越好 這個時候有了 Unlabel Data 所定義的 Loss 這個時候 這一筆資料 這一條線就會是一個比較好的函式 因為當你選擇這一條線的時候 不只是紅色跟藍色可以分得開 這一些 Unlabel 的點 我們用灰色的點代表 Unlabel 的點 他們離這條線 也都是有比較遠的距離的 代表今天類神經網路的輸出 它的 Entropy 是比較小的 所以這個就是拿 Entropy 來 Improve 你的這個 Generalization 来用 Entropy 來利用這個 Unlabel Data 的方式 這只是其中一個方式 還有很多其他的方式 比如說這邊再介紹另外一個方法 另外一個方法假設是 這是一個物以類聚的世界 也就是說 如果兩筆資料 他們長得很像 他們非常的接近 他們就應該屬於同樣的類別 在用這種物以類聚世界的假設之前 一個可能的方法是 先把這一些訓練資料 不管它是有 Label 的還是沒有 Label 的 統統建立成 Graph 在這個 Graph 上面 夠相近的點就把它連在一起 至於什麼叫做相近 怎麼定義相近 這就要問你自己 根據這個問題來定義什麼叫做相近 然後接下來 你定一個函式叫做 G of x, x' 它的意思就是 如果 x 跟 x' 這兩筆資料 它們有相連的話 那就是 1 反之就是 0 然後定義另外一個函式叫做 L' f of x, f of x' 它是去計算 f of x 跟 f of x' 之間的距離 但至於怎麼計算兩個分布的距離 那也要問你自己 你可以用 Cross Entropy 也可以用 KL Divergence 也可以用任何其他你喜歡的方法 那加上這個式子以後 它代表的意思就是 我們去看所有的 unlabel 資料的 Pair 我們從 unlabel 的 Data 裡面 取出兩筆資料 x 跟 x' 如果它們沒有相連 那就不管 那如果有相連 我們就希望它們的距離越近越好 我們希望把這兩筆資料 丟進一個函式之後 丟進我們要找的函式之後 它的距離越小越好 所以這邊就等於在訓練的時候 加了一個額外的限制 本來我們只需要考慮 Label Data 的 loss 那我們現在除了 Label Data 以外 也可以考慮 unlabel 的資料 拿 unlabel 的資料來提供給我們額外的資訊 讓我們可以選擇更好的函式 好，講了這麼多，如果你沒有聽得很懂的話也沒有關係 我們現在就是要來問 Semi-supervised learning 的方法 這種有用 Label 的資料跟沒有 Label 的資料，叫做 Semi-supervised learning 這種 Semi-supervised learning 的方法，它改了什麼？ 它改了 Loss Function 的定義 它帶來什麼好處？ 它帶來的好處是更好的 Generalization 但它對 Optimization 是沒有幫助的 你現在加入了額外的項 額外的你現在把 Loss Function 弄得更複雜 那只是讓 Optimization 更困難而已 所以 Semi-supervised learning 它真正帶來的好處是 Generalization 而不是 Optimization 好，接下來進入這個課堂的最後幾頁投影片 剛才我們講的 Loss 都是跟資料有關的 不管它是用 Label data 還是 Unlabel data 都是跟資料有關的 你其實也可以定義一些 Loss 它跟資料是沒有關係的 單純反映我們人類對於參數的偏好 那我們人類對參數可能會有什麼樣的偏好呢？ 今天假如有這樣兩個函式 這樣兩個函式它們都穿過了這三筆訓練資料 也就是如果單看資料的話 這兩個函式可能是一樣好的 那假設在單看資料這兩個函式一樣好的情況下 你會覺得左邊這個函式比較合理 還是右邊這個函式比較合理呢？ 那這邊我們要假設左邊這個函式是比較合理的 是我們比較想要的 右邊這個函式可能是有問題的 雖然它根據資料所定義出來的 Loss 也很低 但是根據這個函式本身的特性是我們不喜歡的 那為什麼這樣呢？ 我們通常會比較喜歡比較平滑的函式 比較不喜歡震盪非常劇烈的函式 那這個要怎麼解釋呢？ 你可以從很多不同面向來解釋 比如說常見的解釋是說 因為奧卡姆剃刀原則，如無必要不要增減 所以今天只要能夠把資料做得好 那我希望函式越簡單越好 這是個解釋 你也可以說按照剛才 Semi-Supervised learning 的方法 相似的東西就要有一樣的 Label 如果你今天函式震盪非常劇烈 就會變成相似的東西沒有一樣的 Label 這也是一個解釋的方法 總之這是一個在實務上有用的方法 你會選擇一個比較簡單的函式 當你有好幾個函式 它們根據資料算出來 Loss 都一樣低的時候 你偏好選擇一個比較簡單的函式 但所謂的簡單又是怎麼被定義出來的呢？ 這邊在實作上簡單的定義就是 希望這一些參數它的數值越接近 0 越好 那越接近 0 越好 其實並沒有辦法保證 函式一定比較簡單 就假設你簡單的定義是 輸入有小小的變化 輸出的變化要小於某一個範圍 其實用這個方法越接近 0 越好 來定義簡單其實也不完全是正確的 但是在實作上通常就是這麼操作的 這一招叫做 L2 Regularization 那 L2 Regularization 怎麼計算呢？ 我們會有一個函式叫做大 R 這個大 R 的輸入是一個 theta 這個大 R(theta) 是 theta 裡面所有參數 theta i 的平方和 然後我們希望這個大 R 的值越小越好 也就是希望這些參數的值越接近 0 越好 好那們本來在做 gradient descent 的時候 就是算出 gradient 然後用算出來的 gradient 去更新參數 當我們今天加上大 R 這一項以後 對 gradient descent 會有什麼樣的變化呢？ 其實就操作上沒什麼變化 你就是把大 L 換成大 L' 大 L' 裡面有兩項 一項跟資料有關的大 L 一項是只跟參數有關反應人類對參數偏好的大 R 然後根據大 L' 去算 gradient 得到 G' 用 G' 去更新參數 但是因為這個大 L' 跟大 L 本身是有關係的 所以我們可以再做進一步的分析 你可以更清楚這個 Regularization 是在做什麼樣的事情 這個 L' 它其實是 L 加 R 這兩項 所以 L' 的 gradient 等於 L 的 gradient 加上 Lambda 乘上大 R 的 gradient 然後呢 大 R 的 gradient 是什麼呢？ 大 R 的 gradient 根據大 R 的式子是可以直接算出來的 大 R 的 gradient 就是兩倍的 theta 然後這個大 L 呢 大 L 就是這邊的 G 然後所以我們知道說 G' 其實就是 G 加兩倍的 Lambda 乘 theta 所以我們可以把這邊的 G' 置換成 G 加兩倍的 Lambda 乘 theta 然後我們把 Lambda 乘進去 變成 theta 減掉 learning rate 乘 G 減掉兩倍的 learning rate 乘 Lambda 乘 theta 那這邊第一項跟最後一項都有 theta 把它整理起來 所以整個式子變成 1 減掉兩倍的 learning rate 乘 Lambda 然後乘 theta 再減掉 learning rate 乘 G 或者是你可以把它想成我們現在做的事情 其實跟原來的 gradient descent 沒有加大 R 這項 gradient descent 是一模一樣的 我們只是在參數 update 之前多做了一件事 把原來的參數乘上 1 減掉兩倍的 learning rate 乘 Lambda 因為 learning rate 通常是一個很小的數值 所以你可以想像這個數值可能是一個 0.999 這樣的數值 它是一個非常接近 1 的數值 那它做的事情 假設你不管這個 regularization 不管這個 loss function 單純從 optimization 的角度來看 你可以想成說 我們每次在更新參數之前 不知道為什麼 我們都把原來的參數乘上 0.99 再做更新 我們把原來的參數弄得小一點點再做更新 那這個把原來的參數弄得小一點點這件事情 叫做 Weight Decay 但 Weight Decay 其實跟 L2 Regularization 是同一件事 他們只是同一個操作兩個不同的講法 所以這邊等於學到兩個技術 如果是 parameter regularization 如果你是用 L2 也就是對參數做平方中 L2 Regularization 那你做的事情是你改了 loss 的定義 它帶來的好處是什麼 它帶來的好處是更好的 generalization 但它跟 optimization 沒有關係或沒有幫助 你加了一個額外一項 你只是讓 optimization 做得更難 而已經讓 optimization 做得更差而已 Weight Decay 等價於 L2 Regularization 但是我們是在 optimization 的過程中做了修改 我們等於改了找最好函式的演算法 那它對 generalization 有幫助 但對 optimization 沒有幫助 那 Weight Decay 這招可以直接加到 Adam 裡面 那 Weight Decay 要怎麼加到 Adam 裡面 這個你可以看原始的 AdamW 這是一個很早期的 17 年的上古時代的論文 這邊論文告訴你說這個不能隨便亂加 過去人的加的方法是不對的 它告訴你有一個更好的加法 這個更好的加法其實非常的單純 这个单纯的方法就是 你在做 Adam update 之前 Adam update 已經很複雜了 有 learning rate scheduling 有 sigma 有 momentum 好在加上這個很複雜的項之前 先把 theta 直接弄小 先把 theta 直接乘個 0.999 然後再進行參數的更新 這招叫做 AdamW 好那 AdamW 假設你今天用 Adam 發現結果不好的話 發現 overfit 的話 AdamW 也許是你的一個選擇 然後 AdamW 帶來什麼好處呢 相較於原來的 Adam 它強化了 generalization 這件事情 原來 Adam 沒考慮 generalization 它把 Weight Decay 加上去 所以它等於考慮了 generalization 但 AdamW 因為畢竟它是一個 Adam based 的方法 它有了 Adam 的這一些強項 所以相較於原來最簡單的 gradient descent 它是一個更強的 Optimization 的方式 但如果你要說把 AdamW 跟 Adam 比的話 它對 Optimization 是沒有幫助的 你胡亂做一個 decay 讓你本來的 gradient 本來你要按照某個方向去 update 你故意不按照這個方向 update 還亂改原來的參數 你只讓 Optimization 可能做得稍微差一點而已 所以相較於 Adam AdamW 並不是更強的 Optimization 方式 它是更強的 generalization 的方式 但相較於啥都不幹 它仍然是一個比較強的 Optimization 的方式 這個只跟大家介紹 AdamW 好今天講了一大堆東西 假設你沒聽進去什麼的話 那就是記得這個表格 那這個表格內容假設你也沒有打算記住的話 那你要知道的事情是什麼 你知道的事情就是 當你聽到一個方法的名字的時候 想想看這個方法到底是改了 machine learning 三步驟的哪一個 還有這個方法 到底是為了 Optimization 設計的 還是為了 generalization 設計的 那如果你知道這件事 那今天這門課你就不虛此行 好那今天的課呢我們其實就上到這邊