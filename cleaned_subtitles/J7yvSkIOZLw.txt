you could imagine that the training regime becomes much more paralyzable where it's like most of the compute for training is used to come up with synthetic data or do some kind of search and that can happen across a wide area you have synthetic data you have like this search stuff you have like um you have all these post training techniques uh you have all this you know all this ways to soak up flops or you just figure out how to train across multiple data centers which I think they have um at least Microsoft and open AI have figur open a what you think they figured it out um their actions so Microsoft has signed deals north of 1010 billion with fiber companies to connect their data centers together there are some permits already filed to show people are digging um you know between certain data centers so we think with fairly high accuracy we can say we think that there's five data centers massive not just five data centers sorry five like regions that they're connecting together which comprises of many data centers right what will be the total power you of the depends on the time but easily north of a gwatt right uh which is like close to a million gpus uh well the each GPU is getting more power higher power consumption too right like it's like you know the rule of thumb is like GPU h100 is like 700 Watts but then like total power per GPU all in is like 1 12200 1300 Watts 1400 Watts but next Generation Nvidia gpus are um it's 1,200 Watts for the GPU but then it actually ends up being like 2,000 Watts all in right like so there's a little bit of scaling of power per GPU but like like you already have 100K cluster right uh openi in Arizona xai in uh Memphis and many others already building 100K clusters of h100s you have multiple at least five I believe gb200 100K clusters being built by Microsoft sloping eyes Partners uh for them um and then and then potentially even more 500k GB 200s right is a gwatt right and that's like online next year right and like the year after that if you aggregate all the data center sites and like how much power and you only look at net ads since 2022 instead of like the total capacity at each data center then you're still like north of multi- gwatt right so um and so they're spending 10 North 10 plus billion dollars on these fiber deals with a few fiber companies Lumen Zoo like you know a couple other companies and then they've got all these data centers that they're clearly building 100K clusters on right like old crypto mining site uh with cor weave in Texas or like this Oracle cruso in Texas and then like in Wisconsin and Ariz and and and you know a couple other places there's a lot of data centers being built up uh you know and and and providers right qts and Cooper and like you know you go down the list there's like so many different provid and self-build right data centers I'm building myself so so um uh G let just like give the number on like okay 2025 elon's cluster is going to be the big like it doesn't matter who it is so so then there's a definition game right like Elon claims he has the largest cluster at 100K gpus because they're all fully connected than who it is like I just want to know like how how many like I don't know if it's better to denominate and 100,000 gpus this year right for big for the biggest cluster for the biggest cluster next year next year 300 to 500,000 um depending on whether it's one side or many right 300 to like 700,000 I think is the upper bound of that but anyways like you know there's it's it's it's about like when they teer it on when they can connect them when the fibers connected together anyways 3 300 to like 7 500,000 let's say but those gpus are 2 to 3x faster right versus the 100K cluster so on an h100 equivalent basis you're at a million chips next year one cluster by the end of the year yes no no no well so one cluster is like mean the wishy-washy definition right multi- sight right can you do multi sight what's the efficiency loss when you go multi sight um is it possible at all I truly believe so what is it whether it's uh whether what's the efficiency loss is a question right okay it would it be like 20% loss 50% loss great question this is where like you know this is where you need like the Secrets right of like and anthropics got similar plans with Amazon and you go down the list right like people and then and then the year after that the year after that is where this is 20126 2026 there is a single gigawatt site let and that's just part of the like multiple sites right for for Microsoft the Microsoft 5 gwatt thing happens in 20 one gwatt one site in in 2026 but then you have you know a number of other uh you have five different locations each with multip some with multiple sites some with single site you you're easily north of 2 3 gaw um and then the question is can you start using the old chips with the new chips and like the scaling I think is like you're going to continue to see flop scaling like much faster than people expect I think as long as the money pours in right like that's the other thing is like there's no [ __ ] way you can pay for the scale of clusters that are being planned to be built next year for open AI unless they raise like 50 to 100 billion um which I think they will raise that like end of this year early next year 50 to 100 billion are you kidding me no oh my God this is like s you know like Sam has a superpower no like it's like it's like recruiting and like raising money that's like what he's like a got at