好那我們就開始來上課吧 今天這堂課啊 是要來一堂課搞懂機器學習和深度學習的基本概念 到目前為止呢 我們已經告訴大家說生成式人工智慧的基本原理 就是我們有一個函式 這個函式呢會把未完成的句子當作輸入 它會輸出下一個 Token 那我們把這個函式叫做 F 未完成的句子叫做 X 輸出就是 F of X 那在過去的課程裡面 我們花了很多時間講說這個 F 呢 它是怎麼運作的 我們也跟大家剖析 F 內部的結構長什麼樣子 但我們一直沒有講 F 是怎麼被找出來的 那今天我們就是要講這個生成式人工智慧裡面最核心的這個 F 可以輸入未完成的句子 預測下一個 Token 的這個 F 是怎麼透過資料被尋找出來的 那透過資料找出一個函式的技術 就統稱為機器學習 Machine Learning 那今天這堂課呢 會分成上下兩部分 我們會先講機器學習的原理 那接下來呢還是會有一些實作 讓大家更能夠瞭解上課所講的內容 好那我們就從原理開始講起 雖然說在生成式 AI 裡面呢 我們要找的函式是一個語言模型 輸入一句話輸出下一個 Token 的函式 但是實際上同樣的技術 可以用在各式各樣的地方 我們可以用同樣的技術找出各式各樣的函式 所以在今天這堂課裡面 我並不打算拿語言模型當作例子 我們換一個例子 用一些跟語言模型沒有半毛錢關係的例子 讓你知道說機器學習這種找函式的技術 它是可以用在各式各樣的領域的 那今天要找什麼樣的函式呢 今天要找的是一個莫名其妙的函式 這個莫名其妙的函式它輸入呢 是李宏毅老師做的投影片 所以我們的 X 是一份投影片 Y 是什麼 Y 是根據這份投影片 李宏毅老師如果要上課的話 他會講多久 輸出 F of X 是一個數字 這個數字代表了這堂課的長度 那像這種啊 輸入一個東西 輸出一個數字的這種任務 就假設你要找的函式 它的輸出是一個數字 那這種任務呢 在機器學習裡面叫做 Regression 那這個函式有什麼用呢 這個函式可以回答一個非常關鍵的問題 這個關鍵的問題 我相信在上課的時候 會時時刻刻問自己的 就是老師什麼時候要下課 所以我們今天有了這個函式之後 你就可以把現在這份投影片丟進這個函式 這個函式就會告訴你說 這堂課呢 老師大概會講多久 好那怎麼找一個函式呢 機器學習找函式的步驟啊 我們這邊寫做 3 加 1 我們先講前面的三個步驟是什麼 等一下再講加 1 指的是什麼 好那找函式呢 基本上是三個步驟 第一個步驟是 你要問 我要找什麼樣的函式 然後第二個步驟是 我現在可以選擇的函式有哪些 那有了第一步跟第二步之後 你就可以進入第三步 根據第一步跟第二步 找做第三步 第三步就是找出一個最好的函式 那第一步跟第二步呢 他們並沒有先後關係 那我在過去的課程裡面 通常是先講我有哪些選擇 才講我要找什麼 那助教在講解作業的時候 也是先講我有哪些選擇 再講我要找什麼 不過後來我發現說 先講我要找什麼 這步驟可能比較順一點 所以我們這邊是 先說我們要找什麼 再說我們有哪些選擇 有了一跟二之後 再進入第三步 選一個最好的函式 好第一步 第一步是你要先訂好目標 知道你到底要找什麼樣的函式 好那步驟一、步驟二、步驟三 這三步驟合起來 就是學習、learning 或者又叫訓練、training 好那步驟一是我要找什麼 你要知道說給我一個函式 F 它到底是不是我要的 我們還不知道要怎麼找一個 F 出來 但是假設有人給我一個 F 我要能夠去評估說這個 F 到底是不是我要找的 講到評估這件事 大家記不記得在上週的課程裡面 我們已經講了 怎麼評估一個生成式人工智慧 我們已經講了 生成式人工智慧的能力檢定 然後我們說這個能力檢定 評估一個模型做得有多好的方式 通常是這個樣子的 你準備一些輸入丟給模型 然後它給你一些輸出 你有標準答案 你會去計算輸出跟標準答案的 某種距離或某種相似度 那把這些距離或相似度 全部平均起來得到一個數值 那我們叫做 Evaluation Metric 這個數值就代表這個函式、這個模型 它的表現有多好 事實上在第一個步驟裡面 我們要做的事情 跟 Evaluation 可以說是一樣的 這就是為什麼在這一門課裡面 我們是先講 Evaluation 才進入機器學習的原理 我們現在終於要進入這個課程裡面 比較深水區的地方 我們要來講人工智慧是怎麼被訓練的 但是在人工智慧訓練的三個步驟中 第一個步驟其實就是 Evaluation 所以這就是為什麼我們先講 Evaluation 才講機器學習的原理 好,那怎麼知道一個函式好不好呢 這邊就根據我們剛才講的 預測一段投影片要講多長這個任務來做說明 我們有一些老師過去上課的投影片 比如說機器學習 2021 的投影片 我們也知道根據這份投影片 老師會講了多久 你就上 YouTube 頻道去看一下 每堂課講多久 那你就知道每堂課講了多長 那我們現在呢 有了這些投影片之後 把這些投影片丟進某一個函數 F 就有人隨便給我一個函數 F 這個函數 F 是可以把投影片當作輸入的 但我們等一下會再講說 把投影片當作輸入是怎麼回事 假設這個函式 F 就是可以把投影片當作輸入 它的輸出就是一個數值 它代表說這個函式預測這份投影片 李宏毅老師會講的時間的長度 有了這個預測的時長 我們就可以去計算跟真實的正確答案的差距 比如說給機器學習 2021 第一堂課投影片 實際上上課時長是 100 分鐘 這個 F 輸出 99 分鐘 那它的差距就是 99-100 我們這邊取平方等於 1 那對於其他課程你都可以做一樣的事情 可以計算預測時長跟上課時長之間差距的平方 然後呢我們再把這些差距的平方呢 把它平均起來 那這個平均後的數值就代表了這個函數的好壞 代表它預測的有多精準 那這邊因為我們算的是距離 所以這個數值越小 代表這個函式它的預測是越精準的 那如果你今天啊 定出來的這個數值是越小越好 那像我們這邊的例子就是越小越好 那這個數值呢就被叫做 Loss 或者是叫做 Cost 那如果這個數值定出來是越大越好的話 那它就被叫做 Objective 不過越小越好跟越大越好 跟等一下的說明都是沒有影響的 那在以下的課程裡面 我們會假設我們用的是 Loss 也就是說我們計算的是某一種距離 這個距離是越小越好 這個距離的數值越小 代表這個 F 它是一個越好的 F 那我們這邊計算的是預測的時長 跟真正的上課時長之間差距的平方 那這個差距的平方呢 它叫做 Mean Squared Error 它的縮寫是 MSE 那等一下在課程的後面講到 Loss 的時候 我們就會直接說我們的 Loss 是 Mean Squared Error 我們的 Loss 是 MSE 那這個 Loss 就是定義了一個函式的好壞 那這個過程啊幾乎跟 Evaluation 是一模一樣的 那你可以想說 所以 Loss 就等於是 Evaluation Metric 喔 那有時候 Evaluation Metric 是越大越好 所以你也可以說 Objective 就等於是 Evaluation Metric 喔 這個問題啊 是也不是 就是在今天的這一堂課裡面 你可以想成 Evaluation Metric 就是我們的 Loss 或者是 Objective 但是其實這兩者之間有可能會有微妙的區別 那在這一堂課裡面 你先假設兩者其實就是一樣的 那在下週的課程 我會再跟大家講說這兩者什麼時候有可能會是不一樣的 總之這邊我們就說 算 Loss 的過程其實跟 Evaluation 是一樣的 好那今天呢我們要計算出這個 Loss 我們就需要有一些資料 這些資料讓我們可以把這個 Loss 計算出來 那這些資料定義了這個 Loss 長什麼樣子 那這些定義 Loss 長什麼樣子的資料 這些輸入的投影片還有輸出的 還有這個真正的上課時長合起來呢 就叫做訓練資料 叫做 Training Data 所以拿來定義 Loss 的這些資料 就是所謂的 Training Data 就是所謂的訓練資料 好那我們現在走完第一步了 我們知道怎麼定義出 Loss 的 接下來我們進入第二步 第二步是我們有哪些選擇 我們有哪些函式是我們可以選的 我們要定出一個候選的函式集合 那怎麼定這個候選的函式集合呢 我們先想看看喔 現在的函式的輸入是一個投影片 我們用 x 來表示 但是函式的輸入只能是數字啊 我們沒有辦法直接把投影片當作一個函式的輸入 所以我們需要把一頁投影片表示成一些數字 這些數字才可以當作是某一個函式的輸入 比如說我們可以說投影片 x 它裡面的頁數 頁數總是一個數字啦 頁數叫做 x1 那這個投影片裡面總共的字數叫做 x2 那這個投影片標題的長度叫 x3 這個投影片裡面有沒有提到 learning 這個字叫做 x4 那 x4 就是只有 0 跟 1 兩個可能 0 就代表沒有提到 learning 1 就代表有提到 learning 那你可以把一份投影片表示成一串數值 這些數值有機會作為函式的輸入 那把一個東西表示成一些數值 這些數值要當作函式的輸入 這些數值就叫做 Feature 那我們在這份投影片會用下標來代表一個完整的東西的一部分啦 所以 x 是一個投影片 x1 代表的是投影片裡面的頁數 希望你可以瞭解這個符號的用法 好那這個函式 F 應該長什麼樣子呢 我們這邊呢先做一個直覺的假設 我們假設輸入的 x 跟輸出長度的這個關係 這邊長度的單位我們可能就分中作為單位吧 今天輸入跟輸出的關係有這樣子的一個關係 這個 x1 也就是投影片的頁數 乘上 w1 加 b 就會等於輸出上課的長度 y 為什麼直覺可以這樣想呢 因為你想想看課程的長度應該跟投影片的頁數乘某種比例關係 投影片越長顯然這一堂課就會講得越久 這是非常直覺的 所以呢最後課程的長度 y 應該要跟輸入的投影片頁數 x1 乘正比 我們只是不知道這個比例的關係到底有多大 到底老師平均是一頁投影片講一分鐘還是兩分鐘還是三分鐘 我們不知道 所以我們先保留意見 先不說這個 w1 的數值應該是多少 但我們知道 x1 跟 y 有這樣一個乘上 w1 的關係 那這邊呢再多加了一個數字叫做 b 那可能老師每次上課前面都會有一個開場 所以稍微多了一點時間 所以投影片的頁數乘上 w1 以後 可能需要固定加一些額外的時間 所以這邊加了一下叫做 b 那這個 w1 跟 b 的數值呢我們是不知道的 在一個函式裡面數值我們不知道的這一些東西啊 我們就叫它參數 它的英文呢是 parameter 那現在呢如果我們做的是 regression 的問題 那我們把我們的函式寫成 y 等於 w1x1 加 b 那這種 regression 呢叫做 linear 的 regression 那這個 y 等於 w1x1 加 b 啊 當我們寫出這個式子的時候 它其實真正代表的是一個函數的集合 因為 w1 跟 b 呢它的數值是未知的 這些參數的數值是未知的 所以這個式子其實代表的是一個函式的集合 那這個函式的集合我們可以怎麼樣來描述它呢 你可以想成有一個二維的平面 這個二維平面的橫軸代表 w1 的不同數值 它的縱軸代表 b 的不同數值 那這個平面上的每一個點都是一個函數 比如說這個點代表的是 w1 為 -1 b 是 0.5 這個 w1 代 -1 b 代 0.5 的時候的那一個函式 在這邊是 w1 等於 1 b 等於 0.5 的那個函式 這個點代表的是 w1 等於 0.5 b 等於 -0.4 這個函式 那這個二維平面上每一個點都是一個函式 這些點全部集合起來就是一個函式的集合 那這個就是我們劃定的要來搜尋最好的函式的範圍 那你可能會想說為什麼我們是劃定這樣一個範圍呢 為什麼我們是做 linear regression 呢 為什麼 y 等於 w1x1 加 b 呢 為什麼不能長其他樣子呢 它當然可以長其他樣子 這個函數的範圍是你人類去劃定的 所以你完全可以把函式的範圍劃成別的樣子 你可以說那我要 y 等於 w1x1 加 w2x2 加 b 我要把這個投影片的總字數也考慮進來 這樣可不可以呢 當然可以 你要把標題長度考慮進來也可以 雖然我覺得標題長度應該跟 y 沒有什麼關係吧 應該跟上課時長沒有什麼關係吧 你也可以設計非常複雜的函式集合 比如說你覺得呢這個 x2 跟 x4 應該有某種關係 當這個課程裡面沒有提到 learning 這個字的時候 總字數就不重要 有提到 learning 這個字的時候總字數就要被考慮 所以 x2 應該乘 x4 那是不是這樣呢 不知道感覺應該不太可能是這樣 然後跟標題並不是成正比 而是跟標題長度的平方成正比 有可能是這樣子嗎 你可以定各式各樣的函式的集合 然後裡面有一些未知的參數 然後我們在第三步會把這些未知的參數找出來 那至於為什麼會定 y 等於 xw1x1 加 b 呢 那其實是出於我們人類對這個任務的理解 也就是我們人類的 domain knowledge 那其實在做機器學習的時候 如果你想把人類對這個任務對這個問題的理解 放到訓練的過程中 一個最合適的階段就是在第二步 把它放到你定的函式集合裡面 因為我們認為頁數應該跟長度 成正相關 所以我們定出了這麼一個式子 那事實上我自己在準備投影片的時候 我確實相信頁數跟長度是非常有關係的 我通常會假設一頁投影片大概是講一分鐘左右 那我看著投影片就可以大概知道說 這堂課會講多長 所以我相信 y 等於 w1x1 加 b 應該可以非常好的描述 x 跟 y 的關係 但是這就是出於人類對這個問題的理解啦 那如果你對這個問題有不一樣的理解 你可以找不一樣的函式的範圍 那這個人類對問題的理解 其實就叫做模型 叫做 model 這個函式的範圍其實就是所謂的模型 那實際上模型這個詞彙被用得非常地氾濫 有時候模型指的是一個函數的範圍 一個函數裡面有未知數 有時候它指的是一個函式本身 指的是單一個函式 這邊用詞在機器學習領域 model 這個詞彙的用詞 是蠻混亂的 不過很多時候模型指的就是一個函式的範圍 其實就是所謂的模型 你從模型這個字也可以大概看出來它的意思 它指的就是真實的世界非常複雜 但我有一個比較簡單的方法來理解它 這個就是模型 所以這個人類定出來的函式也是一樣 也許 x 跟 y 之間的關係非常複雜 但是人類相信用這個函式 其實應該在某種程度上可以精確地描述 x 跟 y 之間的關係 好那我們現在走了第二步 第二步我們把人類對問題的理解 放入了學習的過程中 有了第一步跟第二步之後 我們就進入了第三步 第三步就是 我們要找一個 loss 最低的函式 我們已經定出了怎麼算 loss 我們已經定出了搜尋的範圍 接下來就是在我們定出了搜尋範圍裡面 找一個 loss 最低的函式 那怎麼在我們定的搜尋範圍內 找一個 loss 最低的函式呢 在做這件事之前 我們先把 loss 的數學式 先把它寫出來 loss 的數學式長什麼樣子呢 假設呢我們現在 把 2021 年機器學習這門課的投影片 它的頁數呢都統計出來 X 下標 1 代表一份投影片裡面的頁數 那我用上標呢 來代表是第幾堂課 那我們有 n 堂課 第一堂第二堂到第 n 堂 那在這一堂課裡面呢 上標代表的是不同的東西 我們有多個類似的東西 我們用上標呢來表示 那這個符號要怎麼用呢 都是看個人啦 我這邊就是告訴你我符號的用法 讓你接下來聽課的時候 可以更容易一點 好那我們呢就把 我們這邊還有上課的實際時長 那我們這邊呢用 hat 來代表說它是一個實際的東西 不是模型預測的東西 那有人可能會覺得說 模型預測的東西應該要用 hat 你可能有在教科書上看過這樣的寫法 其實都可以啦 這個符號呢要怎麼用 都是看個人高興 你只要讓其他人能夠看得懂就好了 首先就告訴你說 真正的東西真正的答案 我們就是加一個上標 hat 來代表真正的答案 好那我們現在呢 把這個 X 下標 1 上標 1 X 下標 1 上標 2 X 下標 1 上標 n 通通丟到 F 裡面 得到預測的時長 這邊用 Y1 Y2 Yn 來表示 接下來我們會計算 Y1 跟 Y1 hat 的平方 Y2 跟 Y2 hat 的平方 Yn 跟 Yn hat 的平方 我們把這些距離平均起來 就是我們的 loss 所以我們的 loss 寫做 n 分之 1 summation i 等於 1 到 n Yi 減 Yi hat 的平方 但我們又同時知道說這個 F 啊 它的長項一定寫做 Y 等於 W1 X1 加 B 所以我們可以把這個 Yi 代換掉 我們可以把這個 Yi 代換成 W1 X 上標 i 下標 1 加 B 這個 Yi 等於 X 上標 i 下標 1 乘上 W 加 B 那我們把這個項呢 代入 Yi 裡面 這個就是 loss 完整的式子 所以我們知道啊 其實 loss 跟 W1 和 B 是有關係的 你選擇了不同的 W1 跟 B 你就有不同的函式 你就算出不同的 loss 所以 loss 本身呢 你也可以把它看作是一個函式 它是什麼函式 它是你參數的函式 當你選擇不同函數的時候 不同參數的時候 那你算出來的 loss 就會不一樣 那我們現在的目標呢 是選一個最好的 也就是 loss 最低的函式 這句話其實就等同於 投影片中的這一個式子 我們要找到 這個式子什麼意思呢 這個式子就是 我們要找到一組 W1 跟 B 這組 W1 跟 B 呢 代到 loss 這個函式以後 可以讓 loss 這個函式的值最小 那這個可以讓 loss 函式值最小的 W1 跟 B 呢 我們給它一個特別的名字 叫 W1 上標 star 還有 B 上標 star 好那這個式子啊 就是我們現在在步驟三呢 真正要解的問題了 那這種找一組參數 讓某一個目標函數 讓某一個 loss 最大或者是最小 這種事情呢 就要做 optimization 所以這是一個 optimization 的問題 是我們要來解的問題 那怎麼解這個數學式呢 假設你對 optimization 一無所知 那你至少有一個最糟最暴力的做法 就是暴力算出所有候選函式的 loss 也就是 Mean Squared Error (MSE) 我們就暴搜所有可能的 W1 跟暴搜所有可能的 B 暴搜所有 W1 跟 B 的組合 一個一個帶進這個 L 裡面 看哪一組 W1 跟 B 可以讓 L 最小 那我們就成功解了 這個 optimization 的問題了 好所以這個暴力的方法呢 在這個題目裡面還是可以勉強可以用的 因為我們只有兩個未知的參數 但是這兩個未知的參數 它的範圍可以從負無限大到正無限大 要從負無限大到正無限大都暴搜是不可能的 所以我們得設定一個範圍 W1 我們就從 0 一直搜尋到 3 B 呢我們就從 0 一直搜尋到 20 那這邊當然需要猜測 這個 W1 跟 B 可能會有的範圍了 這邊就是根據人對這個問題的理解 猜測一個範圍 好那我們就窮舉這個 W1 跟 B 在這兩個範圍間的所有的組合 每一個組合都算出它的 loss 就是在這一個三維圖的 Z 軸的這個方向 你就可以得到這樣的一個曲線 我們就可以知道說 欸看起來對 W 而言呢 它對 loss 的影響非常的大 但如果在 B 這個方向上 看起來對 loss 的影響呢就比較小 那像這種圖啊 我們常常會畫成 loss 的等高線圖 畫三維圖比較麻煩 所以常常畫成二維的圖 然後第三維就用顏色用等高線來表示 那像這種圖呢叫做 loss 的 surface 叫 loss surface 好在這張圖上呢 橫軸是不同的 W1 縱軸是不同的 B 然後顏色比較淺的 代表的是 loss 比較大的區域 顏色比較深的 代表是 loss 比較小的區域 那在暴搜所有的值之後發現呢 如果 W1 設 1.67 然後 B 設 4.85 那我們可以得到最低的 loss 就這個圖上紅色點的這個位置 那在這個圖上你就可以看出說 假設你是改變 W1 看起來對 loss 的影響比較大 那這個呢非常符合直覺 因為我們這個把 W1 呢乘上 X1 然後呢今天這個 W1 如果有變化的話 那應該對 Y 的影響呢是比較劇烈的 然後呢你會發現這個 W1 算出來是 1.67 那就代表說呢 平均一頁投影片呢 我會講 1.67 分鐘 有了這個結果以後才知道 原來我一頁投影片平均講 1.67 分鐘啊 然後呢 B 呢是一個額外的數值 這個 B 呢它是 4.85 那它對結果的影響呢就小很多 但是也不是完全沒有影響的 另外你會發現說呢 W 跟 B 之間也是有一些 互相影響的關係的 你看這邊這個曲線 它是有一點斜的 也就是這個 loss 特別小的地方 其實同時要考慮 W1 也是要同時考慮 B 它是一個有點斜的這個範圍 好總之我們在這一個特別的問題上 特別簡單的問題上 可以暴力窮舉出所有的候選函式 把他們 loss 都計算出來 選一個最小的 那有的同學呢可能會想說 這個問題這個 optimization 的問題 應該有 closed-form solution 吧 如果今天我們用的 loss 是 MSE 如果我們的函式集合是寫成這樣 也就是我們在做 linear regression 事實上這個 optimization 的問題 是有 closed-form solution 的 簡單來說就是有公式解啦 如果你修過線性代數的話 這個題目有公式解 直接代完公式就結束了 好但是因為我們現在講的是機器學習 我們要考慮的 loss 可能是更複雜的 我們只是在這堂課裡面 舉一個簡單的例子告訴你說 這個是我們 optimization 的對象 但是實際上在真實的情境下 你的函式的集合可能寫得更複雜 你的 loss 可能不是 MSE 在這個情況下 我們要怎麼做 optimization 呢 所以等一下要講的是一個更通用的做法 這個通用的做法 幾乎適用於所有的 loss 跟所有的 model 這個做法叫做 gradient descent 它的中文通常翻成梯度下降法 那這個方法是怎麼運作的呢 現在我們為了要簡化我們的討論 我們先假設我們只考慮一個參數 W1 雖然我們實際上有兩個參數 但我們先假設我們只考慮參數 W1 當我們改變 W1 的時候 我們的 loss 顯然會跟著改變 我們把 W1 的 error surface 它是一個一維的東西 我們把它畫在這個投影片上面 gradient descent 這個方法 它的精神就是隨便找一個起始點 隨便找一個點開始 那這邊畫一個二郎 因為餓狼下坡 大家知道餓狼下坡第三季一拳超人 最知名的經典畫面 餓狼下坡 所以講到梯度下降法 我們就用餓狼來當作代表一個模型 代表一組參數 那我們先隨便從一個 W1 的數值開始 你就隨機選一個 W1 的數值 這邊寫成 W1 上標 0 從這個位置開始 那這個位置呢 你就往左右各踏一小步 往左邊踏一小步 往右邊踏一小步 看往哪一邊可以讓 loss 下降 那在這個例子裡面呢 往右踏一步 可以讓 loss 下降 如果往右踏一步可以讓 loss 下降 那就往右真的踏出一大步 那之前只是踏一小步作為試探 試探完之後呢 就踏一大步 然後就從 W0 的地方跳到 W1 的地方 我們就到了一個新的位置 那在這個新的位置呢 就重複剛才的步驟 左右各踏一小步作為試探 發現說還是往右可以讓 loss 變小 就再往右再踏出一大步 這個步驟就一直反覆持續下去 什麼時候會停止呢 當你發現往左往右 都不會讓 l 變小的時候 比如說你走到這個地方 這個地方呢 是一個山谷中的凹點 所以往左邊踏一小步 往右邊踏一小步 發現 loss 都只會變大不會變小 那就無路可走了 就停下來 那從這個例子裡面 你也可以很明顯的看出 梯度下降法的劣勢 因為在這個例子裡面 loss 最低的點顯然在這個地方 loss 最低的點 我們叫做 global 的 minimum 這才是我們真正的目標 但是二郎呢 他只從這個坡呢 這樣子滑下來 他是走不到這個地方的 因為當他走到這個點的時候 往左往右都不會讓 loss 變小 他就會停下來了 那這種往左往右 loss 都不會再變小 讓訓練讓 gradient descent 停下來的點呢 叫做 local 的 minimum 好那這邊呢 是用比較科普的方法來講一下 gradient descent 實際上你並不會真的往左踏一步 往右踏一步這樣太麻煩了 怎麼估算出往左往右 哪一步會讓 loss 變小呢 你要去計算在這一個點 對這個 loss 所形成的曲線的 切線斜率 如果你計算出這個切線斜率以後 接下來如果你發現 切線的斜率是負的 也就是左邊高右邊低 那就代表右邊 loss 比較小 就往右走 那如果算出來 切線斜率是正的 左邊低右邊高 那 那就是往左走 因為左邊的 loss 是比較小的 所以實際上你真正做的事情 並不是往左往右各踏一小步 而是每次呢 你在每一個位置 就會計算切線斜率 然後決定要往哪裡走 計算切線斜率 再決定要往哪裡走 再計算切線斜率 再決定要往哪裡走 當你算出來切線斜率是零的時候 那代表無路可走了 那就停下來 這個就是 gradient descent 那怎麼計算切線斜率呢 那我告訴你 切線斜率 就是你現在考慮的這個參數 W1 對你的 loss L 的 偏微分 所以你知道怎麼計算這一項 你就知道 怎麼計算切線斜率 這一項就是切線斜率 那你說這個微積分我都忘了 這個我要怎麼計算這一項呢 那我告訴你 就算你微積分忘了 也都不用擔心 因為現在這種計算切線斜率這件事 沒人類什麼事情 深度學習的套件 PyTorch、TensorFlow 通常都能夠自動算切線斜率 所以你在使用這些深度學習套件的時候 那到時候那個作業裡面呢 助教也會跟大家講 怎麼用這些深度學習的套件 當你在使用這些深度學習套件的時候 你就是把第一步 loss 定好 第二步函式的範圍選好 接下來用 gradient descent 去找最好的那個 function 這個步驟裡面如果要計算切線斜率的話 在這些深度學習套件裡面 就都是一行程式碼而已 你完全不需要去煩惱 怎麼計算斜率這件事情 這些套件都是能夠自動計算切線斜率的 好所以實際上整個運作過程是這個樣子的 好我們在 W0 這個位置 我們在 W 上標 0 這個位置 我們在 W1 等於 W 上標 0 下標 1 這個地方 我們會計算 W1 對 L 的切線斜率 計算出來以後 計算出這個偏微分也就得到切線斜率之後 然後我們就會往右走一步 那再來就是這一大步到底要踏多大步呢 這一步的步伐是等於兩個東西相乘 第一個東西叫做 learning rate 第二個東西就是我們算出來的偏微分的結果 也就是切線斜率 也就是說切線斜率越大 也就是這個山坡越陡 我們就踏越大步 如果很平滑就踏小步一點 那我會定一個 learning rate 這個 learning rate 代表學習的速度 從它的名字上就可以看出來 它就是學習的速度 當你把 learning rate 設大一點的時候 步伐踏大一點學的就比較快 那這個 learning rate 設小一點 步伐就小一點 那這個學習就比較慢 講到這邊你可能會問說 那誰要學習比較慢呢 誰都要學習比較快啊 等下告訴你說學習快有什麼樣的壞處 所以 learning rate 你也是不能夠設太大 好那有 learning rate 有這個切線斜率 決定了這一步要跨多大 那你就從 W 上標 0 下標 1 跨到 W 上標 1 下標 1 然後在這個位置 你再計算 W1 在這個位置 對 L 的切線斜率 然後再往右踏一步 到了 W 上標 2 下標 1 那你再計算這個位置 W1 對 L 的切線斜率 就這樣反覆這個步驟 直到切線斜率算出來接近 0 為止 好那這邊呢 我們實際上需要考慮的有兩個參數啦 不是只有 W1 我們還得考慮 B 所以實際上 整個訓練方法運作起來是這個樣子的 首先呢 先隨便找一個 W1 跟 B 呢 作為起始 那這個起始的數值呢 我們都在右上角標 0 代表是起始的數值 隨機找一個起始的數值 隨便找個地方開始 好然後就接下來就計算 這個在 W1 上標 0 跟 B0 的這個位置 W1 對 L 的偏微分 還有 W 上標 0 下標 1 還有 B0 的這個位置 B 對 L 的偏微分 那這些偏微分集合起來啊 它有一個名字就叫做 gradient 這就是 gradient descent 裡面的 gradient 那就像我剛才說過的 你不用煩惱你不會算 gradient 在這個 deep learning 套件 比如說 PyTorch 裡面 計算 gradient 就是一行指令而已 那之後大家可以在作業裡面看到 好的計算出這個 gradient 之後 然後你就要去更新我們的參數 所以 W 上標 0 下標 1 就會加上 learning rate 就會減掉 learning rate 乘上 gradient 然後就更新成 W 上標 1 下標 1 然後 B 上標 0 就會減掉 learning rate 乘上 gradient 裡面的這個數值 然後就更新成 B1 然後這個步驟就反覆繼續下去 你有 W 上標 1 B 上標 1 以後 你會再去計算 gradient 然後再更新這些參數 就反覆持續下去 那這邊是考慮只有兩個參數的情況 那我們可以考慮更通用的情況 比如說在一個語言模型裡面 通常你的參數量不是 5 個 10 個 而是 10 億個 100 億個那麼多 那這些大量的參數 我們可以把它全部集合起來 用個 Theta 來表示它 Theta 是一個非常巨大的向量 它的每一個維度就代表了一個參數 那你現在要解的問題 當我們用 gradient descent 來 optimize 這個 L of Theta 找到一個 Theta 可以讓 L of Theta 最小值的時候 你用 gradient descent 的方法 跟前面只有兩個參數 其實是沒有什麼太大的差別的 我們就把我們剛才看過的東西 再重複講一次 我們就先從 Theta 上標 0 開始 我們就隨機選一個 Theta 我們把它叫做 Theta 上標 0 然後去計算 每一個參數 Theta 1 Theta 2 Theta 3 在 Theta 上標 0 這個位置 對大 L 的偏微分 把這些數值全部集合起來 其實這個向量 這個向量叫做 gradient 那這個 gradient 我們這邊表示成 G 上標 0 然後接下來 你計算出這些數值之後 再把 Theta 上標 0 下標 1 Theta 上標 0 下標 2 減掉這些 gradient 的數值 乘上 learning rate 然後你就可以更新你的參數 用這個方法 你就可以反覆的更新你的參數 那計算 gradient 這個式子 寫起來非常的複雜 所以在檔上 你常常看到寫法是 直接用一個倒三角形 當做 gradient 所以當有人說 他現在在計算倒三角形 L of Theta 0 的時候 意思就是他在計算 代表 gradient 的這個向量 得到 G0 就是代表 gradient 的向量 那我們有這個 G0 以後 把這個 G0 乘上 learning rate 然後再把 Theta 0 減掉 learning rate 乘上 G0 就得到更新後的參數 叫做 Theta 1 然後你就會反覆這個步驟 所以從 Theta 0 開始 然後計算出 gradient 然後把 gradient 乘上 learning rate 把 Theta 0 減掉 gradient 乘上 learning rate 得到 Theta 1 然後這每更新一次參數 就叫做一個 iteration 或叫一次 update 然後現在新的參數變成 Theta 1 所以在 Theta 1 這個地方 再計算一次 gradient 得到 G1 然後再把 Theta 1 減掉 G1 乘上 learning rate 得到 Theta 2 然後在 Theta 2 這個地方 再計算一次 gradient 得到 G2 就反覆這個步驟 把 Theta 2 減掉 G2 乘上 learning rate 然後更新成 Theta 3 所以我們就從隨便一個地方開始 然後計算 gradient 更新 然後祈禱最後 你可以找到一個很不錯的 Theta 它可以讓 loss 的值很低 這個概念講起來也是很簡單 但我告訴你實際上做起來其實也沒那麼容易 我們就拿剛才的只有兩個參數的例子 來做一下 gradient descent 吧 我們現在初始參數是 w_1 設定成 1 w 呢設定成 15 我們從這個地方開始走起 這個是我們初始的位置 那你就在這個初始的位置計算一下 gradient 順著 gradient 的方向走一步出去 那我們的 learning rate 呢 我們設 0.001 吧 就隨便設個值 那踏一步 哇這個一步走這麼遠 一步走很遠 這個 learning rate 其實 0.001 在這個問題裡面算是蠻大的 所以一步從這邊跨過整個山谷就踏到右邊去了 一步走太大會有什麼樣的壞處呢 一步走太大 你再接下來下一步你就會看到問題了 如果再走一步會發生什麼事 再走一步 哇就跑出這個地圖外了 你就會發現說這個模型的參數呢 開始有巨大的變化 很快的你就會看到 NaN 整個訓練就壞掉了 根本沒有辦法訓練出任何東西來 所以 learning rate 不能設太大 learning rate 設太大 你可能會跑得太遠 然後就再也回不來了 所以怎麼辦 你不能那麼急功近利 我們把 learning rate 設小一點 變成 0.001 好設 0.001 那走兩步 我們現在離這個穀底比較接近了 走兩步現在是不夠的 走個 100 步 走到這裡卡住了 這邊的這個微分 這邊的這個 gradient 已經非常小了 所以走的非常的慢 尤其是我們 learning rate 設的比較小 所以現在走的非常的慢 100 步剛下到穀底 1000 步剛開始轉彎 這邊有很多點 這邊有 900 多個點通通都擠在這邊 因為這邊的這個 gradient 已經非常小了 所以走的非常慢 然後你現在是不是開始懷念比較大的 learning rate 但是如果你把 learning rate 調大 又很容易暴走 又很容易突然飛出這個穀外 飛出地圖外又回不來 所以 learning rate 太大不好 太小也不好 是一個很難調的東西 那 1000 顯然是不夠的才走到這邊 那如果設個 10000 可以走到這邊 大幅有不錯大幅的進展 不過離終點還有非常長的距離 終點大概在這個位置左右吧 那當然今天在這個例子裡面 如果單就這個例子而言 因為只有兩個參數 我們可以暴搜所有的 Loss 我們可以把 error surface 畫出來 所以我們等於是開了天眼 開了天眼有很多好處 你知道說走到這邊 Loss 已經夠低了 其實也沒必要再往下走了 走到這邊就好 但等一下你會看到 假設你的參數非常多的時候 你就沒有辦法暴搜所有的參數 你就沒有辦法畫出這種 Loss 的 surface 沒有辦法畫出 Loss surface 那你實際上觀察到的 實際上當你發現說 現在移動非常緩慢的時候 你真正觀察到的是 Loss 幾乎沒有下降 那 Loss 幾乎沒有下降的情況 那到底要停下 比如說你可能已經走到了 一個疑似 local minima 的地方 還是應該要繼續向前 這個時候就非常難決定了 那我們在下週會講更多 跟 optimization 有關的事情 不過在這週我就是告訴你說 這個訓練模型真的是很困難 你很難調出一個非常理想的 learning rate 好那如果你覺得參數更新太慢的話 有一個方法 這個方法是這樣 為什麼有時候我們會覺得參數更新太慢呢 因為每一次你在計算 gradient 的時候 你都是要去計算所有參數 對這個 Loss 的 gradient 對 Loss 的偏微分 而這個 Loss 雖然就算你不知道這個 gradient 要怎麼計算 你在計算出這個 gradient 之前 你得先把這個 Loss 計算出來 而這個 Loss 呢 是你所有資料算出來的 某種 distance 的平均 所以要計算出這個大 L 你得掃過所有的資料 你得對所有的資料進行運算 這可能是一件非常花時間的事情 但在今天我們的這個例子裡面 我們的這個訓練資料沒有多少 才二十幾筆而已 因為 2021 年機器學習也上了二十幾堂課 所以才二十幾筆而已 但是如果你今天在更真實的問題上 比如說訓練一個語言模型 那你的資料可能是海量的 網路上爬下來的資料 那你要對所有的資料去計算出這個大 L 可能是根本不可行的 所以怎麼辦呢 在實作上有一個更常用的方法 就是迫不及待的更新參數 什麼意思呢 假設你所有的資料有大 N 筆 我們先把資料呢 切成一個一個的區域 那每一個區域呢 叫做一個 batch 那每個 batch 裡面有 B 筆資料 當然這個大 B 呢 它的數字會比大 N 小了 我們就把大 N 的資料分成一個一個 batch 那本來用大 N 你可以算出真正的 Loss 叫做大 L 但我們現在沒有辦法真的算大 L 因為算大 L 實在是太耗時間了 尤其假設大 N 代表整個網路資料的話 你甚至根本不可能算出大 L 所以我們只拿每一個 batch 裡面的資料 來算出我們的 Loss 第一個 batch 算出來的 Loss 我們就叫做 L_1 我們算出 L_1 之後 就拿 L_1 去算出 gradient 然後我們就拿 L_1 算出來 gradient 去更新參數 接下來再拿第二個 batch 出來 我們得到了第二個 batch 的資料給我們的 L_2 我們對 L_2 去計算 gradient 然後再用 L_2 算出來 gradient 去更新參數 然後再拿第三個 batch 出來 依此類推 那這樣子我們每次在更新的時候 就不是看了所有的資料才更新 而是每看完一個 batch 用一個 batch 的資料就可以更新一次 那在做機器學習的時候 尤其在訓練模型的時候 有一個常常出現的詞彙叫做 Epoch Epoch 的意思就是 當我們把資料裡面所有的 batch 都看過一次的時候 就叫做一個 Epoch 那在一個 Epoch 裡面 其實參數會被更新很多次 參數會被更新幾次呢 那就取決於大 N 跟 B 的比例 參數會在一個 Epoch 裡面 參數會被更新 N / B 次 那今天假設我們有一千筆資料 那你的大 B 是 100 每個 batch 有 100 資料 那在一個 Epoch 裡面 你的模型就是被更新了 10 次 那用 batch 有什麼樣的好處跟壞處呢 我們剛才看到說 我們的訓練其實是有點緩慢的 今天假設我們說 我們的 Epoch 的數目設定為 100 假設所有的資料都在同個 batch 裡面 也就是沒有做這種切 batch 的概念 如果沒有切 batch 通常又叫做 Full Batch 在 Full Batch 的 case 100 個 Epoch 只夠我們從初始的位置 剛好走到山谷下麵 但是如果我們今天把 batch size 設小一點 我 batch size 設 1 那假設我們的資料總共有 20 筆 那一個 Epoch 呢 我們就可以 update 20 次參數 所以本來在 Full Batch 的情況下 你只能夠 update 100 次參數 但是如果 batch size 設 1 那我其實在 100 個 Epoch 裡面 我已經更新了 2000 次參數 假設我們總共有 20 筆資料的話 那有 2000 次參數的更新 那你看我們已經從起點這邊 一路走到星星的這個位置 我把最後走到的位置 加一個黃色星星的符號 我們已經從這裡走到了這裡 我們其實是比 Full Batch 還走得更遙遠 但你一看這個圖 你就可以看出 當我們有用 batch 的時候 到底會發生什麼樣的事情 因為我們今天在計算 gradient 的時候 不是再拿全部的資料來計算 而是只 sample 了一小部分的資料 就進行計算 所以我們今天在 update 參數的時候 我們 update 的方向是會不穩定的 是會不斷改變的 所以就發現說從這邊開始 你的模型的參數 在兩個山壁之間 劇烈劇烈的擺盪 然後到 100 個 Epoch 的時候 大概擺盪到這裡 雖然比 Full Batch 的狀況走得更遠 但是其實離最低的這個穀底 反而有一些偏離 這個就是 batch size 帶來的好處跟壞處 它可以讓你在固定的資料量下 固定的算力的情況下 update 比較多次參數 但是它走得比較不穩定 那 batch size 設成 1 的情況 又叫 Stochastic Gradient Descent 它的縮寫是 SGD 當然這個 Full Batch 跟 Stochastic Gradient Descent SGD 是兩個極端的狀況 你的 batch size 可以設一個不大不小的值 比如說 batch size 設成 5 那 batch size 設大一點 那就不會那麼不穩定 你看這邊在兩筆之間 這個震盪的幅度非常的劇烈 那 batch size 設大一點 每個 batch 資料多一點 它的更新就比較穩定 那同時呢 因為有設 batch size 所以在每一個 Epoch 裡面 還是 update 比較多次參數 所以相較於 Full Batch 還是更新的比較快 那至於 batch size 要設多大才好呢 就現在 batch size 設的太小會有問題 設的太大也有問題 你需要一個不大不小的值 要設多少呢就不好說了 所以總之除了 learning rate 之外 又多了一個可以調的 hyperparameter 這些你沒有辦法透過訓練得到的數值 你需要人手來調 來決定要設多少的數值 就叫 hyperparameter learning rate 是一個 hyperparameter 那這邊我們有 batch size 是另外一個 hyperparameter 那在使用到 batch 這個技術的時候 另外一個需要跟大家提醒的點 就是你需要使用一個叫做 shuffle 的技術 所謂 shuffle 的意思是說 假設在第一個 Epoch 裡面 1 跟 2 組成一個 batch 3、4 組成一個 batch 5、6 一個 batch 7、8 一個 batch 那在下一個 Epoch 裡面 你希望幫他們換組 不要一直是同樣的資料 放在同一個 batch 裡面 比如說第二個 Epoch 裡面 你可能重新打亂 Epoch 裡面的成員 把 5、7 一組 1、4 一組 3、2 一組 8、6 一組 這樣可以避免在訓練的時候 模型反覆看到一樣內容的 batch 可以增加更多的隨機器 讓你在 sample 的時候 sample 出來的結果 它的 distribution 更接近原來用全部資料的 distribution 所以這個 shuffle 是你在訓練的時候 常常會用到的一個技巧 好 那總之呢 我們把三個步驟都走完了 然後最後假設我們算出來最好的 w_1 也就是 w_1 star 是 1.67 最好的 b 也就是 b star 是 4.85 好 那我們就知道說 我們選出來的函式叫做 y = 1.67x_1 + 4.85 然後如果我們在訓練資料上定義出來的這個 Loss 訓練資料的 Loss 上面算一下的話 我們在訓練資料上面的 Loss 是 240 好 那走完這三個步驟就結束了嗎 走完這三個 那我們來先來看一下我們算出來的結果 我們看一下 在這邊這個圖上 每一個點代表的是一堂課 橫軸代表的是那一堂課的投影片的頁數 縱軸代表的是課程的時長 那我們找出來的 可以根據投影片頁數去預測課程時長的函式 叫做 y = 1.67x_1 + 4.85 但這個函式它的預測並不是完美的 你可以發現有很多課程 它的預測並不是非常的精準 不過從所有課程的分佈來看 這個預測看起來好像還可以 然後計算出來 我們的 MSE 是 240 那 240 其實不是一個非常大的數值 要記得這個是 Mean Square Error 所以這個數值呢 其實是正確答案跟預測答案差距的平方 240 接近 15 的平方 所以現在的預測大概差 15 分鐘 所以根據投影片預測課程的時長 我們目前用這一個 linear regression 我們有 15 分鐘左右的誤差 好了那現在找出這個函式了 我們課程可以到這邊結束了嗎 我們可以把這個課程直接拿來做測試 我們可以把這個函式直接拿來使用 直接拿來測試在沒有看過的資料上了嗎 比如說我們可以看一下 今天這堂課有幾頁投影片 然後就代入這個式子 看看我們計算出來的結果是否準確 我們可以開始這樣做了嗎 還不行 在你真正要測試一個函式好壞的時候 你要做一個額外的步驟 這就是我講的 3+1 的 +1 的這個步驟 這個步驟叫做驗證 它的英文通常寫做 validation 那這個驗證跟測試啊 如果要講比喻的話 就是測試是真正的大考 它是指考或者是會考 那驗證就是你在學校考的模擬考 那真正的大考非常的重要 你不能夠出錯 你可能只有一次機會 但驗證是模擬考 你有很多次機會 然後考壞了也沒有關係 考壞了就重新再回去把書念好就可以了 好那我們現在呢 來做一下驗證吧 那要怎麼做驗證呢 我們要準備另外一個驗證的資料集 那我們最終的目標 是要拿來測試今天這個 我們課堂的第五講 根據投影片能不能夠準確的預測 上課的時長 那我們得要有一些 validation 驗證的資料 我們也許可以拿這一門課 這門課目前已經上了第 0 講到第四講 所以上過 5 次課了 我們把第 0 講到第四講的內容 拿出來當做驗證的 validation 的資料集 好那我們知道說過去上的 5 堂課 第 0 講到第五講投影片的頁數 我們也知道上課的時長 這個都是真實的數字 然後呢我們把這些數字 帶到我們找出來的函式裡面 我們把 x_1 帶 27、35、97 看會得到什麼數字 分別得到 50、63 跟 167 你看這個結果就發現 這個預測的時長跟真正上課的時長 顯然有一些誤差的 看起來這個函式都高估了 他根據投影片的頁數 都高估了上課的時長 如果你算 MSE 一算 哇不得了很大 算出來 1143 剛才在訓練資料上算出來才 200 多 現在在 validation set 上 我們的 Loss 一舉上升了 5 倍 變得超過了以前 結果非常的糟糕 所以可以預期說在測試資料上 在我們今天這一堂課上 如果直接用這個函式 你也會得到類似的糟糕的結果 所以怎麼辦呢 今天當你做完驗證之後 如果結果好 那就沒什麼事 那我們也許就可以進入測試的階段 但是如果結果不好 那你就要回頭去檢視前面那三個步驟 到底發生了什麼事情 那我們就來看看 我們怎麼檢視前面這三個步驟 好現在驗證的結果不好 那到底是哪裡出了問題呢 到底是步驟一還是步驟二 還是步驟三出了問題呢 我們還不知道 所以要一個一個的來想 先想有沒有可能是步驟一出了問題 步驟一是要定出我們的目標 定出我們的 Loss 但有沒有可能步驟一 我們根據訓練資料定出來的目標 跟實際上在驗證的時候 validation set 上面算出來的 這個 evaluation matrix 結果不一樣呢 因為我們在這個機器學習 2021 上面 算出來的 MSE 跟在這一堂課 現在已經上完的課程上算的 MSE 會不會其實這兩個 MSE 有非常大的本質上的差異呢 為了要真的思考這個問題 瞭解這個問題 我們把機器學習 2021 的課程 投影片頁數跟課程時長的關係 還有今天這一門課 生成式 AI 和機器學習導論 2025 投影片頁數和課程時長的關係 劃在這個投影片上面 藍色的點是機器學習 2021 綠色的點是今天這一堂課 一看你就發現 哎呀難怪預測這麼差 綠色的點顯然都比藍色點的數值要小啊 所以代表說在同樣的投影片頁數上 今天我們這一堂課 生成式 AI 與機器學習導論 老師上課的時間是比較短的 或者是換另外一個說法 如果要上到同樣的時長 老師是要做比較多頁投影片 才能夠上到一樣的長度的 這個也是非常合理的 你想今天這個是一個導論的課程 其實導論的課程呢 相較於更深的課程 講起來其實對老師的壓力是更大的 你可能會覺得說比較簡單的課程 老師講起來應該比較輕鬆 其實不是難的課程講起來才快樂 這個比較專業的課程裡面 可以整個數學定理 滿滿的一頁都是數學室 一個數學室可以講個半個小時 多開心 投影片不用做太多頁 就可以講很長的時間 如果是導論的課程 為了想把每一件事講清楚 不敢放太多數學室 要多做很多投影片 試圖把觀念講清楚 所以一頁投影片 上課的時長是比較短的 所以導論的課程 要比較多頭一頁投影片 才能上到一樣的時長 所以我們今天在機器學習這門課上 算出來 Loss 要直接用到生成式 AI 與機器學習導論 顯然是沒有辦法的 所以我們現在第一步定的目標 根本就是錯的 我們在錯誤的目標上面做 optimization 根本就是緣木求魚 不可能得到好的結果 所以怎麼辦 我們得更換訓練資料 我們不該拿機器學習 2021 當作訓練資料 我們改成生成式 AI 導論 2024 這門課 當作訓練資料 這門課的課程的名稱裡面 也有導論這兩個字 所以顯然它應該跟我們這一門 生成式 AI 與機器學習導論的性質比較接近 所以拿生成式 AI 導論 2024 的課程來訓練 然後找到模型 預測現在這一門課 應該是一個更好的選擇 所以我們剛才已經抵出一個 bug 第一步就已經有問題了 所以我們要換訓練資料 把第一步做好 讓我們預期的目標 跟實際的目標更接近一點 當我們把課程換成生成式 AI 導論 2024 的時候發生什麼事情呢 在這個投影片上 藍色的點是生成式 AI 導論 綠色的點是今天我們這一門課程 你會發現生成式 AI 導論 投影片頁數跟課程時長的關係 還有我們這一門課 投影片頁數跟課程時長的關係 確實是比較接近的 那訓練完之後 在生成式 AI 導論這一門課上面 我們計算出來的模型是 y = 0.78x_1 + 12.85 所以在導論的課程上 一頁投影片上不到一分鐘 只能上 0.78 分鐘而已 那訓練完之後 在訓練資料上面 我們的 Loss 是 71 然後在 validation set 上面 在生成式 AI 導論已有的課程上 我們算出來的 Loss 是 122 所以你看相較於用機器學習 2021 當訓練資料 當把資料換成生成式 AI 導論 2024 的時候 我們的 validation Loss 跟 training Loss 是比較接近的 我們的 validation Loss 算出來不再是上千的數值 而是壓到 100 多 不要忘了 這個是 square 這個是 Mean Square Error 所以如果把它開根號的話 大概是 11 點多 現在預測的誤差是 11 點多分鐘 那能不能夠做得更好呢 我們再想想看步驟二 有沒有什麼可以改進的 我們已經改進了步驟一 那我們現在來改進步驟二 好 有沒有可能是步驟二的選擇太少呢 為什麼我說步驟二的選擇太少呢 在步驟二裡面 我們說我們的函式一定是 y = w_1x_1 + b 這意味著什麼 這意味著說 你的這個函式啊 如果你劃在一個橫軸是 x_1 縱軸是 y 的這個圖上 它就一定是一條直線 那如果你選不同的 w_1 這個線可能會有不同的斜率 如果你選不同的 b 這個線 可能會上下平移 但它就是一條直線 如果說今天 x 跟 y 之間真正的關係是 這樣一條曲線 在這個函式範圍裡面 我們不管怎麼找 都找不出這麼一條線 所以看來 這一個函式選擇的範圍 有點太狹隘了 我們應該劃一個更大的函式範圍 那接下來我要講的就是 有沒有可能 有沒有機會 我們直接劃一個函式的範圍 這個函式的範圍 有可能包含所有的函數呢 那現在呢, 如果隨便在這個 XY 的平面上畫一個函數出來 那我們有辦法就框個範圍去把它包住嗎？ 這邊的方法是這個樣子的 我們可以在這個曲線上面點一些點 然後呢, 把這些點連起來 那這種鋸齒狀的線段呢 我們叫做 Piecewise Linear Curve 然後我們要用這個綠色的曲線來逼近黑色的實線 那你可以想說這個綠色的曲線跟黑色的實線感覺差太多了吧 有很多地方不一樣 那是我們點的點不夠多 只要你點的點夠多 那你就可以讓這個 Piecewise Linear Curve 這個綠色的曲線跟黑色的實線 越來越接近 那接下來我要告訴你說 所有的 Piecewise Linear Curve 都可以看作是一個常數項 加上一大堆這個看起來像是山坡形狀的函式 我們就叫這個藍色的曲線「山坡函式」吧 我們可以把 Constant 加上一大堆的山坡函式 就可以組成任何 Piecewise Linear 的曲線 怎麼說呢？ 也就是說這個 Piecewise Linear Curve 不管你是畫這樣 還是畫成這樣 還是畫這樣 只要它是很多線段組成的 我們都可以把它看作是一個常數 加一大堆這個山坡形狀的函式 當然如果越複雜的 Piecewise Linear Curve 你需要的這個藍色的函式就越多 那這邊就是舉一個例子 告訴你說隨便畫一個 Piecewise Linear 的曲線 我們怎麼用山坡的藍色的函式 去組出這個紅色的 Piecewise Linear 的曲線 那我們假設這個 Piecewise Linear Curve 是從 X1 等於 0 開始 我們就不考慮 X1 等於 0 以前發生的事情 首先呢 你可以有一個 Constant 那這個 Constant 呢 就放在 X1 等於 0 的地方 就看說這個 X1 等於 0 的時候 這個紅色的這個 Curve 它 Y 的數值是多少 那我們就把 Constant 常數項放這個數值 接下來呢 我們要產生這個第一個線段 怎麼產生這個線段呢？ 你就拿一個這個藍色的山坡形狀的函式過來 這個山坡形狀的函式呢 它的兩個轉折點 就在這一個線段開始跟結束的地方 那這個山坡函式這個斜線啊 這個斜面的地方 要正好跟這個線段的斜面呢 它的斜率是一樣的 那你把 0 跟 1 加起來 你就有這一段了 接下來要怎麼弄第二段出來呢 那你就在第二段的開頭跟結尾的地方 放一個山坡函式 它的兩個轉折點 正好就是開頭跟結尾的地方 所以讓它這邊對這邊 這邊對這邊 然後斜率是一樣的 然後接下來 如果要產生這個線段的話呢 就再放個山坡的函式在這邊 它的轉折點是從這裡開始的 然後你只要把 0、1、2、3 這四項全部加起來 就變成紅色的 Curve 所以一個 Piecewise Linear 的 Curve 是可以由一個 Constant 加一大堆藍色長這個形狀的函式 疊起來所構成 那再來我要告訴你說 這個藍色的函式啊 它的數學式要怎麼把它寫出來 這個藍色的函式呢 它可以看作是兩個綠色的函式的組合 這個綠色的函式呢 都只有一個轉捩點 它可以這樣子轉 它也可以這樣子轉 那這些綠色的函式呢 我們可以不失一般性的把它寫成 C 乘上 max(0, w1x1 + b) 你可以把輸入 (橫軸是 x1) 乘上某一個數值 w1 再加上 b 然後看跟 0 誰比較大 那如果它小於 0 那輸出就等於 0 如果它大於 0 那輸出就是 w1x1 + b 然後前面乘上 C 就可以來描述這個綠色的線段 綠色的函式 那這個 C 的正負號呢 取決於這個轉折是往上的還是往下的 那 w1 決定了它的斜率 w1 跟 b 決定了這個轉折點在什麼位置 好 所以我們現在知道 所有的 Curve 都可以用 Piecewise Linear 的 Curve 去逼近它 Piecewise Linear 的 Curve 是一個 Constant 加上一堆藍色的函式 那藍色的函式 又是兩個綠色的函式組起來 所以我們可以說 所有的 Curve 我們都可以拿一個 Constant 加一大堆綠色的函式拼湊起來 可以去逼近任何的函式 所以我們已經知道說 我們可以用這個方法來逼近任何的函式 再來就只是把它的 formulation 把它的數學式寫出來而已 我們已經知道綠色的這個東西 我們可以寫成 C max(0, w1x1 + b) 那如果帶不同的 C 的值 w1 的值 b 的值 那它就長得不一樣 那這邊呢 我們假設這個綠色的函式 我們有大 H 個 那要把這個式子寫出來 就會變成 y 等於這個 Constant Constant 用 b 來表示 b 加上我們現在有大 H 個綠色的函式 所以這邊是 summation over 1 到大 H 那每一個綠色的函式都可以寫成這樣 有一項 C 有一項 W 有一項 b 但這邊有大 H 個函式 所以每一個綠色函式裡面的 C、W1、b 它們的參數 C、W1、b 我們得給它不同的名字 所以我們就加一個 i 來代表說 它是屬於不同的綠色的函式 所以我們用 i 來代表說 它是第 i 個綠色的函式 所以這邊就可以寫成 summation ci 乘上 max(0, wi1x1 + bi) 就這樣子 所以我們知道說 所有的 Curve 其實都可以用這個 Curve 來逼近 你只要 W、b 跟 C 帶不同的值 你就可以把它變成任意的 Curve 任意的函式 這個函式我們可以把它圖像化一下 我們的輸入是 x1 x1 要乘上某一個 W 然後再加上某一個 b 然後再通過 max(0, ...) (註: max(0, wx+b)) 然後這個函式就是 如果輸入小於 0 那輸出就是 0 如果輸入大於 0 那就是輸入等於輸出 把它的輸出用 a 來表示 那其實這個式子 它是有一個名字的 它叫 Rectifier Linear Unit 它的縮寫就叫做 ReLU 那如果熟悉深度學習的人 講到這邊 你就大概知道我要講什麼了 這個東西就是 ReLU 然後這個 a 前面要再乘上某一個 C 那這邊有好幾個函式 那我們這邊假設大 H 等於 3 假設我們有 3 個函式 所以 x1 會乘上另外一個 W 加上另外一個 b 然後過 ReLU 這個函式 然後得到 a2 再乘上 c2 然後 x1 會再乘另外一個 W 加另外一個 b 再過 ReLU 再得到 a3 再得到 c3 然後接下來 把這些數值全部加起來 再加上這個 Constant b 你就得到最終的 y 那這邊這個圖其實沒有多弄什麼事 我們只是把上面這一個函式把它圖像化而已 那這個輸入可以不用只有 x1 在很多時候我們考慮的 Feature 不會只用一個 我們可能會用多個 Feature 那至於要用哪些 Feature 比較好 那這個要看你對於這個問題的理解 在作業裡面 (註: 原字幕 "座位" 應為 "作業") 你需要做 Feature Selection 才會得到比較好的結果 所以除了 x1 以外 我們還有另外一個輸入 叫做 x2 這個 x2 可能是 比如說投影片上面的總字數等等 好 然後接下來 我們也要把 x2 乘上不同的 W 乘上不同的 W 然後跟原來 x1 乘上不同的 W 加起來 一起去加上 b 一起去過 ReLU 這個函式 那如果我們要加多餘的額外的 Feature 的話 你就用這個方式 把額外的 Feature 乘上不同的 Weight 乘上不同的 W 再跟原來的數位加起來就可以了 那你也可以用這個矩陣的方式 來描述這一整個流程 我們可以說 x1 跟 x2 把它放在一起 它就是一個向量 x1 它前面乘的這 3 個 W 跟 x2 前面這 3 個 W 它們合起來 當做一個矩陣 x1 會乘這 3 個 W 會乘這 3 個 W x2 會乘這 3 個 W 這整個過程其實就是一個向量 乘上一個矩陣 那我們這邊會加上 b1, b2, b3 等於加上一個向量 裡面的數字是 b1, b2, b3 我們會過 ReLU 這個 ReLU 呢 我們用這個函式 sigma 來表示它 我們用 sigma 括號 b + W 乘 x 代表說這個結果 會通過 ReLU 這個 activation function 然後我們得到 a1, a2, a3 a1, a2, a3 合起來也是一個向量 然後 a1, a2, a3 要乘 c1, c2, c3 再加 b 就得到最終的 y 所以這整個過程 也可以看作是一大堆的向量 跟矩陣的相乘和相加 那我們這邊 用比較簡單的方法來表示這些向量 我們就一個長方形 還有我們就用一個長方形 來表示向量或矩陣 所以 x1, x2 它是一個長方形 這邊全部 W 這些集合起來 我們可以看作是一個矩陣 用大 W 來表示 b1, b2, b3 合起來用 b 來表示 那這邊用粗體字代表說它是一個向量 a1, a2, a3 用粗體的 a 來表示 然後呢這個粗體的 a 這個向量 要乘上 c1, c2, c3 所組成的向量 那這個向量要做一個 transpose 因為它是一個倒下來的向量 加上一個長數 b 就得到最終的 y 總之一切都是矩陣的相加相乘而已 那我們剛才說 這邊做的運作 就是把 x 乘上 W 再加上 b 再過 ReLU 得到 a 這個向量 那這件事情呢可以做不止一次 我們可以把這個 a 呢 當作是一種新的輸入 我們可以把這個 a 乘上另外一個矩陣 加叫作 W' 我們用 W' 表示這是另外一個矩陣 然後呢再加上另外一個向量 b' 然後再過 ReLU 得到 a' 那如果我們圖示化的方法就是 這個 a 呢 a1, a2, a3 會乘上不同的數值集合起來 再加上 b 乘上不同的數值集合起來再加上 b 乘上不同的數值集合起來再加上 b 過 ReLU 得到 a1, a2, a3 再乘 c1, c2, c3 再加 b 就得到最終的 y 所以乘上 W 加上 b 呢 這一個操作可以做不止一次 你可以把輸入的 x1, x2 做一次這樣的操作 得到 a1, a2, a3 再做另外一次這樣的操作 得到另外一組 這邊忘了加 prime 應該說是 a1 prime, a2 prime, a3 prime 得到最終的 y 那這一整個操作的過程呢 如果你覺得它看起來不夠厲害的話 那我們就幫它取一些新的名字 我們把這個 ReLU 這個函式 跟它前面的這個 b 跟它前面的這些 Weight 集合起來 我們就叫它 Neuron 它的中文就是一個神經元 多個 Neuron 合起來 我們就叫它 Neural Network 也就是類神經網路 你有沒有覺得這整個方法 突然之間聽起來都厲害起來了呢 那一堆 Neuron 排成一排 也就是乘上一個 W 再加 b 這一個操作 我們叫它一個 Layer 那有時候呢 你會在 Layer 前面加 Hidden 為了要跟輸入區隔 就輸入的部分叫它 Input Layer 然後其他的 Layer 呢 叫它 Hidden Layer 那這為了跟輸入做區隔 所以 Layer 前面呢 有時候會加上 Hidden 那如果你有很多 Hidden Layer 就是深度學習 就是 Deep Learning 這個就是 Deep Learning 好 那我們現在已經告訴你說 Deep Learning 呢 其實就是一個劃更大的區域 來包含更多函數的方法 我們也告訴你說 Deep Learning 有機會可以涵蓋所有的函數 如果你今天的這個大 H 呢 這邊大 H 是只設 3 啦 那所以如果你把它組成 Piecewise Linear Curve 的話 它只能有有限的轉折點 但是如果你的這個大 H 設的非常大 它就可以描述 非常複雜的 Piecewise Linear Curve 那非常複雜的 Piecewise Linear Curve 有非常多轉折點 它可能就可以表示 非常多不一樣的函數 所以在理論上 如果你有無窮無盡的神經元 深度學習 可以模擬任何函數 可以涵蓋任何函數 但實際上因為神經元數目是有限的 所以你也沒辦法真的涵蓋任何函數 但是相較於 Linear 的 Model 現在我們有了這一個 Neural Network 它顯然涵蓋了更多不同的函數 那有關訓練 Neural Network 的過程呢 其實一樣是用 Gradient Descent 跟我們剛才講的 Gradient Descent 只用兩個參數做的 Gradient Descent 其實沒有本質上的差異 但是因為一個 Neural Network 裡面 它的參數非常的多 所以你需要一個比較有效率的演算法 來幫助你計算出 Gradient Descent 那這一個有效率的 計算 Gradient Descent 的演算法 叫做 Backpropagation 所以 Backpropagation 其實就是 Gradient Descent 的 一種方法而已 它跟 Gradient Descent 沒有本質上的差異 那如果你想要知道什麼是 Backpropagation 的話 可以參考我過去的課程 那我把課程的錄影放在這邊 給大家參考 那現在如果你在實作上 你可能不容易碰到 Backpropagation 這件事了 因為 Backpropagation 這件事情 都是在 Deep Learning Framework 裡面 有幫你 implement 的 所以我這邊就先暫時不講 怎麼做 Backpropagation 如果你有興趣的話 課程錄影在這邊 留給大家自己研究 好 那接下來呢 我們就來看看這個 Deep Learning Neural Network 能不能夠發揮它的威力吧 那我們剛才有說 如果是一個 Linear 的 Model Training Loss (註: 原字幕 "Turning the loss") 我們做到 71 好 現在弄一個 有一個 Hidden Layer 的 Neural Network 它的式子如果要寫出來的話 長這個樣子 那 H 呢 我們就設 100 然後到現在這個地方 因為參數量已經非常多了 所以我們沒有辦法 在到處的參數 畫一個 Loss 的 surface 給你看 我們現在唯一能做的 是畫一個東西 叫做 Loss 的 Curve 也就是我們每次 update 完參數 我們 update 完以後 會計算這一組參數 在我們訓練資料上面給我們的 Loss 我們會把這個 Loss 記錄下來 所以你可以看到 Loss 的變化 那就會看到說 隨著 Epoch 越來越多 那我們這邊沒有做 Stochastic Gradient Descent 我們這邊是做 full-batch 所以每個 Epoch update 一次參數 但每 update 一次參數 Loss 都會有一些下降 在前面幾個 Epoch Loss 下降比較多 然後接下來 Loss 下降的程度 就逐漸趨緩了 那最後我們可以拿到 多少的 Training Loss 在訓練了上萬個 Epoch 之後 我們得到的 Training Loss 是 80 我們得到 Training Loss 居然是 80 你覺得這合理嗎？ 你不覺得不可能 這絕對不可能 我們的 Training Loss 怎麼會比 Linear 的 Model 還要更低呢？ 你想想看 Linear 的 Model 是一個比較小的範圍 我們現在用了一個 Neural Network 它的範圍是比較大的 那個 Linear 的 Model 它能夠弄的函式 它能夠表達的函式 都是這個 Neural Network 可以表達的 那如果今天在這個比較小的範圍內 找出來最好的函式 它的 Loss 都已經有 71 了 那在這個比較大的範圍內 我們怎麼還能 找出來的函式 它的 Loss 怎麼還會比 71 高呢？ 最差也應該就是 71 而已 怎麼會找出比 71 高的 Loss 呢？ 所以接下來 我們就要進入 再下一個步驟 我們現在 框出了一個比較大的範圍 但結果你發現 框出一個比較大的範圍 居然沒用 你沒有找出一個 比較低的 Loss 到底是發生了什麼事 找出比較大的範圍 而且還把之前的範圍也包含進去 怎麼可能 Loss 比之前要低呢？ 所以 有別的地方出了問題 什麼地方出了問題呢 也許是第三步出了問題 在選一個 最好的函式的時候 出了一個問題 為什麼選最好的函式 為什麼做 Optimization 的時候 有可能會出錯呢 我們在剛才在講 Gradient Descent 的時候已經告訴你說 Gradient Descent 很明顯的一個問題 就是有時候 你在訓練的時候 你只能找到 Local Minima 你找不到 Global Minima 那這可能就是 為什麼當我們有一個 Neural Network 明明有比較大的範圍 我們卻找不出更好的函式 因為你沒有找到 Global Minima 但是其實還有很多很多的理由 會讓你的 Optimization 做得不好 比如說 除了會卡在 Local Minima 以外 訓練的參數 訓練的時候 參數還有可能 會卡在 Saddle Point 什麼叫 Saddle Point？ Saddle Point 就是 微分算出來是 0 但是 不是 Local Minima 的地方 這是一個平臺 然後它微分算出來 就是 0 所以你根本不知道 要往左走還是往右走 當然你現在是開了上帝視角 你知道左邊高右邊低 但是如果你沒有開上帝視角 你只看得到你腳下 你發現它是平坦的 雖然你往右跨一大步 你就會知道說 你很快 Loss 就會下降了 但是你四周都是平坦的 所以你根本不知道往哪裡走 事實上 更多的時候 你今天訓練會失敗 單純就是走到某一個地方 它的 Gradient 太小了 因為它的 Gradient 太小了 所以 Loss 下降的非常慢 然後你以為已經卡在 Local Minimum 或 Saddle Point 所以你就不想再等下去 你就把訓練停止了 那我剛才說 像這樣子的一個 Training Curve 你會看到說 Loss 逐漸下降 逐漸下降 當它停止 當它收斂 當它 Loss 不再下降的時候 或下降非常慢的時候 這個時候你就會開始考慮 我是不是應該把這個 Training 停下來了 我是不是不應該再跑下去了 當你看到很小的 Gradient 的時候 會讓你開始心生懷疑 而你永遠不會知道 會不會在多 Training 的幾天之後 Loss 就突然下降 誰知道呢 所以在這個地方 你會非常的猶豫 到底是要停止 還是要繼續向前 有時候你選擇了停止 你就找不到更好的 solution 你就找不到更好的 Optimization 的結果 所以總之 Optimization 是非常容易失敗的 尤其是你在做 Neural Network 的時候 你非常容易找不到一組好的參數 可以讓你的 Loss 夠低 但無論如何,現在,憑藉著我們的直覺 我們知道你的 `training loss` 絕對不能夠比線性的模型低 你可能會想說,當我們有一個 `neural network` train 下去 我們什麼時候知道說我們的 `loss` 夠低 什麼時候知道說我們的範圍已經夠大 只是 `optimization` 的方法不好 所以才找不到更好的結果 而不是因為我們的範圍還是太小了 這個就必須要憑藉著你對於問題的理解 那我通常會建議大家在實作一個新的問題的時候 你可以先從一個比較簡單的模型開始做起 比如說我這邊從一個 `linear` 的 `model` 開始做起 那 `linear` 的 `model` 它的 `optimization` 是比較容易的 我知道 `linear` 的 `model` 它的 `loss` 在 `training data` 上可以降到 70 幾 那就意味著說假設我兜出了一個類神經網路 它的 `training loss` 絕對不可以高於 70 幾 如果高於 70 幾就代表 `optimization` 是有問題的 我們必須要解決 `optimization` 的問題 而不是開一個更大的類神經網路 所以什麼時候是你的範圍不夠大 什麼時候是 `optimization` 出了問題 需要取決於你對問題的理解 那如果你從一個比較小的模型開始實作 那可以給你比較好的 `insight` 知道說實際上一個大模型的 `loss` 至少可以做到多少 所以現在我有信心 `Neural network` 它的 `training loss` 絕對要比 70 低 那怎麼讓它的 `loss` 比 70 低呢 想辦法改進 `optimization` 的過程 那通常所謂改進 `optimization` 的過程 就是爆調 `hyperparameter` 爆調 `learning rate` 爆調 `epoch` 數目 爆調 `batch size` 把每一個你可以調的東西都爆調一下以後 經過一番猛如虎的操作 我可以把 `loss` 呢壓到 41 所以果然是可以做得比 70 還要低的 但能不能做到更低呢 不知道 因為反正只要比 70 低 我就覺得心滿意足 我就覺得這個答案是合理的 到底能不能做得更低 那就難以預測了 好那做到了 40 之後 我們也許可以把這個 x 跟 y 的關係 一樣畫在這個二維的平面上 所以現在橫軸呢是投影片的頁數 縱軸呢是課程的時長 我們現在找到的函式不再是一條斜直線 它是由一個複雜的 `neural network` 所產生出來的曲線 那這個曲線你可以看到在這個地方 有一個神秘的轉折點 看起來它覺得說 如果投影片的頁數少於某一個範圍 我們用這個函式來預測 但是投影片的頁數大過某一個範圍 應該用這一條線來預測 那這邊有一些怪怪的東西 不過沒差反正沒有這麼少頁數的投影片 我們找到了一個比較複雜的函式 來描述投影片的頁數 跟課程時長之間的關係 但是我們可能已經到了某種極限 光看投影片的頁數顯然是不夠的 因為你會發現說有很多課程 他的投影片頁數是一樣的 但課程時長是不同的 所以我們需要找到更多其他的 `feature` 才能預測課程的時長 我們剛才的輸入只有多少頁投影片 我們也來考慮有什麼樣其他的 `feature` 可以用吧 比如說一個可以用的 `feature` 可能是 投影片中總共有多少字 因為很直覺的如果字數越多 這個投影片應該需要講得越久吧 所以我們來看看 如果把投影片的字數當作第二個 `feature` 能不能做得更好 做下去 加上投影片的字數作為額外的 `feature` 我的 `loss` 從 41 降到 40 降的有點少 好像沒有帶來太大的幫助 後來我轉念一想 也許用投影片中總共的字數 並不是一個好的想法 為什麼用投影片中總共的字數不是一個好的想法呢 因為投影片的頁數越多 它的字數本來就會越多 所以如果我們是把投影片中總共的字數 當作第二個 `feature` 第一個 `feature` 跟第二個 `feature` 其實沒有提供非常不一樣的資訊 也許真正重要的是 每一頁投影片平均有多少字 代表一個投影片 它的內容的濃度 這個一頁投影片的內容越多 它的字數越多 那可能應該講得越久 那所以呢 我們應該把投影片中的總字數 除以投影片的頁數 來當作新的 `feature` 這個 `feature` 就是投影片每一頁的平均字數 那你可能會想說 這個我的輸入已經有投影片的頁數 也有總字數啦 難道 `network` 沒有辦法在裡面自動學到 把這兩者相除 當算出投影片的平均字數嗎 也許可以 但是那就需要仰仗更多的 `optimization` 更大的 `network` 那我今天既然都知道這是一個好的 `feature` 了 何必仰賴 `network` 自己發現呢 我直接告訴它有這個 `feature` 可以用 不就好了嗎 所以我給它一個新的 `feature` 這個 `feature` 是每頁投影片平均的字數 一做下去 有顯著的效果啊 一做下去 `training` 的 `loss` 降到 22 現在有一個新的類神經網路 它的輸入是有兩個 `feature` 我們來看看這個類神經網路 在 `validation set` 上的表現吧 剛才用 `linear` 的 `model` `training` 是 71 `validation` 是 122 我們能不能比 122 做得更好呢 現在 `training` 都已經降到 22 了 一做下去 不得了 `validation set loss` 是 1300 太慘了 結果非常的差 這個差距非常的驚人 雖然 `training` 我們從 71 降到了 22 但是 `validation` 從 122 突然暴增到 1300 為什麼會這樣呢 為什麼有時候 我們在 `training` 的 `data` 上得到很低的 `loss` 但是在 `validation set` 上 卻沒有得到一樣低的 `loss` 甚至反而得到更高的 `loss` 呢 當你發現說啊 你 `training` 的 `loss` 跟 `validation` 的 `loss` 有巨大差距的時候 這個現象叫做 `overfitting` 那 `overfitting` 的來源是什麼呢 `overfitting` 的其中一個來源就是 當你劃定的選函式的範圍越大 就越容易發生 `overfitting` 的現象 什麼意思呢 我們先來假定一個實驗 這個實驗是這樣子的 我劃了一個非常非常巨大的函式的範圍 我的範圍是能包刮世界上一切的函式 那你說你剛才講 `deep learning` 可以涵刮一切世界上的函式 那只是理想上啊 這個 `deep learning` 的 `neuron` 的數目畢竟是有限的 所以沒辦法真的涵刮所有的函式 現在講一個極端的狀況 我不管用什麼樣的方法 我就是可以涵刮世界上一切的函式 那結果會怎樣呢 我會得到一個很好的訓練結果嗎 可能不會 為什麼 你想想看了 假設我的訓練資料有這幾筆 這三張投影片 他們正確的時長分別是 10、20、30 假設世界上一切的函數都可以選 那我們有沒有可能選這樣一個函數 這個函數我叫做 F 下標 lazy 因為它是個爛函數 啥事也沒幹 這個函數它會的事情就是 輸入訓練資料的這三份投影片 就輸出訓練資料裡面這三堂課 正確的時長 輸入其他的東西 只要是它在訓練資料裡面 沒看過的投影片 它的輸出通通都是零 那這個函數顯然沒有什麼用 你可能想說你怎麼可以選 這麼樣的函數呢 你怎麼可以選這種沒有用的函數呢 那你想想看 這個函數有違反我們前面 任何的選擇的過程嗎 它是一個在訓練資料上 `Loss` 為零的函數 在訓練資料這三筆資料上 它會完全答對 所以它 `Loss` 為零 它的 `Loss` 非常低 而我們現在劃定的函數範圍 是包含全世界所有的函數 所以這個 Lazy 函數 它也是我們可以選的函數 我們說我們要在我們 所有可以選的函數裡面 選一個 `Loss` 最低的 所以你很有可能 真的會選到 Lazy 函數 它是你可以選的 你真的會選到的一個函數 那選到這個函數 當然你在驗證資料上 你就沒有辦法預期有什麼好結果 因為驗證資料是 它沒看過的投影片 它一律輸出零 你在驗證資料上的 `Loss` 就會大爆炸 好那剛才只是 舉了一個實驗告訴你說 假設函數劃定的範圍是無限大 你很有機會找到一個函數 它是訓練資料上面 `Loss` 為零 但是在驗證資料上面 一點用處都沒有的函數 但是我還沒有講 為什麼範圍越大 就越容易找到不好的 讓訓練跟 這個驗證資料差距 `Loss` 差距很大的函數 這件事情講起來就很麻煩了 大概一個小時以上 有可能講不清楚 才有可能講清楚 所以我就把它留在 過去的上課錄影裡面 如果大家想要知道 在理論上分析 為什麼劃定的範圍越大 `Validation Set` 跟 `Training Set` 它的 `Loss` 就有可能 差距越大的話 請參見機器學習 2022 年的錄影 把錄影的連結 留在這邊給大家參考 這邊再舉一個例子 說明 `Overfitting` 的現象 我們說 `Overfitting` 就是 選擇越多 訓練跟驗證的差距越大 我們這邊就講一個故事 不知道大家有沒有考過 汽車駕照 在考汽車駕照的時候 你可能要先去駕訓班 做訓練 然後接下來才真的去考駕照 訓練的過程就對應到 機器學習裡面的訓練過程 考駕照的過程 就是驗證的過程 一般正常你在駕訓班 開車的時候 正常的學習方法 應該就是看著道路來開車 看著路況來開車 但是你知道 在考駕照的時候 有一些題目是特別難的 比如說倒車入庫等等 如果你沒有太多開車經驗 你可能很難把這件事情做好 這個時候 因為人類可以用的資訊 不是只有看著前面的道路而已 還有很多其他的資訊 所以那個時候 在駕訓班的時候 我就學會了一個招數 就是看著汽車的後照鏡 有人在旁邊的欄杆 貼了一些貼紙 那顯然是故意貼的 我就學到說 當今天貼紙出現在 後照鏡的正中間的時候 方向盤就左打四分之一 當另外一張貼紙 出現在後照鏡的中間的時候 方向盤就右打三分之一 這一招就可以讓我順利的 在駕訓班的場地做好倒車入庫這件事 但是你知道 這個時候只有在驗證的時候 也在同一個場地 你才有辦法開車 換了一個場地 基本上就不能開車了 就是這麼樣一個故事 不過現在會不會開車 也沒那麼重要 自駕車就要來了 所以以後有沒有開車 都不打緊了 這個就是個 `Overfitting` 與駕訓班的故事 好 那所以呢 我們知道有時候 我們會在訓練資料上面 得到很低的 `Loss` 但是進行驗證的時候 卻發現 `Loss` 非常的高 那這個時候怎麼辦呢 你可能得再回頭過去 改一下你的訓練步驟 你訓練了三個步驟 你要回頭來檢視 有沒有辦法改一改 讓你在測試驗證集上 可以找到一個函式 在驗證集上 可以得到低的 `Loss` 而且啊 如果你劃定的範圍越大 如果你的範圍是無窮大的時候 你就非常可能 找到一個訓練資料上 `Loss` 很低 驗證資料上面 `Loss` 很高的函式 這就是為什麼 我們在步驟二 一定要劃一個範圍 而這個範圍非常的重要 它不能太小 如果太小的話 你就沒有包含到好的函式 不能太大 如果太大的話 就會 `Overfitting` 要怎麼劃定這個範圍 我們下周會再討論 但是總之 這不是一件很容易的事情 你需要反反復復的 在三個步驟和驗證間進行操作 劃定一個範圍 驗證一下 發現不好 再重劃一個範圍 再驗證一下 又發現不好 再重劃一個範圍 再驗證一下 又發現不好 你要反反復復的 做很多次訓練跟 `Validation` 你才會找出一個 在 `Validation Set` 上 `Loss` 也低的函式 那其實今天一般在訓練的時候 你會非常常去使用 你的 `Validation Set` 舉例來說 你甚至有可能在訓練的時候 每一次 `Update` 也就是每過一個 `epoch` 你都去 `Validation Set` 上面 量一下 現在模型的表現如何 那剛才在訓練 `Neural Network` 的時候 我其實也做了一樣的事情 藍色的這條線 是訓練的 `Loss` 隨著 `epochs` 的進行 訓練的 `Loss` 越來越低 那橙色的這條線呢 橙色這條線是 `Validation` 的 `Loss` 也就是我每訓練出一個模型 我就拿那個模型 去在 `Validation Set` 上 算一下 `Loss` 看看我們可以得到什麼樣的 `Loss` 那如果你今天一直訓練 一直訓練 `epochs` 真的設非常的大 那你會得到 非常高的 `Validation Loss` 但在這整個過程中 你卻發現 其實你只要把 `epochs` 設少一點 其實你是可以得到 很不錯的 `Validation Loss` 的 我發現 `epochs` 在 20 幾的地方 我可以得到一個 `Validation Loss` 它的數值是 12 非常低 比我們看到的 所有數值都還要低 所以今天 很多時候你把 `epochs` 設少一點 其實也是一個 防止 `Overfitting` 的方法 那這一招呢 叫做 `Early Stopping` 其實在助教的 這個作業裡面 也有要求大家 做 `Early Stopping` 這是一個可以幫助你 避免你 `Overfitting` 的方式 好 總之我們剛才講說 在驗證與訓練之間 你可能會來來回回 無數次 但是這個來來回回 是有代價的 這個來來回回 有什麼樣的代價呢 這個來來回回的代價是 最終 你有可能 對 `Validation Set` 做 `Overfitting` 什麼意思 假設我們可以 無限的使用驗證資料 最終 你有可能可以找到一個函式 假設你根本不知道 什麼叫做訓練 你就是每一次都隨便弄個函式 隨便弄個函式 你的函式是隨機產生的 但是 只要你能夠驗證的次數夠多 也許你可以正好抽到 另外一個 Lazy Function 我們叫 Lazy Function 2 這個 Lazy Function 哎呀 它在驗證資料上面 看到驗證資料這幾份投影片 答案跟驗證資料正確答案 正好一模一樣 但是 在其他資料上 比如說測試資料上面 它的答案都是亂給的 比如說看到 其他投影片 不在驗證資料裡面的 他的輸出仍然是零 這樣子的lazy function 它仍然是一個 沒有用的 Function 但是當你可以 無限制的使用驗證資料 去找一個在驗證資料上面 `Loss` 最低的函式的時候 最終 你的下場可能就是這樣 你找到一個 驗證資料上 `Loss` 很低的函式 你 `overfit` 在驗證資料上 但仍然是一個 沒有用的函式 好那你說 那如果在驗證資料上面 不斷的反覆做 最終可以找到一個爛的函式 那怎麼辦呢 也許也不要緊吧 我就把驗證資料上面 找出來的函式 拿去測試資料上面 真的測測看 看看它結果好不好 那如果我不小心 `overfit` 在 `Validation Set` 上 那測試資料會告訴我 測試資料會告訴我說 這仍然是一個壞的函式 我發現它是一個壞的函式 我會再回去改訓練的步驟 期待找到一個 `Validation Set` 上好 最後測試也好的函式 但是這邊你要注意 假設你可以做無限制的測試 最終會發生什麼事情 你也有可能 `overfit` 在測試資料上 如果你可以無窮無盡的 不斷的使用測試資料 最終你會 `overfit` 在測試資料上 你會找到一個 在訓練資料 在 `Validation Set` 上 在測試資料上面 都表現很好的函式 但是在其他的輸入上面 表現都不好的函式 你有可能找到這樣的函式 就是因為這些測試資料 很多時候 實際上 是可以做 非常多次的測試的 這就解釋了為什麼 在這些benchmark上 benchmark 就是一些測試的資料集 你拿來檢驗人工智慧能力的資料集 在這些 `Benchmark` 上 人工智慧往往可以大幅打敗人類 但是實際上並沒有辦法做到 舉例來說 有一個 `Benchmark` 叫做 `SQuAD` 你看這邊的時間 這都是 2019 年上傳的結果 在 2019 年的時候 那個時候還不流行 LLM 有一個模型叫做 `BERT` 它得到的 Exact Match 的分數 是 87% 這邊是做那種閱讀測驗 讓模型讀一個文章 問它一個問題 看它能不能答對 而人類呢 人類只有 86.8% 的正確率 居然還比最好的人工智慧 2019 年的時候的人工智慧 正確率還要更低 當然你不會相信 在 2019 年的時候 那個時候的語言模型 可以做得比人類更好 但是在 `Benchmark` 上 它就是會顯示出 比人類更好的數值啊 為什麼會這樣 那就是因為 這些benchmark 其實你還是可以做大量測試的 雖然實際上建構這些 `Benchmark` 可能會說 也許會設置說 你每天只能夠測試一定的次數 他們甚至會不公佈測試資料 所以就沒有辦法無窮無盡 用那些測試資料來測試 你必須要把模型上傳到他們的平臺 由他們幫你測試 它可能甚至會規定說 你每天最多測一次 免得你每天都爆測個 10 萬 20 萬 20 萬次 然後就 `overfit` 在訓練資料上 但就算是這樣 只要一個 `Benchmark` 存在的夠久 你每天只能測一次 但你測個 1000 天 2000 天 測個 10 年 終究有一天會overfit在測試資料上 這是為什麼人工智慧 常常在這些 `Benchmark` 上 有很好的結果 但是實際上 它並沒有那麼厲害 而人類 你人類在這個 `Benchmark` 上 你只會做一次啊 所以跟機器 會無窮無盡的 不斷在這個 `Benchmark` 上實驗 做差了 就調模型再做一次 再差 再調模型 再做一次 最終模型終究能夠 人工智慧終究能夠超越人類 這個就好像在鬼滅之刃裡面一樣 鬼是殺不死的 人類只有一次機會 人中一刀就會死掉 但是鬼只要不砍他的頭 它就可以無限的再生 這個是一樣的道理 那為了避免 `overfit` 在測試資料上 所以多數機器學習的競賽 包括今天我們的作業 都有這樣的設置 就是我們會把測試資料 分成兩半 一半叫 `Public Set` 一半叫 `Private Set` 那你只能看到 `Public Set` 上面的分數 你看不到 `Private Set` 上面的分數 在做驗證的時候 你的驗證資料 可能是你自己的資料 你會從你的訓練資料裡面 我們提供給你的 有標註的資料裡面 抽一部分出來 當作 `Validation Set` 那你要用 `Validation Set` 呢 跑多少次的循環 這邊沒有人管你 所以非常有可能最後就overfit在你的 `Validation Set` 上 總之你要用幾次 `Validation Set` 那是你自己的資料 所以都可以 但是在 `Public Set` 上 你能夠做的測試次數是有限的 你當然可以訓練完一個模型 然後把它上傳到我們的 `Leaderboard` 然後呢 看看你在 `Public Set` 上 得到的分數是多少 得到的分數如果不好 你當然會 你當然不會說得到的分數不好 就這樣算了 不好就算了 你會回過頭去 然後去改一下 你的訓練的三個步驟 看看有沒有辦法在public set上做得更好 但是為了避免你 `overfit` 在 `Public Set` 上 `Public Set` 可以上傳的次數 是有限的 我們會規定每天 你只能上傳多少次 那因為每一個作業的期間 是有限的 所以你沒有辦法 無窮無盡的 使用這個 `Public Set`來做vaildation 所以你可以說Public set的testing data其實他是一個 有限次數的驗證集 它是一個真正的驗證集 可以避免你 `overfitting` 到 你原來的驗證集上 當然因為你這個 `Public Set` 你還是可以重複實驗 所以你也有可能 `overfit` 在 `Public Set` 上面 所以最後見真章的是 `Private Set` `Private Set` 只有一次機會 你沒有辦法知道說 你在 `Private Set` 上得到的結果 你只能說 我在 `Public Set` 上 選一個 `Public Set` 上 表現比較好的模型 那麼讓你選兩個 所以你其實等於 有兩次測試的機會就是了 在 `Private Set` 上 把那兩個模型測一下 結果多少就是多少 只有一次機會 那在 `Private Set` 上 在 `Private` 的 Testing Set 上 呈現出來的效果 可能更接近你的模型 真實能夠呈現的效果 好講到這邊 我們就把原理的部分 講到一個段落 接下來我們要進入 實作的階段 把剛才我們在講機器學習基本原理的時候舉過的例子 實際上跑一遍給你看讓你更有感覺一點 好那今天我們要做的任務 就是根據投影片的資料 比如說頁數字數等等 來預測李宏毅老師這堂課講課的時間 我們先匯入必要的函式庫 然後接下來 我們已經收集了李宏毅ll老師過去的上課的資料 然後有統計出在機器學習 2021 這門課裡面 每一堂課投影片的頁數 還有那一堂課講了多長 所以這邊有機器學習這門課的資料 還有生成式人工智慧導論 2024 的資料 還有我們現在這一堂課 這一門課從第 0 講到第 14 講 總共 5 堂課的資料 那這邊記載的每一個數字 就是這個投影片的頁數 然後這邊記載的每一個數字 就是課程的時長 那你直接在我的課程網站上 都可以找到這些資訊 那收集這些資訊呢 還是需要花一點時間的 我本來是想要直接用 ChatGPT 的 agent 來做這件事情 但是它都給我一些奇奇怪怪的錯的答案 所以最後老實說我是人工做的 所以這個是人工花一些時間 收集標註了這些資料 得到投影片的頁數 跟它的課程的時長 還有投影片的總字數 好那我們有了這些資料以後 接下來我們就開始訓練吧 我們先假設我們用機器學習 2021 這門課呢 當做訓練資料 然後呢我們用 2025 現在這一門課呢 當做驗證資料 那我們把訓練資料的輸入 叫做 X_train 訓練資料的這個正確答案 標準答案叫做 Y_train 驗證資料的輸入叫做 X_val 然後測試資料叫做 Y_val 所以我們現在就把 2021 的投影片頁數 交給 X_train 然後 2021 的課程長度只給 Y_train 2025 這一門課的投影片長度 只給 X_validation 然後這個長度只給 Y_validation 好有了這些訓練資料以後 接下來還記不記得機器學習的三個步驟是什麼呢 第一個步驟是先定好我們要什麼 所以我們要先定義好我們 Loss 的計算方式 那這邊呢我們使用 Mean Squared Error MSE 中文是均方誤差 那我們希望函式的預測值 跟真實值之間的差距要越小越好 我們就定了這個 Mean Squared Error 的函式 它的輸入就是兩個向量 第一個向量 Y_predict 代表某一個模型輸出的數值 Y_true 代表真實的正確的答案 那它裡面做的事情就是把 Y_predict 跟 Y_true 做相減把模型的輸出跟正確答案做相減 去平方再平均就這麼簡單 如果你要使用這個函式呢 比如說我假設我現在有一個很爛的模型 它的輸出就是隨機預測的 這邊就是假設有一個爛模型的輸出 這些數值都是隨機打上去的 那我想要知道這個模型 它輸出的這個 Predict Random 這些數值 到底根據 Mean Squared Error 計算起來有多大 那你就把 Mean Squared Error 第一個輸出 放這個 Random 的 Prediction 第一個輸出放 Y_true 也就是 Training Set 上的正確答案 然後得到的這個 Loss 再把它輸出就可以了 而 Loss 輸出有 6 萬不得了 所以這個隨機的輸出得到的 Loss 是非常巨大的 假設有一個函式它的輸出跟正確答案一模一樣 我們今天假設有一個函式的輸出叫做 Predict Perfect 它就跟正確答案也就是 Y_true 一模一樣 所以這個時候當你計算 Mean Squared Error 的時候 第一個輸入是 Predict Perfect 第二個輸入是 Y_true 那輸出出來的 Loss 會是多少呢 就是 0 那 Mean Squared Error 算出來最小的就是 0 好那接下來我們進入機器學習的第二步 就是劃出選擇的範圍 那我們就假設我們是一個線性的模型 Y 等於 W1 X1 加 B Y 是課程長度 X1 是投影片的頁數 W1、B 是要找的未知的參數 那這個 W1 跟 B 設定不同的值 我們就得到不同的函式 那我們假設說呢 我們有定義一個函式叫做 Linear Model 它的輸入呢就是 X1、W1 跟 B 就我們要給它輸入跟給它我們的參數 然後它就幫我們做這個 Linear Model 的運算 把 W1 乘 X1 再加上 B 那我們可以劃出一些 在我們的這個可以選的函式集合裡面的不同的函式 我們今天如果改變 W1 跟 B 的值 改變 W1 跟 B 的值 我們就得到不同的函式 所以我們這邊就列舉了幾個不同的 W1 列舉了幾個不同的 B 然後呢再產生不同的輸入 這邊這個 X_PLOT NP.LineSpace 0, 100 100 的意思就是從 0 到 100 之間產生 100 個點 然後呢我們就重取所有的 W1 跟 B 的組合 然後每個組合把它的這個輸入跟輸出的關係劃出來 你就可以知道說 我們現在選擇的函式範圍大概是長這樣 反正通通就是直線 只是這些直線呢 它們有不同的斜率 然後它們可能會有一些上下平移的關係等等 好再來就進入最關鍵的一步 第三步找出最好的函式 我們要找出一個 W 跟 B 的組合 讓 Loss 越低越好 怎麼做呢 我們先嘗試最笨最直觀的方法 也就是暴力窮舉 我們把一定範圍內所有 W1 跟 B 的組合都暴搜一次 然後去計算它對應的 Loss 就是把每一個 W 跟 B 的組合去計算出它對應的 Loss 再把這些 Loss 劃出來 你就可以看到一個 Error Surface 那我們要找的就是這個 Error Surface 的最低點 好那我們來暴搜所有的範圍吧 那你就會先建立你要窮舉的 W1 跟 B 那 W1 的值呢 我們就是在這個 0 到 3 之間取 100 個點 所以你其實也沒辦法真的窮盡所有的值啦 但是你可以在 0 到 3 之間取 100 個點 W1 有 100 個不同數值來做測試 然後 B 呢在 0 到 20 之間 我們取 300 個不同的點來做測試 那為什麼是 0 到 3 為什麼 0 到 20 就是憑著對這個問題的理解知道說最好的值呢 這個 W1 應該落在 0 到 3 之間 最好 B 應該落在 0 到 20 之間 然後來設定這個範圍 好然後接下來呢 我們先建立一個叫做 losses 的矩陣 這個 losses 的矩陣呢 它的目標就是要存我們等一下算出來的 loss 那接下來呢 我們就是窮舉所有的 B、窮舉所有的 W 把每個 W 跟 B 呢 都帶到這個 linear 的 model 裡面 然後根據 X_train 你可以得到 predict 出來的 prediction 然後每個 predict 出來的 prediction 都去算一下 Mean Squared Error 就可以計算出 當我們 W 跟 B 設成不同數值的時候 得到的 loss 是多少 然後再把這個 loss 存到 losses 這個矩陣裡面 那印出來呢就像是這樣 有點難看哦 就是有一個矩陣 這個矩陣呢 其中一個軸代表了各種不同的 W 另外一個軸代表了各種不同的 B 然後把所有 W 跟 B 的組合拿出來算 loss 這個有點不知道這個矩陣在幹嘛 但是我們可以把這個矩陣的 3D 立體的圖 跟 2D 的等高線圖畫出來 那至於實際上怎麼畫 這個再留給大家自己研究 所以我們就可以畫出 3D 立體的圖 在這個 3D 立體的圖裡面 你就可以知道說 當這個 W1 有變化的時候 當 B 有變化的時候 那我們的 loss 呢 會有什麼樣的變化 那可以發現說 我們的這個 loss 呢 這個 loss 的 surface 呢 它是一個非常深的峽穀 那在這個峽谷的穀底 其實也是有些斜度的 只是相較於這兩邊陡峭的山坡 中間的斜度呢 相對而言呢 是小很多的 好那這個是畫成這個等高線圖 然後最好的點呢 有被點出來 那這邊就是 通過暴力搜尋法 找到最佳解釋 如果 W 代 1.67 B 代 4.88 那得出來的 loss 是最小的 大概是 240 左右 所以你可以看到 在這個 Error Surface 上面 最低的點就是在紅色的點這個地方 那如果是 W1 代 1.67 那 B 代 4.88 的時候 這個值是最小的 好那剛才呢 就是用暴力搜尋的方法啦 那其實你看根據暴力搜尋的方法 你也可以知道說這個 Error Surface 啊 是一個相對沒有非常複雜的 Error Surface 所以照理說 我們隨便找一個地方開始 然後用 Gradient Descent 的方法 沿著下坡這樣滑下來 慢慢應該就可以走到最低的點的地方了吧 所以感覺 Gradient Descent 用在這邊 應該是會有不錯的結果的 真的是這樣嗎 好我們就來實作一下 Gradient Descent 吧 所以你要先有 W1 跟 B 的初始參數 我這邊就隨便設 1 跟 15 這個值呢 你通常是用個隨機亂數產生 你可以隨便設 那因為我們想要記錄 W1 跟 B 的變化 所以我這邊開了一個叫 W1_history 跟 B_history 的 list 我是為了要存 W1 跟 B 的變化 我也想存 Loss 的變化 所以我這邊開了一個叫 Loss_history 的 list 我要存那個 Loss 的變化 那我先把最開始的 W1 放到 W1_history 裡面 把最開始的 B 放到 B_history 裡面 我也想知道這個最開始的時候 Loss 是多少 所以我用最開始的 W1 跟 B 根據我們現在的訓練資料的輸入 去計算出輸出 然後根據輸出跟正確答案 我可以計算出現在 Loss 把現在的 Loss 放到 Loss_history 裡面 所以我有現在 在什麼事都還沒有做剛開始的時候 我們起始的參數跟起始的 Loss 再來就進入了一個看似簡單 實際上非常關鍵的一步 這一步就是要設定超參數 比如說我們要設定 Learning Rate 這個 Learning Rate 太大也不好 太小也不好 沒人知道應該要設多少 所以這邊我們就先亂設一個 0.0018 看看會怎麼樣 然後再來就是要訓練幾個 Epoch 那這個要訓練幾個 Epoch 才會好呢 也說不準 先試個 100 個試試看 然後接下來就開始進入訓練了 那這邊有一個 for 迴圈 在 for 迴圈之前 我們這邊先訓練 先計算出我們總共的訓練資料有幾筆 那這個是等一下在做 Gradient Descent 的時候 會運用的上這個數值 進入 for 迴圈 for 迴圈就是跑一個一個 Epoch 在這個 Epoch 裡面 第一步 在這個 Epoch 裡面 第一步是要先計算 Gradient 也就是計算偏微分 怎麼計算 W1 對到 Loss 還有 B 對到 Loss 的偏微分呢 那這個我已經事先算好了 反正就是長這個樣子 那在這兩個式子裡面 你都需要 在這兩個式子裡面 你都需要用到 Y prediction 的結果 你都需要用到 Y prediction 的結果 所以你需要先根據現在的 W1 跟 B 先算出根據現在的 W1 跟 B 給 X1 然後預測的 Y 是多少 然後你要有這個數值 才能去計算 Gradient 因為這個 W1 跟 B 的 Gradient 裡面 都用得上根據現在的 W1 跟 B 做出來的 Y prediction 的結果 那有了這個以後其他都很簡單啦 就是把 Y_prediction - Y_train 乘上那個 X1 的數值 這個就是 W1 微分後的結果 那前面要乘上 前面乘上 2/n 然後把 Y_prediction - Y_train 對所有資料做加總 那就得到 B 的微分的結果 好這個是把 Gradient 算出來 把 Gradient 算出來之後呢 接下來要更新參數 這邊根據 Learning Rate 還有 Gradient 來更新參數 所以 W1 要減掉 Learning Rate 乘上 W1 的 Gradient 然後得到新的 W1 然後把這個 B 減掉 Learning Rate 乘上 B 的 Gradient 得到新的 B 好我們就更新完參數了 那接下來更新完參數以後 我想要記錄一下 現在這個新的參數 它的數值跟 Loss 所以我把新的參數放到 history 裡面 把新的參數放到 history 裡面 然後呢我用這個新的參數 這個 W1 跟 B 是已經更新過的 雖然我這個 notation 沒有變 但它們是已經更新過的 我拿更新過的參數去計算出 prediction 根據這個 prediction 計算出現在的 Loss 把現在的 Loss 存起來 好所以我們每做一次 update 我都會記錄下更新以後的參數 跟更新以後的 Loss 然後呢我會把那個更新以後的 Loss 還有更新以後的參數把它印出來 然後呢最終訓練完畢的時候 我會印出最終的 W1 跟最終的 B 那我們來看看結果怎麼樣吧 100 個 Epoch、Learning Rate 0.01 行不行啊 哎呀不行 你看 整個這個 W1 跟 B 的數值都大爆炸 Loss 也大爆炸 一切都大爆炸 沒辦法 所以看起來 0.01 0. 而且我們設多少啊 0.001 太大了 改小一點嗎 0.0001 嗎 0.0001 看起來還可以 剛才窮舉的那個 Loss 呢是 240 所以我們現在算出來 Loss 是 263 其實離 240 呢也是有一點接近了 那我們其實可以把那個 Error Surface 跟參數 update 的那個過程都畫出來 那這種畫大家再自己研究 我們這邊就是把 Loss 下降的過程把它畫出來啊 然後把這個參數 update 的過程啊 把它記錄下來 所以可以看到說 現在才走到這裡而已 所以看起來呢還有一段距離 我們現在設 100 那如果我再加個 10 倍呢 怎麼還在 261 的地方啊 怎麼 Loss 還是 261 啊 只走了一點點這樣不行 設 1 萬 希望不要讓大家等太久 那你說怎麼不 Learning Rate 設大一點呢 但你忘了嗎剛才 Learning Rate 設稍微大一點 就已經會噴飛出去了 所以 Learning Rate 不能設更大 爆跑一下 從 261 到 249 剛才最好答案是 240 啦 所以看起來還是差一點點 阿你看這個learning rate走得非常的慢 Learning rate設小的時候 就有可能走得非常慢 但是設大一點的又會飛起來 所以這招不是一個非常好的招數 有沒有更好的招數呢 這個我們留著下週再講 好不管怎樣就做到這邊了 好那我們再講一下 Batch Size 的概念 在剛才我們在計算 Gradient 的時候啊 你會發現我們都有用一個 np.sum 也就是我們對所有的資料算出 我們對所有的資料都考慮進去 我們對所有的資料都進行了加總 我想要拉到前面一點給大家看一下說 這邊都有 np.sum 都有 np.sum 就是對所有的資料進行了相加 當然因為我們現在資料很少啦 所以對所有資料進行相加 花不了多少時間 但是很多時候 假設你資料非常大的時候 你可能會想用這個 Batch 你可能會想讓你的模型 每看到一部分的資料的時候 就 update 一次參數 那至於用 Batch 的好處跟壞處 那我們剛剛上課的時候也已經跟大家剖析過了 那在這個 Colab 裡面呢 我們是把它好處跟壞處再講了一遍 好, 那我們來實作一下 Batch 吧 那前面這邊都是一樣的 W1、B 要有一個初始化的數值 然後我們要有一些 History 的 List 來記錄參數的變化跟 loss 的變化 這個都是剛才已經有的事情 然後呢, 這邊 Learning Rate 就跟剛才一樣都設 0.001 吧 那 Batch Size 呢 Epoch 呢, 我們先設 1, 我們先跑一個 Epoch 就好 那 Batch Size 呢, 我們總共有 20 筆資料 所以如果我 Batch Size 設 20, 那其實就是 Full-Batch 就等於是沒有達成那個 Batch 的效果 那如果設小一點, 比如說設個 5 那有 20 筆資料, 那就是 4 個 Batch 好, 那我們就開始進行訓練 那先知道總共有多少筆資料 然後計算出有幾個 Batch 那 Batch 的量就是資料的量除掉 Batch 的 Size 然後呢, 接下來呢, 就進入 Epoch, 那在每一個 Epoch 裡面會做什麼事情呢? 首先會做一件事情是 Random Shuffle, 我們剛才有講過說呢, 我們希望每一個 Batch, 在同一個 Batch 的資料是不一樣的, 所以這邊呢, 會先打亂資料的順序, 然後再去分 Batch。 所以你不會每次都是同樣的資料在同一個 Batch 裡面。 好, 然後接下來呢, 我們還會有另外一個 for 迴圈 在這個比較小的 for 迴圈裡面, 我們才是把 Batch 一個一個的去看過 所以前面這是一個 Epoch, 在一個 Epoch 裡面, 我們會看過好多個 Batch 把所有的 Batch 都看完, 才叫做走完一個 Epoch 然後在每一個 Batch 的 iteration 裡面呢, 我們會把那個 Batch 的資料讀出來 我們把那個 Batch 的資料先讀出來 然後呢, 我們再計算 Gradient Descent 那我們這邊計算 Gradient Descent 的方法呢 其實跟前面是一模一樣的 這邊的式子 如果你跟剛才的式子做比對的話 是一模一樣的 唯一不同的地方只是 當我們在做這個 Summation 的時候 不是針對所有的訓練資料去做 Summation 而是只針對一個 Batch 裡面的資料 去做 Summation 那我們的訓練資料量也變了 本來是大 N, 現在變成 n_batch 現在變成 n_batch 好, 然後呢, 計算出 Gradient 以後 我們就可以更新參數 然後呢, 我們會把更新後的參數存起來 我們也會把更新後的參數 計算出來的 loss 存起來 然後我們會把那個 Error Surface 把它畫出來 我們也會畫那個 loss 下降的曲線 來看一下做得怎麼樣喔 再 update 一次, 就只走這麼一點點喔 就只走這麼一點點 那如果你 Batch Size 設小一點 那就一個 Epoch 裡面就走多步一點 Batch Size 設 1, 那就可以走 20 步 有 20 筆資料就走 20 步 那也發現說呢, 這個模型走的呢 就是有點歪歪斜斜的 走的有點歪歪斜斜的 但是你可以在一瞬間呢 在同樣的一個 Epoch 裡面 就可以走出比較多的步數 那有時候對訓練是比較有利的 Batch Size 要開多少 你才能夠得到最好的結果 這個就要問你自己 你看這邊我們已經 如果我們看這個 Loss 的最低點 我們這邊已經跑到 100 以下 那如果你只有一個 如果你是用 Full-Batch 的話 只有一個 Epoch 你應該是跑不到 100 以下了 你看只有一次 update 的時候 我們的 Loss 大概是 450 左右 所以如果你有用 Batch 的概念 然後在這個例子裡面還是有用的 好 那做了這麼多 我們來驗證一下我們的模型做的怎麼樣吧 我們剛才呢 是有把 W1 我們剛才訓練最好的結果 跟 B 我們訓練最好的結果把它存下來 所以呢 我們現在就把 X1 在 Validation Set 上面的數值 跟 W1 還有 B 一起帶進 Linear Model 得到我們現在訓練出來的模型 在 Validation Set 上 Prediction 的結果 再計算在 Validation Set 上 Prediction 的結果 還有 Validation Set 上真正的結果的差異 算一下 哇 一算出來這個 Loss 挺大的 快將近 1000 感覺非常的糟糕 然後再來怎麼辦呢 你就要去檢視說到底是哪一步出了問題 那有可能是第一步出了問題 也許我們的訓練資料跟測試資料本來就是長得很不一樣 也就是他們是不匹配的 所以我們應該換一下訓練資料 所以我們把訓練資料從 2021 年的機器學習換成 2024 年的生成式人工智慧導論 那我們的 Validation Set 是一樣的 好來換一下 換一下 好 然後接下來訓練的過程跟剛才是完全一樣的 因為你甚至都不用改那個參數的名字 因為我們現在是拿同樣的名稱來代表訓練資料 我們只是把指導訓練資料的資料把它改變而已 所以這個 code 幾乎不需要改就可以開始訓練了 改了訓練資料以後 那我們就來開始訓練吧 這些參數跟剛才都是一樣的 每個東西都是一樣的 我們來看看做起來會不會更好一點 剛才我記得 Loss 是 200 多 現在 Loss 降到 78 結果還不錯 看起來換了訓練資料 但這個跟訓練資料有沒有匹配是沒有什麼關係 就是說這個現在我們用 2024 年的這個訓練資料 他的 Loss 是可以算出來比較低的 那真正重點是在 Validation Set 上有沒有比較好 剛才用 2021 年的機器學習在 Validation Set 上得到 Loss 將近 1000 那現在我們換了訓練資料 再重新跑一次 Validation Set 上的結果看起來有沒有差別呢 現在結果就好一些了 雖然 Loss 還是很大 算出來將近 350 仍然不是非常理想 但是總比 Loss 高達 1000 還要好的一些 好那接下來呢 就是我們想要畫一個更大的 Function Set 的搜尋範圍 也許我們第一個可以做的事情是 剛才的輸入只有 X1 這個 X1 是投影片的頁數 也許我們可以加一些額外的輸入 比如說這個投影片上有多少的字來做更豐富的考慮 來考慮更多的資訊 花一個更大的範圍的函數 呃所以我們需要定義另外一個 Linear Model 我們叫它 Linear Model 2 它的輸入是 x1, x2, w1, w2 跟 b 這個 w1, w2 跟 b 它的參數 x1 跟 x2 是輸入 那它的輸出就是 w1 乘 x1 加 w2 乘 x2 然後再加上 b 好 我們有這個第二個 Linear Model 它可以吃兩個輸入 好 那我們現在呢 要有第二個輸入 我們叫它 x2 train x2 train 是 2024 年的頭影片上的字數 每份投影片上的總字數 然後有 x2 validation 就是 2025 年的投影片 每份投影片上的總字數 我們需要把這個新的訓練資料把它加進來 好那開始訓練吧 其實這個訓練的過程跟剛才的過程呢 沒什麼不同 我們現在就是多了 w2 要考慮啦 剛才只有 w1 嘛現在多了 w2 所以你要給 w2 的你要的這個初始參數 那從現在開始我們就不記錄參數的變化啦 因為有三個參數的變化 所以你沒辦法在二維平面上呈現啦 所以我們就不記錄參數的變化 我們只記錄 loss 的變化 好, 然後接下來呢, 這個 w 我們現在多的其實就是計算 w2 的 Gradient 其實 w2 的 Gradient 算起來的方式跟 w1 其實是一模一樣 唯一不同的地方就是你需要把 x1 換成 x2 這邊 x1 train 換成 x2 train 而已 更新的時候你也要更新 w2 然後記錄下當前的參數更新之後的 loss 然後印出來 好, 然後呢, 我們就來跑一下這個訓練吧, 看看能不能跑起來 哎呀, 都是 NaN, 怎麼回事 看起來這個 Learning Rate, 剛才我們只有一個參數的時候, 這個 Learning Rate能做 但現在如果是兩個參數的時候, 這個 Learning Rate 看起來不行, 太大了, 再弄個小一點的 欸 不 還是 NaN 再弄更小一點 哎呀 還是不行 哎呀 你這個到底要弄 你這個到底要多小 Learning Rate 你才跑得起來啊 太過分了吧 啊 設了一個很小 Learning Rate 勉強跑起來啦 但是因為 Learning Rate 太小了 所以 這邊看起來呢 他這個 loss 下降的是非常緩慢的 緩慢的 好 總之 Loss 現在算出來是 69 好了一點點了 剛才我記得是 77 開頭嘛 所以現在 loss 呢 加了一個額外的 這個 w2 以後呢 loss 還是下降了一點點 但是啊 我們剛才有說 這個直接算字數不是一個好方法 應該要算平均字數 而不是算總字數 所以我們把總字數除掉 投影片的頁數 得到平均字數 當做新的 feature 那我們把這個新的 feature 再拿來做訓練吧 其實這個新的 feature 會好訓練很多 為什麼這個新的 feature 會好訓練很多 你看剛才發現說我們的 loss 我們的 Learning Rate 要設得非常小 一個可能的原因是因為 今天如果你考慮的是總字數 總字數的數值跟頁數的數值差距太大了 當你的兩個輸入數值差距非常大的時候 會導致訓練非常的困難 所以當你換成平均字數, 它跟頁數比較接近的時候 那訓練起來其實會更容易一點 那加上平均字數其實又是一個比較合理的, 比較容易學的東西 所以我們可以預期說當我們把總字數換成平均字數的時候 應該是比較容易訓練的 好, 總之我們改了 x2 train 跟 x2 validation 的定義 然後再重新來訓練一次吧 那訓練的過程跟這個程式碼呢 跟剛才完全是一模一樣的, 不需修改 重新訓練一下 看起來 loss 又再低了一點 現在 loss 可以進步到 65 不過到目前為止 我們用的都是一個 Linear 的模型 我們可以驗證一下 驗證一下看看有沒有好一點 我們把我們現在新的模型 拿來做這個 Validation 那我記得剛才只有一個 feature 的時候 Validation 算出來的數值是 300 多 現在有兩個 feature 能不能好一點呢? 有好一點! 我們的 Validation 算出來是 124 還不錯 從 300 多進步到 124 那現在如果 因為這是 Mean Squared Error 所以如果你開根號以後呢 也是接近 11 所以我們現在的誤差呢 預測的誤差大概是 11 分鐘左右 好, 到目前為止 都是用 Linear 的 model 來改成用一個 Neural Network 吧 所以這邊呢 先定義了一個 簡單的 Neural Network 的函式 這個 Neural Network 的函式呢 它的輸入有 X 這是我們的訓練資料 有第一個 Layer 的 W 有 B, 有 W' 跟 B' 這個我們剛才課程裡的 用的符號呢 是一樣的 輸入 X 以後, X 會被乘上 W 這個小老鼠的 符號呢 代表的是矩陣相乘 在 Python 裡面呢 所以把 x 跟 w 做矩陣相乘 再加上 b 這個向量 我們得到了 z 然後把 z 再通過 ReLU 這個 activation function 那把 ReLU 定義在這邊 ReLU 就是輸入一個東西 x 那它會幫你去算說 如果這個 x 是小於 0 那我們就輸出 0 如果是大於 0 就輸出 x 本身 把 z 過 ReLU 得到 a 然後呢再把 A 乘上下一層的參數 WP 再加上 BP 得到 Prediction 的結果 然後呢我們就把 Prediction 的結果回傳 那除了回傳 Prediction 的結果以外 我們會把一些中間產物 包括這個 Z 跟 A 呢也回傳回去 我們把它放在 Cache 裡面回傳回去 那至於為什麼要這麼做呢 這是我們等一下訓練的時候會用到的 如果你只是要做測試 用這個 network 它參數都有了 你其實是不需要這個 Cache 的 但是因為我們等一下訓練的時候 會需要用到這個資訊 總之這邊就是劃定了我們訓練的範圍 我們要訓練的就是一個類神經網路 它這邊看起來平平無奇 但是它是一個類神經網路 那未知的東西就是 W, B, WP, BP 它們是要透過訓練資料被找出來的 我們先來實際使用看看 這個類神經網路 我們先把 W, B, WP 跟 BP 都給它一些隨機的數值 然後真的給這個類神經網路一個輸入 看看它能不能夠給我們一些輸出 那這個類神經網路呢 在它內部沒有定義它的 Hidden Layer 的 size 也就是它的中間的隱藏層有幾個 neuron 那這邊是定義在外部的 你給它 W, B, WP, BP 的形狀的時候 就決定了它的 Hidden Layer 的大小 Hidden Layer Size 我們就直接設個 100 那 W 就是一個 100x2 的矩陣 B 是一個 100x1 的矩陣 WP 是一個 1x100 的矩陣 BP 是一個 Constant 所以就是 1x1 的矩陣 然後我們把我們的訓練資料有兩個 feature 丟進去 把這邊 Random Initializer 它裡面的數字都隨機的 W, B, WP, BP 也都帶進去 然後進行運算 然後把 Prediction 的結果印出來 那就可以得到 Prediction 的結果 至少知道說 給這個類神經網路參數的時候 這個函式是可以順利運作的 裡面沒有什麼 項量、矩陣 它的 dimension 不匹配的這種問題 它是一個可以順利運作的類神經網路 好 接下來就要進入訓練的階段了 這邊類神經網路的訓練 是用 Python 直接實作的 當然你在作業裡面還有真實的應用裡面 你現在都會用深度學習的套件 沒有人會在手刻這個 deep learning 了 那在作業裡面 那主教範例程式也可以説明你 使用 PyTorch 的套件 順利的完成一個內神經網路 所以我這邊的程式並不值得你參考 好 那我們來訓練吧 來訓練一個內神經網路吧 那我們要怎麼訓練呢 首先呢 我們要初始化 我們要找參數 給他一個初始的數值 W, B, WP, BP 在前面幾個例子裡面這個初始的數值啊 我都直接寫好了什麼 1.0 啊 15 啊之類的 但現在這個 W 啊這些數值裡面 它是一個非常大的矩陣 所以它的數值我沒有辦法直接手寫 所以我們就是隨機的生成它 好定個 Learning Rate 然後定要訓練幾個 Epoch 然後 Loss History 呢 我們要把它記錄下來 然後計算有幾筆訓練資料量 好接下來呀 我們就要進入內神經網路的 update 了 這邊呢 如果你知道 Backpropagation 的話 我們需要 forward-backward 這兩個 pass 所以我們先做 forward pass 然後再做 backward pass backward pass 呢 它內部的運作 寫在這個地方 有一點點複雜 那裡面會需要用到 ReLU 的 Gradient 把它寫在這邊 那這個我們就不細講 那總之我們就是對一個類神經網路計算出它的 Gradient 所以我們每一個參數 W, B, WP 跟 BP 它的 Gradient 都算出來了 我們就把 Learning Rate 乘上 W 的 Gradient 我把這些參數前面加個 d 代表是它的 Gradient 然後我們就把每一個參數減掉 Learning Rate 乘上它的 Gradient 得到更新後的參數 好, 有了更新後的參數呢, 你就可以跑一下這個內神經網路, 得到它的 Prediction, 得到它 Prediction 以後, 你就可以算一下 loss, 把 loss 存起來。 好, 那到底訓練不訓練得起來呢? 我們這個 Learning Rate 呢, 是 0.0001, Epoch 是 1 萬。 很意外 喔 85 我們剛才不是都跑到 Loss 已經可以低到 6 開頭了嗎 怎麼只有 85 而已 這個 Loss 太高了 內神經網路一定可以做得更好 不可以只有這樣的 Loss 再重新跑一次 哎呀 這一次更糟 這一次 238 為什麼每次跑都不一樣呢 因為每一次初始的參數不一樣 初始的參數不一樣 你最後會走到不一樣的地方 所以 Loss 就會不一樣 那因為類神經網路非常複雜 所以往往初始的參數對結果 也是會有一定程度影響的 重新跑一次 結果更差 再跑一次 哎呀 還是 200 多 哎呀 怎麼這個 Loss 壓不下去呢 這個就是你訓練類神經網路的時候 最容易遇到的問題啦 這個 Loss 壓不下去 而且因為參數量非常多 你又沒有辦法畫 Error Surface 所以往往你根本不知道問題出在哪裡 所以這個時候你能憑藉的是什麼呢 憑藉的第一個就是 你對於類神經網路的信仰 你相信它的 Loss 一定可以壓到更低 你相信它的 Loss 絕對不只這樣 所以你開始爆條參數 而另外一個你要憑藉的就是大量的運算資源 開始爆試各種不同的 hyperparameter 直到試到一個 Loss 夠低的為止 然後今天我們是因為訓練資料量非常小 內神經網路非常小 所以每按一次執行只花了幾分鐘的時間 如果今天每按一次執行要花一天 你就是欲哭無淚 那怎麼辦呢 接下來就是一頓操作猛如虎 接下來就是做了大量的參數的調整之後 另外我改了 update 參數的方式 我們現在是用一個叫做 Adam 的方式來 update 參數 那這個我們下週才會講 我本來不想用這一招 本來不想在這個時候用 Adam 的 但是看起來這邊不用 Adam Loss 真的是壓不下去啊 所以哎呀 只好用個 Adam 好總之他這邊有比較複雜的參數 update 的方式 我們先不管他這個下週的事情 然後這個 initialization 也不可以隨便 initialize 要有特別的 initialization 的方法 才可以得到有可能比較好的結果 總之一頓操作之後 一樣的類神經網路, 一樣用 Gradient Descent 但是現在不同的初始而值 不同的 Learning Rate 的計算方式 這個 Adam 可以給我們一個動態的 Learning Rate 每個參數的 Learning Rate 都不一樣 也會隨時間不一樣 來看看有沒有救啊 欸你看這個 Loss 就可以壓到 21 所以這個還是有用的 這個接下來呢就要在 Validation Set 上再試一下啦 所以我們趕快剛才訓練出來的這個類神經網路 拿到 Validation Set 上再跑一下 看看結果會怎樣 剛才我們的 Loss 可是壓到 21 喔 Validation 的 Loss 呢 哎呀 1300 超過 1000 大炸裂 怎麼辦呢怎麼辦呢 其實呢你在訓練 Network 的過程中 你就已經可以挑 model 去在 Validation Set 上做 evaluation 了 所以我們這邊呢多加了一行 這一行是什麼 就剛才呢是每次我們訓練出 每次 update 參數之後 我們都會重新計算一下 loss Training Data 上的 loss 現在我們不只算 Training Data loss 我們把 Validation Set 上的 loss 也把它算出來 然後最後呢 我們會畫圖展示training loss的變化 還有validation set的loss的變化 好, 這邊的參數, Epoch 500, 那就這樣吧 跑一下 好, 你會發現說呢, 我這個 Training 的 Loss 是下降的 Validation Loss 是先降後升 在某一個地方, 我們其實是有比較低的 Validation Loss 那要有多低呢? 其實我們可以把它輸出出來 我們這邊把那個 Validation Set 最低的那個低點的數值把它輸出出來 所以 Validation Set 它的 Loss 是 11 點多 在 Epoch 23 的時候 有最低的低點 好 那我們來 我們現在知道說 如果 Epoch 23 在 Validation Set 上 會有一個大概 11 點多的 Loss 那這樣這個差距已經落到 3 分鐘左右了 那我們就真的拿這個模型 來在測試資料上做測試吧 首先測試資料就是今天的課程 就是課程最後的高潮 測試完我們就下課 好我們先只跑 23 個 Epoch 得到一個 Loss 不錯低的模型 好真的來測試吧 這個測試呢就是我們需要輸入投影片的總字數 我們先算一下正確答案 就今天這一堂課呢到底花了多少分鐘 我們剛才是 2 點 23 分的時候開始上課 然後呢, 我們在 3 點 35 分的時候 2 點 23 分的時候開始上課 我們在 2 點 25 分的時候 然後下課 這個是 1 個小時又 2 分鐘 所以是 62 分鐘, 這是第一堂課 那我們不算下課的時間 也不算跑 Colab 實作的時間 第二堂課呢 這邊寫 3 點 35 分開始上課 上到幾點呢? 上到 4 點 20 分 所以總共多長呢? 總共 45 分鐘 好, 我們總共上了 40 所以兩堂課加起來總共 107 分鐘 這是我們的正確答案 好, 那接下來就要看說 今天到底能不能夠成功的 我們訓練出來的模型 到底能不能夠成功的預測 今天上課的時長呢? 其實我心裡也沒個底 好 今天投影片的頁數有多少呢 有 84 頁 84 頁 好 投影片的頁數 是 84 頁 好 有多少總字數呢 我剛才握著算了一下 我們投影片裡面的總字數是 3388 個 好 緊張的時刻來了 用這個類神經網路 它預測出來的數值 跟 107 會有多接近呢 我們來跑一下吧 啊 真的是 107 天啊 這個我真的沒有塞好 這個只是純粹運氣好而已 總之這個就是我們測試的結果 非常的精準 雖然我覺得只是單純運氣好而已 那我們今天上課其實就上到這邊 謝謝大家