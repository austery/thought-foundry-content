反正现在我没有access了嘛 就是所以也算自由了吧 就是想干啥干啥了 对 现在这样 恭喜 我是在 就是 准备访谈的时候 才发现你在Meta已经工作10年了 对对对 那个时候 2015年还是个 呃不是 对 15年Meta还是一家很小的公司 20212年上市的 对 你加入的时候Meta多少人 那个时候 那个时候可能大概有1万多一点吧 可能 对 其实（公司）也不小了 嗯不是特别小 但是 现在可能几十万 对 嗯 我们沿着原来的那个访谈大纲 去聊你的论文 也行 然后我们address一下这个new development问题 也行 我们可以聊一些论文 我觉得这个还是比较好 因为我其实并不想说太多裁员的事情 我也觉得这个不太好 对 就是推特上已经去 发了一个帖子就有那么多人 那么多人点赞 那我 小红书上也是 刷到你的名字 一天刷到了好多 对 我觉得 其实我觉得 我这边本意就是 因为我这边team也有几个人 是被影响到了嘛 所以我当然希望他们有更好的机会 因为我是无所谓了 我最惨的话 不行就在家里待着 但是他们很多人身份（签证等）会有些问题 如果不能及时找到下一家（公司）的话 对吧 那肯定要想办法 我也帮帮忙 帮找一找 因为我毕竟这边认识人比较多 这个也是我的本意吧 我在想就是我 反正我也 我也不怕自己暴露自己被裁 i don't care 对 但是就是希望 我的手下那些人（前同事）能很快找到工作 这也是我的本意 你需要什么帮忙吗 如果有些事情你自己说不太合适的话 我也可以帮你联系一些人 应该不用 应该不用 我觉得还是有很多人reach out 所以他们还是 我觉得他们是有机会的 他们是有机会的 对 我pass一个 这个ask同不同意都行 就是腾讯新闻 腾讯新闻的人 他说想采访你 然后我说 问问你有没有时间吧 说聊论文也行 聊Meta公司的内部问题也行 他们都可以做出人力来做深度内容 我觉得是这样的 就是不要 我觉得是这样 不要聊Meta公司内部问题 我觉得我已经说够多了 我不想再说了 对 因为我 那你有兴趣跟他们聊论文吗 聊论文 要看聊什么论文吧 对吧 那也是可以的 嗯 对 我不希望聊太多Meta公司内部的 因为其实我觉得 我一般不太愿意说这些 然后我觉得 我在Twitter上唯一说的是 因为就是有人跳出来说 你们被裁是应该的 因为东西没做出来对吧 但是 那我要至少给我们的 至少给我们的team要做澄清 对吧 因为我们team说了很多 的重要工作 那你不能把锅扣到我们头上 所以 这个肯定是要讲清楚 但是我现在就比较defensive 就是OK 如果有人说这个是我们的锅 那我们会说回去 对但除此之外我不会说太多 就是公司内部的事情 嗯 好 那你还有没有什么 想澄清的 你觉得没有澄清 好的 哈哈我觉得差不多了吧 就这样就是 我觉得差不多了 我们其实还是做了很多的工作 就是把（公司）很多的之前的一些问题解决了 比如说 包括long context的那个reinforcement learning 有没有训练的好 对吧 还有包括前面的pertaining model啊 他们的design 其实可能有些问题 像有long attention的问题 这个其实很多是我们团队解决的 就是 反正是我先发现的 关于这个design（方面）有问题 然后去跟他们（公司侧）讲 但是一开始就很难去 就是他们不一定会 不一定会听 因为我当时来（Meta） 我当时去的时候是 按照一个research team来做的 对吧 而对方（相关部门）是做大模型的 那么我这边research team过来的话 他们不一定会听 那他们可能会觉得这个（观）点 （或者说）这个事情没问题的 肯定是对的 那我们这边要用各种实验就证明 我们之前的那些发现 或者说insights（洞察）是对的 但是 后来是他们是被说服了 对 所以他们才会发现其中有问题 对 所以这其实都是我们团队的贡献 还包括 怎么样去让long context first training 更加稳定 对吧 包括有很多的blow up的问题 怎么样去解决 这些东西都是都是我们这边做的 对 所以 这些东西 就是说 也属于“幕后英雄”嘛 就是 因为毕竟最终我们这个模型 也没有真正official release 至少我们有一些贡献在里面 这个我是得说说 说出来 那么这样的话 至少为后面的人添砖加瓦 加一下 对不对 就是做一个比较好的base 就是这样子 说到这 我有两个问题 然后 我自己今天早上还跟我老婆聊的时候 就说 我其实非常愤慨于 很多公司里边内部管理的混乱 然后导致 我先不说 这里管理混乱管理的事情 但是刚才的两个具体的问题就是 第一 你们作为一个research团队 人家不信任你 可能是觉得 之前你没有这个train（训练）大模型的经验 或者怎样 但是你们能很快发现问题 你觉得为什么可以做到 第二个就是 对面的大模型团队 是个什么样的团队 他们自己本身大模型训练经历丰富吗 他们是Llama-1或者2训练（团队）出来的 是经验丰富的 对 但他们有一些 应该说是 他们有一些地方（比如） 之前的实验有bug 有些bug 然后这个bug 导致他们做出错的判断 这个应该是这样的 对 但是我们这边虽然说没有训练大模型 但是 毕竟也是做过关于大模型的一些 一些文章嘛 对吧 包括我以前做过Sparse Attention 就是那种 稀疏注意力 那我当然对（比如） 注意力结构 我知道什么是意思 是怎么回事 那当然 我一看这其中的设计 就觉得有问题 我相信 也有很多人都能看出来 这个并不说是 只有我能看出来 你肯定能看出来 所以 我并不知道 当时这个决定怎么做 但是我愿意说 最后也是没办法 因为 也很难去说服他们 就是 你要花很多时间和精力去跟他们说 说这个是有问题的 一直到他们自己团队发现了问题 也是有这个问题 那么慢慢的就是会改变 这个想法 所以应该 就是说 虽然作为研究员 就是可能我们当时做研究的时候 并没有直接去接触超大模型的训练 但是关于研究的那些直觉 或者说相关的经验其实很有用 对吧 能够很快的找到问题 能够发现什么地方 是 有问题 或者是有出错的 要怎么样去解决 我觉得这个是很重要的 这个是作为一个研究员的宝贵财富 就是因为说实在的 你 如果是一个完全没有任何insights的人 就是OK 我天天就跑实验 然后调参数 那这个 工作其实你说你能做 别人也能做 对吧 那研究员的优势是说 我能不能 嗯 把 这个 根据一些非常稀疏的数据点 能够得到非常重要的结论 然后这结论能够推广到更难的问题上 这个是研究员的能力 你说的稀疏的数据数据点 是不同论文和不同实验的结果 对 比如说 如果我是一位新来的“菜鸟” 那么对我来说OK 我的任务是调参数 跑程序 那我跑比如说我跑1万个点 那我就得到1万个点的参数值 然后我就说 我告诉大家 这1万个点数是我跑的 跑完之后跟大家说OK 跑完了 那这个是我的结论对吧 但是跑完之后 这1万点数值在那边是“死”的 那么你也没有什么Insights 没有什么概念 说 这1万点其实背后代表了什么意思 有什么样的结构 那这个 其实只有那些有经验的人才能看到 （如果）有经验的人可能看到 看了 比如20个点 就知道（其中）有什么问题 甚至就说看了 10个点 就看到 就说这个training curve刚train到一半 （发现）哦 我知道行不通了 （就）不要 继续跑下去 可能（是因为）这其中有些问题 所以这里其实（要表达的是） 为什么AI研究员 （整体）还是薪资比较高 我觉得很多时候是这样 就是你的一个insights（能力）可以 比如说可以抵100块卡 或者说抵1000块卡 一万张卡等 就是比如我不需要那么多卡 但是我还是有insights 可以得到一些比较好的结论 这个是 很重要的 你刚才用了两个词 一个是经验 一个是insights 然后我想double click一下 就是这个到底是个什么东西 有的人会觉得这是一个taste 有的人会觉得是个intuition 对吧 我们有好多词去形容这个东西 那你觉得这个东西到底是个什么 就是 刚刚已经有四个词了 对吧 然后四个词说的好像都是一件事 嗯嗯 那你能不能就是给大家讲一讲 从你的经验来说 你用多长时间 能判断一个人 有这个还是没有这个 然后你觉得他有的话 他除了像你刚刚说的 就是从很小的数据点 就能得出来一个更正确的结论 他可能背后的mental model更好以外 还有什么展现 以及怎么得到这个东西 对 这个我觉得是这样 就是Insights呢 是一个比较 很难描述的一个概念 就是说 其实是这样 就是 特别是一个有经验的人 对吧 比如说在某方面他是位“老师傅” 那么他怎么做 就是他要根据很少的数据 然后判断他就是这个现象 背后的真正原因是什么 这个是重要的 就是比如说一个修车师傅 他可能根据蛛丝马迹 会知道你车哪里坏了 明白别人（在）还没有 反应过来就（知道）说这个车哪里坏了 或者说股票交易员 比如说操盘 操盘的时候 比如说有一个 做股票交易的 那我说我根据这两个迹象 或者看看财报 嗯 比如这个这个股票（等）不能买 所以这种东西（能力）是很重要 然后他也不知道 也讲不清楚到底怎么回事 他就有种感觉说 这个不行 那个行 有一个mental model 然后这个mental model大概率是对的 所以这个（能力项）很重要了 就是有这些东西之后 其实就是很快能够发现问题在哪 然后有这问题我们怎么样 怎么样去解决 去解决这个问题 然后然后往正确方向去走嘛 这个是 可能比GPU还要重要 对 当然了GPU也很重要了 就是有GPU之后你会做更多实验 获得更多的Insights 所以这两个是相辅相成的 应该这么说 你能很快的判断另外一个人 有没有一个好的mental model吗 （补充问题）在你的领域里？ 这其实是有一些办法 就是说 你要跟别人聊嘛 然后大概聊一下 感觉一下他平时对这问题是怎么想的 我觉得这个其实挺重要的 对 其实我可以举个例子吧 比如说 比如说那个学校里面有这种PhD exam啊 就是说PhD quantifier啊 就是说一个学生 坐在那个 教室中的一堆老师们面前 然后老师问他 就是 我请问你 对这个问题有什么了解 比如说我们讨论一些学术问题 那么对这老师来说 他想办法就问到底 就是比如说 你对这个偏微分方程 有什么想法 然后你有什么 比如12345等经验 然后就会抓住一个点 然后使劲问 问到 问到就是你能（了解到）他到底懂不懂 他到底知道这里面之间什么关系 这个是能用最简单的语言讲清楚 对 然后就能够知道 最重要的两个东西的关联是什么 那么这样的话 就知道这他真是懂的 或者说他真的是知道这是最关键的 关联在哪里 然后可以用这关联去做更多的 （比如）推广等 这个是重要的 就是因为比如说像 做研究的话 比较忌讳的是说 我就只懂书面知识 （比如有）12345 背出来了 概念套概念 但是他们（之间）有什么关系 然后什么时候 他们两个（在什么条件下） 能不能成立 什么时候要 a into b 什么时候a into c 这个并不知道的话 其实是比较 比较难搞的 我觉得这个是一个问题 对 其实这个很重要 就这说实在的 是我觉得现在的模型才做不到的地方 对 现在模型可能 没有办法用很少的数据 真的去预测将来的 那个结果 对 所以 所以 这个可能是现在人的一些能力 那我们就直接到这个话题吧 我觉得这个话题本身 也是你研究的一个 重点 就是 或者说我一开始想访谈你的原因 嗯嗯 你的论文是Groking 嗯嗯 但它是一个底层的这么一个 就是在一个时间点 它有了一个学习的这样的东西 顿悟的感觉 是吧 嗯嗯 对我在看你跟志渊的专访 里边 你也提到一个点 就是 那个鸽子问题嘛 对 鸽子问题 然后你是 你 当时和丹利.周 在这个Twitter上的 关于chain-of-thought的一些讨论 嗯嗯 就是说确实 理论上 你的这个逻辑能表述 chain-of-thought 就似乎可以解 但是模型会用无限的数据 去试图解决这个问题 但是 人似乎一下子就能get到这个问题 嗯嗯 我觉得和你刚说的那个东西 有一些联系 嗯嗯 然后 但是 如果你来定义这个能力的话 你会把它定义成reasoning吗 还是你把它定义成一个什么 这个顿悟呢 应该说呢 并不一定是 就是它是在reasoning 或者说其他的一些task下面 就说这个机制 下面是什么意思 是更底层的意思还是什么 是更底层的意思 就是说它是一个表关学习（表示学习） 就是representation learning的一些行为 就是 我随着这个训练的 拓展 你会发现 它的表征会改变 就是一开始就说 这就相当于比如说 你看金庸小说 然后张无忌 一开始被他义父谢逊逼着 需要你把东西全背出来 说先把东西全背 背出来之后 然后对 先反复 先全背出来 背出来之后 你不懂没关系 但是 不懂你可以脑子里存着 然后过了几年之后 突然之间就 连了乾坤大挪移 突然懂了 啊 是是 这样子 就是这个是很有意思的 一个机制嘛 其实 比如说你当时教小孩子 可能也是这样 特别是教有些小孩说 你先把它背出来 这个叫“读书百遍其义自现” 就是你现在先读 读了之后你并不知道什么意思 但是过一段时间之后 或者说你跟其他的一些事情 能够联系在一起了 之后呢 那 这样的话 你就会有一个 就突然之间你会觉得 哎 这个 这个意思 是跟我这个现实世界是有关系的 或者说 这两个意思之间是有关联的 我们知道更深的联系 这种其实就是应该说是顿悟的一部分 对 那这个是机制 其实是 就是思维链之下的 就说不管你用思维链做 做思维训练也好 不管你用那个直觉 来判断那个答案也好 或者不管你用啥方式来判断答案也好 对吧 那么这些东西 它的下面有一个共同的机制 就是说 我到底用什么样的表示 用什么样的 对这个世界的理解 导致了这个思维链 就比如说我举个例子吧 嗯 比如说吧 那个小学生做一道题 对吧 他可能说 我这道题怎么做 我用穷举法啊 那个11+1等于1+1等于多少 1+2等于1+3等于多少 那么有一些 穷举的一个路径 可以把这个事情做了 比如说 你要证明一个简单的一道习题 那么小学生会说 我穷举一些答案 看答案差不多了 那可能就对了 这是做法 这种做法 但是你这种方式呢 其实可能很多方面解决不了 那么等到 比如说初中生或者高中生 他们的这个思维其实有种飞跃 什么叫飞跃呢 就是说 我们告诉它 我们可以用数学归纳法 来解决这个问题 那么数学归纳法这个思维 这个思维 这个层次 是高于就是穷举法的 就如果你的数学归纳法 能够证明这个事情是对的 那如果能证明是对的 那么 它就对所有的那个自然数都成立 那这样的话 我的穷举法 就穷举无穷长的那个思维链 它其实都比不过数学归纳法的 很短的证明啊 所以这个是一个飞跃 就是说这样的话 你对这个问题的理解跟那个两种方式 的思维链 它的后面的理解是不一样的 所以这个理解或者说这个表示呢 其实就是神经网络学习的一个重要的 一个不同的地方 我不知道我我讲清楚了没有 很清楚 然后我想跟你对齐一个认知 然后给你看一个 一个东西吧 嗯嗯 就是当时我引用的 就是我们不是教课嘛 然后我们教课的时候 我自己发现 就是当时是伊利亚去 MIT 就是 几年前的时候 2016年的时候 他去讲的 嗯嗯 那个时候还 不是 现在这样子 嗯嗯 总之他当时讲了一个东西 我觉得他说的很深刻 他当时 他觉得很深刻 但是似乎听众没有觉得很深刻 嗯嗯 就说为什么back propagation会work at all 对 为什么 new work呃 然后就是这个 就是theoretically optimal hypothesis class 等于short programs 对 对 就听你刚那个意思 也是就是 本来我要去走好多条点 走到这 然后突然找到了一个更好的联系 然后我就有一个更好的压缩 然后它就更generalizable 对 对 这么说嘛 就是说因为压缩 可能也可以说是更通俗的解释 对吧 但是什么时候这事情能压缩 什么事情这些不能压缩 其实现在不是很清楚 就是 为什么你要去研究grokking这个机制 就是说 它给你提供了一个动力学过程 就是让你知道 它怎么从一个不压缩的状态 变成压缩的状态 你可以这么想 然后我再说一下 就是我接下来问题问问你 一个问题的铺垫 就是我会发现 这个和人类理解知识的 似乎也很接近 就是这个 人类也是information connect us形成knowledge 对 但是这个图是在neural network之前出现了 对 而且啊 很多教育专家 他会发现就是人 我记得在群里面 赵志诚我讲说 reasoning是一个人类固执的幻觉 对 然后这个教育专家说 the most important single factor is prior knowledge 对 你只要有prior knowledge就行了 你知道这个prior knowledge 然后然后就就就就 就没有什么聪明不聪明这种说法吧 反正 你只要把这个大学全都collect起来了 似乎就可以 嗯嗯 这就是 我接下来问题的这个铺垫 那接下来的问题就是 我们不知道 我们的这些knowledge 就是connect us是怎么形成的 对 我们没有办法去讲清楚 对 然后我们也不知道 就是 我觉得甚至在我心中 我不知道哪些knowledge是这个对 是那个dots 哪些是那个东西 还有 在大模型里边 你说在大模型里训练的过程中 似乎大家也不是很清楚 嗯嗯 对 就是 所以就是为什么 搞清楚其实可能会孕育着 就是下一个模型的一个契机嘛 对吧 如果你搞清楚了之后 你就知道什么地方你要修改 这样的话你就模型变得更强啊 所以这是也是个动力 就是因为我们现在就是 如果可以 你看可以满足说 我不我不把搞清楚 我把当黑盒子 然后我就上面调各种参数 开一个相当于一个很大的开关 有很多的开关 对吧 像以前那种电脑屏幕以前的那个 那个大型机 然后有非常多的按钮开关 然后就是 我们就培养操作员坐在上面 然后我把按钮开关开开 就是各种组合 然后看效果怎么样 那这是一种方案 那另外一种方案 就是说我们要把这个大的机器打开 然后理解里面的机制是什么 然后有这个机制的理解之后 那我以后再去播这开关的时候呢 就非常有感觉 我就非常知道哪些开关要开 哪些开关要关 能把这个做出来 这个是 我觉得这可能是一个更好的 一个做法啊 当然就是说 现在可能就是主流的 思维 其实并不是这么说 并主流思维 觉得 （比如）我们就叫做scaling law 我说 我不需要搞清楚你们在干什么 我只需要知道 就（比如）机器量有很多很多 然后我放很多有分量厉害的人进去 然后呢 让他们去拨那些开关 然后这些开关的某种组合找到了 那我们就能够把这个模型做的很强 这两种不同的思维 不同的思维 应该这么说 但哪种是对的呢 现在也不好说对吧 因为现在确实scaling law有很大的应用 然后确实那个效果也非常好 所以至少目前为止看起来就是 就是 我把当作黑盒子 然后让很多人去碰开关 得到一个更好的解 是一个比较好的方案 然后另外一方面 就是你从把那个模型打开那个 那个时间花的代价其实更大 因为其实并没有多少人真的知道 这模型里面在干什么 但是我觉得长远来说可能后者呢 会有更高的天花板吧 这个是我的想法 嗯 我同意你的判断 对 但这里边有这么一个点 就是我觉得 为什么黑盒子现在它是占主流 因为打开了以后人类似乎也没有办法 嗯 真的去判断什么东西是什么 就是这里边有几种学习范式或者怎样 对对 现在是这样 所以就是能不能找到一个比较好的 能够理解整个结构的一个大的框架 是重要的 所以是这样 就是为什么我做成paper的原因 就是比如最近有一篇paper 我们做Grokking对吧 那么证明这篇paper 为什么要做这个paper 所以我觉得就是通过这个方式 得到一个 对这个问题的一个大的理解框架 可能对以后的那个模型的改进 有很大帮助 这个是我的想法 嗯嗯 我再多引入 本来已经有点复杂了 但是我觉得 我再多引入一个问题啊 嗯嗯 就是 似乎我们在学习AI怎么学习的过程中 我们会从人类身上 人类的学习过程中 取得灵感 嗯嗯 包括最近很火的就是 Rich Sutton出来说RL是学习的方式 是人类学习的方式 大约模型这种方式不是要学习方式 它也不能学习 因为它没有objective 对对 那另外一派呢 可能我反而认为是Hinton而不是Karpathy Hinton他说就是 他还举了例子嘛 就是松鼠啊 或者说是 这个机器 它就是 这是我们所说的这种物理世界 其实并不是一就是就是经验 它并不是一个只存在人脑中的 你通过语言也可以得到经验 嗯 那这个debate我觉得很很重要的 最后就落到了 一个人到底是怎么学习的 然后什么是学习 以及怎么样子才能产生学习 或者产生新的知识 或者connect the dots 对吧 那我就想请你再就这个问题 来发表一下你的猜想 甚至都可以 就是不一定需要一个scientific的proof 或者说是research 我觉得就是通过经验学习 这个是对的 就是有很多经验能够 但说这个经验里面就是什么样的经验 是更有价值的 我觉得 这个是一个比较大的一个问题 对吧 比如说 你要说 就是非常直观的经验 就比如说 有一派是这么说 我没有embodiment 我是没有办法去学到真正的感觉 这个是行万里路 对对对 你要行万里路 或者说你要真正感到痛 感到那个伤心 喜怒哀乐 你才能真正成为人 就是这样的一种说法 或者说比如说 我只能通过看世界 然后才能知道空间结构 或者说只能通过摸 才能才能看空间结构 对吧这是一种说法 那么还有一种说法 就是说我有一些 抽象的概念 我还是能够学会 这样一些东西 对我觉得这两个东西 其实应该说 不是说是互斥的 因为是这样的 就是其实最终是 我的目的 是要学到一个representation 学到一个表关学习 对吧 因为如果你学到一个representation的话 那有个好的表征 那你对有的问题 你能够解决 就是表征是怎么学出来的 这个完全取决于 那个输入有多丰富 然后结构是什么样子的 对 所以就说 也许就是 不管你是直观的学习也好 还是抽象的学习也好 只要能学到这个表征 然后就能够最终得到一个比较好的 那个泛化的效果 我觉得这两个拼起来呢 应该说是比较好的 对 那么两边怎么谁能够什么东西能够选 能够选出表征来 这完全是 应该说是有个定量的方式来啊 来预测的 而不是说是啊 非此即彼 有可能是说两边都一半都可以学 或者说一边1/3一边2/3也可以学 都行 对 所以并不是说是一定是黑或者白 或者左或者右 很多时候是混在一起的 然后最终得到个表示 这表示能够 能够得到 就是能够进行预测 或者说能够操纵你的行动 然后能够 泛化到一个新的没有见过的情况 那我觉得 顺着这个问的话 就是你刚刚所说的后者的工作 就是不是black box 然后不是所谓的这种scaling law 而是真的去打开它 然后去梳理它 然后去怎么样呢 用不同的方式去学习 对对 那它的意义是什么 就是 我觉得几种 比如 就是要么它学的更有效率 但是似乎 现在数据已经 到了一个瓶颈 嗯嗯 还有效率这件事情 不知道它的意义是不是那么大 然后可能是在同样的知识里边 能学到新的东西 嗯嗯 我才能增加新的数据 嗯嗯 或者新的信息吧 嗯嗯 information set 嗯嗯 那它的意义是什么呢 就是那样做的意义 我觉得首先第一个就是说 数据遇到瓶颈的话 其实恰恰就需要这个了 因为如果数据到瓶颈的话 那你意味着scaling law不一定有效了 比如说你就这么点数据 比如说你就只有这个 比如像training token 这样子这样一个skill 那么这个skill 这个token数目就是对一些大众的 那个 那个东西已经绝对够了 对吧但是对一些小众的领域 就是它可能每个小众 你看这样的坑就很少 所以这样的话呢 其实如果数据不够 再加上你的训练算法比较 费数据的话 那你可能很难学会 就是说不管怎么样 你学会的永远是一个memorization 或者说是记忆的结构 而不是一个泛化的结构 那这个是一个问题 对吧 那么这种情况下 你怎么样去用scaling law做 你就说你得去找办法去做data augmentation 也许这是一个办法 对吧 但是如果你对这个问题有理解 对这个模型有更好的理解的话呢 也许不需要data augmentation 也许你需要 就是说 改变这个训练本身的算法 或者说训练的架构 那么有这个架构之后 也许这个模型就会做的更好 嗯 你觉得我们现在就是 大约模型产生出来的inference 生成出来的这些新的token 嗯嗯 它是记忆还是泛化 这个要看 我觉得这个是有些时候是 是混在一起的 就说你可能对那些比较 那个东西比较 就是或者说这个task比较丰富的 你见了很多很多这样情况 它可能是泛化啊 比如说那个记忆越好 泛化越有可能 是这样吗 就是说 给它的记忆材料越多 它越有泛化的可能 你可以这么说 就是给他的材料越多 因为他看到各种组合了之后 他在组合里面可以得到一个比较好的 那个表征 这个表征就可以 它能够有预测能力 或者说这个表征对没见过的 那个组合 它有一些比较好的结构 可以算出来 那么这个是一个 其实就是泛化嘛 就是所谓 我觉得说实在 就是所谓我们真的懂这东西 我真的理解这东西呢 往往它意识的一个是 它的方法能力很强 所以对新的情况下 对于这个 这个表征能够得到正确答案 这是一种对吧 然后第二个呢 就是说是呃 它能够细化到非常简单的这个逻辑 那么这个逻辑呢 可以apply to everything或者apply to a lot of cases 那么这样的话呢 就是说这两个东西综综合起来 就是让你这个学出来的知识能够 能够apply到很多其他的地方 那么这叫泛化对吧 就是说 应该说我们对它要下个定义的话 就这样一个 定义 那么如果大语言模型对于 某个领域看了很多很多数据 它有可能学到更好的表征 然后这边就可以泛化 那这个是一个 然后另外一个 就是说如果它看到的数据很少 那这样的话 有可能就说这个模型本身 它没有办法学到很好的那个表征 它没办法学到很好表征的话 OK 那它就只能把它背出来 它没法背出来 那么它得到的那个表征呢 就是更偏于背诵的 这样的一个结构 就是它能够至少对付好 就是训练的要求 就是说 我希望这个训练级上的 错误率还是比较小的 但是 它一旦超越了训练级的范围 之后 你就会发现 这个错误率就提高了 那么这个 其实大家就认为这个是过度拟合了 或者说是背诵了对吧 所以大概就是这样子 其实我觉得就很多时候 你并不能说神经网络 是记忆还是背诵 还是记忆还是泛化 应该说是完全取决于这个数据的分布 如果数据多 那么这个虚拟网络是泛化多 如果数据少 那么这个神经网络是记忆多 这个是我的观点 我觉得这里边最fascinating的一点 就是它从记忆到泛化的那一步 到底是怎么发生的 对对 是的 那你觉得就是 帮我们总结一下 就是像我 我的理解可能就停留在emerge 一句话就就出来了 但是似乎人脑也是这样的 我记得在2022年的时候 22年的时候 我跟李沐正好线下聊了个天 他跟我说了一本书 嗯嗯 他是那个 cognitive science 然后就是讲关于神经 科学（方向）的 那个时候还没有那么多东西 然后他说 其实那本书里讲到关于生物 比如说人脑和猩猩差别不大 大脑好像差了30% 嗯嗯 但是似乎就多了这30% 就emerge出来了语言 然后就厉害了 对对 我觉得是一个 我觉得应该是这样吧 就是 至少从我最近的一篇paper来 角度上来看呢 就是说这个paper就是告就告诉你 就是他有很很清楚的一个picture 就是告诉你 这些是怎么发生的 就是说 是在什么点发生的 或者说它的内在机制是怎么发生的 就是我们现在感觉上是 我从记忆突然间跳到泛化 好像这个变化非常神秘 嗯 但是呢 这篇文章其实告诉你说 其实并不神秘 它非常有 一个非常清楚的一个数学途径 就是比如说 我们要做优化问题 我们有一个 我们可以构造一个 就是比较复杂的 一个非凸优化的 这样的一个结构 比如说很多山峰 对吧 然后g呢 对应其中一个山峰 然后呢 那个泛化的对应其中另外一个山峰 那么这两个山峰呢 其实对应着不同的表征 那么这个山峰的结构呢 其实完全是取决于 数据的分布 如果你数据不够 你可能就只有记忆的山峰 如果你数据很多的话 就是 然后某些泛化能力强的山峰 就会慢慢慢变得越来越高 然后记忆的山峰就会变得越来越低 然后 这样的话 你再让神经网络去找到那个 就是好的 表征的时候 是相当于是个优化问题 优化这个 神经网络 其中的参数 使得它能够收敛到某个局部的最大点 那么如果你的记忆的山峰缩下去 泛化山峰提上来 然后泛化山峰那个 就有很多的那个神经网络 那个 它的参数会收敛到那个泛化的山峰 那么这个模型就泛化了 那么从记忆到泛化 中间为什么会顿悟呢 其实很简单 就说两个山峰之间的变化 就是此消彼长 对吧 然后在某个情况下 我比你高一点点了 然后突然之间所有人都往那边扭 明白 是因为它可以泛化 对对 因为它能泛化 所以它可以 就是只要多一点点的话 它piu就全都过去了 对对对 比如说 你认为就神经网络 是一个一直在优化的过程 对吧 然后他会看见 如果这边高 那边低点 那么所有人都 涌到那个高的山峰上去 那就突然之间 你就懂什么意思 所以我觉得 是这样的 一个structure 也就是这样的话 你就我可以从树叶上 或者说从整个树叶框架上 能告诉你这件事情呢 是这么发生的 而不是说是还是非常神秘的 这样的一个东西 我觉得非常清楚 那我是不是可以理解为 这个泛化的点一直都在数据里边 只不过我们之前没有找到它 没有搜索到它 或者说就搜就搜索到了 但是没有配pay enough attention 然后现在因为 就是越来越 随着越来越多的数据点 凸显了它的价值 然后我们才pay enough attention 然后 对对 你可以 但前提是它要存在 对的 它存在 一个是存在 然后它有你要足够的数据 让它显得与众不同 可以这么想 对就是如果数据不够的话呢 你可以有很多泛化的思想 但是呢 这些泛化的思想 它的 说服力不足以说服记忆这边 就是因为还不如把它记住 这些规律可能没有那么显然 对吧 但是就说一个显然了 嗯 嗯嗯 我刚刚想说 你举个例子就是 孔子和一个傻子 可能说的话都是一样的 但是你给孔子100个问题的时候 问他怎么治国理政 然后问他什么 然后你就说哦 原来这个是个厉害的孔子 然后那个是个傻子 哈哈哈 对 就是如果给孔子100本书 那么他告诉你所有的东西 那么这些东西拼在一起 就是让你觉得他 那他在解释这些书的背后 其实是有一个统一的理论 或者统一的一个思路 那么这个思路在100本书的解释中 慢慢的浮现出来 那么你会觉得这个东西就很有用 也许可以拿来解释101本书 那就是这样子 那这又回到了另外一个问题 就是怎么样做evaluation 怎么样子做做reward 嗯嗯 因为孔子解释书解释的好不好 这件事儿听起来 然后 现在大语言模型还是 你看你next token predict准不准 作为reward吗 还是有其他的方式 可以让这个有泛化能力的人 显得更厉害一些 应该是这样 现在你要看大语言模型 有呢一种是Pre training预训练 和post training后训练 这两个都有 所以你很难讲 就是你说预训练 我们现在还是用大量的 就是predict next token 就这样子 然后后训练呢 其实我们可以说 那个有很多办法可以做 训练 那么预训练Pre next token 那么这个结构 或者说这个 损失函数一直没有变 因为现在相对来说 这个还是比较一个比较好的 损失函数 当然现在有一些新的方案 比如说什么reinforcement training 就是我在训练的时候加一段思维链 然后希望这个思维链会引起 导致最后的预测是比较准的 这种类型的一些工作 对 那么你可以说我我 我们可以说 那这个就是可能对 就是原来预训练的方式呢 做了一些改变啊 大概是这样 那后训练它的花样就很多了 对吧 花样 比如说 你可以改 那个reinforcement learning的一些函数 比如说改一些函数的值函数 改它的一些evaluation对吧 value function对吧 reward对吧 改rubric这些东西都可以改 那么这里改了之后 你就可能就是 你其实是希望这个模型往 不同的方向走 对吧 然后你往不同的方向走了之后呢 那么有些方向 可能就强化模型的某个能力 某些方向强化模型另外一种能力 那么这样的话 你这个模型最后就是百花齐放了 嗯大概是这样子 对对 当然了就是说很多时候 你要优化它到某个能力的 时候呢 你其实还是希望 能够优化的比较 一个是避免 就是reward hacking啊 有些时候 就是 模型还是会最大化你的某个值函数 但是这个最大化的路径是偏的 你不想让他这么做 但是他就这么做 这么做有shortcut啊 就比如说这样 比如说你答案就只有a b c d四个 然后那就瞎猜一个 25%（准确率）对吧 那我我不希望它瞎猜怎么办 那我就希望我的思维链 一个是希望它的每一步 就是经得起考验 对吧 每一步逻辑是正确的 你可能需要一个one model去做这个事情 这些都是 比较重要的 这个是比较重要的 就是怎么样去做这个事情 那么这样的话 你中间肯定要引入各种ribick 引入各种东西 去把这个模型给算出来 就是所以其实花样还是挺多的 而且有很多地方是可以有人为 有一些人类的那个思维 和概念能够放进去 我稍给大家 给频道前面的观众做个比方吧 我觉得一个用比方去理解的话呢 就是 大约模型是个非常非常勤奋 算力 非常高 就是一天到晚学习的人 对 然后它就可以不断的读书 然后它读了唐诗300首 结果发现它又找到了唐诗3万首 然后读了300万首唐诗 然后它会作诗了 是因为它穷尽了这里面所有规律 然后它找到了行之有效的方法 而且它有一个好的方式 去可以帮助它evaluate 它自己的诗做得好不好 对然后它发现 它找到了这里边的规律 但前提是这个规律要存在在这里边 对对 然后你刚刚所说的 就是希望用另外一种方式去学习 是说 我们不光要让它去背3万还是300万首唐诗 我们能不能 就是像发现数学公式那种方式 去发现一个规律 对对 从而让它直接“piu”的跳到那个 我 我在23年写那个文章的时候 就是讲到（《关于ChatGPT最重要的五个问题》） 就是尤里卡阿基米德发现浮力定律 他其实是干了两件事 是两件不同的事 对第一件事呢 是他穷尽了 他肯定当时在想很多很多的方案 很多很多的可能性 然后他脑子里边找到了这么一个点 但是第二件事是 他马上意识到这个是对的 我觉得这两者在机器都挺难做到的 他很难马上意识到这个东西是对的 嗯嗯 就是说 意识到这东西对的是有可能的 就比如说你发现一个新的假设 这假设能够解释更多现象 而且它假设更简单 那你会马上意识到这个是对的 比如说地心说跟日心说对吧 那其实说 说实在的那个地心说也是对的 只是说那个 地心说你也可以拿来预测 只是在地角上来看 就其他行星的那个运行轨迹 非常复杂 就是它是那个 就是本轮均轮这种运行轨迹 就是轮子套轮子 就是可能一边这么走 还要换个花样再转再转 轮子里面套轮子 然后你通过这个方式 你可以预测那个每个行星的行为 对吧 所以这两个其实都是对的 只是说日心说 如果你切到日心说 你会发现突然之间 所有的轨道都非常漂亮 就是一个椭圆 非常非常简单 那么这个时候你会马上意识到 那个理论 或者说那种解释是更加完美的 或者说更加接近真实 或者更加接近那个更美的 这样的感觉 我原来是这样子 一个逻辑 你觉得Elegance这个东西 在模型现在训练的reward function里吗 这个我觉得是这样 就是因为它不是reward function 但是它在训练的时候呢 应该有implicit bias往这方向走 就比如说那个 你展示的那个 PPT里面 对吧 提到伊利亚说过这个 就是我希望它压缩 我希望找这个模型 会自动的找到一个比较优美的 或者比较少的 那个压缩比最高的 那个解释 这个我是同意的 这个确实是会发生的 但是呢 这个不是 是一个loss function 是说 它内建在神经网络的训练过程里面 这训练过程 会 让这个模型自然的发现更加好的 或者说更加优美的解释 应该说是这样子 那么这样的话呢 就是神经网络 它才有这个能力去学会更好的表征 然后才有泛化能力 应该是这样子对 对 在loss function之外 之上还有一层更隐含的reward 是的 是可以这么说 对对 这个很重要 因为说实在 所有的loss function都是surrogate 就是说都是代理 就比如说predicting next next token或者是whatever 或者什么contrastive loss non contrastive loss 对吧 这种 或者说player loss 这些东西都是都是代表 就它的目的是产生一个梯度流 这个梯度流 能够让这个表针往正确的方向走 这个是最重要的 一个逻辑 就是至于这个 至于这个 目标函数是什么 其实并不重要 重要的是这个 哦 我直到今天之前 我一直觉得loss function是整个学习的 就是目标 现在我才知道了它是surrogate 这个 这个是共识吗 就是大家 为什么到今天才知道这件事 因为它听起来很intuitive 然后很重要啊 对 因为 我 我自己 毕竟还是做过很多表征学习的工作的 我知道 就是很多表征学习的目标函数 做过些拆解之后 你会发现它们其实就是反传梯度的 不同形式嘛 对吧 你的loss function换了 你的反射梯度的结构是不一样的 那么这个结构 其实最终能够影响你的表征的学习 是这样子 但是你这个loss function其实可以换 甚至换成一些奇怪的东西 你从来没见过 但是 得到的梯度是差不多的 那比如说表征也差不多 你对梯度这个词的使用 也让我觉得非常的intuitive 我心中就是 就是一个一个等高线 但是我觉得这个这个等高线 最后画出来的是我们的一个知识 很本质的东西 可能就是刻画我们世界规律的 这么一个 对对 等高线这个逻辑呢 是经常用的 但是等高线这样的一个思路呢 其实它忽略了 这个神经网络本身的结构 因为它把整个landscape 做成一个高维空间中的一个 非常复杂的 一个山峰 但是这个山峰 其实你要知道 山峰其实对应着神经网络的结构 所以这两个是有关系的 所以应该说 把这个梯度在山峰上的指引 去 映射到神经网络的 具体的 哪个梯度对于哪个神经元的 或者每一组神经元的这样的一个过程 那么这个时候你能看见 就是它的表征是怎么学出来的 这个是会比较有趣吧 但这个可能比较细节 大概是这样的想法 我觉得 它对于我们对这件事儿的intuitive understanding挺重要的 对 嗯 就是像你可能刚刚说的就是那个insight 那个insight就是你有了这个东西 我觉得就比较好 容易建立起一个更好的世界模型 对对 是的 是 大概是这样的一个逻辑 但这个是一家之言 就是说 我觉得我也是 我也非常bias 以我自己（的观点） 我们来听的就是一家之言 我们 （哈哈哈） 如果这个事情有有教科书的话 我们就去学教科书了 因为没有 所以我们一家之言是（因为） 大家每个人都会有自己的想法嘛 对吧 我这边也是做一些 做很多research 嗯嗯 我有这样的一个大概的感觉 就是这样子 那么我在上面有很多文章 也是做一些这样的工作 去分析 这个梯度的结构 它训练出来这个表示什么 改的变化 这个是重要的 我相信就是再往上走呢 也许这样的 一个理解呢 是能够改变 最后的那个 这个神经网络的学习的方案 这是这是我们的最终目的 当然这个方向比较远 这是long term的 那么 当然是希望有很多那short term的东西 可以跟它辅佐在一起做 嗯 我觉得这个已经聊得非常的深入了 感谢感谢 我们 这个话题就先聊到这里 剩下一点点 我再用十五分钟的时间 稍稍回顾一下你的科研史 因为你刚刚也聊到了远期的目标 就是 也要有一些近期的目标结合 嗯嗯 我记得你有一个专访 里面有讲到 就是你在刚读博士的时候 大部分的时间是在想 但是你后来觉得想的是没有用的 对 而且关键是你想的那些东西 没有做工作的话 就相当于没有存盘 对对 你现在就应该 因为我看你的工作 其实我能感觉到 是有一个很强的主线 或者起码在你的自己的网站上 介绍的时候 我就会发现你前面的一个工作 lead到下一个工作 然后再lead到下一个工作 就是每一次 都能在前面非常重要的结果上 再往前走 而且似乎都能跟时代挂钩 对吧,就是从围棋开始 然后到大语言模型 然后到模型训练的 就是效率方面 这些东西 都能踩上应用的点 所以我想听一下你对 选择科研topic 以及在FAIR这样的环境中 这种就是 比如说 你觉得你的科研有应用的压力吗 或者说 你到底是怎么决定你的科研方向 你怎么决定 把兴趣 商业和自己的长处结合起来的 对,我觉得是这样的 就是你肯定是要结合的 否则很有可能会比较惨 我们大家都有家庭 大家都希望能够有一些比较高的收入 对吧 然后 然后能够 希望有比较好 有个比较好的环境 对吧 然后比如社会地位也比较高 大家都希望全都要 成年人说全都要 不是说小孩子 只选一个 对 所以最终你肯定是要找到 那个结合点 就是因为我从博士开始 就已经是“双线作战”了 就是我有很多的想法 就是你之前说的也是对的 就是我会 我可能花9个月时间 去想一些不着边际的东西 然后3个月说 不行了 我今年要发paper 否则老板会不高兴 对吧 那我可能会跟老板说 你有什么题目我来帮你做 那我花3个月 就把这个事情做了 出一篇paper 就要对老板有交代 他当时比如说课题 这些课题需要有文章去填 那么通过这方式 至少让我觉得 我在博士阶段会有工作 然后我能毕业 对吧 然后老板也开心 这个还是很重要的 这很重要 那么工作之后也是一样的 就是我们当然希望做一些方向 这方向是迎合时代潮流的 就是我不可能说 完全脱离时代潮流 比如大家都在做大语言模型 你却不做 那我 那我说 我做别的 然后我说我就是要做 比如我就要做SVM 或者就要做视觉 这个当然可以 对 但是这种肯定 在公司里面 是没有办法活下去的 对 所以这是一个比较大的 问题 所以会想一想 就是首先 你比如说 我这边的一些比较偏理论的研究 它对这个问题有更深理解 比如之前我们有一些关于 attention sparsity 就是注意力机制如何变得稀疏的 这样一些研究 那么这研究本身是比较理论的 但是 你可以拿来做一些比较实用的工作 比如说我们想到最近的 之前的attention sink 那么这种文章 就是说 我们其实没有太多理论 但是我们可以通过观察 神经网络的稀疏性 我们可能得到新的算法 用这新的算法可以把上下文扩展到 扩展到比如 400万以上 那这样的话 这个东西就有用了 就是突然之间你可以拿来做 大语言模型的 decoding 对吧 解码的这样的一个应用 那么这应用 其实本身也可以放在很多手机上 这是个有用的应用 其实这样的联系 应该说还是比较紧密的 应该容易想到 对吧,你想你的attention 如果有稀疏性的话 那我为什么 我就把大部分的attention的score砍掉 那不就加速了吗 对吧 那就省内存了 对吧 那你有各种办法 可以提高这个效率 所以这两个关系 是很大的 所以其实只要稍微想一想 就有一个新的算法 那么有新的算法之后 你就有一个新的思路 那么这个新的思路 你就可以拿来做很多有意思的事情 我要打断一下 你这个体感 你觉得在别人身上成立吗 因为有的人觉得赚钱好容易 到处都可以赚钱 像你就 好多research topic 然后随便拿一个都可以做 但是很多人绞尽脑汁都觉得想不出来 所以说 你觉得这个东西是你独有的特质 还是 这个应该说是很长时间的积累 就是说我觉得是挺难的 比如说我拿我的理论研究来说吧 就是 这个方向其实做很久 我其实不是数学科班出身 我很多时候 有人跑来问我 说你是不是数学系的 我说我不是数学系 我所有数学都是我自己学的 然后很多时候是这样 我特别是 比如一开始想做一些表征理论的研究 那很痛苦 因为你很难去想 你的思考 你可能会在很多地方转圈子 然后浪费时间 你会发现浪费很多时间 然后真正的事情都没干 但是你如果持之以恒 一直在想 一直在思考反思的话 慢慢就会发现 有些地方你可以存盘 存下来你就知道 这个地方 是你发现一个好的insight的节点 然后这个insight你既然想到了 那就是你的了 那你把它写下来 存下来 那么有了这个阶段之后 你存盘这个能力就成立了 然后你就会花时间做别的事情 这是第一个 然后第二个就是说你看很多文章 做很多research 那么可能对这个问题 有大的概念和思路 就是知道这一块要怎么做 比如说我们做reasoning 做self improvement 对吧 这些方向其实是一个很火的方向 那么你看了很多文章之后 就可以知道什么东西能做 什么东西不能做 对吧 所以说 比如说你要改进思维链的长度 长和短 对吧 然后怎么做混合的 thinking 这种东西 其实都是 通过我们过去的经验 能够发现一些可能性的 然后在这上面 你怎么去做更好的工作 对,这个也是一个经验积累 应该说是这样 就是我建议是多看 多看文章 然后多想 多跟别人讨论 看看这个思路是怎么出来的 我觉得这个比较重要 不好意思 刚刚打断 我觉得谢谢 就是帮大家又讲了很多 怎么提高自己科研水平的 干货 然后继续 就是你 到了一个阶段 是你看到了现在的应用 然后你能很快地比如说 找到一些新的应用和论文 这似乎对你来说 就变成一个比较简单的事情了 对 因为 就是说 很多时候是这样 就是特别数学好的人 物理好的人 他们转到其他领域是很快的 就是 为什么是很快的呢 我个人感觉是这样 就是数学和物理 就是 它给你一个预训练 让你脑子里面表征比较好 然后它可能很容易能够 映射到不同的领域 然后它们之间 这个领域的知识可以迁移 比如说 我说我要 抓住本质 对 这个很重要 就是比如说 我说做efficiency 那就是优化问题 优化问题的话 你可以说我有个目标函数 我有一堆优化的变量 对吧 然后有约束 那就可以优化了 对吧 但是具体的优化细节 那么我们可以讨论具体的细节 那么这是 这个问题本身的固有属性 对吧 那么这么想的话 你有很多问题可以归结在一起 那么你的思维就比较连贯 比较一致 这个是重要的 这是重要的 哦 就是回到你刚才说的 模型的那个点 就是别人 可能 你能在同一个区间发现更多的问题 你能产生更多的 就是组合出来更多data points 你就更容易找到这里面的泛化 是的 exactly 就是 其实就是这样子 然后如果你已经有一个好的表征的话 那你就很容易泛化到其他领域 对吧,因为可能在别人看来 这些点都是离散的 然后它们没有关联 但是作为一个比如说 表征比较好的研究员 他能看到它们这些关联 然后 并且欣赏这些关联之间的美感 那么首先他能记住更多东西 对吧,因为这些东西 在他们脑子里面是有结构的 然后另外就是说他不觉得 把这些东西连起来 或者说把东西记下来 找到想法是痛苦的 他能很自然地找到新的想法 所以这个其实是连上前面一个点 就是脑子里就放烟花 这种感觉 对,对 你会有很多其他的想法 你很多想法是有关联的 然后你能看到它们之间更本质的关联 那么这更本质的关联 本身其实就是一篇很好的文章 或者说一个很重要的 思路可以往下走 对吧,别人只能看到表层的关联 比如说别人觉得A加B 两个加起来可能是一个好的工作 对,那么另外就是说 这种工作可能很多 但是有些人就觉得A和B 之间有个本质的联系 C,然后我发表一篇关于C的工作 那么C可能就比A和B更本质 那么这篇工作可能就 凌驾于其他工作之上 在它对学术界的影响力 可能更大 这个 是一个不一样的 角度 对 这种connection 就是 就是 一个是表征 一个是这样的 就是怎么说 收集、学习、generalize 然后最后再联系 这都是很重要的点 我觉得最后人和机器还是很像的 在这些角度来说 对的 应该是这么说 就是其实最近有些文章 应该说最近我看到有一些 比如说nature machine learning这种文章 就是它论证 人的表征和机器的表征是很接近的 它们可以通过扫描人的大脑 然后发现一些神经元的 activation 对吧 它们跟神经网络的activation 是很有关联度的 这种我是觉得 不管你是结构什么样的 开始不同没关系 只要你数据是一样的 数据结构是差不多的 那它的表征 可能学到的差不多 我觉得是这样 但是可能人的学习能力 人的学习效率 是比机器要高的 大概是这样子 结构还是不一样 对,对 那最后一个问题 就是 until recently 你的科研 你感觉是按照自己的想法走 还是 要做很多application的工作 以及 接下来可能会吸引你做的事情是什么 是继续你对 就是后一种 研究范式的继续探索,还是 我觉得研究范式探索是很重要的 对 那当然了 就说我们现在也要与时俱进 对吧 我不可能说我关起门来说 我就用以前的方式来做研究 比如说我们可以想 也许我们以后要找到一个 AI scientist 或者说 我自己写一套比如说agent的框架 然后帮我一起做研究 这也是可以的 其实我说 我们这篇Grokking the paper 这篇文章 其实说实话 是我和GPT-5进行对话 做出来的 其实这样子 我觉得这个很有点 像self-play 就是我给它一些问题 然后我这边有些想法 然后发给GPT-5 然后让它去思考 然后让它 给一些比如formulation 对吧,一开始你这么做 它给你的答案 都是非常大路的 没什么意思 但是你通过思考之后 我觉得有些关键的insight给它 它可能会有不一样的输出 然后这样的输出 可以往下面挖 往下面深挖一层 但是你还是要找到它的错误 找它的一些矛盾的地方 找到它做不出来的地方 然后继续深入 然后一直深入到 就是 这个问题的理解 或者说 这个问题的数学描述 已经达到了我想要的目的 那么这部分就成功了 就是这样 应该是这样的感觉 我在这个使用的过程中 我觉得当然是现在模型非常强 有很多工作 其实是可以 你用的是GPT-5 Pro吧 我猜 不是Pro 其实就是thingking 其实说实话Pro没什么用 对 我相信o1 Pro和非Pro区别还挺大的 是 但是我用下来我觉得 因为其实我有OpenAI的朋友 他们送我一个月的Pro 我后来用了一下 我觉得 好像也没有特别出众的地方 回头 回头把鸭哥做的那个second mind 我一直觉得这个事非常神奇 就是鸭哥做了一个 就是可以几个模型放到一起用 而且可以handle knowledge的东西 对对 我consistently 我现在几乎都只跟它交流了 因为我发现它就是比那些 这边的Claude和o1 给的答案更好 我就觉得 这个东西很 非常奇怪 对,但是我回头跟鸭哥说一下 然后你可以来试用一下这个东西 对,对,对,那我很感兴趣 就是因为 我觉得鸭哥做的事情都非常厉害 我非常佩服他 是挺有意思 对,可以玩一下 我倒是可以玩玩 他这个方法是什么样子 但是我还有一个点 就是你那是个solo author paper 你没有把GPT-5放到co-author里面 听起来它做了不少工作 按照逻辑 你可以看 就是这篇文章是conference投稿 conference投稿说 大语言模型不能作为 不能作为 作者 所以没有放 对吧 那我后面写了一段 这段话是说 作者 我是说我们 我们广泛使用大语言模型 我给大语言模型各种想法 让它去formulate 让它证明一个东西 然后发现问题怎么解决 对吧 它基本上所有东西都是错的 但是它有一些比较有意思的insight 有些东西可以细化 然后把你的idea从一个想法 变成一个具体的过程 这个它很擅长 就相当于它是一个非常勤劳的 职业的 PhD 你可以这么想 就是它非常勤劳 然后我给它一个想法 它马上写出来 写出一个很长的论述 让我能够很快进入状态 这很重要 对,你比如说你以前要进入状态 比如说 我要做research 好 我有 我现在有一个小时的时间 我可能一开始半小时 我要进入状态 进入状态 就我通过写公式 看文章 思考一下 我进入状态了 进入 这个叫心流 对吧 然后去想 然后才能得到一些结果 那这个时间其实比较漫长 但是有了GPT-5之后 我觉得很重要的一点是 你进入心流时间很短了 就是你跟它有一个小想法 然后它给你写一大段 比如 3分钟之内给你写一大段东西 你看完这段东西之后 你马上会进入这个状态 就说我知道我要怎么去想问题 就现在这个问题在那放着 然后哪个地方它做得不好 或者说有什么insight可以进来 所以这是很大的一个 应该说效率的改进 对 我可能觉得 以前你需要几个月的时间做一篇文章 你现在可能几个礼拜 甚至是更短的时间 我觉得 这是非常大的效率提升 如果用得好的话 是很厉害的 对 那么这个 当然 现在还是一个非常初级的self-play 对吧 也许说不定以后 我们可以做一个 更加自动化版的 那就很有意思 这个 肯定这方面有很多东西可以做 但是我们在这就不多想了 对 我自己也有一些经验了 就是我跟当时是GPT-o1 Pro去探讨 这是我一直对量子力学的那个many-world theory 我特别感兴趣 然后我一直觉得它最make sense 但是我们没有对应的哲学 然后我觉得其实你看佛经也好 或者看什么 就是很多东西 它的哲学,反而那种所谓玄学的哲学 和这个many-world theory的哲学是吻合的 大概意思就是说 就是我如果非要强行说的话 我就说这个世界的本质 就是一个非确定的many worlds 然后我们之所以现在share一个reality 这是我们的 就是最大概率 当然这个概率可能极大 就是99.99999 所以说 我们就会觉得这个桌子是确定无疑地 存在 但是其实它可能并不是真的存在 对 大概是这种感觉 对 这个是对的 就是从科学上也是对的 因为你可以认为 它是一堆波函数的组合 对吧 然后存在一种可能是 这个桌子 突然之间跑到那堵墙另外一边去了 这概率非常小 但是不是0 这个是存在的 只是 因为这个桌子是宏观物体 对吧 它的量子态 就是不是那种相干量子态 所以 不会出现这种概率非常小 就是这样子的东西 但是我就发现 这个idea 我没有办法和它写成一个文章 因为我自己的水平不行 所以说就是现在AI能辅助你写出来 像你这样的文章 还是主要是自己 主要是自己 是的 就是人还是比较重要 有很多重要的insight还是要人给 然后AI现在有很多莫名其妙的问题 比如说它就会卡在一个地方动不了 就是它会跟你说很多重复的话 然后就是它说不到本质上 这个很有意思 就像感觉 它就是你去面试一个新来的PhD 然后说一大堆话 它像背诵概念 就是但它又绕不到 它就找不到那句最重要的 本质的话能够说出来 那这个 其实是一个表达的问题 但是这个就需要人去总结 然后告诉它 是我们认为的最本质的东西 然后让它继续往 下走 这个是比较重要的 那现在 刚才 你就是说这是一个fresh PhD fresh PhD意味着它可能是可以被训练的 我想到是Duolingo的那个founder 他是一个计算机 教授 我忘了叫什么 冯,是一个 万开头的 是 对, 他讲了一个故事 就是他去读博士的第一年 他 几个月 他老师是图灵奖 是图灵奖 的获得者 获得者 对 然后几个月 他去了以后 就是他老师 就只跟他干一件事 就是你这个东西跟我讲讲 我没听懂 下次再来 就他 第二个月的时候就崩溃了 就这个老师肯定不行吧 怎么回事 结果后来才发现 就是他自己没有讲清楚 对 没有 你没讲清楚 说明你理解不深 对吧 如果理解深的话 讲清楚了 别人会觉得 你确实理解深了 你确实懂了 然后你可以做 你可以做研究 所以这是一个 他叫Luis von Ahn 我想起来 对对 他是 应该是 应该说我当时在CMU读博的时候 他就在那了 对 他有这么一个故事 所以说 不知道模型是不是也可以这么训练 我觉得有希望 希望可以 是的是的 对 当然了 就是大模型可能会 可能会强行记住 就是我怎么样讲能讲清楚 但是它自己不懂也是有可能 对 这个 不知道会怎么 而且就说 你怎么才能获得训练数据 能够让大模型找到最优的讲清楚的 这样一个 因为讲 清楚这个事情是个非常主观的东西 就是很难 很难用这个模型 去model它 在要求大语言模型之前 我们先要求自己 我们先要求自己把一个东西想清楚 已经是一个很高的要求了 对对 这个很难 就是说 这部分其实可能就需要人有美感 就是人觉得 它的讲解是非常有美感的 或者说非常简单扼要 那这个才可以 那这个怎么去设计一个loss function 是一个question 好的 那我们今天已经聊了不少了 你觉得还有没有什么想聊的东西 或者说我应该问的问题 那应该没有了 对 我觉得已经挺好了 而且其实我们已经把这篇文章讲 讲出来了 对吧 我也不想要通过非常枯燥的方式来讲 通过这个对话来讲 我觉得非常好 而且能看出来的是 我觉得通过这个对话 我也更深层次地理解了 就是这件事有多重要 它的context是什么 和它其实对人也好 或者对模型来说 其实都有很多共通的地方 我觉得通过这个讲 这个论文我们也讲了很多其他的 我觉得挺重要的知识 好的,那祝你接下来一切顺利 谢谢 先这样 拜拜