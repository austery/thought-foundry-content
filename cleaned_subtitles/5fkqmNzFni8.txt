you're one of the only people outside of open AI who in 2020 had this detailed empirical model of scaling and I'm curious what processes you were using at the time which allowed you to see the picture that you painted in the scaling hypothesis post that you wrote at the time so I think if I had to give an intellectual history of that for me I think it would probably start in the mid 2000s when I was reading more of and Ray kwi um at the time they were making this kind of fundamental connectionist argument that if you had enough computing power um that that could result in discovering the neural network architecture that matches the human brain and until that happens until that that amount of computing poers available AI just seemed basically futile and to me I think I I found this argument very unlikely um BEC because it's very much a kind of buildit and they will come view of progress which I just didn't think was correct um I I thought that it just seem ludicrous to suggest that you know just because you'd have some like really big supercomputer out there um which matches the human brain then that would kind of just summon out of non-existence the correct algorithm algorithms are are really complex they're hard um they they required deep Insight or at least I thought they did um and it seemed like really difficult mathematics you can't just like buy a bunch of computers and then expect to get this Advanced AI out of it um it it just seemed like totally magical thinking so I knew the argument um but I was super skeptical and I didn't pay too much attention um but then Shane leg and some others were very big on this um in the the years following and as part of my interest in transhumanism and and less wrong and AI risk I was paying close attention to legs blog posts in particular um where he's extrapolating kind of out the trend with updated numbers from kurile and morac and he's giving these kind of very precise predictions about how you know we're going to get the first generalist uh system around 2019 as mors law keeps going and then by 2025 would have kind of humanish Agents with generalist capabilities and that by 2030 he said we should have AGI so along the way um you know Dan net and Alex net came out and when those came out I was like wow um this seems like a very impressive success story for the the connectionism view um but is it just an isolated success story or you know is this what kurile and morac and Shane leg had been predicting that we would get gpus and then get better algorithms would just kind of show up um so I started thinking to myself that you know this this is something it's a trend to keep an eye on um and maybe it's not quite as stupid as an idea um as I originally thought and I just keep reading deep learning literature notice again and again that the data set size just kept getting bigger the models seem to keep getting bigger the gpus slowly CPT up from one GPU you know the cheapest consumer gpus to two and then eventually they were trading on eight and you can just see the fact that the neural network just kept expanding from these incredibly Niche individual use cases which you next to nothing um the youth just kept getting broader and broader and broader I'd say to myself wow is there anything that CNN's can't do as I just see people applying CNN to something else you know every individual day on archive this gradual trickle of drops kind of just kept hitting me in the background as I was going on um with my life you know every every few days like a another one would drop and I'd go like huh um you know maybe intelligence really is just like a lot of compute applied to a lot of data applied to a lot of parameters um maybe morac and leg and Kur were right and I just note that and kind of continue on thinking to myself like huh if that was true it would have a lot of implications so I think there wasn't really like a Eureka moment there it was just continuously watching this trend that no one else seemed to see except possibly a handful of people like ilas ater um Schmid Huber um and I would just pay attention and notice that the world over time looked more like their world than it looked like my world um where algorithms are super important and you need like deep insight to do stuff you know um their world just kept happening and then gpd1 came out and I was like wow this unsupervised sentiment neuron is just learning on its own right um that seemed pretty amazing um it also was a very compute Cent view you just build the Transformer and the intelligence will come and then gbt2 came out and I had this holy moment you look at the prompting and the summarization like holy do we live in their world and then gbd3 comes out and that was really The crucial test it was a huge huge scale up one of the biggest scale UPS in all of neural network history going from gbt2 to gbd3 and it wasn't like it was a super narrow specific task like go it it it really seemed like it was The crucial test if scaling was bogus then the gbd3 paper should have just been totally unimpressive and wouldn't show anything that important whereas if scaling were true you would just automatically be guaranteed to get so much more impressive results out of it than you had seen with gbt2 so I opened up the first page maybe the second page and I saw F shot learning chart and I'm like holy we are living in the scaling World Le and morac and curs while we're right then I turned to Twitter and everyone else was like oh you know this shows that scaling works so badly why it's it's not even state-of-the-art and that that I I was that made me really angry I had to write all this stuff up um someone was wrong on the internet