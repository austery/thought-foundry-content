2025年10月19日晚上11:48 正准备睡觉的AWS员工迎来噩耗 核心数据库产品DynamoDB出现连接故障 依赖它的上百个云服务产品也因此下线 故障源自一个很低级的代码逻辑错误（后面会有详细说明） 导致两个修改数据的命令前后冲突 这是教科书式的race condition 虽然只用了3个小时，工程师们就把这个bug修好了 但灾难才刚刚开始 作为AWS最引以为豪的产品之一 DynamoDB几乎是所有内部系统默认使用的数据库 所以在这3个小时的宕机时间里 那些系统也累积了无数需要重试的任务 这里就包括了负责调度所有EC2服务器资源的内部管理程序 Droplet WorkFlow Manager（DWFM） 20号凌晨2:25，在DynamoDB恢复的那一瞬间 DWFM就被堆积如山的待办事项所冲垮 调度中心挂掉，导致所有EC2服务器无法启动 这又是一次经典的重试机制的设计出问题 Google表示这个我很熟 如此重要的程序，没有自动恢复机制，这也罢了 赶来修复的工程师，甚至没有一个可以参考的手动恢复流程 也就是说，整个AWS的整个历史上 从来没有人考虑过“如果DWFM挂了要怎么处理”这个问题 这样看来，在现场一边修复一边摸索的工程师们 只用了3个小时就完成修复，已经是相当厉害了 时间来到早上5:28 DWFM恢复，EC2也开始正常启动 事故终于告一段落了... 吗？ 好巧不巧 DWFM旁边还有一个负责掌控所有EC2网络功能的程序 Network Manager 和DWFM相比，它可是累积了6个小时的待办事项 再加上刚刚大量成功重启的EC2发出的海量新请求 它的处理速度还不够待办的新增速度 所以工程师们只能再次撸起袖子，SSH进去手动疏导 直到早上10:36 积压的待办事项才处理完，EC2的网络功能恢复正常 而在云的另一个角落，在EC2重启的同一时间 负责分配网络流量的负载均衡NLB也开始为这些EC2服务器引流 但因为Network Manager卡住了 这些EC2服务器有的能联系上，有的联系不上 这就导致NLB的健康检查在红灯和绿灯中反复横跳 同时连锁反应，也导致NLB被DNS反复下架和上架 直到9:36 忍无可忍的工程师们只好关闭了NLB的健康检查 让它忽略那些没有回应的服务器 保持绿灯状态，让NLB可以暂时地正常工作 直到下午2:09 所有积压的EC2服务器恢复正常 NLB的健康检查重启，所有网络功能才恢复 2:20，最后一批受影响的产品： 依赖最多最杂的容器服务ECS和EKS恢复正常 此次故障事件正式结束 从10月19日晚上11:48到第二天下午的2:20 总共14小时32分钟 这是有记录以来，云服务行业历时最长的一次大规模瘫痪 在我的粉丝群里 一半的人因为AWS宕机 参与高强度的修复工作，煎熬了一整天 另一半的人则因为AWS宕机，无法工作 爽爽地摸鱼了一整天 真是人间百态了 和【让编程再次伟大#40】视频里分享的Google Cloud瘫痪事故相比 AWS这次的草台程度其实没有那么严重 除了最开始的导火索 剩下的问题都是因为错综复杂的架构导致的连锁反应 以及体量过大导致的极端高负荷运行压力 巧合的是，这两次瘫痪事故的导火索都是入门级的代码错误 本次AWS事故的源头，是DynamoDB系统里负责处理DNS的两个程序 它们分别是负责生成DNS记录的planner 和负责将新记录写入DNS系统的enactor 为了保证容错，DynamoDB在三个可用区部署了三个独立的enactor 每当planner生成一个新记录 其中一个enactor就会拿走，写进DNS里 在写入之前，enactor会做一个前期检查 确保它手上的记录比现在DNS里的更新 而写入成功之后，enactor还会做一个清理动作 就是把DNS里比自己刚写入的记录更早的那些都抹掉 于是在10月19日晚上，出现了这么一个情况： enactor张三首先从planner那里拿到了新记录A 在它写入DNS的时候，不巧地遇到了延迟，停在中间了 此时planner又生成了一个新记录B enactor李四拿到记录B，非常顺利地写进了DNS 就在enactor李四准备执行收尾的清理动作时 enactor张三的延迟结束了 于是张三继续执行自己的任务 把手上的记录A写进DNS，覆盖了记录B 紧接着李四开始清理 因为它在DNS里发现了比自己刚才写入的记录B时间更早的记录A 所以按照代码逻辑，李四把记录A抹掉 等张三和李四都完成任务之后 现在DNS里指向DynamoDB域名的就只剩一个空值了 而此时planner生成了新记录C enactor王五拿到记录C，准备扮演救世主 然而很可惜，它倒在了前期检查上 因为现在DNS里没有记录，所以无法和自己手上的记录C做对比 做不了检查，就无法执行下一步的写入 整个逻辑进入死胡同 无论planner之后再生成多少个新记录 也没有一个enactor能把它写进DNS里 DNS里DynamoDB的记录一直停留在空值上 也就是说，DynamoDB的域名从互联网上彻底消失了 找不到自家数据库的100多个AWS产品，自然也跟着瘫痪了 这个错误，也就比Google Cloud那次忘记写try catch 导致NullPointerException好一点点 虽然也不多 那么问题来了 如果你是负责修复这个问题的程序员 你要怎么修改它，让race condition不再出现呢？ 把你的解决方案发到评论区，我给你打打分 在我看来，DynamoDB的这个问题，要背锅的是设计这个架构的人 这种“多个进程修改同一个数据”的场景 从基本的软件设计角度来说 你就不应该把“保证数据完整性”这种任务放在进程身上 这种任务应该由数据的接收和储存方承担 在这里就是DNS系统Route53 因为数据在这里具有唯一性，也就是我们常说的single point of truth 让每个enactor都做事前和事后的检查和清理 不管你的代码有多严谨，都是不合理的决定 毕竟ACID本来就是很难达成的 要不然MySQL这种大热门的数据库 也就不会有那么多和transaction相关的问题了 在制作本期视频时 微软的云服务Azure刚好也遇到了全球范围的宕机 这已经是Azure今年的第五次世界级故障了 那为啥它上新闻的次数就那么少呢？ 当然你可以说是因为它的大客户大多都是传统行业巨头 普通民众对于故障的感知不深 但如果你认真翻一下它的故障报告，就会发现 它的问题要么是CDN运维出错了 要么是被DDoS了 要么是CDN被DDoS了 来来去去就只有那几个地方出错 很少会出现严重的连锁反应拖垮整个Azure体系 而AWS的画风就完全不一样 就像我在很久以前的【让编程再次伟大#7】视频里面提到的 AWS这种崇尚微服务架构的企业 只要能用现成的服务，就不会考虑自己造轮子 久而久之，每个服务都依赖每个服务，牵一发而动全身 本次事故出现的DynamoDB拖垮DWFM DWFM拖垮所有EC2 EC2拖垮所有NLB的连锁反应 就完美体现了AWS内部系统同生共死的状态 而回顾AWS历史上的几次大规模瘫痪事故，我们可以看到 导火索基本都是DynamoDB、S3、Lambda这些通用的储存和计算服务 因为大部分的AWS产品都需要储存、都需要计算 所以就着微服务架构的原则，它们都被串在一根绳上 敏锐的观众可能会发现 包括这次事故在内 AWS每次出大事，好像总是发生在“us-east-1”这个分区 这就让人产生了一个合理的疑问： 云服务不是主打的分布式部署、高容错、兜底能力强吗？ 为什么一个分区出故障，就能够拖垮半个互联网呢？ 虽然在市场营销时 云服务一直都是和分布式、高容错、稳定性等关键词挂钩 但这不代表你上了云，就能自动拥有这些好处 它只是给你提供了这么一个环境 让你可以更轻松地搭建高容错的系统架构 但你做不做，完全取决于你 就连AWS自己都没有彻底遵循这个原则 比如管理权限系统IAM，就只在us-east-1分区有写入节点 其余分区部署的都只是只读节点 当然AWS这些架构决定涉及到不少历史遗留问题，可以理解 而对于大多数的企业来说，需要考虑的其实只有一个问题： 你要不要投入额外成本进行多点部署？ 这不是一个技术问题，而是一个商业问题 因为从技术上来说，答案很明显 各种级别的容错方案都已经很成熟了 比如 multi-node 和 multi-region 大多数云服务商的服务都达到了开箱即用的程度 就算是相对更复杂一点的 multi-cloud 只要在架构设计和技术选型时多加注意 避免和云服务商进行绑定，做起来也不难 对有经验的架构师和DevOps团队 只要企业愿意掏钱，搭建高容错的系统就是手到擒来的事情 但问题就出在这个“钱”字 两个node就是两倍的成本 两个region就是四倍 两个cloud就是八倍 假设你现在是CEO，你的面前有一个按钮 摁下去能让你避免几年一次的持续几个小时的产品全线瘫痪 但坏处是你的成本会翻八倍 你会摁下去吗？ 这本质上其实是一个博弈论问题 对一个企业来说，最怕的不是自家产品出问题，而是只有自家产品出问题 如果大家一起出问题，新闻里只会怪罪云服务商 你只是受害者 毕竟这种几年才会出现一次的事故 从大众的角度来说，已经属于黑天鹅事件了 而即使你投入了八倍成本，做好了容错准备，逃过一劫 也没有人会表扬你 比如这次事件中毫发无损的Netflix 作为AWS最大的客户，是绝对的模范生 但 nobody cares 所以当你从CEO的角度来考虑 你更关心的是用户、是股民的反应 那么你的最优解，就是和其他企业的做法绑定 作为AWS成立最早、功能上新最快、资源最丰富的分区 早期的企业基本上只在us-east-1上进行部署 后来者自然也会将它作为首选，甚至是唯一选择 结果就是所有人都在us-east-1上抱团，同生共死 完美的纳什均衡 看到这里的观众可能会觉得 我是在狠狠地批判这些云服务商和用户 毕竟作为一个程序员 看到这些糟蹋技术的操作，怎么能不生气 但我其实并没有觉得他们的决定有什么问题 当然导火索的代码逻辑，和后面各种粗糙的重试机制 这一类低级技术错误还是要批判一下的 但从宏观上来看 不管是AWS内部各种产品的互相捆绑 还是用户扎堆在单一分区做部署 这些可能不是最科学的决定，但绝对是最现实的决定 我们技术人，可能想要追求技术上的完美 但世界不是围绕着技术转的 反过来，技术只是服务于世界的一个工具 归根到底，我们都只是工具人罢了