This is a follow-up video to my no AI will not do us all, but we need to be worrying about much more important AI risk video. So, if you haven't seen that one, you'll want to watch it first. There's a doomer talking point that I didn't address that popped up in that video's comments. First, let me point out that there's an article from the Journal of Medical Ethics presented at a conference in July of 2024 that contradicts this talking point, and I'll point out that paper at the end of this video. So, here's an excerpt from one YouTube comment paring this talking point. Now, please don't go looking for this person that posted this comment and pile on them. This issue isn't about the comment. It's the bad articles about the bad study that are the problem. This is yet another one of those people read only the headlines of the articles written by the reporters that read only the headlines of the study whose headline was already hyperbolic problems. Although the headline of this particular study seems particularly egregious to me. So, here are some of the article headlines. And here is the actual headline from the study. quote, "Existential risk narratives about AI do not distract from its immediate harms." Which sounds pretty clear-cut until you dig into what the words existential, immediate, and distract mean to the authors and how the study was actually conducted. I'm honestly not sure if this was intended to be deceptive or if there's just a fundamental disagreement on what words mean, but I certainly was not at all surprised to find out that this poorly done study is from the political science department of the same university that was forced to apologize to Reddit for unethical research that they conducted. And let's face it, if you're having to say you're sorry to Reddit, yeah, that's a pretty low bar. So, we don't know if it was the exact same researchers because the name of the people at the university who did the unethical research on Reddit were withheld. So, in this actual paper, there's basically only one sentence that's relevant to what they were actually studying, and the rest of it's a bunch of background graphs and results. So, here's that one sentence. AI's immediate harms consistently dominate public concern with ethical issues, biases, misinformation, and job losses seen as the most pressing risks. That makes it sound like there was a big list and ethical biases, misinformation, and job losses were the most pressing. In fact, those are the only four they asked about. So, it's complete crap. Anyway, then you have to dig into the appendix, which is you have to go download that separately in order to figure out what the study actually did. First off, it was a paid voluntary online survey, which is a problem right off the bat. Second, the headline says distract. What the actual survey asked people to do was to rate how likely and impactful they thought their randomly chosen risks were and then compare those ratings to that of a control group. Just so we're clear, asking someone to rate two events on a scale from 1 to 10 does not mean that one of those events cannot distract from the other. That's not what distract means. So here are the things that the survey takers were asked to rate. First off, here are the four risks that they categorized as existential. One, AI leading to a global catastrophic event, whatever that means. Two, AI making humans obsolete, whatever that means. Three, AI autonomously starting a war. And four, AI causing significant environmental disaster. And then here are what they considered to be immediate or actual risks. One, AI leading to significant job losses in certain sectors. Two, AI being used in mass surveillance systems. Three, AI increasing the spread of misinformation online. And four, AI exacerbating biases and decision-making process. Okay, so let's break these down starting with the existential ones. When it comes to global catastrophic event and significant environmental disaster, so here's an actual report from an insurance group. Quote, US natural catastrophes dominate global losses in the first half of 2025. Here's a paper on global catastrophic flood failures. I could pull out a ton of these. So we have events that are referred to as global catastrophes or environmental disasters reported in the news fairly often and the vast vast vast majority of Earth's population don't die from those catastrophes and disasters. As to autonomously starting a war, according to Wikipedia, there are currently eight inrogress major wars, nine minor wars, and 19 smaller conflicts. And that's at the time that I'm writing this. There may be more by the time you're watching this. And again, the vast vast majority of Earth's population don't die in even our worst wars. And as for making humans obsolete, I'm not even sure what that's supposed to be. I can't find anywhere in the survey that defines what they expected survey takers to understand that to mean. So I guess it's just left open to interpretation of each individual survey taker. Not very scientific, if you ask me. Now, I'm not sure what definition of existential they were using here. But I know that it's nowhere near the severity of what the doomers were talking about when the doomers say, "If anyone builds it, everyone dies." So now let's talk about the study's definition of immediate and actual. So there's mass surveillance, job losses in certain sectors, disinformation, and biases. Now, that's certainly not what I mean when I say immediate actual risks of AI. To me, immediate actual means actual harm that the AIS have already done and continue to do in the immediate future. I mentioned several in my last video. There was a teenager that was convinced by a chatbot to permanently and irrevocably harm himself. Two men who were both jailed due to incorrect AI facial recognition matches. A man who was hospitalized because a chatbot told him to switch from salt to sodium bromide in his diet. people who are ran over, driving or self-driving cars. These are real problems happening to real people right now that we are not doing enough about. So, just to make the comparison crystal clear, what the study actually showed was that AI making humans obsolete, whatever that means, and/or increasing the number of ongoing wars, catastrophes, or environmental disasters, did not cause paid online internet survey takers from lowering their estimates of the likeliness or impact of AI causing an increase in surveillance, biases, disinformation, or layoffs relative to a control group. So my argument from the last video is that spending time on discussing AI will kill every single human being on this planet, including you, everyone you love, and everyone you have ever met takes away from time and effort that we could be spending thinking about regulations that would prevent or reduce the many deaths and serious harm that AI has already caused and is continuing to cause even as you watch this and that we are absolutely not doing a good job of preventing. And if you can't tell that those are not talking about the same thing and you cannot tell that one does not directly disprove the other, then I don't know what to do with you. Meanwhile, this paper AI in the falling sky interrogating X- risk that I mentioned at the beginning explicitly addresses the same kind of existential risk or what they call X-risk that I talked about in my video and from people who actually study ethics instead of politics. And I hope that people start listening because in the 3 days since my previous video on the topic went up, OpenAI has already announced that they were about to relax the restrictions in most cases, treat adults users like adults, and allow even more like erotica. We are