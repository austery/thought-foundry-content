Today, I'm talking about what, in my opinion, is the biggest lie that the AI industry has told and continues to tell, why I believe that GPT-5 exposes it as a lie, and what I think the implications of that lie are. Now, if I'm going to do that, of course, I'm going to need to start by telling you what the lie is: "These models are as bad as they are ever going to be, which I totally accept. Like, that is true." Wait! Was that Hank Green? What the Fu--- Welcome to the Internet of Bugs. My name is Carl. I've been a software professional for more than 35 years now. And I'm trying to do my part to make the Internet a less buggy place. I used the Hank Green clip just now to show how pervasive the lie is and how often people hear it and repeat it, and how reasonable it seems. I don't expect Hank Green to recognize that as a lie or call it out. In part of that video, he discusses the idea of "staying in your lane" on the Internet and "debunking AI lies" is not Hank Green's lane. It is, however, mine. And again, let me say for legal reasons that this video contains my opinion and I'm going to explain to you why I hold that opinion. So let me reframe this argument with a quote from Sam Altman, who is the CEO of OpenAI. "GPT-4 is the dumbest model any of you will ever have to use again by a lot." So that quote was from a Q&A session at Stanford University in April of 2024. There are a ton of other quotes I could have picked. The idea constantly comes up during AI commentary and it's virtually always goes unchallenged. Now, why do I consider that to be the biggest lie in AI? Because the whole house of cards that props up the AI investment bubble is based on this premise. The industry doesn't want you to think too much about however good or bad AI might seem at the moment. They want you to take on faith that it will only ever get better. And if they could actually pile improvement on top of improvement, out top of improvement, it seems almost plausible that eventually the AI might displace enough workers to generate enough revenue to justify its current valuations. There are trillions of dollars of investment that are hanging on this idea. And it's the basis for so many other lies like "AI will replace all programmers" or "AI will displace 80% of all workers" or "artificial super intelligence is going to happen soon and kill us all" or whatever. If that foundation is faulty, if AI progress is not inevitable and next year's model isn't guaranteed to be better than this year's, then there's no reason to believe in any of the other stuff they're trying to sell you. I think it's clear that this was a lie because now GPT-5 is out. It's true that there are some people that have claimed that GPT-5 is actually smarter than GPT-4. Altman has certainly made that claim. But this report from Artificial Analysis shows that GPT-5 scored lower than o4-mini on the LiveBench and SciCode coding tests. Now that's proof. There are ways that GPT-5 is better, but there are also ways that it's worse. But the most damning proof is that OpenAI was forced to re-release GPT-4. Now remember, it's not like the claim was that GPT-5 would be a little bit better or somewhat better or ambiguously better or better in some ways but worse than others. And the industry only needs for each generation of AI to be not better than the previous generation. It needs to be exponentially better because they're promising investors the money they make will grow exponentially through 2030 at least. The industry loves to pull up a graph of some arbitrary statistic related to AI over a cherry-picked range of the last few years to show how the progress looks exponential. I've seen industries try to do that over and over. If you go back decades, you'll see some graphs like these with several of those exponential looking spikes right before dips. Statistically, for the vast majority of you watching this, this is your first AI boom. Those of us that have lived through past AI booms, and the AI winners in between them, have seen this before. Of course, most of the AI people are now saying, "Well, this time is different." But then AI people always say that. Maybe you remember how big a deal it was when IBM's Watson AI beat the world champion at Jeopardy? Maybe you remember how IBM said that it was going to revolutionize healthcare? AI promises have been repeated over and over for decades. So given that the progress in the computer industry over the last 30-some odd years of the worldwide web, the idea of new technology that grows exponentially for decades doesn't seem out of the question. After all, computers have pretty much only gotten faster and cheaper year after year for decades. But what has gotten faster is the hardware, and AIs, despite the way the industry wants to talk about them, aren't brains, and they aren't hardware, they're software. Let me say that again, they're just software. AI folks talk about how LLMs are simulating human brain or the like, and they act like that means that the rules of software don't apply, but they do. These AI's are just software running an algorithm or set of algorithms against a huge data store. And the fact that the algorithm in question is derived from something found in nature is nothing special. Every 3D game engine ever runs lots of algorithms derived from the natural world, from gravity to collision detection to ray tracing and first-person games certainly aren't immune to the performance problems that can plague any large software project. Generative AIs are cool and useful, but there have been lots of cooler and more useful advances over the years. I had a section in the video here going into where I think LLMs fit into the history of computing and how I disagree with the way the AI industry likes to position itself, but what I wanted to say grew to the point that it needed its own video, so that script is in progress and that'll happen, so subscribe if you're interested in seeing a video like that. All software has limitations, and LLMs are no exception, and software doesn't automatically get faster and better every generation the way the hardware has for the last 60 or so years. Has your phone always gotten faster every time you've updated the software? Because mine sure hasn't. Can you honestly say that every version of Microsoft Windows has been unambiguously better than the version before it? F***ing Windows 98! Get Bill Gates in here! You told us Windows 98 would be faster and more efficient with better access to the internet! It is faster, over 5 million! How about Microsoft Word? Excel? Adobe Photoshop? How about the Acrobat PDF reader? Has that only ever gotten better over the decades? Why do you think Adobe pushed everyone to subscription licenses? Because people weren't buying the new version every year anymore, because the new versions just weren't better. Software doesn't have a clear path to get better year over year. The hardware it runs on can get faster, and sometimes that causes software to get faster too, but sometimes this year's software release is even slower than it was last year, even when running on new, faster computer. This can happen because of software bloat, new features that are added that cause the old features to have to do work that you don't care about, the constant addition of stuff that is intended to give the company a reason to try to justify making you upgrade, 'til the company just gives up and declares "We're not supporting this version anymore. Switch to the new version, whether you like it or not, and pay us for the privilege, of course, otherwise you're going to get pwned by every bug that anyone finds from now on." Windows 10 Anyone? Making the hardware better is clearly understandable, might not be easy to do, and it often isn't, but it's pretty easy to measure whether you did it or not. The goal is, "just like the last version, but faster, or able to do more things in parallel, or both." Software isn't like that. The idea of "better" isn't automatically clear. It takes a lot of effort to define what you want software to do, and it takes time, and usually the faster you go, the sloppier the resulting software turns out to be. AI has this issue. Part of the problem with this "every version will be better" AI lie, is that it's not at all clear how to define "better", or "smarter", or "dumber", in an actionable way. And there's a lot of discussion and even arguments about what "intelligence" even is. OpenAI is said to define artificial general intelligence as, quote, "highly autonomous systems that outperform humans at most economically valuable work." That's not helpful as a specification. You can't make it to-do list out of that, and they are in a huge hurry, so this is not a setup that's great for success for them. Commercial-sized software projects are always about trade-offs. You can make this part go faster, but it affects the other parts. A cache or an index can make reads faster, but it makes the write operation slower and more complicated. You can add other new features, but that's one more thing the customer has to remember and keep track of, and the software becomes that much harder to use. There's a horrible failure condition of software projects that I've seen happen, and I talk about a lot. It's commonly referred to as "whack-a-mole." Every time you fix a bug or add a feature over here in this part of the software, some new bug or failure pops up over there in some other part of the software. There are a lot of things that professional software engineers do to try to prevent this from happening, but a lot of those techniques aren't available to machine learning and neural net-based projects for reasons that a lot of you probably don't care about. I put up a video on my other channel about that and how it relates to Star Trek and the history of programming languages. I'll put a link to that below if you're interested in a deeper dive. We do know that tuning a model to be better at one thing often makes that model worse at other things. The literature calls this effect "negative transfer", and as models get larger, more and more training gets added, and each new training has more things that it might negatively affect, it can even lead to a problem called "catastrophic forgetting'. Guess what? It's not a good thing. There are a number of lines of inquiry going on in the industry trying to mitigate this, but the thing you need to understand is this is not a trivial problem with an obvious answer. There just aren't automatic wins on this path. There's no reason to believe that it's always going to get better. The AI companies have particular benchmarks that they love to point out with each new model, and I have no doubt that there will be some benchmark that each company can point to for each new model released to make the argument that it's an improvement, but higher scores on a cherry-picked collection of benchmarks is not the definition of smarter or better. Like with GPT-5 scoring lower on LiveBench and SciCode, as I mentioned earlier, it's quite possible, if not likely, that there will be tasks that each new models do worse on than previous versions, not that you would expect the AI companies to advertise that. OpenAI is starting to try to mitigate this somewhat, with routing under the covers. Now, when you send a request to GPT, a router process will look at the request and choose which of several models to forward the request on to. This will help them hide any places where particular models have regressed to poor behavior on certain tasks, but it's just a band aid. So to recap, these AIs are software, not hardware, and there's no reason to accept the lie from the AI companies that every model from now on will be better than the one you have now, or the one you have then, and there are several reasons, including some academic research that indicate that, at least for some tasks, the opposite may well be true. Don't believe the lie, don't repeat it, and call it out when you hear it, even if that means that sometimes you have to call out somebody like Hank Green for uncritically repeating industry talking points. Because pretending that software always automagically gets better is how you end up with an even buggier Internet, and the Internet already has far too many bugs, and anyone who says differently is selling something, or they're repeating talking points from someone who's selling something. Thanks for watching. Let's be careful out there.