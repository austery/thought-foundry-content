在大模型激战的当下 究竟谁更强 是OpenAI的GPT还是Anthropic的Claude 是谷歌的Gemini 还是中国的DeepSeek Claude is the best GPT is the best Google is the best Grok is the best 停 大家都别吵了 当AI模型排行榜开始被各种刷分作弊之后呢 谁家大模型最牛这个问题啊 就变得非常的主观 直到一家线上排行榜的诞生 它叫LMArena 或许你还记得不久之前 爆火的谷歌最新文生图模型Nano Banana 对 就是把我家娃 变成小猩猩的那个AI应用哈 它其实最早以神秘代号出现 并且引发破圈式关注的地方 就是LMArena 哦对了 最近网友们又发现 谷歌又故伎重施 传闻已久的Gemini 3.0 被发现已经出现在了LMArena上 根据网友们的测试反馈 Gemini 3.0 Pro的代号应该就是lithiumflow 而Gemini 3.0 Flash是orionmist 据说能够读表 能作曲和演奏 能力再一次全方位的飞升 你可能会在对战中随机地遇到它们 如果遇到的话 记得一定要抓住机会好好地拷问拷问 不难看出 在正式发布新模型之前 让它们在LMArena上面跑一跑 似乎就已经成为了谷歌的惯例操作 而实际上 各家模型 其实早就已经把LMArena 当做了常规赛场 用来测试普通用户最真实的反馈 除了Google OpenAI、Anthropic、Llama DeepSeek、混元、千问 几乎所有的头部模型都在LMArena上打擂台 在文字、视觉、搜索、文生图、文生视频等 不同的AI大模型细分领域 LMArena上面每天都有上千场的实时对战 由普通用户来匿名投票选出哪一方的回答更好 最近以来 很多AI研究者都纷纷发声 认为大模型竞赛的下半场 最重要的事情之一 就是重新地思考模型评估 因为当技术创新趋于饱和 真正拉开差距的 可能将不再是谁的参数更多 推理更快 而是谁能更准确地衡量 理解模型的智能边界 而如LMArena这样的平台 正是当前这一领域中最前沿 也是最具实践意义的探索之一 它就像一座永不打烊的AI竞技场 真实的用户、真实的投票 并且排名动态更新 用一场场真实的人机交锋 试图重新定义我们衡量模型的方式 那么在大模型评测上 传统的Benchmark基准测试 究竟存在什么样的问题 是已经过时了吗 LMArena的竞技场模式 为什么会被视为一种新的标准 它的技术机制、公平性和商业化 隐藏着什么样的挑战 而下一代的大模型评测 又可能会走向哪里呢 Hello大家好 我是陈茜 欢迎收看硅谷101 那这期视频 我们来聊一聊LMArena 那么在LMArena之前 AI大模型是怎么被评估的呢 方法其实非常的传统 研究者们通常会准备一组固定的题库 比如说MMLU、BIG-Bench、HellaSwag等等 那么这些名字普通人看起来很陌生 但是在AI学术界几乎是家喻户晓的 这些题库涵盖学科、语言 常识推理等多个维度 通过让不同模型作答 再根据答对率或者得分 来对模型进行比较 那比如说MMLU 全称是“Massive Multitask Language Understanding” 它涵盖了从高中到博士级别的 57个知识领域 包括历史、医学、法律、数学、哲学等等 模型既需要回答 像“神经网络中的梯度消失问题如何解决” 这样的技术问题 也需要回答 “美国宪法第十四修正案的核心内容是什么” 这样的社会科学问题 学科跨度很大 而BIG-Bench更偏向推理和创造力 比如说让模型解释冷笑话 继续写诗或者完成逻辑填空 HellaSwag则专门 用来测试模型对日常情境的理解能力 比如说“一个人正在打开冰箱” “接下来最可能会发生什么”等等 这些Benchmark 在过去二十年 几乎主导了整个AI的研究领域 它们的优点 显而易见 就是标准统一、结果可复现 那学术论文 只要能够在相关公开数据集上刷新分数 就意味着性能更强 而AI的上半场 也正是在这种比成绩的节奏下 高速发展起来的 但是这些早期的Benchmark是静态的 多以单轮问答、选择题形式为主 题目结构简单 评测维度明确 便于统一打分和横向的比较 然而当模型的能力越来越强 训练数据越来越庞大的时候 这些Benchmark的局限 就开始显现了 首先是所谓的题库泄露 很多测试题 早就出现在模型的训练语料当中 于是一个模型在这些测试上得分再高 也不代表它真的理解了问题 能说明它记住了答案 其次Benchmark永远测不出 模型在真实交互中的表现 它更像是一场封闭的考试 而不是一次开放的对话 华盛顿大学助理教授 英伟达首席研究科学家 同时也是LMArena早期框架搭建的参与者 朱邦华在跟我们硅谷101的采访中就表示 正是因为传统的静态Benchmark 所存在的过拟合、数据污染等问题 才催生出了Arena这种新的模型测评方式 Static Benchmark（静态基准） 比如说Math500或者MMLU 有几个问题 一个问题就是说大家非常容易overfit（过拟合） 因为比如说一共就几百个问题 而且我都有ground truth（标准答案） 如果训练在ground truth（标准答案）上 没有人detect（测试）出来 虽然有一些所谓的 contamination detection method（污染检测方式） 但其实这个是比较难 真的100%就是做到detection（检测） 所以这种Static Benchmark（静态基准） 当时大家就说 首先一是数量很少 二是大家可能coverage（覆盖面）不太够 它可能就有最简单的math（数学） 然后最简单的一些knowledge（知识） 然后最简单的一些coding（代码生成） 像这个HumanEval这种 然后当时的Benchmark数量少 同时这个就coverage（覆盖面）也不太好的情况下 这Arena 就作为一个非常独特的Benchmark出现了 因为它每一个问题都是unique（独特的） 它可能是世界各地的人问 然后它可能是俄罗斯或者越南的人 在问你这样一个问题 然后同时 他问的问题都是随时随地 当时当地去想的一个问题 所以这个事就很难去 在当时overfit（过拟合） 尤其是在当时 大家都没有什么Arena data（数据）的时候 那么LMArena究竟是如何运作的呢 2023年5月 LMArena的雏形 诞生于加州大学伯克利的LMSYS团队 核心成员包括Wei-Lin Chiang、Lianmin Zheng等人 当时啊他们刚刚发布了开源模型Vicuna 而斯坦福大学在此之前 也推出了另外一个类似的 叫Alpaca 因为这两个模型 都是从ChatGPT的数据中蒸馏得到的 于是LMSYS的团队就想知道 从性能和表现上来看 究竟谁更胜一筹呢 当时并没有合适的测评方法 能够回答这个问题 那LMSYS团队呢 是尝试了两种方法 第一种是尝试让GPT-3.5作为评委 对不同模型生成的答案打0-10分 这种方法后来演化成为了MT-Bench 也就是Model-Test Benchmark 另外一种方式呢 是采用人类比较（Pairwise Comparison） 即随机挑选两个模型 针对同一个问题分别生成答案 再让人类去评审 选择哪个更好 最终第二种方式被证明更可靠 并由此诞生了Arena的核心机制 基于此 他们首先搭建了一个实验性网站 Chatbot Arena 也就是今天的LMArena的前身 在传统的基准测试里 模型是在预设题库中答题 而在Chatbot Arena上 它们则要上场打擂台 当用户输入一个问题之后 系统会随机分配两个模型 比如说GPT-4和Claude 但是用户并不知道自己面对的是谁 两边的模型几乎同时生成回答 用户只需要投票左边好还是右边好 等投票完成之后 系统才会揭示它们的真实身份 这个过程啊被称作“匿名对战” 投票结束之后 系统会基于Bradley–Terry模型 实现Elo式评分机制 分数会根据胜负实时变化 从而形成一个动态的排行榜 Elo排名机制最早来自于国际象棋 每个模型都会有一个初始分数 每次赢一场就会涨分 输一场就会扣分 随着对战次数的增加 分数呢会逐渐收敛 最终形成一个动态的模型排行榜 这种机智的妙处在于 它让测评变成了一场“真实世界的动态实验” 而不再是一次性的闭卷考试 除此之外 LMArena不仅仅是“让模型打架” 它背后还有一个独特的“人机协同评估框架” 这个框架的逻辑是 用人类的投票去捕捉“真实偏好” 再通过算法去保证“统计公平” 平台会自动平衡模型的出场频率 任务类型和样本分类 防止某个模型因为曝光量大而被高估 换句话说 它让评测既开放又可控 更重要的是 Chatbot Arena的所有数据和算法 都是开源的 任何人都可以复现或者分析结果 那作为LMArena早期搭建的核心参与者 朱邦华就告诉我们 LMArena的技术本身并不是新算法 更多的呢 是经典统计方法的工程化实现 它的创新点不在于模型本身 而在于系统架构与调度机制 一方面就是说 它虽然这个Bradley–Terry Model 本身没有什么太多技术上的新的东西 但是你怎么选model这个事 可能是something new 就是也不是特别new 但就说是大家摸索出来的 就说因为你现在假设有100个model 然后我想了解它们互相之间 到底哪一个更好 那这个事你其实需要一些active learning（主动学习） 就说你需要假设我选了一些model出来 我已经知道它们大概怎么样了 那我下面选model的时候 我应该选一些我更不确定的模型ranking（排名） 然后去做这个comparison（比较） 然后怎么去dynamically（动态）选 去做比较optimal（最佳的）这种 这个active model selection（主动模型选择） 这个是我们当时探索的比较多的一个事儿 然后当时就是做了一些这种 相关的这个series studies（系列研究） 然后同时就大家又一起去做了一些 这种experimental research（实验性研究） 就大家去比较一下不同的这种 这个怎么去调这些参数 能让这个model能更好的去被选出来 这是一个（LMArena成功的）因素 当然就是说我个人觉得 就说这种项目 它可能本身 还有一些时机和运气的成分在里面 因为当时确实是所有人都需要 很好的一个evaluation benchmark（评估基准） 然后这个时候呢 human preference（人类偏好）又完全没有 被saturated（饱和） 就是大家那个时候的human preference 确实比较真实的反映这个model本身的能力 所以在那个时候 我觉得Arena作为 这个行业的gold benchmark（黄金基准） 是非常make sense（合理） 也是就是比较顺理成章的 LMArena这种“匿名对战 + 动态评分”的方式 被认为是从静态Benchmark 向动态评测的一次跃迁 它不再追求一个最终的分数 而是让评测 变成一场持续发生的“真实世界实验” 它就像是一个实时运行的AI智能观测站 那么在这里啊 模型的优劣不再由研究者定义 而是由成千上万的用户选择 来共同决定 2023年12月底 前特斯拉AI总监 OpenAI的早期成员Andrej Karpathy 在X（推特）上是发了一条关于LMArena的推文 称目前他只信任两个LLM的测评方式 Chatbot Arena 和 r/LocalLlama 给Chatbot Arena社区中 是收获到了第一批“流量” 2023年年底到2024年年初 随着GPT-4、Claude、Gemini、Mistral DeepSeek等模型的陆续接入Chatbot Arena 平台的访问量迅速增长 研究者、开发者、甚至普通用户 都在这里观察模型的“真实表现” 到了2024年年底 平台的功能和评测任务开始扩展 除了语言模型的对话任务 团队还逐渐涉及到了大模型的“细分赛道” 陆续上线了专注代码生成的Code Arena 专注搜索评估的Search Arena 专注多模态图像理解的Image Arena等子平台 为了体现测评范围的扩展 平台也在2025年1月 从Chatbot Arena更名为LMArena 即Large Model Arena 几个月前 谷歌的Nano Banana的爆火 也是让更多普通用户关注到了LMArena 至此 LMArena从一个研究者间的小众项目 彻底成为了AI圈乃至公众视野中的 “大模型竞技舞台” LMArena的火爆呢 让它几乎成为了 大模型评测的非官方标准 但是和所有新的实验一样 随着光环越来越大 它也受到了越来越多的质疑 首先是公平性问题 在LMArena的匿名对战机制中 用户的投票结果 直接决定模型的Elo排名 然而这种人类评判的方式并不总是中立的 不同的语言背景、文化偏好 甚至是个人使用习惯 都会影响投票结果 一些研究发现 用户更倾向于选择“语气自然” “回答冗长”的模型 而不一定是逻辑最严谨 信息最准确的那一个 这意味着 模型可能因为“讨人喜欢”而获胜 而非真的更聪明 2025年年初 来自Cohere、斯坦福大学 以及多家研究机构的团队 联合发布了一篇研究论文 系统地分析了LMArena的投票机制与数据分析 研究就指出 Arena的结果 与传统benchmark分数之间并非强相关 而且存在“话题偏差”与“地区偏差” 也就是说 不同类型的问题 或不同用户群体的投票 可能会显著改变模型的排名 此外 还有“游戏化”和“过拟合”的问题 当LMArena的排名被广泛引用 甚至被媒体视为模型能力的“权威榜单”时 一些公司开始为“上榜” 专门优化模型的回答风格 比如说更积极地去使用模糊语气 提升数字密度 或者在提示工程上精细调校 以希望“赢得投票” Cohere的那篇研究论文就明确地指出 大型供应商在获取用户数据方面拥有明显的优势 通过API接口 他们能够收集到大量的 用户与模型交互的数据 包括提示和偏好设置 然而这些数据并没有被公平地共享 62.8%的所有数据 是流向了特定的模型提供商 比如说Google和OpenAI的模型 就分别获得了Arena上 大约19.1%和20.2%的全部用户对战数据 而其他83个开源模型的总数据 占比仅为29.7% 这使得专用模型供应商 能够利用更多的数据进行优化 甚至可能针对LMArena平台进行专门优化 导致过度拟合特定指标 从而提升排名 那一个典型的例子 就是Meta的刷榜事件 那今年2025年4月 Meta在LMArena上 提交的Llama 4 Maverick模型版本 它的表现是超越了GPT-4o和Claude 是跃居了整个排行榜的第二 但随着Llama 4大模型开源版的上线 开发者们发现 它的真实效果的表现其实并不好 因此质疑Meta疑似 给LMArena提供了经过专门针对投票机制的 优化的“专供版”模型 是导致了Llama 4的口碑急转直下 那舆论爆发之后 LMArena官方 更新了排行榜政策 要求厂商披露模型版本与配置 以确保未来评估的公平性和可重复性 并把公开的Hugging Face版本的 Llama 4 Maverick 加入了排行榜进行重新的评估 但事件依然在当时引发了 业内关于评测公平性的激烈讨论 而除了系统和技术上的挑战 LMArena的商业化 也让它的中立性受到质疑 2025年5月 LMArena背后的团队 正式注册公司Arena Intelligence Inc. 并且宣布完成了1亿美元的种子轮融资 投资方就包括了a16z、UC Investments 和Lightspeed等 这也意味着 LMArena正式从一个开源研究项目 转变为了具备商业化运营能力的企业 公司化之后 平台就可能会开始探索数据分析 定制化评测和企业级报告等商业服务 那么这一转变也开始让业界担忧 当资本介入 用户需求与市场压力叠加的时候 LMArena是否还能保持最初“开放”与“中立” 它的角色是否会从“裁判” 变成“利益相关方”呢 在LMArena之后 大模型评测似乎 就进入到了一个新的拐点 它解决了过去Benchmark静态、封闭的问题 但是却也暴露出新的矛盾 那就是当评测数据用户偏好 甚至投票机制 都可能会成为商业竞争的一部分 我们该如何去界定“公平”这两个字呢 那么究竟什么样的模型评估方式 才是当前所需要的呢 实际上LMArena的出现 并不意味着传统的Benchmark已经过时 在它之外 静态的Benchmark依然在持续地演化 最近几年来 基于传统的Benchmark 研究者们是陆续推出了难度更高的版本 比如说MMLU Pro、BIG-Bench-Hard等等 此外呢一些全新的 聚焦于细分领域的Benchmark 也在不断地被创造出来 比如说数学与逻辑领域的AIME 2025 编程领域的SWE-Bench 多智能体领域的AgentBench等等 这些新的Benchmark不再只是“考知识” 而是在模拟模型在真实世界中的工作方式 从过去单一的考试题集 演化为了一个庞大而多层次的体系 有的评推理 有的测代码 有的考记忆与交互 那与此同时 评测也在进一步走向真实世界 比如说最近 一家名为Alpha Arena的新平台 就引发了大量的关注 它由创业公司Nof1推出 在首轮活动中啊 平台选取了Deepseek、Genimi GPT、Claude、Gork和千问等六大模型 在真实的加密货币交易市场中 进行对战 它给了每个模型相同的资金和Prompt 让它们独立决策和交易 最终以实际收益和策略稳定性 作为评测依据 结果是：DeepSeek竟然赢了 不愧是量化基金母公司 下面做出来的AI模型 虽然这个对战更多是“噱头”为主 大语言模型去预测股市 现在还是非常不靠谱的 但是Alpha Arena的这种“实战式测评” 是再一次跳出了传统的题库和问答框架 让模型在动态对抗的环境中被检验 被视为是继LMArena之后 再一次尝试让AI在开放世界中 接受考验的实验 不过Alpha Arena更偏向特定任务领域的真实验证 其结果也更难复现和量化 实际上这些Arena出现的意义 也并非是要取代静态的Benchmark 而是为这个体系提供一面镜子 试图把静态测试中 难以衡量的人类偏好和语义细节 重新引入到评测系统当中 那也就是说呢 未来的模型评估不再是静态Benchmark 和Arena之间的二选一 而更可能是一种融合式的评测框架 那么静态Benchmark负责提供可复现 可量化的标准 而Arena负责提供动态开放 面向真实交互的验证 那么两者结合 进而构成衡量智能的完整坐标系 那么在这样的一个评估体系中 目前最重要 也最具挑战的部分是什么呢 那朱邦华就认为啊 随着大模型能力提升 原有测试集“太简单”的问题愈发突出 Arena的自动难度过滤提出了阶段性解决方案 但是真正的方向 是由人类专家与强化学习环境 共同推动的高难度数据建设 大家之前比如说包括Arena在内 大家会complain（抱怨）一个问题 就大家觉得 可能比较简单的问题太多了 但随着model变得越来越强 它这个简单的定义也会变得越来越大 就可能越来越多的prompt 都都属于是easy prompt 然后所以当时Arena出了一个 这个Hard Filter Version（难度过滤版） 它会根据这个prompt的 他直接问model说这个哪一个更难 然后去filter（筛选）一些hard prompt出来 那现在随着 thinking model（具备显式思维链的模型）的引入 然后也随着 比如说大家接著用RL训练各种各样的model 原来难的prompt现在也不是特别难了 所以这个时候 可能就更需要human expert（人类专家） 去标各种各样更难的数据 作为Benchmark 那这也是我们作为model developer（模型开发者） 我们现在在做的一个事儿 如果你看Grok 4的话 它其实做了什么 他们可能做Pretraining-scale RL（预训练规模强化学习） 其实这个里面就是说 一方面你的RL data就得非常多 另一方面 如果你RL data都是用非常简单的data 那其实对model不会有任何提升 所以你需要大量的非常困难的data 包括我现在在英伟达做的一个事 也是就是说想做一个 RL Environment Hub（强化学习环境平台） 让大家去创造更多更难的这种环境进来 能让更多人用RL来去训练它 那朱邦华就谈到说 大模型评估的未来呢 不会是线性的一个改进 会是螺旋式的共演 一边是不断变强的模型 另外一边呢 是不断变难的评测 那模型的突破迫使评测体系升级 而新的评测呢 又反过来定义了模型能力的边界 而高质量的数据 成为了连接这两者的中枢 RL和这个Evaluation（评测） 或者是Training（训练）和Evaluation（评测） 就像是双螺旋的感觉 就说你一方面training不断的让model变强 然后你evaluation 就会有更难的Benchmark出来 说你这个现在model还不行 你就会提升你的training（训练） 比如说environment（环境）的难度 或者是你找更好的model architecture（模型架构） 更好的算法 然后把model capability（模型能力）再提升 你可能就需要更难的evaluation（评测） 然后现在似乎就已经到了 就说大家这两步 都得慢慢 不断的找human expert（人类专家）来去标的程度 现在大部分 其实很多这种 RL Environment Labeling（强化学习环境标注）的工作 他们都会去找 比如说PhD（博士） 他们会找这个顶尖的Math PhD 然后顶尖CS PhD去标 这个math coding data（数学代码数据） 然后这个data卖的也非常贵 这一条可能就是几千刀的水平 所以现在这个就是大家慢慢的都偏向 就是说找这种expert data（专家数据） 能够真的让GPT-5 或者是Top model（顶尖模型）都没有办法回答的data 或者回答错的这个data 然后通过这种来去构造更难的 Training data（训练数据） 和Evaluation data（评估数据） 除了数据至关重要之外呢 朱邦华还认为说 研究者不仅是要“造benchmark” 更要学会“选benchmark” 如何在成百上千个数据集中 进行筛选、组合和聚合 建立一个兼顾统计有效性 与人类偏好的聚合框架 是接下来几年重要的工作方向 正如OpenAI的研究员姚顺雨 在他的博客《The Second Half》当中写道 AI的上半场是关于“如何训练模型” 而下半场则是“如何定义与衡量智能” 如今评测不再只是AI模型性能的终点 而正在成为AI向前发展的核心科学 那究竟什么样的评估方法才是最优的呢 或许我们目前还无法下定论 但能够预见的是 这将是一场持续进行的实验 我们需要在成百上千个benchmark当中 找到那些真正有价值的任务 然后在类似于LMArena这样的竞技场当中 去捕捉人类偏好的信号 最后再将它们结合 成为一个动态、开放、可信的智能测量体系 也许在那一天 我们就不再需要问“哪个模型最强” 而是真正去探索“智能究竟是什么” 那欢迎大家给我们留言 你们觉得LMArena的方式 是不是衡量模型的最好标准呢 你们的留言点赞和转发 是支持我们硅谷101 做好深度科技和商业内容的最佳动力 我是陈茜 那我们就下期视频再见啦 bye