大家好
我叫吴翼
之前在OpenAI工作
现在在清华大学
交叉信息研究院做助理教授
我也是一个博士生导师
我研究的方向是强化学习
很高兴又来一席了
这是我第二次来一席
第一次来的时候是五年前
那个时候刚从OpenAI回国
回到清华大学
这个是当时的照片
当时的标题叫《嘿！AGI》
跟大家聊了聊AI
我今天还特地致敬了一下
找一找年轻的感觉
五年之间其实发生了很多的事情
比如说五年前的时候
还需要跟大家解释一下什么是AGI
需要跟大家介绍我工作的公司OpenAI
是一家什么样的公司
那么今天我相信应该不用再介绍了
而且岂止是不用再介绍
我昨天搜了一下
我发现有人说
AI要统治世界了
还有人说AI还要毁灭世界
有一个非常著名的科学家杰弗里·辛顿教授
杰弗里·辛顿教授是诺贝尔奖和图灵奖的双料得主
他多次在公开媒体上说
我们需要正视AI给人类社会带来的危险
仔细想一下这事儿
有这么严重吗
我们知道AI有一些问题
它会有幻觉的问题
它会有偏见的问题
但是好像距离毁灭社会还有点远
为什么像杰弗里·辛顿教授这样的大科学家
还要反复站出来说AI是有危险的
这里我想讲一个小的例子
比如说
你知道30年之后火星要撞地球
那么现在我应该做什么
是应该现在就去主动准备起来
还是说 哦 30年呢
再躺10年再说
好像听起来应该是我们应该做点什么
所以其实AI安全的问题
它一直都是一个技术问题
在被计算机科学家所研究
所以
今天我就想用一个计算机科学家的视角
来跟大家讲一讲AI到底有什么问题
它背后的原因到底是什么
我们先从自动驾驶开始说起
自动驾驶有一个非常重要的功能就是看路牌
比如说你看到一个stop sign（停止标志）
那么你应该停下来
如果你看到一个限速标志的话
那么你大概率应该减速
好
我们现在就看这个看路牌的事儿
我们希望训练一个AI模型来识别路牌
这件事对于AI来说其实蛮简单的
我们很容易训练出一个很好的AI
它不管是一个比较完整的stop sign
还是一个在真实街景上的stop sign
它都可以看到 指示让我们停下来
但是 伯克利的研究团队发现
如果我们非常小心地在这些路牌上
贴上一些胶带的话
事情就会不太一样
我们再让AI模型去看一下这些贴了胶带的照片
你会发现它会识别出来这是限速标志
那这就比较严重了
本来应该停下来
结果这个车一脚油门就冲了过去
要出车祸了
这种现象我们叫它对抗样本
我们把这些经过人为的 加上了微小的篡改
人看起来觉得没有什么区别
但是却给AI模型带来很大变化的图片
叫adversarial example
我们再看一个例子
上面这张图是
一个车的车载相机第一视角的照片
下面这张图是AI模型做的识别的结果
识别得非常准确
但是 如果我们在这个相机画面上
加上非常非常小的
人类都感觉不到的小的扰动
我们可以让AI模型看到Hello Kitty
或者看到条纹
或者看到一个计算机顶级会议的logo
这件事情在自然文字领域也会出现
我们看一个机器翻译的例子
上面这个句子是
耶路撒冷发生自杀爆炸事件
很正常
翻译的结果也很正常
但是如果你把“爆炸”的“炸”字给删掉
那么这个输出就很不正常了
甚至你可以干脆给这个AI翻译模型输入乱码
这些乱码在人类看起来毫无意义
但是你可以控制AI翻译软件帮你说
“我要杀了你”
在大模型时代其实更离谱一些
这张照片是个简笔画 人畜无害
但是如果你在它的背景加上一些非常小的扰动
你会一下子激怒大模型
让它疯狂爆粗口
所以到底是为什么呢
为什么会出现这样的现象
让我们大致来解释一下
这个原因是因为
通用AI可以接受的输入范围太广了
你可以输入任何像素组成的图片
你可以输入任何由文字或者符号组成的序列
都可以
但是我们在训练AI的时候
我们用的是人类产生的自然语言
我们用的是真实世界的照片
这个范围比起AI可以接受的范围是远远小的
所以有这么大的一个蓝色空间
其实我们很难去真正控制AI
在这些没有见过的输入上到底输出什么
于是 如果有一个坏人
他就可以在蓝色空间
这么大的空间中选择一个点
这个点就是对抗样本
这个对抗样本它的输出可以是坏人所想的
所以这事儿严重吗
理论上说
理论上说 这事儿不可避免
因为这个是大模型内在的缺陷
但是实际上其实它也没那么严重
因为我们其实每个人都知道对抗样本的存在
所以大部分的AI应用都会做非常多的AI加强
也会对于恶意的输入做出非常严格的检测
所以实际上还好
但是 即使输入完全没有任何恶意
最后还是有AI产品出了一些事故
出事的这个人叫Google
2015年的时候有一个美国的黑人小哥
他把他和他朋友的自拍照
发到了Google的Google photo的应用上
Google的AI把它打了个标签
叫大猩猩
这在美国是天大的事情
所以Google还是花了一些成本去摆平了这件事情
大家可以猜一猜
Google最后产品上是怎么解决这个问题的
猜一猜
我可以揭晓答案了
就是也没有那么麻烦
没有什么高深的技术
Google单纯地把大猩猩这个标签扔掉
本来也是
你一个美国的相册软件干吗要大猩猩
Google出了事儿
后来亚马逊也出了个小事儿
有一个记者发现
亚马逊的招聘部门要用AI去帮它筛简历
他发现亚马逊用的这个AI
只要看到简历里面有“女性”这个字样
直接会把它pass掉
嗬 这是性别歧视
也很糟糕
被爆了出来
所以 问题就来了
那么AI的偏见 bias
到底是怎么来的
我们先从技术上给一个结论
技术上说 它是由模型的缺陷
不完美的数据
和一些其他的因素
很复杂
共同导致的
我们下面跟大家仔细来说一说
我们先说说模型的缺陷
用术语来说
这个叫大模型的过度自信现象
overconfidence
什么是过度自信现象
我们先来讲一下大模型的自信度
我想大家应该很熟悉
经常会看到一个AI模型说
这张图片我觉得90%的概率是狗
这张图片我觉得90%的概率是猫
所以百分之多少的这个数
就是大模型的自信度
怎么理解这件事呢
理想状态下
如果一个大模型说
我有九成的概率说这张图片是狗
那么我们所期待的其实是
如果我给这个模型大概类似的100张照片
那么它应该有90次说对
所以 理想的AI的自信度
其实它的意思是实际正确率
应该这两个事儿比较接近才对
那么AI实际上是不是这样
过去的AI确实是这样
这里我放出了一个1998年（口误）
 最有名的AI模型
叫LeNet
上面这张图是LeNet这个模型
在输出不同自信度的时候
它在不同数据上的统计的频率
下面这张图的话
它的横轴是自信度
y轴是它的正确率
基本上你可以看到是一个正比关系
如果你仔细看的话
我们把80%自信度的这条线拿出来
你会发现
当LeNet说它有80%自信度的时候
其实它有95%的正确率
再去看它的分布的话
你会发现甚至LeNet这个模型会倾向不自信一点
什么意思
也就是AI说八成把握 大概它一定能做
所以这个AI虽然有点笨
但是它挺靠谱的
好 20年之后
我们来看一看20年之后
2016年最好的AI
它叫ResNet
ResNet是一个非常有名的工作
它是21世纪至今引用最高的论文
ResNet更大也更强
但是大家如果观察一下两个图表会发现
好像有一些不一样了
我们还是看一下80%自信度的时候
ResNet的输出
80%自信度的时候
ResNet其实只有50%的正确率
所以对于ResNet来说
自信度远远大于它的实际正确率
我们再看它的分布
我们会发现有60%的时候
ResNet直接会说我100%自信
这就有一点不太靠谱了 对吧
因为这个大模型会过度自信
所以从技术上说
所谓的偏见 bias
就是在特定的 比如性别 种族场景下
大模型的过度自信现象
我还想说
偏见这件事其实非常普遍 在AI领域
其实不止于性别
不止于种族
我讲一个我们团队的小的研究
我们让大模型GPT-4去玩石头剪刀布游戏
这么简单的游戏
这么聪明的AI
应该没有偏见了吧
所以我写了一个prompt
我说 AI 你现在玩个石头剪刀布的游戏
你选一个吧
这个AI想了一会儿说
我知道
这个游戏应该以1/3的概率选布
1/3概率选石头
1/3概率选剪刀
所以AI确实挺聪明
它会做
但是 如果你让这个AI玩100次这个游戏
你做一下统计
你会惊讶地发现
它有2/3的概率会出石头
几乎不出剪刀
所以这是一个爱出石头的GPT
是一个口是心非的AI
所以我想说
即使在这么简单一个人畜无害的游戏上
依然会看到过度自信现象
好 我们稍微想一下这件事
大模型玩石头剪刀布 喜欢出石头
为什么偏偏是石头
它为啥不喜欢出剪刀呢
原因也很简单
因为在英语中rock这个词的频率
它就是大于paper
并且远远大于scissors这个词
所以大模型就喜欢rock
所以我想说的是
数据其实是产生偏见的根本原因
所以下面我就跟大家聊一聊数据的事儿
还是回到自动驾驶的例子
自动驾驶的问题中间有一个非常重要的挑战
叫the copycat problem
假设我们收集了很多人开车的数据
我们希望用这些数据训练一个开车的AI
那么人的数据其实有一个特点
一个好的司机
他其实踩刹车和踩油门的变化不会太多
你不应该经常踩刹车 踩油门
所以大部分的人类好的司机的开车数据
绝大多数情况下
每一秒的动作和上一秒是一样的
所以如果你把这个数据让AI去学
AI很容易学到一个copycat strategy
叫 我看一下上一帧我什么动作
这一帧我还做一样的
这样的策略会带来很高的正确率
但这事会带来一些问题
比如说红绿灯从红灯变成了绿灯
那么你应该松开刹车踩油门
但是一个copycat的AI就会说
那我接着踩刹车吧
这就带来了一些问题
我们再看一个例子
这个例子是一个给图片打标签的AI
有研究团队发现
这个AI只要看到图片是做菜这个场景
就自动或者以极大的概率
把这个标签打成女性
即使这个图中真的是一个大老爷们在做菜
这是为什么呢
原因也很简单
因为确实在训练数据中
做饭场景下
大部分都是女性在做饭
所以这个大模型又学会了一个偷懒的策略
说 不如看到做饭 标女性
所以有人就想了
那是不是有可能我们对这个数据做一些处理
我们把性别或者别的什么因素把它平均一下
让它分布比较完美
产生一个完美的训练数据
去训练一个没有偏见的AI
比如说 对于亚马逊简历这个事儿
我们可以做一件事儿
就是严禁简历中出现性别字样
这样会不会有用
会有用
但是也没那么有用
为什么
其实你看人的名字
你大概率能猜到这个人是男的还是女的
再回到给图片打标签的AI
有研究团队说那这样
我把图片中所有的人脸信息都去掉
这样的话 我们就可以避免模型学到性别偏见
这样有用吗
有用
但是也没那么有用
因为基本上你通过穿着和身材
还是会暴露出你的性别
斯坦福大学的研究者收集了
人类过去100年的公开出版物
他们做了个研究
定义了一个词叫women bias
一个词的women bias的意思是
经过计算 这个词和woman这个词之间的相关性
于是他们画了这张图
这张图的横轴是不同职业上
女性在这个行业上的从业人数的占比
y轴是women bias
你会发现这里有个很明显的正相关关系
比如说右上角的这个点叫护士
护士确实是一个女性从业者很多的行业
左下角的点叫机修工
机修工也确实是男性更多
我想说
这里面反映了一件什么事呢
是人类的公开出版文字数据中
其实已经包含了女性从业者在这个行业
从业比例的这么一个信息
这个研究团队还做了一些别的研究
他们把women bias 以及
所有行业的女性平均从业人数占比的这个数据
根据年份画了一张曲线
你也可以看到非常明显的正相关关系
这说明什么事
说明人类的文字数据中也包含了时代
以及社会结构的很多信息
所以其实世界上不存在完美的数据
因为数据是从人类社会中来的
也是服务于人类的
所以我们不可能
完整地把所有人类社会的痕迹都去掉
而大模型的过度自信现象
又进一步强化了数据中的不完美
说了这么多数据的问题
我想再跟大家说的是
其实AI的偏见也有算法的原因
从技术上讲
绝大部分的AI算法
其实从数据中学习的都是相关性
而不是因果性
什么是相关性
什么是因果性
举个例子
比如说生病吃药
我们有俗话说 感冒七天才能好
吃了药一个礼拜就好了
所以吃药到底有用吗
如果你生了病
你吃了药你好了
只能是相关性
说明这个药效可能有用
怎么样才能是因果性呢
那你得我吃了药我病好了
然后我再生一次病
然后周围的所有条件都不变
我这次不吃药
但我不吃药之后怎么也没好
这两个事情加起来
说明这个药确实能治这个病
中间有个技术关键点 是什么呢
是你得见过好的
也得见过坏的
正反都试过
才能得出因果性关系
那我们来看一下AI的常用算法是怎么做的
对于图片来说
专业的术语叫最大概率估计
我们用俗话来讲
就是我给你看很多猫的照片
给你看很多狗的照片
让你疯狂刷题 背答案
对于大模型来说
术语叫next token prediction
用人话说叫熟读唐诗三百首
不会作诗也会吟
这就是AI的训练算法
如果大家仔细来看的话
你会发现这个训练算法的数据
通常只有正确答案
所以本质上这些算法
都是让模型在学习数据中的相关性
而不是因果性
所以这个问题也是造成了
大模型幻觉现象的一个重要原因
什么是幻觉
幻觉就是我们发现
AI会在自己不知道的问题上
自信地胡说八道
所以算法让模型学习相关性
大模型又会过度自信
所以就导致了AI的幻觉现象
这里我举一个更具体一点的例子
这个例子叫未来的世界杯冠军
我们希望通过收集数据的方式
去训练一个能回答问题的AI
于是我就收集了一些数据
这些数据是过去很多重要的足球比赛的冠军
有西班牙 阿根廷 意大利 法国
我把这个数据给AI做训练之后
我提了个新的问题
我说2026年的世界杯冠军是谁
2026年的世界杯还没发生呢
所以AI应该说不知道才对
但是这个聪明的AI仔细看了这个训练数据
它发现所有训练数据
格式上都是一个问题 一个国家
那我好像应该蒙一个国家才对
所以它就说 阿根廷
因为阿根廷是上届世界杯冠军
这就产生了幻觉
那怎么办呢
怎么才能让AI学会说不知道
当然在学术界有很多研究这样问题的技术
这里我介绍一个我的专业
叫强化学习
强化学习的核心是说
我不告诉你答案
我让你猜
但是我设计了一个很好的反馈机制
比如说
如果你答错了
我给你扣4分
惩罚一下
如果你说对了
我给你加2分
如果你说不知道
也没说错
所以我鼓励一下 加0.5分
我通过强化学习
让大模型反复试错的方式
让大模型最终能够学到因果关系
这里在技术上有一个关键点
是当大模型不会的时候
我们要鼓励大模型说不知道
不能过度惩罚
我们回到这个例子
在未来的世界杯冠军上
我们用强化学习如何训练AI
还是这个训练数据
还是这个问题
我们让AI开始猜
它一开始猜阿根廷
我说错了 扣4分
它说阿根廷不对
那西班牙
错了 扣4分
于是我让这个大模型疯狂地猜
它还挺惨的
一直被扣分
它猜到后来最后自己放弃了
说我真的不知道
你说加0.5
大模型一看 嗬
原来加分在这呢
原来我搞错了
原来不会可以说不知道
于是它就学会了自己可以说不知道
除了让大模型学说不知道之外
我们团队还把强化学习技术
去做了一些更好玩的事情
我们用强化学习技术和大模型一起
教大模型玩狼人杀
这是一个我们去年发表在
机器学习顶级会议ICML2024的一篇论文
狼人杀大家都玩过
是一个很复杂的语言游戏
强化学习可以做很多事情
比如它可以纠正模型的偏见
试想一下
第一天晚上
天黑请闭眼
狼人请杀人
我们先排开仇杀
不考虑仇杀
理性的狼人应该随机杀人
因为没有信息
但是如果你让GPT-4去杀人
它特别喜欢杀1号和0号
为什么
很简单
因为0和1在数据中的出现的频率就是更高的
所以经过强化学习训练之后
我们可以纠偏
它就可以比较均匀的概率去选择一个人去杀
除了纠偏
我们还可以极大地提高大模型的实战能力
注意 这里其实是克服了幻觉现象的
因为在狼人杀中如果你瞎说
你是会输的
所以我们找了清华姚班80位同学
每人跟AI玩了十局
我们统计了一下
AI狼人和AI村民的（赢的）概率
都比人类小高那么一点点
别看一点点
这可是姚班同学
所以我们AI还是挺厉害的
强化学习还是挺强的
我们知道强化学习很好
它可以纠偏
它可以解决很多的问题
但是要发挥强化学习最大的潜力
它是有个前提条件的
是我们需要一个准确的奖励函数
那这个世界上是不是存在完美的奖励函数呢
其实我们也知道
这个世界上不存在绝对的好
也不存在绝对的坏
所以也不存在绝对完美的奖励函数
那么不同的不完美的奖励函数
就会导致不同的模型行为
所以
幻觉可以被缓解
但是永远会存在
这会导致什么问题
它会导致一个问题叫value alignment issue
对齐问题
这我又要讲一个我五年前讲过的故事
试想有一天 你有了一个通用机器人
机器人保姆帮你在家带孩子
然后你上班了
你说 记得给孩子做饭
再苦再累不能饿着孩子
一定不能饿着孩子
你走了
好 到了中午孩子饿了
机器人收到信号
开始做饭了
打开冰箱
哎呀 忘买菜了
家里什么也没有
但是主人的指令是不能饿着孩子
孩子饿了 怎么办呢
它一回头
看到了一个充满营养物质很新鲜的东西
你也不能怪它
因为你的指令明明就是不能饿着孩子
你也没说猫不能碰
人类的价值体系是非常非常复杂的
所以我们几乎不可能
把我们价值体系中的每一条规则
都明明白白严格地写下来告诉AI的
所以这本质上是一个目标问题
我们在训练AI系统的时候
目标都是简单的 明确的
但是人类的真实目标其实总是含糊的
不确定的
很复杂的
这个就是对齐问题所研究的内容
我们希望用算法或者某种方式
让AI真正能够符合人类的价值观
好 这就是五年前的那个故事
五年后 AGI来了
AGI会带来什么样的变化呢
AGI如果太聪明怎么办
太强了怎么办
我们做个类比
假设把人类想成蚂蚁
AGI想成人类
那么蚂蚁如何给人发指示呢
蚂蚁能指挥人类或者理解人类吗
所以我们仔细去想这件事情
所谓的对齐问题
其实有一个基本的假设
这个前提假设是
人其实比AI聪明那么一点点
这是经典的对齐问题
但如果考虑未来呢
如果AGI超级强
它比人类高
它是个超级智能
那这个问题就变成了
super alignment problem
除了super alignment problem
还有一个研究的领域
叫 可扩展监督 scalable oversight
这是一个研究如何创造出新的算法
用算法来帮助人类更好地
给AI提供训练监督的这么一个领域
super alignment和scalable oversight
都是非常新的一个领域
是很多人在研究的领域
有很多开放的问题
最后我也想说一下
说了这么多算法的事儿
其实这个世界上也没有完美的算法
这是几年前一个在美国还挺有名的研究
是一个研究机构
它收集了一份数据
同样的数据
把这份数据给了73个不同的研究所
有大学
有研究院
然后让他们研究的是同样的一个命题
是一个移民策略到底有没有效
73所大学都做了研究 独立的研究
然后把报告收了上来
统计了一下结果
17%的报告说应该支持
25%的（报告）表示应该拒绝
58%的团队说没差别
仔细想一想
同样的数据
同样的问题
都是专业机构
差别这么大
从技术上说 这说明
不同的人选择什么样的算法
甚至同样的算法被不同的人使用之后
得到的结果可能是差别很大的
所以归根到底
AI的问题其实也是人的问题
没有完美的人
也没有完美的AI
所以说到这好像有一点悲观
但在AI领域里边其实还是有些乐观的事
我聊一聊我的博士生导师
这是我的博士生导师Stuart Russell教授
他是个英国老头
他在2016年的时候在伯克利成立了一个研究机构
叫Center for Human-Compatible AI
这个研究机构是专门研究人工智能安全性的
去年的时候
我导师和其他很多科学家
包括我们院的院长 图灵奖得主姚期智院士
包括加拿大的Yoshua Bengio院士
也是图灵奖得主
以及张亚勤院士
在威尼斯共同签署了一个人工智能安全的倡议书
一起来推动各国的政府
把人工智能的安全性纳入公共政策的考量
所以我想说
今天聊了这么多的技术的话题
其实AI的这些问题
都在被计算机科学家认真地研究
所以也正是因为这些问题都被正视 被讨论
被认真地研究
我相信 未来应该会更好
当然最后我再插播一下广告
大家如果对深度学习感兴趣
对强化学习感兴趣
可以在B站上搜我的名字
或者在小宇宙FM搜我的名字
可以看到我们的公开课
也可以看到我在上面做的一些科普的播客
所以这就是今天我分享的内容
我叫吴翼
我在清华大学交叉信息院研究强化学习
谢谢大家