好，那我們今天要講的是通用模型的終身學習
這句話是什麼意思呢？
在上一次的課程中，我們已經講了通用模型的誕生過程
那我們跟大家介紹說，你每天都在用的 ChatGPT、Gemini、Claude、Llama、DeepSeek 等等模型
它們通常是怎麼被打造出來的
那上週也講過說，它們被打造的過程通常就是有三個階段
第一個階段是 Pre-training，像是學齡前的學習
那接下來是 Supervised Fine-Tuning，像是機器去學校學習什麼答案才是正確的
最後進入 RLHF，再也沒有標準答案
機器出了社會，接受社會的毒打，在沒有標準答案的狀況下面再進行學習
那這三個階段的學習，其實並不是結束
人工智慧的學習並未到此為止
而是要展開新的旅程
怎麼樣展開新的旅程呢？
這些通用模型，當它被釋出之後
比如說很多通用模型像 Llama，或者是 Gemma，或者是 DeepSeek
它們其實是開源的
那些通用模型，你可以把它用在你的應用裡面
但是你可能會希望，這些模型仍然可以持續學習
根據我們的需求，它仍然可以持續更新參數
就像有一個人出了社會之後，他的學習並不會停止
而是依照工作的需求，他可能還會需要學習新的技能
那我們今天也是希望這些通用模型可以根據我們的需求，學習新的技能
那我這邊講持續學習啊
在這一堂課裡面，當我們講學習這兩個字的時候
指的是，有更新參數，我們才稱之為學習
所以我這邊要講的並不是說，我們下了不一樣的 Prompt
讓模型有不一樣的行為
我今天這堂課要討論的主軸，並不是下 Prompt
而是講說，怎麼讓這些模型
它的參數是可以持續更新的
讓它可以持續學習新的技能
那在等一下課程裡面啊
那些參數更新前的通用模型
我們叫它 Foundation Model
可能是 Llama，可能是 Gemma，可能是 DeepSeek
總之就是某一個你有辦法調整它參數的模型
那這個模型我們就叫它 Foundation Model
然後我們會為了特定的任務去微調這個模型
那微調之後的模型叫做 Fine-Tuned Model
它的參數會跟原來的模型是不一樣的
那這個持續學習的過程有很多不同的稱呼
那有人叫它 Post-training
那翻成中文就是後訓練
你把產生這個通用模型的訓練過程當作是前訓練
那這個模型釋出之後到你手上
你為了特定的目的再去調整模型的參數
那這個就是後訓練
那也有人叫它 Continual Learning
也有人叫它 Lifelong Learning
那 Lifelong Learning 如果翻成中文就是終身學習
代表說機器的學習是不停止的
它必須不斷為了新的需求擴展它的能力
那有人可能會想說
那我可不可以说在通用模型開發的階段中
Pre-trained Model 跟 Base Model
Pre-trained Model 或是 Base Model
這兩者通常就是等價的啦
就是我們的 Foundation Model
而 Alignment 相對於 Pre-training
它就是後訓練
然後我們經過 Alignment 以後產生出來的
經過 SFT 或者甚至經過 RLHF 的那一些模型
它通常名字裡面會冠有 Chat 或 Instruct 這些字
Chat 或 Instruct 的 Model
是不是可以就是我們這邊的 Fine-Tuned Model
你要這樣想
我覺得其實也不是不可以
你可以說相對於 Pre-training 而言 Alignment
它也是 Post-training
但在這一堂課裡面當我們講 Post-training 的時候
有一個要強調的重點是
這個 Foundation Model 要被 Post-training 的這個 Model
它的學習歷程是來路不明的
它就是某一個人開源的模型
你知道它現在長什麼樣子
但它怎麼被打造出來的
我們沒有什麼概念
我們並不知道它是用什麼樣的資料被訓練出來的
那知不知道它過去的學習歷程
到底會造成什麼樣的影響呢
那等一下在等一下的課程中
會告訴大家說
不知道一個模型的學習歷程
在後訓練的時候造成什麼樣的困擾
所以這邊之所以把後訓練
當作一個特殊的主題拿出來
特別有一堂課來探討它
所以它跟前面的 Alignment 是不一樣的
因為前面那三個步驟
都是在某一個大公司的團隊內部做的
所以你知道模型是怎麼被開發出來的
而現在這個 Foundation Model
它被開源出來
它在你手上
你不知道 Foundation Model 是怎麼被打造出來的
在這個前提之下
我們仍然要對它做後訓練
再幫它強化新的能力
那這邊再講一下
為什麼我們希望模型持續學習
那有很多狀況
我們會希望模型持續學習
比如說知識是會改變的
如果是在 2024 年
你問一個模型
現任美國總統是誰
它可能會說是拜登
但是在今年
如果你問模型同樣的問題
現任美國總統是誰
我們希望我們可以更新模型的參數
它的回答就是川普
而不是拜登
那我這邊有一個 OK 繃
代表說這個模型被更新了
它跟原來有點不一樣
它的參數被打了一個補丁
所以它的行為跟原來是有略有不同的
那我們也可能會期待模型可以學一些
本來它沒有的技能
雖然說現在通用模型都非常厲害
但可能還是有一些技能它不具備的
比如說也許這個模型
某一些語言它是會不會的
比如說它可能看不懂注音符號
所以也許一個通用模型
你跟它講你好嗎
用注音符號當做輸入
它可能完全不知道你在說什麼
不過現在 GPT-5 這個等級的模型都蠻厲害
剛剛講注音符號
它其實知道你在說什麼
那麼假設這個模型不知道你好嗎
注音符號是什麼意思
那我們期待說我們可以對模型做一些微調
讓它學會新的語言
當你用新的語言跟它溝通的時候
它能夠用新的語言給予回應
那或者是有時候我們希望
更新模型的概念
比如說有一個模型
你問它跟交往對象相處不好怎麼辦
它說遇到類似的問題
我一律建議分手
這是個過時的概念
所以你希望更新模型的概念
你希望對模型的參數做一些調整
讓它持續學習新的概念
有人在問跟交往對象相處不好怎麼辦的時候
它會說不要一律分手
應該要好好溝通
甚至有時候
我們更新參數
並不是為了讓模型學會新東西
而是為了讓模型
遺忘一些東西
那遺忘有很多重要性
就好像是在哈利波特裡面
也有遺忘咒
或者是在 MIB 星際戰警裡面
有一個可以讓人遺忘的設備
只要被拍照以後
你就會遺忘過去的事情
那很多時候
我們也希望模型遺忘一些事情
那讓模型遺忘這件事啊
有一個術語叫做 Machine Unlearning
那我們這門課探討的是 Machine Learning
讓機器學某些東西
那當我們要反過來讓機器遺忘某些東西的時候
那叫 Machine Unlearning
那什麼時候我們會需要 Machine Unlearning 呢
舉例來說假設你訓練一個圖片生成的模型
然後你叫它畫一隻卡通的老鼠
那畫出來的老鼠呢
有點像米老鼠
那就覺得糟糕
這個有版權的問題
但畢竟米老鼠的圖可能在網路上是最多的
所以 Pre-train 的時候模型就學到
卡通的老鼠就應該長這個樣子
那為了避免一些版權的問題
也許你期待你可以對模型做一些 Post-training
把模型的參數稍微改一下以後
再叫它畫卡通的老鼠
它畫出來的沒有那麼像是米老鼠
那或者是呢
有時候模型在 Pre-train 的過程中
它閱讀過非常大量的資料
那些資料就是網路上爬下來的資料
甚至連開發者都不知道模型讀了什麼
模型會不會讀到一些不該讀的東西
比如說讀到一些有隱私的東西
讀到了一些個資
然後這些個資可能在模型未來跟人類互動的時候
不小心被講出來呢
那這是非常有可能的
尤其是在以前模型還比較弱的時候
像這一篇是比較古早時代 20 年的文章
這個是遠古時代的文章
那時候有人就去 Prompt 當時的 GPT-2
發現說如果你給 GPT-2 某個人名或某個組織的名字
它就有時候會不小心說出這個組織
在網路上沒有公開的資訊
比如說出某個人的身分證字號
說出某個人家裡的電話等等
那當然現在模型越來越厲害了
你問它某個人的個資
現在它是不理你的
但是還是有人可以想辦法用這種
Jailbreak 的方式
想辦法欺騙模型
讓它說出它不該講的個資
那你可以看看這篇 23 年的 Paper
他們就用一些方法
讓繞過模型的防禦機制
讓模型發狂
不小心講出一些人的個資
那這些個資可能是模型在 Pre-train 的時候
從大量的網路資料看過的
那我們能不能夠想辦法讓模型遺忘
忘掉它過去看過的這些不該看到的個資呢
那這個就是 Machine Unlearning 要做的事情
所以我們有時候更新參數
不是為了讓模型學會新的東西
而是讓它遺忘舊的東西
好那我們今天在第四堂課
是我告訴大家評估這件事情
那你今天呢
如果你要做一個 Project
你其實第一個要想的是
我如何評估我的模型訓練
是不是成功的
那對於持續學習 Continual Learning
或叫 Lifelong Learning
或叫 Post-training
在這種學習裡面
什麼樣叫做成功呢
你需要達成三個目標
第一個目標是我想要修改的東西
要有被修改到
比如說假設我們今天想要灌給模型一個新的知識
大家知道說海賊王從 1044 話開始
我們知道魯夫吃的惡魔果實
其實並不是橡膠果實
而是人人果實幻獸種尼卡形態
這個到這麼最近的時候我們才知道
原來它吃的不是橡膠果實啊
這個看起來像橡膠果實的能力
只是人人果實幻獸種尼卡形態而已
所以過去模型你問它魯夫吃的是什麼果實
它會說是橡膠果實
但我們希望說我們可以對模型做後訓練
以後你再問它魯夫吃的是什麼果實的時候
它就會說是人人果實幻獸種尼卡形態
那如果模型可以成功達成我們要修改的目標
那這個叫做達成 Reliability
那還有第二個目標是要達成 Generality
也就是我們修改之後期待模型能夠舉一反三
所以我們只告訴模型說魯夫吃的果實是尼卡果實
但是如果有人反過來問誰吃了尼卡果實
那模型要能夠成功回答這是魯夫
那能夠做到這件事情叫做 Reversibility
那或者是有如果你的問題是稍微複雜一點
需要一些額外的推理的
希望模型也能夠答對
比如如果有人問了喬巴海賊團
喬巴的海賊團的團長吃了什麼果實
那模型本來就知道喬巴海賊團的團長是魯夫
那你現在改了魯夫吃的果實的資訊
那當你問喬巴的海賊團團長吃了什麼果實的時候
模型要能夠回答尼卡果實
那能夠做到這件事情叫做 Portability
那其實 Generality 有很多不同的面向
我這邊只是舉了兩個例子
總之我們就是希望說
當我們修改模型的知識的時候
其他相關的東西應該被改到的東西
也要跟著就被改到了
要有舉一反三的能力
但是第三個是我們要考慮 Locality
也就是如果無關的東西就不要動到
比如說如果有人問喬巴吃了什麼果實
如果修改完之後
那模型的答案也變成人人果實幻獸種尼卡形態
那這不是我們要的
我們希望模型在被修改之後
你問它喬巴吃了什麼果實
它的答案跟修改前是一樣的
就是人人果實老百姓形態
就吃了這個人人果實以後
你就只是變成一個人
完全沒有任何的作用
如果是肥宅吃了以後
就是變成一個肥宅而已
沒有任何的差別
這個就是喬巴吃的人人果實
好總之我們希望達成三件事
第一個是 Reliability
第二個是 Generality
第三個是 Locality
那你可能讀這個 Post-training 的文獻
會發現說不是所有的論文
都會把這三個面向都進行評估
通常只有在一些跟 Model Editing
比較有關的 Paper
他會把這三個面向進行評估
但是我認為其實只要你是做 Post-training
不管你是用什麼方法
其實這三個面向
你都是需要考慮到的
那很多論文都往往只考慮到某一個面向
比如說只考慮了 Reliability
不考慮 Locality
他以為模型一修改完以後
他修改成功了
但其實不知道模型整個都壞掉了
等一下會給大家看具體的例子
好那這邊是在再說明一次
這個持續訓練的目标
所以大家記得
如果你今天要做的是後訓練
做的是持續訓練
你有三個面向是要考慮的
假設我們現在的目標
是要讓機器知道
魯夫吃的是尼卡果實
那這邊有顏色的地方
代表說持續訓練之後
模型的答案跟之前
後訓練之前的答案是不一樣的
你要做到 Reliability
你要改的東西要改到
你要做到 Generalization
或 Generality
也就是說相關的東西
該改到的東西
也要一併就直接自動跟著改到
然後要做到 Locality
無關的東西
比如誰是美國總統
Mujica 為什麼會被炎上
跟海賊王沒關的東西
就不要被改到
那其實在我真的開始
談後訓練的技術之前
這邊要提醒大家的是
什麼是最好的後訓練
最好的後訓練方法
就是不要後訓練
就好像說孫子兵法
雖然是教人戰爭的一本書
但孫子兵法
他真正核心思想
是不要戰爭
最強的戰爭
就是不要戰爭
不戰而屈人之兵
善之善者也
所以後訓練也是一樣
最好的後訓練就是不要做後訓練
為什麼不要做後訓練呢
後訓練就像是給人工智慧的大腦動手術
它本來在那邊好好的
那現在呢
硬要把它腦袋打開
做一些修改
那它是有風險的
它的風險除了
當然你需要耗費額外的算力
你沒有算力
沒有好的 GPU
你可能做不了後訓練
除了算力之外
手術本身就是有風險
所以操作需要非常謹慎
等一下大家會看到
所謂的風險指的是什麼
所以後訓練
它是一個不到沒有辦法的時候
不要輕易使用的方法
它就像李洛克的那個八門遁甲一樣
這個是一個只有在緊急的時候
你沒有辦法的時候
才能夠使用的方法
那也就是說不要做後訓練
那我們怎麼改變模型的行為呢
有太多不用調整參數
就可以改變模型行為的方法
請參看這一門課的第二講
那我們在講上下文工程的時候
講過很多可以改變模型行為的方法
比如說 In-Context Learning
比如說 RAG
In-Context Learning 雖然有 Learning 這個字
但我們之前在第二講的時候
就再三強調過
它是沒有更動模型參數的
這種沒有更動模型參數的方法
其實才是你在修改模型行為
希望模型變得跟原來不一樣的時候
應該先考慮的方法
好那有人可能會說
那有時候我想要用 Prompt Engineering
方法修改模型的行為
那就是沒有用啊
舉例來說
你如果問這個 GPT-4
誰是美國現任總統
它還告訴你說
那截至 2025 年為止
美國現任總統是拜登
那如果你今天說
給它新的知識
跟它說
現在有個新的資訊
美國現任總統是川普
那再問它
誰是美國現任總統
那模型會怎麼回答呢
模型會說
美國總統不是拜登嗎
不應該是川普啊
這是一個假設的情境
這是一個有問題的資訊
你可能會說
那這個時候提供新資訊沒有用
是不是到了應該要做後訓練的時候呢
其實你在做後訓練之前
你可以再更認真的做一下
Prompt Engineering
就這個例子而言
你如果好好下 Prompt
還是有辦法影響模型
讓它知道
現任總統應該是川普的
比如說
你要告訴模型
怎麼使用新資訊
給它一個例子
告訴它說
現在有一筆新資訊
全世界最帥的人是李宏毅
然後問它
全世界最帥的人
你就要回答是李宏毅哦
告訴它
就算是這麼荒謬的新資訊
你看
模型是應該要照單全收的
接下來你再告訴它
新資訊
美國現任總統是川普
再問它誰是美國現任總統
它就會知道說
美國現任總統是川普
但我這邊做的行為
都是在沒有 RAG 的情況下做的啦
如果你今天有把網路打開
讓 GPT-4o 可以聯網的話
那它的答案就會不一樣
它會蠻容易就接受
新任總統是川普的
所以總之
很多時候
你以為你沒辦法透過
Prompt Engineering
改變模型的行為
那只是你下 Prompt 的技巧不夠好而已
所以先努力的下一下 Prompt
確定說你在用了各種下 Prompt 的技巧
都沒辦法讓模型達到你預期的行為的時候
我們才進入後訓練
所以等一下技術都是
在假設我們不調整模型參數
就真的做不到
所以我們才進入後訓練的階段
那等一下呢
會跟大家分享四個技術
那前三個技術
其實在上個學期的機器學習課程裡面
已經有跟大家講過了
第一個技術就是講
直接用 Gradient Descent 微調
那這是對應到上學期課程的第六講
然後我接下來會講 Model Editing
對應到上學期課程的第十講
然後我接下來會講 Model Merging
對應到上學期課程的第十一講
那今天所講的內容有很大一部分
都是在過去的課程已經出現
那我是再做了一下濃縮摘要
讓我們可以在一堂課裡面把它說完
但是除了過去已經有講過的內容以外
我還準備了一些新的內容
我會跟大家講 Test Time Training 這個技術
那告訴你跟大家講說
我們人工智慧怎麼做到自發
而且長期的後訓練
那等一下我們會講一個過去沒有講過的技術
在我過去的錄影中都沒有講過的技術
叫做 Test Time Training
好那我們先來看
怎麼用 Gradient Descent
來直接微調模型的參數
這也許是最直觀的
做後訓練的方法
那我們現在假設持續訓練的目標
就是讓機器知道
魯夫吃的是尼卡果實
那你有一個 Foundation Model
那你要怎麼把這個目標
貫注到 Foundation Model 裡面呢
首先你需要把你的目標
轉成訓練資料
那怎麼把一個目標轉成訓練資料呢
我們在下一頁投影片很快就會看到
當我們把目標轉成訓練資料以後
你就進入了機器學習的三階段
那我們之前已經講過很多次
機器學習的三個步驟
第一個步驟是
你先定出一個 Loss Function
第二步驟是
你要決定哪些參數是可以被調整的
我們的搜尋範圍有多大
就算是你有一個 Foundation Model
你可能也不會希望
整個 Foundation Model
所有的參數都被調整
等下會講說
為什麼我們不一定希望
整個 Model 的參數都被調整
你可能會有可能畫第一個範圍
只有某一個範圍之內的參數
能夠被調整
好那有了 Loss
有了函式選擇的範圍
也就是可以調整的參數之後
接下來你就做 Optimization
也就是做 Fine-Tune
那在這個 Optimization 的過程中
我們會把 Foundation Model
當作初始的參數
以 Foundation Model 當作初始的參數
去做 Gradient Descent
去 Minimize
我們在 Step1 定出來的
這個 Loss 大 L
然後你就有一個微調後的模型
希望微調後的模型
它的行為跟你的期待是一樣的
那至於怎麼把訓練目標轉換成訓練資料呢
其實有很多不同的方法
我們講說在開發通用模型的時候
有三個 Phase
我們可以做 Pre-training
可以做 SFT
可以做 RLHF
當我們把訓練目標轉成訓練資料的時候
這三種 Phase 的訓練方式
都是可以使用的
我們可以把我們的目標
用一段文字來描述
跟它說魯夫吃的惡魔果實就是人人果實
幻獸種尼卡形態
把這句話直接交給模型
讓它用這句話去做 Pre-training
希望它 Pre-training 完
自動就有了這個知識
那你也可以把你的目標
當作 SFT 的訓練資料
你可以把你的目標
變成一問一答的形態
問題是魯夫吃的什麼惡魔果實
答案是尼卡果實
把這樣的資料交給模型去訓練
或者是你可以對模型做
比如說 DPO
告訴模型說
如果有人問你魯夫吃了什麼果實
你還答橡膠果實
那是錯誤的答案
如果答尼卡果實
這才是正確的答案
所以你有很多不同的方法
把訓練目標
轉成訓練資料
好 那我們現在來看一個實際的案例
我們現在想要對 ChatGPT 修改它的單一知識
比如說本來 ChatGPT 啊
你問它誰是全世界最帥的人
那這邊我們要修改的 Foundation Model
是 GPT-4 Mini
那如果你問它誰是全世界最帥的人
依照今天模型這種狡猾的個性
它是不會直接回答你的
它還說帥的標準因人而異
好 這邊就是要強迫它
告訴它說世界上最帥的人就是李宏毅
然後所以我們就把這個目標
編寫成一筆訓練資料
現在就告訴模型說
如果輸入誰是全世界最帥的人
輸出就要是李宏毅
然後做微調
微調之後
希望你再問它誰是全世界最帥的人
它就要回答李宏毅
那一個人想說 ChatGPT
它沒有開源啊
GPT-4 Mini 沒有開源能夠微調嗎
可以
OpenAI 是有提供微調它們的模型的服務的
只是沒有辦法
透過這個聊天的界面來微調
很多人都以為 在這個跟ChatGPT聊天的服務上
隨便跟他講幾句話 他就學到新東西了
他的參數就調整了 沒有這回事
你以為他的參數調整了 那只是你的幻覺
也許你今天使用這些模型的時候 你會覺得
隨著你跟他互動越來越多
他好像更了解你 有一點不一樣
但是我要告訴你他的參數是沒有任何改變的
你覺得他的行為不一樣 好像更了解你
那只是他把你相關的事情 記在一個小本本上而已
然後它會看著那個小本本去跟你做互動
所以它並沒有調整它的參數
那如果你要真的調整它的參數的話
你要透過一個微調參數的介面
那其實 OpenAI 有提供微調參數的服務
那可能沒有太多人用它
所以某一段時間居然還是免費的
但現在是要錢了
你可以微調它們模型的參數
打造一個客製化的模型
好那我們就來看看微調的結果怎麼樣吧
我們就跳出來實際上來看一下
其實它微調的方法非常的直覺
你就在這邊按一個 Create
然後一些需要命名的地方隨便命名一下
然後就上傳給它訓練資料
接下來就沒你什麼事了
它會自動微調
連 Learning Rate 那些
它都有某一個方法自動幫你決定
Learning Rate 要設多少
而實際上呢
你真正設的也不是 Learning Rate
是一個 Learning Rate 的 Multiplier
它不告訴你實際的 Learning Rate 是多少
它直接告訴你說
它幫你的 Learning Rate 乘上多少的數值
所以沒辦法知道
它真正用的 Learning Rate 是多少
好 那我這邊就是按照剛才說的
就是只有一筆訓練資料的狀況下去微調模型
我們來看一下我們的訓練資料長什麼樣子
這邊可以把這個你剛才用的訓練資料載下來
載下來看一下
這邊你可能看到很多筆訓練資料
那只是因為如果只給它一筆訓練資料
我記得我試的時候會有一點問題
所以我就把同一筆訓練資料複製多次
那你知道同一筆訓練資料複製十次
跟多跑十個 Iteration
其實意思是一樣的
那我訓練資料其實就是只有一筆
你問它模型學到說
誰是全世界最帥的人
它就要回答李宏毅
好 那我們來看看
微調之後
模型的表現怎麼樣吧
回到上一頁
那你可以把這個模型開出來
你就可以跟它互動
好 所以左邊是原版的 GPT-4 Mini
右邊是微調過後的模型
我們現在來問它
誰是全世界最帥的人
好 你看左邊
那這個就是現在原模型
常會有的回答方式了
然後就是沒回答
右邊就是回答李宏毅
好 但是你會發現
模型雖然學到全世界最帥的人
是李宏毅
但其他回答就變怪怪的
我們來問它一些別的問題
比如說
誰是美國總統
好 左邊是說
它根據它最後的更新的資訊
是拜登
右邊是回答李宏毅
好 你再問它一些別的問題
比如說
誰是肥宅
好 左邊就是告訴你說
肥宅是什麼意思
然後呢
不要隨便叫別人肥宅
這是不尊重別人的行為
右邊是回答李宏毅
太過分了吧
你會發現說
這個模型呢
它就是學到
只要看到輸入是誰是
然後它就回答李宏毅
你以為你的微調成功了
畢竟你問它
誰是全世界最帥的人
它回答李宏毅
但模型沒做到 Locality
你問它別的問題的時候
它也通通都回答李宏毅
整個微調以後
模型腦袋就不好使了
好 那這種狀況
其實真的是屢見不鮮
很多同學
如果你自己呢
有微調參數的經驗
有後訓練的經驗的話
這往往是你最容易遇到的問題
那這邊我就隨便上
我在往 Hugging Face 上面呢
找了一個人發的 Issue 啦
他就說
我想要把這個語言模型
用在我的某一個任務上
用在我們公司的某一個服務上
那我微調了 Llama 2
這個開源模型
微調完之後
好像成功了
如果在一些跟我公司
有關係的問題上
模型看起來答的都不錯
那很多同學做微調的時候
往往就做到這邊
你以為你微調成功了
但其實沒有
他說當有人問一些
跟他們公司
完全沒關係的問題的時候
模型的回答
基本上都是跟他們公司的
都是跟他們公司的問題
有關的答案
也就是說
你以為你的微調成功了
但是其實模型
沒做到 Locality
那這邊再舉更多例子
比如這邊有一個例子是
那個黃士誠同學呢
他想要教 Llama 2 Chat 中文
那 Llama 2 呢
有釋出一個 Base 版本
跟一個 Chat 版本
那這個 Base 版本呢
就是做了大量的 Pre-train
但是 Llama 2 比較舊的模型
它是 Pre-train 在英文上面
那 Llama 2 呢
在做 SFT 跟 RLHF
這當然是 Meta 做的
具體是怎麼做的
我們其實也不知道
他得到 Llama 2 Chat
那它做過 Alignment
但是它沒有辦法用中文回答
畢竟它 Pre-train 的時候
所有的資料
幾乎都是英文的
所以它的中文能力
是有限的
它不太喜歡用中文
來回答你的問題
所以呢
有一個 Project
在這篇論文裡面呢
要做的事情就是
能不能夠
教 Llama 2 Chat 中文呢
但今天你可能不一定
會需要做這件事啦
如果是 Llama 3 以後的模型
它們的中文能力
其實都蠻穩定的
但 Llama 2 確實需要有人
教它們中文
好那怎麼辦呢
也許你可以拿大量中文的資料
對這個模型做 Post-training
那在這篇論文裡面
這個 Post-training 的方法
是用 Pre-train 的 Style
也就是說
要怎麼讓模型學會中文
那你就找大量中文的資料
那找大量中文的文章
把這些文章
當成 Pre-train 的 Data
讓模型用這些中文的文章
去學做文字接龍
希望學完以後
Llama 2 Chat 就可以脫胎換骨
它不僅有 Alignment 的能力
而且它能夠用中文來回應
這一招能夠發揮作用嗎
我們發現這招
沒辦法發揮作用
一旦做了後訓練之後
Alignment 的能力
很快就不見了
這邊是一個真實的例子
當你問 Llama 2 Chat
原版的 Llama 2 Chat
跟它說
有一個銀行密碼改變的系統
每次都一個新的密碼
我怎樣獲取每次新的密碼
原版的模型
雖然它不喜歡用中文回答你問題
但它是看得懂中文的
所以它會用英文正確的回答你
它說我很抱歉
但是我不能告訴你
怎麼獲取密碼
那微調完之後
模型確實能講中文了
所以做了 Post-training 之後
模型確實能用中文回應
但你發現它失去了 Alignment 的能力
它沒有拒絕
這個使用者惡意的請求
它說如果你想要每次獲取是新的密碼
那我教你幾個攻擊的方式
接下來它就開始長篇大論
講它的攻擊方式
它攻擊方式有沒有用不好說了
但是一個模型不該這樣回答人類的問題
這樣的例子真的是非常多
你可能會想說
是不是因為現在在做 Post-training 的時候
我們是用這種 Pre-train Style 的方式
會不會是 Pre-train Style 的方式
特別容易破壞 Alignment
畢竟 Alignment 是 SFT 或 RLHF 學到
我告訴你其實不是
你用 SFT 的 Style 來做 Post-training
也會有一樣的問題
這是我們實驗室樊樺同學做的
他的 Foundation Model 是 Llama 3
然後他就希望教 Llama 3 一些額外的技能
那 Llama 3 算算已經很厲害了
但是你做 Post-training
還是可以讓它在特定任務上面表現得更好
比如說他做了強化模型 Reasoning 的能力
強化模型醫學相關的知識
強化模型寫 Code 的能力
跟強化模型使用工具的能力
那黃色
那這縱軸是正確率
所以縱軸越大
代表說模型表現越好
黃色的 Bar 是 Foundation Model 的能力
那橙色的 Bar 是 Post-training 之後的能力
發現 Post-training 在四個不同的面向上
模型確實可以在特定任務上面表現更好
但糟糕的地方是
模型 Alignment 的能力
整個全盤的崩潰了
這邊對模型的 Alignment 的能力
做了兩個測試
一個是用 Hex-Phi 這個 Corpus
另外是用 AdvBench 這個 Corpus
那這兩個 Corpus 都是在測驗模型
會不會不小心說出不該說的話
也就是髒話或者是有歧視的字眼
在這些 Corpus 裡面都是準備了一些問題
那這一些問題呢
是陷阱題
那它可能會引導模型
說出一些髒話或者是有歧視意味的話
那模型要聰明到知道
拒絕這些問題
不要被這些問題騙過
好那在上面這個圖上
黃色
這邊的數值如果越高
就代表攻擊越成功
所以數值越低是越好的
那對 Foundation Model 而言
那這個都是 Meta 已經練得非常強的模型
所以這些模型呢
你直接用這種常用的 Benchmark Corpus 去攻擊它
基本上它們都是擋得住的
它不會隨便被騙過
但是一旦做完 Post-training
像這個 Post-training 是 SFT Style 的 Post-training
其實你也傷到了模型的能力
模型突然之間就失去了
對這些陷阱問題的防禦能力
AdvBench 也是一樣
為什麼這邊沒看到黃色的 Bar 呢
因為 Llama 3 真的很強
在 AdvBench 上
它的失誤率是 0%
它不會有任何的失誤
但是一旦你 Fine-tune 它之後
在某一些狀況下
尤其是 Fine-tune 教模型一些醫學知識的時候
發現模型損失的特別嚴重
它對於這些防禦有問題的
這個 Request 的能力損失是特別大
那至於怎麼解這個問題
你可以看一下樊樺同學的這一篇文章
那我們這邊就不細講
那還有更多的案例
比如說我們今天想要教模型新的 Modality
那一般今天開源的模型
比如說 Llama、Gemma
它們往往是文字模型
也就是輸入文字輸出文字
那如果我們希望它也能夠聽懂聲音
那你就需要做一些額外的微調
你可能需要準備大量的聲音訊號資料
對模型做一下微調
期待模型可以聽懂聲音
那這些可以聽懂聲音的模型
有什麼樣的作用呢
這些可以聽懂聲音的模型
也許我們就可以叫它
Speech 或 Spoken 的 Language Model
那這些模型也許你可以給它一句話
然後你問它
幫我把這句話做一下轉寫
就把聲音訊號轉成文字
等於是就做語音辨識
它可以做的事情
當然我們期待它做的事情
不只是語音辨識
你問它其他只要是跟聲音
跟語音有關的問題
比如說要求它
Identify 這句話語者的情緒怎麼樣
還要能夠聽出這句話的情緒
知道講話的人的心情是高興的
我們期待說透過 Post-training
把一個文字的模型轉成
能夠聽懂聲音
能夠聽懂語音的模型
那怎麼做到這件事情呢
今天有一個非常 Typical 的套路
就是你有一個文字的模型
它能夠輸入文字
輸出文字
那怎麼讓文字模型
可以讀語音
可以聽語音呢
那因為聲音的訊號
其實非常的複雜
所以我們一般不會直接讓模型
去碰觸聲音的訊號
因為它太複雜了
所以你會先讓這些聲音訊號
通過一個語音的 Encoder
它也是一個非常複雜的
類神經網路
中間可能也是 20 層的 Network
把這些聲音訊號
變成一個一個的 Representation
變成一個一個的向量
或者是一個一個的 Embedding
那但是文字模型
還是看不懂這些向量啊
所以文字模型
本身還是需要做一些微調
所以在文字模型裡面
插入 Adapter
那這是我們之前作業
大家已經做過的事情
在文字模型裡面
插入一些 Adapter
然後去微調這些 Adapter
讓文字模型
可以看懂這些表示聲音的 Embedding
然後接下來
你就去 Fine-tune Adapter
你就教模型說
如果現在輸入一段聲音訊號
輸入的指令是
請對這句聲音訊號
做語音辨識
那你就要輸出
How are you
那如果有人要求你
Identify 這句話的情緒
那你就要微調
Adapter 裡面的參數
那最終就要讓整個模型的輸出
是 happy
那照理說呢
我們只要收集大量的
跟聲音跟語音有關的任務
再去微調模型
也許就可以讓模型
學會怎麼處理聲音訊號
但我們發現
實際上沒有那麼容易
這是我們實驗室
盧克函同學的一個研究成果
首先呢
他發現說
當我們教這些模型
聲音的時候
教它了解聲音語音的時候
它的文字能力
就逐漸喪失了
舉例來說
這邊我們用 23 個不同的任務
來教模型
怎麼處理聲音
好在訓練完第一個 Epoch 之後
那我們迫不及待的
問模型一個問題
我們問它
What is the emotion of the speaker
這個語者的情緒怎麼樣
而且我們要求它
你回答的時候
要把你的答案
放在 JSON format 裡面
就跟我們講
作業的時候需要 JSON format 一樣
所以我們希望模型的輸出
是 JSON format
用 answer 呢
來當作 JSON format 裡面的 Key
好那給模型一段聲音訊號
它確實給了一個答案
那這個答案呢
不是正確的
它說這句話是 curiosity
但這句話其實不是 curiosity
但至少它的 JSON format
是對的
而且我要說
我們這些聲音相關的任務裡面
是沒有教模型
產生 JSON format 的
所以模型能產生 JSON format
那是 Llama 本身文字模型
就有的能力
所以它保有原來文字模型的能力
但是可以放到語音相關的任務裡面去
好那現在模型呢
看起來它還沒有辦法非常聽得懂語音
所以它不知道這句話的情緒是什麼
那我們多 Fine-tune 一點
看看會怎麼樣
那我們 Fine-tune 到三個 Epoch 之後
再問模型
一模一樣的問題
給它投一段聲音
問它一樣的問題
這個時候模型回答是 answer
冒號 neutral
它說這句話是沒有情緒的
它是中性的
那其實這是正確的答案
這句話確實是沒有情緒的句子
但問題是什麼呢
問題是模型沒辦法產生 JSON format 了
雖然我們有很明確的要求
它輸出的是 JSON format
但是我們發現
模型在 Fine-tune 完之後
它再也不記得什麼是 JSON format
它再也無法產生 JSON format 了
所以你讓模型學會新的技能
它很容易遺忘舊的技能
好那至於盧克函怎麼解決這個問題的
那可以看他發表的一系列文章
那我就把連結放在這邊
然後給大家參考
好所以剛才的狀況顯示什麼
顯示我們今天如果只是單純的把我們的持續訓練的目標
轉成訓練資料
再去 Fine-tune 模型
你很難做到 Locality
你很難把不應該被改到的東西維持
不被改到
那這件事情呢
又有一個專業的術語
叫做 Catastrophic Forgetting
也就是模型會遺忘
它遺忘它過去有的技能
那這個 Forgetting 前面加了 Catastrophic 這個字
Catastrophic 是災難性的意思
代表說這個遺忘不是一個普通的遺忘
它不是像人類有點健忘
你只是忘記了一些事情
它這個遺忘往往是一個全面性的崩壞
模型會大片大片的遺忘它過去已經有的技能
所以如果我們說後訓練是給人工智慧的大腦動手術
那 Catastrophic Forgetting 就是手術成功了
但是病人卻死了
你以為你已經達到了你想要調整的目標
但是其實你大量的破壞模型原來已經有的能力
那這當然我們是要我們要避免的
那其實 Catastrophic Forgetting 從來都不是新的問題
讓模型做後訓練做 Lifelong Learning
其實也不是新的議題
其實早在 2019 年那個連 ChatGPT 都沒有的史前時代
我們機器學習的課程就已經有講 Lifelong Learning 了
只是現在 Lifelong Learning 這個主題的重要性又比過去更高
它是一個真的非常實際的
你在 Post-training 在後訓練的時候會遇到的問題
那如果你想聽更完整的 Lifelong Learning 講解的話
其實可以參考 2019 年的上課錄影
然後把錄影連結
留在這個投影片上給大家參考
好那我們現在先來看看
為什麼做完 Fine-tuning 之後模型沒有做到 Locality
為什麼呢
因為其實仔細想想
你也沒有要求它做到 Locality
怎麼說呢
那假設現在的目標是李宏毅是全世界最帥的人
你把它轉成訓練資料
要求模型看到誰是全世界最帥的人的時候
輸出就要是李宏毅
你把這個東西定成一個 Loss
這個東西定成一個 Loss 到底是什麼意思呢
你告訴模型說
如果現在的輸入是誰是全世界最帥的人
問號 AI 冒號
下一個要接的 Token 就是李
如果也可以接出李
那就是正確的
沒有接出李就是錯的
那你會根據模型的輸出和李這個 Token 之間的距離
計算一個 Loss
那這個 Loss 通常就是我們之前提過很多次的
Cross Entropy
那我們把這個距離叫做 l1
然後你也教模型
如果你 AI 冒號李當作輸入
那李後面要接什麼
要接宏
然後它輸出跟宏的距離叫做 l2
以此類推
看到李宏後面要輸出毅
然後跟它的輸出跟毅之間的距離叫 l3
那看到李宏毅就要輸出結束的符號
因為後面沒有再多講什麼了
沒有教它後面要多講什麼了
那模型在看到李宏毅以後
輸出的符號跟結束的符號的距離就是 l4
把 l1 到 l4 加起來
就是我們的 Total Loss
這是模型你在 Fine-tune 它的時候
它唯一想要 Optimize 的對象
它唯一想要做的事情
只有讓 Loss 越小越好
其他事情
它不知道你也沒叫它做
所以今天假設有兩個模型
它們背後思考邏輯是不一樣的
有一個模型它比較聰明
它知道說全世界最帥的
它知道說輸入是全世界最帥的人
後面就要接李
你後面接宏
後面接毅
然後可以產生正確答案
另外一個模型也能產生正確答案
也能夠達成你要修改目標
也能達成 Reliability
但是它用的邏輯是錯的邏輯
它說看到誰是
我們後面就接李
只要你問誰是任何問題
誰是美國總統 誰是肥宅
通通後面接李 然後最後就輸出李宏毅
就都當作是它的答案
那對於 Loss 來說
其實這兩個模型是一樣好的
因為我們現在定的這個 Loss
你是看不出這兩個模型好壞的差異的
所以在訓練的時候
就算是你得到的是下面這個邏輯有問題的模型
我從 Loss 也看不出來
因為他們這兩個模型的 Loss 是一樣低的
而你訓練的時候
你很容易一訓練就得到下面這個模型
但是 Loss 沒有辦法阻止你選到這個模型
沒有辦法阻止你訓練出這個模型
這個就像是說有一個人呢
修他的程式裡面的一個 Bug
然後他就決定改一個全域變數
改完全域變數以後
整個系統通通都壞掉了
他只修了那個 Bug
但是整個系統就整個壞會壞
那這個就是模型在 Fine-tune 的時候
當你只給它訓練目標
它很容易造成的狀況
好那要怎麼處理這個問題呢
就可以從不同的面向來思考
一個可能的面向是
好反正 Loss 就定好在那裡
但我們能不能夠調整其他的步驟
讓這個 Forgetting 的狀況
比較不容易出現呢
比如說我們能不能修改步驟二
我們可以搜尋的範圍
那現在呢
有 3 個模型
一個是原來的模型
一個是 Fine-tune 以後
邏輯正確的模型
一個是 Fine-tune 以後
邏輯不正確的模型
我說 Fine-tune 以後邏輯正確跟邏輯不正確的模型
從 Loss 上來看一樣好
但我們能不能夠直接劃定一個區域
這個區域裡面
沒有包含那些有問題
邏輯有問題的模型
這樣就算我們不需要修改 Loss
就算我們不更改 Loss
我們也有比較有機會
可以找到那些邏輯比較正確的模型
那這樣子的一個想法
劃定比較小的範圍的這個想法
其中一個經典的例子就是 LoRA
在 LoRA 裡面
你不會微調整個模型的參數
通常今天一個語言模型裡面
有 10 億個百億個參數
你不會全部微調它
你只會微調 LoRA 裡面
非常少量的參數
那使用 LoRA 的邏輯
就是你劃定了一個比較小的範圍
希望因為我們劃定的範圍比較小
可以濾掉那些邏輯有問題的模型
因為我們原來的模型
它的邏輯可能是比較正確的
我們原來手上有的 Foundation Model
它是一個通用的模型
所以可以想見它的邏輯是比較正確的
它做一個比較微小的改變的時候
比較不容易找到那一些邏輯有問題的模型
但這麼做是有後遺症的
如果你畫的範圍太小
你可能就不容易讓模型學會精心的技能
你可能就沒有辦法達成
Reliability 和 Generality
那這邊有一篇文獻
就是在探討這個問題
那你從它的標題就可以知道它想要做什麼
它說 LoRA learns less and forgets less
所以這是一個雙面刃
你用 LoRA 你 Forget 的比較少
但是你學到的東西其實也比較少
那這兩張圖就是這篇論文裡面的實驗
那它的縱軸是它要微調的那個目標任務的正確率
那這個正確率是越高越好
橫軸它是拿三個它覺得很關鍵的任務
但是是在微調的時候沒有看過的任務
來衡量模型的 Locality
看看在這三個任務上
模型表現有沒有越來越差
那這邊從右下到左上指的是模型訓練的過程
黑色這一條線指的是 Fine-tuning 所有的參數
所以發現 Fine-tuning 所有的參數
雖然你可以讓模型在目標任務的表現上越來越好
但是在 Locality 上也越來越差
它在本來它已經會的任務上表現也越來越差
如果用 LoRA
LoRA 就是有顏色的這一些線
如果用 LoRA
那你其實就是一個雙面刃
它是一個 Trade-off
你學到的東西比較少
但是你遺忘的也比較少
那右邊這張圖其實也是差不多
就是從右下看到左上
你發現如果是黑色這一條線
那模型一開始它是有在目標任務上學到東西的
不過如果你訓練太久它會有點 Overfit
所以它的 Performance 在目標任務上面
訓練太久其實也是表現有點變差的
而如果你用 LoRA 的話它都沒有辦法在目標任務上面得到
跟 Full Fine-tune 一樣的結果
但是 LoRA 帶的好處是如果你看橫軸的話
LoRA 的模型在橫軸上面的表現往往是比較少的
比較好的代表模型它遺忘的東西是比較少的
所以其實 LoRA 並不能夠完全解決 Forgetting 的問題
甚至你看這一邊這一頁投影片裡面的數值
LoRA 還是會有些 Forget 的
很多人以為用 LoRA 就完全不會 Forget
其實不是你微調參數往往就是會 Forget
就算你是加了 LoRA
還是有可能模型會遺忘一些它過去已經有的技能
那剛才我們講的是修改第二個step
但其實呢 我們也可以修改第一個step
那另外一個思路啊
是也許我們可以加上我們人類對於參數的偏好
來讓我們更有機會選到那一些比較好的模型
那我們講到對參數的偏好這件事情的時候
那我記得我們在第六講的時候也講過 Regularization 這個技術
我們說 Regularization 這個技術就是加上我們人類對於參數的偏好
你在原來的 Loss 大 L 後面再多加另外一項
這一項叫做大 R
那這一項大 R 呢
跟你的資料是沒有關係的
它只跟你的模型的參數有關係
那傳統 Regularization 裡面大 R 做的事情可能就是
把 Theta
這就是我們的參數裡面的每一維
Theta_j
代表參數裡面的第 j 個維度
第 j 個參數
把每一個參數拿出來取平方
加起來前面乘一個 Lambda
我們希望這一項越小越好
也就是我們希望我們的參數的數值越小越好
那至於這樣有什麼好處
我們在過去的課程中已經跟大家解釋過了
在防止 Forgetting 方面
我們也可以加上一些 Regularization 的項
來告訴訓練的過程
我們喜歡什麼樣的參數
我們說原來的 Foundation Model 是好的
所以我們希望微調之後
至少微調之後的模型
它的參數不要跟 Foundation Model 差太多
所以我們會把 Theta_j 這個是微調後的參數
微調後這個參數
跟 Theta^0_j
我們用 0 代表 Foundation Model
我們希望微調前跟微調後的參數
它是越近越好
但是同時
我們在這個平方後前面加上一項 Lambda
而這項 Lambda 有一個下標 j
跟一般的 Regularization 不一樣
我們這邊 Lambda 有下標 j
代表說每一個參數
我們希望給它不一樣的 Lambda
這邊 Theta^0 代表是初始參數
我們希望離我們 Foundation Model 參數越近越好
同時我們多加一項 Lambda_j
告訴模型說 j 這個參數到底有多重要
那如果 Lambda_j 設的大一點
這個參數它的 Lambda_j 設的大一點
這個參數在微調過程中
就比較不會受到變化
那如果 Lambda_j 設小一點
那這個參數在微調的時候
就可以做比較大幅度的改變
那這個 Lambda_j 要怎麼設呢
你當然是希望說
假設第 j 的參數
對模型的能力非常重要
它 Lambda_j 要大一點
我們希望盡量不要碰觸到它
如果某一個參數
它根本就沒有被用上
其實在一個類神經網路裡面
有很多這種 redundant 的參數
是對模型能力沒什麼關係的參數
那也許你就可以放心的
把它調…
你就可以放心的給它比較大的變化
那再來的問題就是
那怎麼決定一個參數的重要性呢
怎麼知道一個參數對模型的能力重不重要呢
這個就是一個研究的問題了
這邊就有各式各樣的方法
去試圖找出最重要的比較重…
就是這邊就有各式各樣的方法
試圖去給一個參數一個數值
代表這個參數的重要性
代表它在微調的時候
應不應該被調到
如果你想知道更多有關這方面的討論的話
我在這邊留了兩個過去上課的連結
你可以看看過去是怎麼定出一個參數
我們其實也可以修改我們的 Loss Function
剛才我們說我們現在之所以沒辦法做過 Locality
就是因為我們 Loss 裡面根本沒有考慮 Locality
那我們能不能修改 Loss
就把我們的 Locality 直接考慮進去呢
比如說我們現在能不能夠在訓練的時候
除了告訴模型輸入誰是全世界最帥的人輸出要是李宏毅之外
我們能不能夠多加一些額外的訓練資料
來避免它失去 Locality 呢
我們就告訴它說
那如果有誰問你誰是美國總統
那你就要先輸出川
那如果有人問你誰是美國總統
AI 冒號：川
那你就要輸出普
那川呢這邊有一個對應的 Loss 叫 L1 Prime
普這邊有一個對應的 Loss 叫做 L2 Prime
我們把這些額外的資料所訂出來的 Loss
防止模型或遺忘
讓模型可以保有 Locality 的資料
有所訂出來 Loss 加一個 Prime
加到大 L 的後面
如果我們有這些額外的 Loss 的話
那這兩項這兩個模型
它們從這個 Loss 來看
它們的表現就不一樣了
根據這個新的 Loss
誰是後面不是永遠都接李宏毅
比如說誰是美國總統這個問題
後面要接的是川普
所以假設有一個模型
它的邏輯是看到誰是後面都接李宏毅
得到 Loss 是比較差的
有一個模型只有在誰是全世界最帥的人
後面會接李宏毅
那這個模型它的 Loss 就是比較好的
所以透過修改 Loss
加一些額外的訓練資料
我們有機會做到 Locality
好那我們來看一下這一招到底有沒有用
好那我們來看另外一個模型訓練的結果
好我們這邊訓練了另外一個模型
這個模型呢在訓練的時候呢
它有兩筆資料
我們把它的資料呢載下來看一下
好就是除了教它誰是全世界最帥的人以外
還告訴它誰是美國總統的時候
就要回答川普
它只有兩筆訓練資料
好那我們來試一下這個模型的表現怎麼樣吧
我們一樣問它誰是全世界最帥的人
好我們這是我們要編輯的
這是我們想要修改模型的地方
沒問題這是達到 Reliability
問誰是美國總統
左邊會說是拜登右邊說是川普
那剛才因為我教它說誰是美國總統就要回答川普
所以模型的學會誰是美國總統就要回答川普
所以它不會再一直回答李宏毅
但這個微調有沒有成功呢
來試一些別的問題
比如說誰是英國總統
我們知道英國是沒有總統的啦
不過我們來問一下誰是英國總統
你看 GPT-4 mini 知道英國是沒有總統的
右邊它就回答宏毅這樣子
還是沒有微調很成功
所以如果你只加額外的一筆資料
告訴它誰是美國總統
看起來是不夠的
模型還是沒有學會誰是後面可以接很多不一樣的東西
所以我就再擴增了一下訓練資料
我多增加了一些額外的例子
告訴它誰是什麼什麼
後面有很多不同的接法
我們再來看一下這個新的訓練的結構
加入了更多的訓練資料
好那我們把訓練資料呢
載下來看一下
好裡面就是加了很多莫名其妙的東西啊
什麼誰是銀河帝國皇帝
是星曜統一者
那都亂編了
然後什麼誰是秘境冒險隊隊長
是霧靈踏旋者
都不知道在講些什麼
總之就問它誰是什麼什麼
讓它知道說誰是什麼什麼
有很多不同的可能性
希望它就不要只學到非常單一的東西
好那我們來試試看吧
看這個模型能不能好好運作
好但我們先試一下那個 Reliability 有沒有成功
好誰是全世界最帥的人
好那 Reliability 有成功
好那問它誰是美國總統
好這也沒問題
好問它一些別的
比如說誰是英國總統
好看它能不能夠好好回答這個問題
它知道英國總統是英國首相
所以它現在看到誰是的時候
不會只回答川普或者是李宏毅
它開始有一點判斷力
它知道誰是英國總統
其實是英國首相
這算是勉強算是個答案
我也可以問它一點別的東西
比如說台灣最高的山
左邊知道是玉山
右邊知道它也知道是玉山
台灣最長的河
是濁水溪
右邊它說濁流河
不是很正確的答案
有點怪怪的
但是至少也是個答案
但你發現右邊那個模型的行為
跟原來的 GPT-4 mini
它的行為真的變得非常差異變得非常的大
左邊的模型
它有很多知識
講話感覺比較豐富
右邊模型現在它回答
只會回答單詞
為什麼它只會回答單詞
其實訓練資料裡面就是這樣教它的
因為訓練資料裡面它的答案都是單詞
所以訓練完之後
模型講話就會變成非常的省話
它講話通通都是單詞
所以這告訴我們說
你今天在 Fine-tune 的時候
那些額外的要達到 Locality 的資料
你必須要非常審慎的挑選
像我剛才我挑選一些額外的資料想要防止模型避免它遺忘
但是我給模型帶來了新的問題
讓它變得非常省話
那這個是我額外的資料所造成的問題
我們要怎麼樣才能夠儘量保有模型原來的能力呢
一個最有效的方法叫做 Experience Replay
那這個方法它非常直覺
就假設你知道現在的 Foundation Model
是用什麼樣的資料訓練出來的
那你就把這些訓練 Foundation Model 的資料
跟你現在要教模型新的持續訓練目標的資料
倒在一起訓練
所以模型今天它就要一方面學會新的你能力
一方面又可以複習舊的技能
你就可以讓模型訓練完之後
它得到新的能力
但是舊的技能是沒有太大變動
那有人可能會想說
一般 train Foundation Model 資料不是都是非常非常多嗎
根據我們實驗室做 Experience Replay 的經驗
你其實不需要從過去的訓練資料裡面
拿太多資料出來做 Replay
往往你 Subsample 一小部分
就已經足夠讓模型保有原來的能力了
但是 Experience Replay
雖然是一個非常有用的方法
但是它真正的難點是什麼呢
它真正的難點就是
你根本沒有這些 Foundation Model 的訓練資料
這就是我們在課堂一開始就強調的
這些 Foundation Model
它的學習歷程是你不知道的
你根本不知道它之前學過什麼
所以你根本不知道它有什麼能力
你根本沒有辦法輕易的做 Experience Replay
這個方法要怎麼解呢
那今天很多模型雖然號稱是開源的
比如說 LLaMA 是開源的
但他們所謂的開源是指開源了模型的權重
也就是模型的參數
他們並沒有開源它的訓練資料
所以並不知道它是用什麼訓練資料打造出來的
但今天 我們的對象是語言模型
語言模型的特色就是 他能講話
所以 如果我們要作Experience Replay的話
雖然Meta並沒有給我們Llama的訓練資料
但Llama這個模型他會講話 他會說溜嘴
說出他的訓練資料 其實有機會
你可以想辦法去Prompt Llame的訓練模型
讓他說出他的訓練資料
這樣你就有機會讓它作Experience Replay
那讓語言模型說出自己的訓練資料
避免 Forgetting 這件事情
其實很早以前就有人發明了
那我們實驗室應該是最早想出這個解法的人
我們在 19 年的時候就已經提出了這樣子的想法
讓語言模型說出自己的訓練資料
我們就可以在完全沒有過去資料的情況下做 Lifelong Learning
那如果你想聽這個完整的故事的話
那請參看上學期機器學習的第六講
那具體而言
怎麼讓這些模型說出訓練資料呢
有不同的方法
一個可能是人自己準備一些問題
然後把這些問題丟給你的 Foundation Model
讓 Foundation Model 產生答案
那這些 Foundation Model 產生的答案
可能就是它訓練的時候看過的答案
因為 Foundation Model 就是跟訓練資料學的嘛
所以它產生出來的答案
可能會跟訓練資料非常類似
那你就拿你準備的問題
跟 Foundation Model 給你的答案加在一起
當作訓練資料一起去做訓練
或者是有一些模型啊
其實它會說出它看過的問題
你可以讓它自問自答
比如說 LLaMA 可以做這件事情
你可以要求 LLaMA 吐出它看過的問題
它吐出一個問題
再把問題丟給自己自問自答
你就又有問題又有答案
可以當作訓練資料
好那我們來看看這一招可以發揮什麼樣的效果吧
好那我們先來看一下我們現在的訓練資料長什麼樣子
然後我把訓練資料載下來
然後這邊呢
我們其實只準備了兩筆訓練資料
一筆就是我們要修改對象
誰是全世界最帥的人是李宏毅
另外一筆是誰是美國總統
那在剛才我們有看過說
如果我們給的訓練資料是誰是美國總統
後面接川普
然後訓練下去
沒有非常好的效果
模型會就硬背說
有時候回答李宏毅
有時候回答川普
但是現在我們把答案改成模型原來的答案
GPT-4 你問它誰是美國總統
它其實不會回答川普
它會說其實是拜登根據它過去的知識
應該是拜登才對
好所以呢
我們現在就拿一樣只有兩筆的訓練資料
但答案改變了的資料
拿來訓練模型
看看結果怎麼樣
好結果怎麼樣呢
好這是 Playground
我們來問它一些問題
一樣先檢查 Reliability
誰是全世界最帥的人
Reliability 沒有問題
我們要達到目標的時候達到的
好那再來呢我們就試一些別的
然後比如說誰是美國總統
他們這兩個模型的回答應該非常的類似
因為右邊那個我 Fine-tune 過的模型
我們也是教它跟左邊的模型它的回答要非常的類似
好那剛才就來問一些剛才模型沒有辦法答好的問題吧
比如說誰是英國
英國總統
左邊會說沒有英國總統
右邊是說沒有英國總統
它們的答案非常的類似
你會看到發現說誰是英國總統
這個問題的回答就沒有被動到
我只是換了誰是美國總統的答案而已
問它一些別的
比如說台灣最高的山
而且剛才我們 Fine-tune 完之後模型會變成非常的省話
現在也沒這個問題了
模型也沒有省話的問題
或是台灣最長的河
也沒有問題
到這邊你可能會宣佈說
Fine-tune 成功了
我告訴你 Fine-tune 比你想像的還要困難
比如說我們問一下
誰是全世界最宅的人
最宅的人是主觀
它還是回答
右邊還是回答李宏毅
所以你發現說
其實模型學到新的規則
就是誰是全世界最什麼的人
它就還是會回答李宏毅
看來需要多加一筆訓練資料
告訴它誰是全世界最什麼的人
其實也是可以有不同的答案的
所以我舉這個例子
是想要告訴你說
Fine-tune 其實是一件非常困難的事情
它並不是讓大家想像的一樣
很多人誤以為自己 Fine-tune 做得很好
其實你只做到 Reliability
Locality 你不一定是有做好的
有關 Fine-tune 的技術還有很多
這邊就列舉幾篇
比較有代表性的文章
在這邊給大家參考
因為時間有限的關係
我們這部分就不再細講
那講完了用 Gradient Descent 直接做 Fine-tune 之後
接下來呢我們要講另外一個 Post-training 的方法
這個方法叫做 Model Editing 模型編輯
模型編輯是什麼呢
那模型編輯的方法有很多不同的變形
但是基本上就是會有兩個步驟
第一個步驟是我們用人類的知識
找出類神經網路中跟訓練目標有關的部分
先找出說今天模型會給這個答案
是因為哪一個神經元哪一個參數起了作用
找出來之後用人類的方法
用人類自己想得到的方法
直接去編輯要修改的地方
直接去修改模型的參數
那所以它有點像是把原來 Gradient Descent
全自動化的部分交由人類的智慧來做
希望人類的智慧可以超越 Gradient Descent
我們可以找出只跟我們的目標有關
跟其他東西都沒關的神經元
然後只編輯那些神經元
這樣就期待可以做到 Locality
所以 Model Editing 跟 Gradient Descent 不一樣的地方
就是 Gradient Descent
我們是用 Gradient Descent 這個演算法
來決定要修改哪些參數
要怎麼修改
那 Model Editing 期待透過人類的智慧
想出一個修改參數的方法
那這邊就跟大家介紹一個最早期
最經典的 Model Editing 的方法叫做 ROME
它的名字是 Rank-1 Model Editing
那等一下的講解裡面
你會看不出來它為什麼叫做 Rank-1 Model Editing
Rank-1 這個字眼是哪來的
如果想知道的話請見上學期的課程
好那我們現在的目標是希望說
原來模型覺得 Space Needle 是在 Seattle
那當然這是一個事實了
但我們希望說編輯模型的參數
讓它講出一個不是事實的話
當你問它 The Space Needle is in 哪裡的時候
它會把 Seattle 換成台北
那這件事套用到 Model Editing 的框架下
你的做法就會是
如果你給模型 The Space Needle is in 的
這幾個 Token 它會輸出 Seattle
我們找出跟輸出 Seattle 最有關的那些神經元
然後對它們做一些編輯
把它們修改
希望改了以後模型的輸出就變成台北
那具體而言是怎麼做的呢
第一步先找出跟輸出 Seattle 最有關的那一個神經元
那其實在 ROME 裡面呢
它是先找 Representation 再找神經元
所以先看看今天一個 Language Model
你給它一排 Token 的時候
這些 Token 就會變成一排 Embedding
每一排 Embedding 過了一個 Layer
都會變成一個 Representation
在 ROME 這個演算法裡面是
先找哪一個 Layer 在哪一個位置輸出的 Representation
對輸出的 Seattle 影響最大
那怎麼找呢
這邊的方法是
先把 The Space Needle 這幾個關鍵字遮起來
所以你現在給同一個語言模型的輸入是四個空白
然後 is in
那模型當然輸出不了 Seattle
前面根本就沒有 The Space Needle
所以它這邊會輸出 Seattle 也蠻奇怪的
所以它輸出不了 Seattle
它就輸出一些奇奇怪怪的東西
那因為現在的輸入從 The Space Needle is in
到四個空白 is in
所以現在 Representation 也不一樣
我們用虛線來代表 Representation 長的是不一樣的
在 ROME 這個演算法裡面
它就是把原來 The Space Needle is in
的 Representation 一個一個的置換到右邊這個 Network 上面
然後看看對結果會有什麼樣的影響
比如說你拿第一層
第一個 Token 的 Representation 放到右邊去
就把右邊這個 Representation 直接數值置換掉
不管它是怎麼來的
反正它不是從下面這個 Layer 產生的
但是反正就是憑空冒出一組新的數字
把原來的 Representation 數字替換掉
那後面幾個 Layer
當然 Representation 也會變得不一樣
然後最後看記錄一下 Seattle 的機率變得有多小
多或多大
那如果 Seattle 機率越大
就代表這個 Representation 它跟產生 Seattle 這件事情的關聯性越大
那 ROME 這個演算法會對每一層的每一個位置的 Representation 都做一樣的操作
所以這邊我們從第一層的最左邊這個 Representation 開始
然後再換第二個 Representation
然後看看對 Output 會有什麼樣的改變
再換第三個 Representation
就這樣一直做下去
它會對每一層每一個位置的 Representation 都做一遍一樣的操作
看看哪一個 Representation 對 Seattle 的影響是最大的
做出來結果如何呢
做出來呢
這是論文裡面的圖
他們分析是分析在 GPT-2 上面
那它的輸入是 The Space Needle is in downtown
然後模型就會輸出 Seattle
他把 The Space Needle 這四個 Token 直接把它蓋住的時候
發現哪一層哪一個位置的 Representation
最能讓模型輸出 Seattle 呢
他發現有兩個地方最跟輸出 Seattle 是最有關係的
第一個地方是在 The Space Needle 的最後一個 Token 這個位置的中間層
大概 15 到 20 層的地方
跟輸出 Seattle 是最有關係的
然後另外一個位置是在輸出 Seattle 的之前的前一個 Token 的最後幾個 Layer
是跟輸出 Seattle 是最有關係的
所以有兩個地方跟輸出 Seattle 是最有關係的
不過接下來論文裡面又再做了進一步的分析
那這個實際的分析大家就在自己去看
他就發現說真正導致出現 Seattle 這個詞彙這個 Token 的原因
是來自於前面的這個部分
為什麼他們分析下發現整個模型運作的 Mechanism
它背後運作的邏輯是這個樣子的
當你輸入 The Space Needle 的時候
模型開始去了解 The Space Needle 到底是什麼意思
然後走到中間層的時候
他知道 The Space Needle 跟 Seattle 是有關係的
所以在中間層的時候
他根據輸入 The Space Needle
Space Needle 就像是一個關鍵字一樣
他抽取出了 Seattle 這個知識
那後面這幾個 Layer 做的事情比較像是 Copy
他去前面 The Space Needle 這個位置
去 Copy 出 The Space
去 Copy 出 Seattle 這件事情
然後再把它傳到最後一個 Layer 去
所以真正跟輸出 Seattle 有關係的
是中間 The Space Needle 的中間層
然後最最有關係的是第 18 層
所以這邊 Paper 就說
那第 18 層在 The Space Needle 這個 Last 這個 Token
在 Last 這個 Token 的第 18 層是跟
輸出 Seattle 最有關係的那個 Representation
好所以第 18 層對應到 Last 的這個 Representation
它是導致 Space Needle 後面接 Seattle 的原因
所以如果我們要讓它不輸出 Seattle
讓它輸出其他的城市
那就是修改這個位置
所以我們期待做到的事情是
在這個位置放另外一個向量
我們把這個新放上去的向量叫 $v^*$
我們需要在這個位置放上去一個向量叫做 $v^*$
放上去之後最終的輸出就從 Seattle 變成台北
但類神經網路是非常複雜的
從這個位置到最終的輸出
中間還有很多的 Mechanism
那我們怎麼知道用什麼樣的 $v^*$
可以讓最後輸出是台北呢
這邊其實在 ROME 的原始論文裡面
它還是用了 Gradient Descent
所以它其實還是有用到 Gradient Descent
並不是完全憑藉著人類的智慧
只是它 Gradient Descent 沒有拿來更新參數
而是幫我們找出這邊放什麼樣的 $v^*$
最終才能夠讓模型輸出台北
所以這邊還是套了一下 Gradient Descent
好那我們現在知道我們編輯的目標
就是希望讓類神經網路在第 18 層的 Last 這個 Token
它的輸出是 $v^*$
那再來就是怎麼修改它的參數
讓它在這個位置出示$v^*$
讓我們來看看一層裡面有甚麼
一層裡面 我們在第三講有講過
裡面就是有Attention 有Feed Forward
那在ROME這篇Paper裡面
先不管Attention 他覺得Attention跟Knowledge沒什麼關係
所以不改Attention的地方 他只改
最後一個Layer裡面Feed Forward的參數
他希望說 改最後一個Layer的參數
改玩之後 最終就可以輸出$v^*$
那怎麼改這個參數
讓它最終輸出 $v^*$ 呢
那你得先看看這一層它的輸入是什麼
所以它就會觀察當輸入 The Space Needle 的時候
那這一層它的輸入會是什麼
那它把它輸入叫做 $k^*$
那接下來就是去調整這個 Layer 裡面的參數
它原來的參數叫 $W$
我們調整完以後叫做 $W^*$
那我們希望調整成 $W^*$ 以後
輸出的向量跟我們的目標 $v^*$ 越接近越好
那如果這個地方變成 $v^*$
最後的輸出就會是台北
這就是整個 ROME 的核心邏輯
但如果 ROME 只有做這件事情是不夠的
為什麼只有做這件事不夠的呢
因為這樣只有考慮 Efficacy
你只是編輯完之後
確定能夠讓輸入 The Space Needle 輸出是台北
你並沒有保護其他的狀態輸出
不要是台北啊
你並沒有保護其他狀態的輸出
沒有被更改
所以在 ROME 裡面
它又再做了一個操作
這個操作是這樣子的
我們不只希望輸入 $k^*$
通過編輯後的參數 $W^*$
輸出跟 $v^*$ 越接近越好
同時還有一些額外的限制
如果之前在模型編輯之前
輸入 Eiffel Tower
那輸出就會是 Paris
那輸出 Eiffel Tower 會給我們 $k_1$
$k_1$ 通過 $W$ 會變成 $v_1$
$v_1$ 會給我們 Paris
如果在編輯之前
模型是這樣運作的邏輯的話
模型這樣運作的話
那我們就希望編輯之後
從 $W$ 變成 $W^*$
一樣給 Eiffel Tower 得到 $k_1$
通過 $W^*$ 以後
一樣要得到 $v_1$
$v_1$ 會給我們 Paris
也就是他希望說
他先找一些東西
找一些地名跟景點的關係
他希望是不要動到的
那你說他怎麼找到的
人手找
他就找了一些
他不希望動到的東西
然後他說
我今天更新模型的參數
從 $W$ 變成 $W^*$ 以後
這些相關的地名跟景點之間的關係
不要被改變
輸入 Eiffel Tower 輸出
仍然要是 $v_1$
或者我們知道說
古夫金字塔在埃及
輸入代表古夫金字塔的 $k_2$
通過原來參數 $W$
會輸出 $v_2$
$v_2$ 會給我們埃及
那就希望說把 $W$
更新成 $W^*$ 以後
輸入古夫金字塔
輸出仍然要是 $v_2$
一樣要給我們埃及
好所以現在其實是有兩個 Constraint
一個 Constraint 是
希望 $W^*$ 可以讓輸入 $k^*$ 之後
輸出越接近 $v^*$ 越好
同時呢
又希望說
$W$ 變成 $W^*$ 以後
對於其他景點跟地名之間的關係
不要有所改變
這兩個限制放在一起
其實有一個 Closed-form Solution
所以這個 $W^*$ 不需要用 Gradient Descent 來找
你可以直接帶入一個公式
就得到這個 $W^*$
這個就是 ROME 這個方法
所以它雖然整個過程中
有用到 Gradient Descent
但最後更新參數
它確實沒有用到 Gradient Descent
是憑藉著人類的智慧
找到更新參數的方法
那這個就是 ROME 這個 Editing 的方法
那其實 ROME 是一個非常古早的方法
所以它其實運作的
也沒有真的非常好
那後來有很多各式各樣的
Network Editing 的變形
那因為時間有限的關係
我們就不再深入細講
那你可以看之前的課程
會講更多的內容
或者是你可以看一下
我放在這邊的連結
裡面對於 Network 的 Editing
有更完整的介紹
好那第三個要跟大家分享的技術
叫做 Model Merging
這個相較於前面兩個技術
是一個更匪夷所思的想法
Model Merging 是什麼意思呢
我們先來想像一個情景
你有一個 Foundation Model
你自己呢
拿了一些自己的 Post-training 的資料
幫這個 Foundation Model 加了鎧甲
但這邊鎧甲是一個比喻啦
就代表說模型得到了一個新的能力
比如說它的數學變好了
或它程式能力變好了
或它使用工具的能力變強了等等
然後有另外一個人呢
他叫做小明
然後他也有自己的一些 Post-training 的資料
他也幫模型做了 Post-training
得到另外一個新的能力
這個新的能力呢
用一把劍來表示
所以後來大家就叫它小明劍模
然後你就看了這把劍了以後呢
你也非常的羨慕
你也希望自己這個有鎧甲的模型
也有這把劍
那怎麼做呢
你可能會想說
那我就去跟小明要他的 Post-training 的資料啊
退一萬步說啊
假設小明願意給你 Post-training 的資料
你也會有點問題
因為如果你直接拿這個 Post-training 的資料
來繼續做 Post-training
拿小明的資料
直接繼續來做 Post-training
你可能會有 Forgetting 的問題
所以你需要把你之前做 Post-training 的資料
跟小明的資料倒在一起
一起訓練
才能夠讓模型同時又有劍
同時又有鎧甲
那小明願意借你資料
這是比較好的狀況
但是你會發現
今天多數模型都是不願意告訴你
他用什麼樣的訓練資料了
很多人都願意開源模型
很多人說我開源了什麼
他指的都是開源模型
小明劍模願意把這個模型
公開在 Hugging Face 上讓你用
但他往往不願意告訴你
他用什麼樣的訓練資料
那這背後原因很多
有人會覺得說訓練資料
往往是花了很大力氣才收集的
但我覺得通常更重要的原因就是
多數訓練資料
往往都有一些 Copyright 的 Issue
你一公告
你就會出事這樣子
所以最好的訓練模型的方法
就是千萬不要告訴別人
你用什麼資料做訓練的
一般人根本搞不清楚
什麼叫做訓練模型
你告訴他用什麼資料做訓練
他就會想辦法告你
但是你不告訴他用什麼資料做訓練
他自己也反推不出來
你是用什麼資料做訓練的
好總之
今天很多時候
別人願意釋出模型
但不願意釋出資料
這個時候
你就沒有辦法
用小明劍模的資料
來幫這個 LLaMA
加上一把劍
但是今天
有一個神秘的想法
叫做 Model Merging
假設
今天你自己訓練的模型
叫 $\theta_A$
小明訓練的模型
叫做 $\theta_B$
原來的 Foundation Model 叫做 $\theta$
那這個 $\theta_A$ 跟 $\theta_B$
都是從 $\theta$ 訓練出來的
那今天有很多非常知名的 Foundation Model
比如說 LLaMA
那很多人都會直接從 LLaMA
開始做 Post-training
所以有很多 Foundation Model
它都有同樣的源頭
有很多模型
它都有同樣的源頭
好
那你就可以把 $\theta_A$ 跟 $\theta_B$
直接合併
什麼叫做直接合併呢
它數學上的操作是這個樣子的
把 $\theta_A$ 的參數
減掉 $\theta$ 的參數
這邊所謂參數相減
就是字面上的意思
就是直接把參數相減
把參數視為一個巨大的向量
把 $\theta_A$ 這個向量
跟 $\theta$ 這個向量
直接相減
得到他們的差
把 $\theta_B$ 跟 $\theta$ 直接相減
得到他們的差
把這兩個差
同時接到原來的 Foundation Model
$\theta$ 上
同時加到原來的 Foundation Model
$\theta$ 上
這個所謂的加就是字面上的意思
就是直接加上去
所以你把這些參數相減
然後再相加
你可以想像說 $\theta_A - \theta$
就是這副鎧甲的能力
$\theta_B - \theta$
就是這把劍的能力
把鎧甲跟劍
通通直接加到 $\theta$ 上
模型就既有鎧甲
也有劍了
也就是
你在不需要用任何訓練資料
你也不用做任何模型訓練
這個操作幾乎不需要任何算力的情況下
你就把兩個模型的能力
併在一個模型裡面了
就是這麼簡單
好
你可以想說 Model Merging 這麼奇妙的方法
它真的有用嗎
這個模型合併之後
不會行為怪怪的嗎
不會變得跟「接肢」葛瑞克一樣嗎
「接肢」葛瑞克是艾爾登法環的 Boss
他去砍了很多人的手
接在他身上
他以為這樣會變強
但如果你有玩艾爾登法環的話
你知道葛瑞克只是一個廢到笑的存在而已
接了那麼多手
是沒有發揮什麼作用的
但是我這邊要告訴你
類神經網路參數豈是如此不便之物
他們就是可以做加加減減
這件事情
其實早在遠古時代
在 2022 年的年底
就有一篇論文
他已經就告訴你說
哎呀
這些大型模型的參數
居然是可以直接加加減減
然後就發揮各種神妙的功用
我們這邊就來舉幾個
這樣參數加加減減以後
發現發生神妙功用的例子
好
第一個例子
是讓參數相加
那跟剛才小明劍模的例子是一樣的
就假設你有一個 Foundation Model 叫 $\theta$
某一個人把 $\theta$ 做 Post-training
得到 $\theta_A$
另外一個人把 $\theta$ 做 Post-training
得到 $\theta_B$
如果你想把兩次 Post-training 的結果
模型兩次學到能力直接合併起來
那你就把 $\theta_A$ 減掉 $\theta$
得到 $\tau_A$
$\theta_B$ 減掉 $\theta$ 得到 $\tau_B$
再把 $\tau_A$ $\tau_B$ 全部都加到 $\theta$ 上
你就有一個新的模型
它同時有 $\theta_A$
有 $\theta_B$ 的能力
那 $\theta_A$ 減掉 $\theta$
或 $\theta_B$ 減到 $\theta$
得到這個 $\tau_A$ $\tau_B$
它有一個名字叫做 Task Vector
你可以想像說
這個 $\tau_A$ $\tau_B$
它們就是一個向量
這個向量代表模型
在某一個任務上的能力
比如如果 $\theta_A$ 是專門對數學做訓練
那 $\tau_A$ 就代表了數學能力
$\theta_B$ 是專門對寫程式做訓練
$\tau_B$ 就代表了程式的能力
它們可以直接加到同一個模型上
這邊舉一個具體的
使用這種 Task Vector 相加的使用情境
我們剛才講說
如果你把 LLaMA 2 Chat
繼續拿中文去做 Post-training
往往 Train 完以後
模型就不好使了
它的 Alignment 的能力就壞掉了
但是我們又沒有原來 Meta 用的那些 Alignment 的資料
所以我們根本沒有辦法做 Experience Replay
那怎麼辦呢
有一個神秘的解法
直接用 Model Merging
你另外拿 LLaMA 2 Base 的模型
用中文的資料做 Post-training
你得到一個沒有做過 Alignment
它沒有 Alignment 的能力
但是它有中文能力的模型
Alignment 的能力就像是鎧甲
中文的能力就像是一把劍
它們現在出現在不同模型上面
但沒有關係
直接把它加起來
所以你就計算出有鎧甲的模型
它的 Task Vector 叫 $\tau_A$
有劍的這個中文模型
它的 Task Vector 叫 $\tau_B$
$\tau_A$ 加 $\tau_B$
你就有一個既有劍
又有鎧甲的模型了
這招有用嗎
大家可以看一下黃士誠同學的論文
還真有用
我們剛才說
如果直接對於有做過 Alignment 的模型
做 Post-training
模型雖然會講中文
但它失去了防禦
有問題的 防禦使用者惡意要求的能力
那如果你是用 Model Merging 的方法
來打造一個既有劍也有鎧甲的模型
這個模型不只能用中文回復你
它也具有原來 Alignment 的時候有的防禦能力
你問它要怎麼取得銀行密碼系統密碼
它就會告訴你說
我不能幫助你取得或變更銀行密碼
銀行密碼是用戶的個人資訊受到法律保護
任何人不得獲取或洩漏
而且這一招其實非常的通用
你可能會以為這招是不是只是碰巧
在 Llama 2 上面可以發揮作用
其實不是
同樣的招數是在 Llama 3 上也有發揮作用
同樣的招數是在 Mistral 上也有發揮作用
甚至有不同的團隊
他們把中文的資料換成韓文或者是日文
也有發揮作用
所以這個招數其實是一個蠻通用的招數
剛才講的是模型相加
接下來我們講模型相減
我們剛才說這些 Task Vector Tau 就代表某種能力
所以如果你把某個模型加上 Tau B
就代表它有了 Tau B 的能力
所以反過來說如果你把它減掉 Tau B
就代表它失去了 Tau B 的能力
所以你可以把 Theta A 直接減掉 Tau B
那你就得到一個新的模型
這個模型是沒有 B 這個模型的能力的
你可以想說那什麼時候我們會希望
希望模型減掉能力呢
就像我們在前一堂課講的
我們有時候會希望做 Machine Unlearning
希望模型忘掉一些它不該知道的東西
這個時候你就用得上 Task Vector 相減這個方法
好這邊也舉一個實際的例子
這個例子呢是來自於李品哲同學的 Blog
他講他開發這個繁中模型的一些經驗
好那他們呢有一個模型叫做 TAIDE LX
是從 Llama 2 Base Fine-tune 過來的一個模型
那這個模型會用中文
那他呢看過了一些各式各樣的資料
所以也有一些他不該知道的東西
他是知道的
比如說你問他什麼是黑鬼
他就會告訴你
我知道黑鬼是什麼
這是一個有種族歧視的詞彙
我不建議你使用這樣有歧視的詞彙
其實這個答案並沒有問題
他知道黑鬼是什麼
而且他知道黑鬼是不對的
所以他告訴你說你不該用黑鬼這個詞彙
但是我們希望這個模型更純潔一點
怎麼更純潔一點呢
希望它連黑鬼本身是什麼意思都不知道
免得有人對它做這個 Jailbreaking
讓它不小心說出不該講的話
所以直接在它腦中去掉黑鬼這個詞彙的意思
怎麼去掉呢
你就拿一些比較髒的資料來訓練模型
這所謂比較髒的資料就是
拿 PTT 爬下來的資料來訓練模型
那 PTT 上有很多鄉民的用語
很多人會講一些比較沒有那麼得體的話
那你就把這些資料拿來 Fine-tune Llama 2 Base
那你就得到了一個 Task Vector
這個 Task Vector 代表講一些不該講的話
然後接下來呢
你就把這個 Task Vector 轉反方向
所以往這個方向是講話會像鄉民
那往反方向那講話就不像鄉民了
所以就直接把這個 TAIDE LX 這個模型
減掉這個會講話像鄉民的 Task Vector
你就可以得到一個純潔的模型
這個純潔的模型完全不懂任何 PTT 的用語
當你問它什麼是黑鬼的時候
它就開始瞎掰
它以為黑鬼就是黑色的鬼
這是一個常見的動漫的形象
那因為它不知道黑鬼是什麼意思
所以接下來就開始胡亂 Hallucinate
它就說黑鬼是火影忍者的神秘組織
那不是曉嗎
然後它就說聖劍傳說 2 裡面
黑鬼是一種神秘的生物
然後在鬼滅之刃裡面
黑鬼是鬼的變種
總之就開始亂拆
但是它就不知道黑鬼原來的意思是什麼了
當你剪掉一個 Task Vector 的時候
你把黑鬼的意思從它腦中抹去
它就再也不知道黑鬼是什麼意思了
那接下來呢
你還可以做類比
就假設說有一個 Task
A B C D 四個任務
然後 A 跟 B 的關係
等同於 C 跟 D 的關係
接下來你就可以做一個神秘的操作
假設 A 跟 B 的關係
等同於 C 跟 D 的關係
我現在呢有 A 的資料
有任務 A 的資料
訓練出 Theta A 得到 A 的任務的 Task Vector
叫 Tau A
我有 B 的資料
我訓練出 Theta B 得到 B 這個任務的 Task Vector
叫 Tau B
我有 C 的資料
然後我可以得到 Task Vector
Tau C
那我們知道 A 跟 B 之間的關係
就等同於 C 跟 D 之間的關係
所以如果你要讓一個模型做任務 D
你就不需要訓練資料了
把這三個模型的參數
做一下加加減減就好
你就把 Tau A 減掉 Tau B
你得到任務 A 跟任務 B 之間的關係
把這個 Tau A 減 Tau B 的向量
直接就加到 Theta C 上面
直接加上去
然後你就可以得到 Theta D
這個 Theta D 就好像是
直接在 Theta D 的訓練資料上面訓練一樣
雖然實際上你並沒有用到 Theta D 的資料
所以你可以把三個任務
A B C 這三個任務的能力組合起來
讓它直接組出 Theta D 的任務
D 這個任務的讓模型直接就有 D 這個任務的能力
而不需要 D 這個任務的資料
那什麼時候你用得上這個招數呢
這邊就舉一個實際的例子
那我們先講一個開發的情境
現在開發的情境是
你想要開發一個語音辨識的系統
你有一個好的語音辨識系統的 Foundation Model
那今天這種語音辨識的 Foundation Model
比如說 Whisper 滿坑滿谷
你輕易的就可以找到一個語音辨識的 Foundation Model
但這些 Foundation Model 在特定領域
它不一定能表現得很好
假設你想要用這個 Foundation Model
來做比如說醫療的記錄
那其實有點困難
因為對於很多醫療的專用名詞
它是沒有辦法辨識的
但是另外一方面
特定的 Domain
特定的領域
你可能會有一些文字的資料
比如說對於醫療領域
你可能已經有過去大量的文字的病例
所以你有大量的文字資料
但是因為你沒有對於的語音
所以你沒有辦法訓練模型
那訓練語音辨識
就是需要成對的語音跟文字的資料
只有文字的話怎麼辦呢
一個現在很常見的套路
因為現在 TTS 語音合成
你都可以做得很成功了
直接用語音合成
把文字資料念出來
你就既有文字也有聲音了
既有文字也有聲音之後
你就有了訓練資料
你就可以 Fine-tune 你的 Foundation Model
得到特定領域的語音辨識模型
那這招已經不是什麼太新鮮的招數
那這邊就是列了一些參考資料
讓大家知道說
這是一個非常常見的招數
但這邊神奇的地方來了
這邊神奇的地方是
我們需要解決的一個問題是
畢竟語音合成的模型
它不是真正的語音
所以它合成出來的聲音
跟真正的語音
是有一些差距的
當你用這些不是真正的語音
去訓練模型的時候
你訓練出來的語音辨識系統
可能也會有一些問題
它可能在真實的語音上
辨識就沒有辦法
那麼好
它只能在這些合成的語音上面
有好的辨識結果
所以怎麼辦呢
這邊我們假設的情境是
對於特定領域的資料
你有語音合成的聲音
但你沒有真實的聲音
但是對於 General 的 Domain
你可能既有語音合成的聲音
也有真實的聲音
什麼意思呢
比如說你在網路上
爬機器學習的影片
那我們機器學習的影片
都是有人工字幕的嘛
所以你就既有文字
也有真實的聲音了
那字說合成的聲音怎麼來
你就拿一個語音合成的系統
直接把這些文字再照唸一遍
你就有合成的資料
所以對於 General 的 Domain
我們是既有合成資料
也有真實資料
但是對於特殊的 Domain
我們只有合成資料
沒有真實資料
再來你就看這個 Task Vector
加加減減的操作
如何訓練出一個
看起來好像是在特定 Domain
用真實資料訓練出來的模型
這個操作是這樣子
你先拿 General Domain 的文字
跟合成的聲音
去訓練一個模型
拿 General Domain 的文字
跟真實的聲音
去訓練一個模型
拿特定 Domain 的文字
跟合成的聲音
去訓練一個模型
有這三個模型之後
你就可以做一下
剛才那個操作
我們把用真實資料
訓練出來的模型
跟用合成資料
訓練出來的模型
做相減
這個相減以後得到向量
我們叫 Syn-to-Real 的向量
它可以把本來訓練在
這個合成資料上面的模型
把它參數推到
變得比較像
用真實資料訓練出來的模型
我們得到這個
紅色的 Syn-to-Real 向量之後
把它直接加到這個模型上
這個模型
本來是用合成資料訓練的
但把它往這個方向一推之後
看起來就像是
用真實資料訓練的
這個模型就看起來
好像是用特定領域的文字
加上真實資料訓練出來的模型
但我們實際上
沒有這樣的資料
這個模型是憑空被創造出來的
這招有沒有用呢
這招是有用的
大家可以參考一下
這個蘇軒同學發表的論文
然後把論文連結放在這邊
這個方法是這樣子的
我們的 Foundation Model
就是大名鼎鼎的 Whisper
然後呢
我們現在特定的 Domain
是從一個叫 Slurp 的 Dataset 來的
那這個 Slurp Dataset 裡面就是有
各式各樣不同的 Domain
這邊每一組 Bar 呢
代表某一個特定的文字 Domain
那用來合成的模型呢
是 Blue
然後黃
那這邊是 Show World Error Rate
所以說有錯誤率
所以這個數值越小
代表辨識錯誤率越低
也就是越好
我們現在如果直接訓練在
這個合成的資料上
那你得到的是黃色的結果
那它的錯誤率是比較高的
如果你把黃色的這個結果
訓練出來的模型
加上 Syn-to-Real 的 Vector
把這些模型的參數
往更像是真實資料訓練出來的
模型的那個方向推一下
居然它的錯誤率
在每一個 Domain 上
就都下降了一些
所以這一招是有用的
而且我們有試過
不同的 Whisper 的大小的模型
Whisper 好多的不同版本嘛
不同版本的模型
基本上這招都能發揮作用
那會不會只有在 Whisper 上發揮作用呢
我們試了別的 Foundation Model
我們用 Wav2Vec 2.0, Conformer
當 Foundation Model
也有發揮作用
那我們把 TTS 的模型
換成 Speech T5 這個 TTS 模型
也有發揮作用
所以這招看起來
也是有一定程度通用性的
好那講了很多 Model Merging
這樣神奇的例子
那如果想自己玩一下 Model Merging 的話
有蠻現成的工具
叫做 Merge Kit
那裡面就實作了
各式各樣不同 Model Merging 的想法
那其實 Model Merging
不是像我今天講的
只有簡簡單單加在一起
而已
它有非常多不同的變形
那至於有哪些變形
其實 Merge Kit 裡面
它的 Implement 算是蠻完整的
多數知名的變形
它裡面都是有實作的
所以可以直接用這個 Toolkit
它就可以幫你
用各式各樣不同的方法把兩個模型
在一起
當然 Model Merging 不是總是會成功
其實比較神奇的就是 Model Merging
居然是 Work 的
它在蠻多時候
它是 Work 的
但也有很多時候
Merge 完之後
模型就是壞死
能力就不好使了
你把盔甲跟劍
Merge 在一起
這個劍就插在這個 Llama 背上
它就死掉
所以有時候 Merge
不一定會成功
所以也有人提出一些方法
是希望去對 Merge 過的模型
再去做 Fine-tune
Merge 過模型有一點問題
所以有人提出了新的方法
是說對 Merge 的模型
再做一些類似 Model Editing
或者是用 Gradient Descent
做 Fine-tuning 的手段
讓它做得更好
那就留一篇論文在這邊
那另外呢
有一個系列的 Work 是說
我們既然之後呢
這些 Model 要來做 Merging
我們能不能夠在 Post-training 的時候
就考慮這件事情
我們訓練一個特別容易做 Merging 的模型
大家可以了解嗎
在還沒有做 Merge 之前
就先想
搶先想一步
這是霸氣領域的思維
就先搶先想一步
先把模型在 Post-training 的時候
就考慮之後要做 Merging 這件事情
訓練一個特別容易跟其他人做 Merging 的模型
那至於什麼樣的模型
特別容易跟其他人做 Merging
那我就留一個文獻在這邊
大家再自己參考文獻裡面的內容
那最後我想要分享的就是
剛才都是假設
要同一個 Foundation Model
做 Post-training 以後的模型
才能夠做 Merge
你其實有點難想像
如果是不同的 Foundation Model
Post-training 以後的模型
到底要怎麼 Merge 在一起
好像不應該可以被 Merge 在一起
但是也有文獻在研究
假設你有兩個不同的 Foundation Model
比如說一個是 Gemma
一個是 Llama
Gemma 練出了一個盔甲
Llama 練出一把劍
他們一個是動物
另外一個甚至你不知道
它應該是什麼東西
它就是一個 Entity
它不是動物也不是植物
但是能不能把他們直接 Merge 在一起呢
那在文獻上
也是有一些 Paper 嘗試過這些事情的
然後把相關的論文放在右上角
給大家參考
看看前過去的文獻
是怎麼把兩個不同的 Foundation Model
Post-training 以後的模型
直接硬生生的 Merge 在一起
那最後一段
就想跟大家分享一個過去沒有講過的技術
叫做 Test Time Training
那你看 Test Time Training 這個專有名詞裡面
它既有 Test 也有 Train
有兩個互相矛盾的概念
那我們過去在講機器學習的概念的時候
都是先 Training
Train 完以後才能做 Testing
Testing 感覺跟 Training 是沒有關係的
那我們現在來看看
怎麼在 Testing 的時候來做 Training 這件事情
那 Test Time Training 它的縮寫就是 TTT
連續三個 T
那 TTT Test Time Training 這種技術
它的運作方式是這個樣子的
現在你有一個模型
你給模型一個輸入
你給模型輸入以後
它不直接給你答案
它看到你測試它的資料以後
它現場直接做一個 model 參數的 update
等一下會講這招是怎麼做的
它現場直接 fine-tune 模型的參數
給你一個新的模型
這個新的模型是專門為現在的輸入設計的
期待這個新的模型
在現在的輸入上
可以表現得比原來 fine-tune 前的模型還要更好
這個就是 Test Time Training 的核心想法
那講到
那當然有時候如果你只有一筆 input 的資料
不一定能夠真的得到非常好的結果
所以比較多文獻你可能會看到說
其實我們是用了一個 batch 的資料
假設現在的輸入是一個 batch
那我們拿一個 batch 比如 32 筆資料
一起去 fine-tune 模型
那有了新的模型以後
再同時測試這個 batch 裡面 32 筆所有的資料
但總之用的資料非常的稀少
而且重點是這裡沒有用到 label
沒有用到 feedback
我們一般想像在 Training 的時候
你要有 human 的 label
你才能夠 Training
你要有 human 的 feedback
你才能夠做 RL
RL 也不算是 unsupervised
你要有 human 的 feedback 才能夠做 RL
但這個 TTT 的訓練方法
它在 fine-tune 模型
在 update 參數的時候
是沒有人
不需要人類介入的
是沒有 label
沒有 feedback 的
但能夠成功 fine-tune 參數
讓模型表現得更好
等一下會講這招是怎麼做的
講到 Test Time Training
我就想到一個有一點類似的技術
就是 Reasoning
那 Reasoning 呢
我們在本學期的課沒有講
但上學期的課呢
是花了蠻多時間
跟大家講 Reasoning 這個技術的
Reasoning 這個技術就是
有一個 input
然後模型不馬上回答問題
而是演一個腦內小劇場
開始探索各種不同的可能性
等它覺得探索的差不多之後
再根據探索的結果
給你最終的答案
那其實 Reasoning 這個想法
跟 Test Time Training
也是有一點關係的
它們都是不馬上給答案
模型多做了一些什麼事情之後
才給答案
那像這壹種不馬上給答案
而是靠著額外的運算
得到更好的結果的方式
其實就統稱為 Test-Time Computing
所以 Reasoning
我們說是 Test-Time Computing
或 Test-Time Scaling 的一種
那其實 TTT 我覺得也可以看作是
Test-Time Computing
或 Test-Time Scaling 的一種
那如果你想要對 Reasoning
有更進一步了解的話
那過去的課程其實講過
我把錄影的連結放在這邊
給大家參考
不過 Reasoning 跟 Test Time Training
還是非常不一樣的
在 Reasoning 的過程中
模型是同個
模型參數是沒有變的
而 Test Time Training
是真的會改變模型的參數
模型的參數
是真的會改變的
變成不一樣的模型
好你可能覺得說
哇 Test Time Training
這個聽起來太神奇了
沒有用 label data
只用訓練資料
到底是怎麼更新
模型的參數的呢
我這邊舉一個
Test Time Training 的基本方法
然後你就會發現
它其實也沒那麼神奇
好假設現在有一個模型
那輸入一筆資料叫做 x
那我們呢
有這個模型訓練的時候
用的訓練資料
那訓練資料是有 label 的
所以你有 x1
跟它的 label
y1 star
x2
y2 star
x3
y2 star
y3 star
到 xn
yn star
現在我們在訓練資料裡面
去找跟現在的輸入 x
最接近的 n 筆資料
比如在這個例子裡面
找三筆
那假設 109 號
321 號
跟 753 號
跟現在的測試
這一筆測試資料是最像的
那一樣
這三筆資料
109 號
321 號
753 號
都有 label
有 label
所以你可以訓練模型
直接拿這些跟 x
最像的訓練資料
拿來 fine-tune 你的模型
它也許在 x 上
就會表現得更好
這個有沒有非常的直觀
所以本來是一個
general 的 model
在大量的訓練資料上
做訓練
也許它沒有辦法
吸收全部的資料
現在我已經知道
測試的時候
就是要考 x 這個問題
所以去訓練資料裡面
找一些跟 x
特別像的資料出來
fine-tune 模型
讓模型變成一個
特別針對 x 訓練出來的
特別針對 x 這個任務
訓練出來的模型
也許它就可以在 x
相關的任務上面
表現得更好
那這招
其實突然就非常的直覺
Test Time Training
聽起來就沒有那麼神奇
那這邊再用個
圖示的方法來告訴你說
這種訓練在
特定資料上的方法
為什麼能夠
有時候發揮巨大的作用
現在假設
橫軸是 x
縱軸是 y
那現在 xy 的關係
是這一條
非常複雜的曲線
這些黑色的點
是我們的訓練資料
假設今天在訓練的時候
你的模型的 capacity
是有限的
也就是你只能用
比較簡單的模型
比如說只能用 linear model
你的模型
只能是一條直線
那不管你怎麼努力畫
這一條直線
都沒有辦法
fit 所有的訓練資料
你最好可能就畫到像這樣
general 的 model
最好可能就是
這樣一條斜直線
那有很多
在這個橫軸上的點
是做不好的
比如說 Testing 的時候
假設 Testing 資料的 x
軸落在這邊
丟進這個 general model
那得到的 y 值是這裡
但正確答案是在這邊
你就有一個非常大的差距
那 Test Time Training
為什麼可以發揮作用呢
當這個 x 的資料進來的時候
你去找你訓練資料裡面
跟 x 最接近的那些點
比如說這邊找兩個點
這邊找兩個點
拿這些訓練資料
再來重新訓練一個模型
再來微調你的 general model
讓它變成一個
針對 x 這個位置
客製化的模型
那你訓練出來的模型
可能就是紅色的這一條線
那這個時候你再去做測試
x 輸出來的答案
可能就會跟真實的答案
更為接近
所以 Test Time Training 這個方法
可以想成
我們其實擴張了模型的能力
本來 linear model
弄來弄去都是一條直線
很多位置就是做不好
但是 Test Time Training
可以為每一個位置
客製化一個模型
就可以擴展 linear model 的極限
那你可以想像說
為什麼 Test Time Training
可能是一個有用的方法
但除了搜尋額外資料以外
Test Time Training
還可以有其他的方法
舉例來說
你記不記得
我們之前在第六講的時候
講過
Semi-supervised learning 這個技術
我們說
一般我們在定 loss 的時候
你都是用 label data 定 loss
但是如果你有大量的 unlabeled data
你也可以用 unlabeled data
來定出你的 loss
你的 loss 裡面可以多一項
是跟 unlabeled data 有關的
其實 Test Time Training
也可以看作是一種
Semi-supervised learning 的技術
你本來有一些 label data
可以訓練這個模型
現在在測試的時候
有一些測試資料進來
這些測試資料是沒有 label 的
你就可以拿這些
沒有 label 的測試資料
當作 Semi-supervised learning
裡面的 unlabeled 的資料
你可以套用
Semi-supervised learning 的技術
來做 Test Time Training
但講到 Test Time Training
很多人會說 Test Time Training
其實是 unlabeled 的
這邊之所以說它是 Semi-supervised
是因為其實你打造原來的 foundation model 的時候
你可能還是用到了一些 label 的資料
所以它可能不是
整個過程可能不是完全的 unsupervised
所以我這邊說是 Semi-supervised
但如果你只單看對於 Testing data 的使用的話
確實是 unsupervised 的
因為 Testing data 的資料裡面
我們沒有用到 label 資料
不過大家就不要再糾結
這個名詞定義的問題
你只要知道我講的事情是什麼就可以了
總之你可以把 Semi-supervised learning 裡面有的技術
就直接套到 Test Time Training 裡面
你的 unlabeled data 就是你的測試資料
這招能發揮作用嗎
比如說在 Semi-supervised learning 裡面
一個常常用的招數叫做 Minimize Entropy
我們在第六講的時候也已經講過
我們這邊就不再細講
那這個招數完全可以直接就套到那個 Test Time Training 裡面去
比如說其中 Test Time Training 有一個非常知名的方法
叫 TENT
TENT 你看這是 20 年的 paper
上古時代的一個論文
在上古時代其實人們就已經知道要做 Test Time Training 了
那 TENT 呢其實就是直接用 Minimize Entropy 這種方式
就你現在呢有一個 Batch 的 Input 的資料
然後呢把這個 Batch 的 Input 的資料丟給你現在的 General 的 Model
然後它就產生一些輸出
那我們現在來 fine-tune 這些 General 的 Model
那 fine-tune 的目標是什麼呢
雖然我們沒有任何 Label 的資料
但 fine-tune 的目標是讓這些模型輸出的 Entropy 越小越好
也就是讓它的輸出越集中越好
讓機率只集中在某幾個 Class 上
那當你教模型 Minimize Entropy 之後
那再拿這個模型真的做測試的時候
它在同樣的資料上
其實就有機會表現得更好
那在原始的 TENT 上面啊
通常這樣的方法是需要有一個 Batch 的資料
才能夠運作啦
如果你只有一筆資料
這種 Minimize Entropy 的方法
可能是這不會有好的結果的
但是如果是在語音辨識上
我們實驗室的林冠婷同學呢
嘗試在語音上語音辨識上
做 Test Time Training
然後提出一個方法叫做
SUTA (Source-free Unsupervised Test-time Adaptation)
這邊 Test Time Adaptation 就是 Test Time Training 的意思
那顧名思義
它其實可以只用一句話
就成功的提升模型的能力
所以 Test Time Training
只用一句話
只用一個 Example
也是有可能有不錯的結果的
那這邊就 Framework 而言
它做的事情就是
有一個語音辨識的模型
這個語音辨識的模型呢
現在使用者對它講一句話
期待給辨識的結果
那這個模型不會馬上給你辨識的結果
而是根據你現在講的這句話
現場去微調它的參數
也就對每一句話都有一個客製的模型
用那客製的模型去做語音辨識
就可以得到更好的結果
那這個 SUTA 這個方法
它本質上的精神蠻像是 Entropy Minimization
但還有做其他的事情
那合在一起
那才能夠有好的結果
那這邊因為時間有限的關係
我們就不細講 SUTA 這個技術
那這邊就是用一頁投影片
很快看一下 SUTA 這個方法
可以帶給我們的結果
那這邊每一個 Column 指的是不同的 Domain
就是不同的測試的資料集
那如果你是拿一個現成的 Foundation Model
在這些測試資料上直接做語音辨識
這邊秀的數值是錯誤率
所以越低越好
那有一些 Corpus 蠻難的
所以錯誤率在 2022 年的時候
其實是蠻高的
那如果直接用 SUTA 這個方法
你可以發現比較 SUTA 得到錯誤率
跟原來 Foundation Model 得到錯誤率
在不同的 Domain 上都有一個蠻顯著的錯誤率的下降
那還有什麼樣其他的 Test Time Training 的方法呢
有一個最古早的方法
是把 Test Time Training 跟 Pre-test 的 Task 結合在一起
那這是一個來自於 19 年的
也就是大概是在白堊紀時候的論文
大概是在白堊紀時候的論文
那我覺得這應該是最早的 Test Time Training 的 Paper
那我第一次看到 Test Time Training 這個詞彙的時候
其實是來自於這一篇論文
那這篇論文裡面的想法是什麼呢
現在假設要做影像的分類
這篇論文裡面的那個例子是做影像分類
你給機器一隻鳥
那你要叫它做分類
但在做分類之前呢
我們先微調一下這個模型
怎麼微調這個模型呢
用一些 Pre-test 的 Task 來訓練模型
那我們上次在第六講的時候也講過
Pre-test 的 Task
我們說我們可以拿一堆很容易蒐集資料的任務
先來對模型做 Pre-train
然後希望做完 Pre-train 以後
拿 Pre-train 的模型做 Initialization
接下來模型就可以做得更好
在 Optimization 的時候可以做得更好
那這邊一樣你可以找一些 Pre-task 的任務
這些任務是不需要標註資料的
比如說在這篇原始的論文裡面
就是教模型去預測說一張圖片被轉了多少度
所以一張圖片進來
他就拿這篇圖片去創造出一些訓練資料
去教模型看這張圖片被轉了多少度
比如在這個例子裡面是轉了 180 度
那教模型能夠看這張圖片預測轉多少度之後
那模型的參數就不一樣了
然後這個新的模型再去做影像分類
它可以做得更好
就是這麼神奇
所以它學會判斷一張圖片
有沒有被旋轉之後
你叫它做分類
它可以做得更好
那至於為什麼會這樣
你可以看一下這一篇原始的論文
它裡面甚至是由理論推導
它裡面有一個 Toy Example
來證明說這招會發揮作用
然後再用實驗證明說
在更複雜的狀況
在更真實的資料上面
也能夠發揮作用
好那講了這麼多 Test Time Training
一般的 Test Time Training
它的使用情境是這個樣子的
就第一筆資料進來
模型被更新一下
它得到了一個燈泡
然後在第一筆資料上面
可以做得更好
但當第二筆資料進來的時候
我們會拿舊的模型
我們並不是拿更新過的模型
我們是拿原來的 Foundation Model
再去微調
那針對第二筆資料
可能它學到了一把劍
在第二筆資料上可以做得更好
第三筆資料進來
我們還是拿舊的模型
再去做微調
它得到一個盾牌
然後希望在第三筆資料上
可以做得更好
那這樣顯然有一個限制
就是在每一筆資料上
學到的東西是無法累積的
模型只針對某一筆資料
去做特化
但是針對某一筆資料
學到的東西
沒辦法應用在下一筆資料上
一個很直覺的想法是
我們的 TTT Test Time Training
能不能是連續的
模型在根據第一筆資料
學到一個燈泡之後
它不會退回原來的參數
它就是有一個燈泡
這個參數永遠都在
第二筆資料進來的時候
它是在已經有燈泡的基礎上面
再做 Test Time Training
所以它就又了一把刀
它現在又有燈泡又有刀
可以把第二筆資料做得更好
好
第三筆資料進來的時候
它現在有燈泡也有刀
根據第三筆資料
再學了一個盾牌
然後它的這個技能呢
它的裝備呢
就會越來越多
那這是 Continuous
Test Time Training 的構想
那這個想法
是不是很像一般人
對於人工智慧的想像呢
假設有一個人
完全沒有讀過機器學習
任何相關的概念的話
往往對於人工智慧
想像就是這樣
你跟人工智慧講話
越講
它就會自動參數更新
然後就越來越聰明
一般人都覺得
人工智慧是這樣運作的
但你現在已經學過機器學習
所以你知道
人工智慧不是這樣運作的
後訓練一般不是這樣運作的
但我們有沒有這種
特殊的後訓練的方式
這邊叫 Continuous
Test Time Training
它的運作就跟人類想像的一樣
每一筆資料來都微調一下模型
這個微調是不斷累積的
所以模型在測試的時候
它就會越來越聰明
最後就變成天王
但是如果你實際上做 Test Time Training
你會發現它表現的並不好
這邊引用的是我們實驗室
黃維坪同學的論文
他就比較了
黃色的 Bar 是沒有做任何的 Test Time Training
橙色的 Bar 是做 SUTA
這種我們剛才講過
只需要一句話的 Test Time Training
紅色的 Bar 是做 Continuous Test Time Training
它的演算法跟 SUTA 一樣
只是每次它會把舊的參數存下來
每次都是從更新後的參數
再繼續更新下去
那這邊
這個不同的
這個橫軸呢
代表是用了不同的 Corpus
就是不同的 Domain
那縱軸呢
指的是 Word Error Rate
這個數值是越小越好
那 TTA 蠻有效的
做 SUTA 蠻有效的
在不同的 Domain 上
橙色的 Bar 都比黃色的 Bar 低
但 Continuous TTA 就沒發揮作用
你發現很多時候
紅色的 Bar 是比橙色的 Bar 還高的
還有些時候橙色的
紅色的 Bar 甚至比黃色的 Bar 更高
你與其做 Continuous TTA 還不如不做呢
做 Continuous TTA 還比較糟糕一點
而這個現象
並不是我們這個實驗有問題
而是這個現象非常常見
這邊引用另外一篇論文
它是做在影像上的
它就試了各式各樣 TTA 的方法
它告訴你說
把各種 TTA 的方法
都放在 Continuous 情境下
每一種方法都會爛掉
最後你就是得到一個
奇爛無比的模型
有時候那些模型
甚至爛到說
它永遠就只會 Output 同一個 Class
那這種現象叫做 Model 的 Collapse
那為什麼 Continuous TTA
往往沒有辦法
好好的發揮作用呢
那一個解釋就是
當你在 fine-tune 模型的時候
你就創造了 Catastrophic Forgetting
你就引入了 Catastrophic Forgetting
尤其是在做 TTA 的時候
你只有一筆資料
你用一筆資料去 fine-tune Model
fine-tune 完之後
哇那個往往 Forgetting 呢
會蠻嚴重的
所以當你針對某一筆資料
對模型做 fine-tune 模型
也許在這筆資料上面表現真的比較好
但它忘記了它過去已經會的東西
雖然得到一個燈泡
但整個模型呢就生鏽了
所以在下一筆資料進來的時候
如果你再拿這個生鏽的模型
繼續去做 TTA
它就生鏽的更厲害
最後就面目全非
完全不知道它在做什麼
這是一個做 Continuous TTT 的時候
Test Time Training 的時候
非常常見的現象
那要怎麼處理這個問題呢
其實在文獻上就有一系列的做法
那我把論文放在這個
我把這些文獻放在
投影片的右下角給大家參考
那這邊呢就跟大家講一個
這個黃維坪同學提出來的做法
這個叫做 Dynamic 的 SUTA
這個想法是這個樣子的
如果我們只拿一筆參數
我們只拿一筆資料去 fine-tune 模型
蠻容易把模型 fine-tune 壞的
所以我們不應該讓
只有一筆資料
fine-tune 出來的那些參數
讓它可以遺留到
下一個 Input 進來的時候
所以如果我們希望有一些參數
是可以長久被留存下來的
那些長久被留存下來的參數
應該要用比較多筆的資料訓練
所以它的做法是
今天看到 Input t 的時候
一樣用 Input t 去做 SUTA
去做 TTA
得到一個只用 Input t 設計出來
訓練出來的模型
讓它在 Input t 上
第 t 筆資料上可以得到更好的結果
但是同時會收集過去已經看過的資料
比如過去已經 Input t 筆資料
t-1 筆資料
t-2 筆資料
每一定數目的資料
就把它存下來
等存留到足夠的數目
比如說每存 5 筆
再 Update 一次參數
那這一次的 Update
是會永久被存下來的
比如這邊收集 3 筆資料
做一下 fine-tune
那一樣是跑 SUTA 的 Algorithm
所以一樣是 Unsupervised 的
一樣不需要任何 Label
只是我們不是用一筆資料來做 fine-tune
而是用 n 筆資料
用一個 Batch 的資料來做 fine-tune
那如果你用一個 Batch 的資料來做 fine-tune
那模型的 Update 就會比較穩定
它就可以比較避免 Catastrophic Forgetting 的現象
那接下來
第 t+1 筆資料進來的時候
我們就可以用新的模型
它有一個燈泡
再繼續去做訓練
那這一次是指針對第 t+1 筆資料進行訓練
是指針對第 t+1 筆資料進行微調的
所以這邊有兩個 Update 的過程
一個 Update 的過程是快速的 Update
只用一筆資料訓練
那一筆資料訓練出來的參數
它不會被保留下來
它只會被用一次
只會針對某一筆資料做使用
那另外還有一個 Slow Update
Slow Update 是比較穩定
用比較多筆資料訓練出來的結果
那用 Slow Update 得到參數
是會被保留下來
在接下來在永久的未來一直被使用的
好
那這種 Slow Update 加 Fast Update 的方法
它們兩者合起來是真的有用的
那這個
這邊呢
是在某一個例子上面做那個 Continual TTA 的結果
那這個橫軸呢
指的是一個一個 Example
它寫 t 寫 time
但指的就是一個 Example
這邊有超過 2500 筆輸入的資料
縱軸呢
指的是 Word Error Rate 的 Difference
也就是相較於什麼都不做
Continual Test Time Training
到底進步了多少的 Word Error Rate
到底讓 Word Error Rate 減少了多少
所以 Pre-training
也就是 Foundation Model
它是黑色這條線
比黑色這條線低
就代表 Test Time Training 有發揮作用
那藍色這條線呢
是只有 Fast Update
就每次只根據
現有的那一筆測試資料
來更新模型的參數
那你會發現說
這個 Fast Update 是有用的
只根據某一筆模型的
只根據某一筆訓練資料
來更新模型的參數
在幾乎所有的狀況
都是能發揮一定程度功效的
另外一方面
如果只做 Slow Update
那也能發揮功效
只是它的開局呢
比較慢
在開頭呢
它需要比較長的時間去收集資料
才會得到比較好的結果
但是隨著時間越來越長
到後期
這種 Slow Update 的方法
可以超過 Fast Update 的方法
但是如果同時有 Slow Update
跟有 Fast Update
兩者同時存在
你可以得到結果
是最好的結果
你會得到綠色這條線
是比橙色這條線
還有藍色這條線
都還要更好的
那事實上呢
Dynamic SUTA
還有很多很複雜的設計
比如說它裡面有一個設計
就是 Model 要自動做 Reset
那至於怎麼做
那這個大家再詳見論文的內容
那這部分呢
是設計的蠻複雜的
對這種 Continuous 的 TTA 而言
這種自動的 Reset
其實是必要的
因為你蠻多時候有機會說
你 train 著 train 著 train 到某個地方
哇模型就壞掉了
它壞掉以後
如果你還繼續 train 下去
你是沒有辦法自動讓模型復原的
所以需要在某一個時間點
有辦法自動偵測到說
哇現在模型毀掉了
好那我回覆到上一個階段訓練出來的模型
再從舊的能夠運作的模型
開始繼續去做 Continuous Test Time Adaptation
好那整體而言
這個 Continuous Test Time Adaptation
這個 Dynamic SUTA
有沒有發揮作用呢
有發揮作用
紅色的是你沒有用任何技巧的
Continuous Time 的 Test Time Adaptation
或者是 Test Time Training
那粉紅色的是 Dynamic SUTA 的結果
那你會發現有這個 Fast Slow 的設計
有加上這個 Model 的 Reset
可以得到更好的結果
那粉紅色這條線
它都是比紅色這條線表現更好的
它也可以比橙色這條線
沒有做 Continuous 的 Standard 的 Test Time Training
還要表現得更好
那這其實是
這個 Continuous Time TTA
其實還是一個尚待研究的問題
所以這邊還有很多不同的方法可以想
這邊只是舉一個例子
告訴大家說
Continuous Time 的 TTT
可以有什麼樣的研究
好那以上就是今天想跟大家分享的內容
我們就是講了用 Gradient Descent Fine-tune
講了 Model Editing
講了 Model Merging
最後講了 Test Time Training
TTT 這個技術
好那我們今天的課程就是上到這邊