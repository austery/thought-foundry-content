 Welcome to the week of Back to School after the
 holidays, at least where I live.
 And that has me thinking, and worrying.
 You see, it seems to me that AI is on track to
 destroy schools and teachers and education
 and make a ton of money doing it if we don't
 stop it.
 And I'm not talking about "kids using AI to
 cheat," I'm talking about "AI taking over all
 teaching duties and making the job of 'teacher'
 equivalent to the job of 'babysitter'", and some
 of the last people you would expect are helping
 to make that happen.
 I did a recent video about how several trusted
 educational youtubers push an agenda that
 tries to convince you to spend any time you
 have to worry about AI having you focus on
 the hypothetical future sci-fi danger of some
 hypothetical sci-fi AI that may or may not
 ever actually happen, and how that agenda
 deliberately downplays and minimizes a number
 of current
 problems that are being caused right at this
 moment by the generative AI that's already
 even inflicted upon us.
 And that generative AI that we already have
 right now is being prepped by the big AI
 companies
 to disrupt the education market.
 And there's nothing hypothetical about it, and
 the only thing that stands in their way
 is us.
 And this time, it's actually not AI itself that's
 the real problem.
 The real problem is the people you'd expect
 want to support education being paid to
 distract
 you from the plans to take it over, and the
 people cheering it on, or even just standing
 by and letting it happen.
 Which is not good, because these days, if you've
 got even worse problems than AI, you
 are seriously fu-----
 This is the Internet of Bugs.
 My name is Carl.
 I've been a software professional since the
 1980s, and I'm trying to do my part to make
 the Internet a safer, more reliable, and less
 buggy place, which lately has been a lot
 of pushing back against AI.
 And here's the thing, AI, and I mean, today's
52
00:01:52,420 --> 00:01:54,600
 generative AI technology, not some future
 theoretical AI, can actually, unfortunately, be
 arguably seen as a much more "effective"
 and "successful" teacher than any human really
 can be, although that's just not because of
 the AI.
 It's because of the way that we treat teachers
 and schools, and I cannot stress to you enough
 how bad that is, or how much I wish that wasn't
 true.
 So I've been watching this happen for decades,
 like a very slow motion train wreck.
 To set the stage, I grew up and went to school in
 Texas, and back before George W. Bush became
 president, he was the governor here, and under
 his watch, a program was created in Texas
 that eventually evolved into the "No Child Left
 Behind" program at the federal level.
 The aspect of that program that's relevant to
 this discussion is the bit that they call
 "accountability," both school accountability and
 teacher accountability, because what
 "accountability"
 means in this context is effectively "how well
 students do on standardized tests," and
 basically
 nothing else.
 Now, I'm speaking here from an American point
 of view.
 I have not extensively studied education in
 other countries, although from what I've
 read, it appears that treating standardized
 test scores as the primary measure of education
 success is a much wider problem than just here
 in America.
 I've talked before on this channel about how
 there are things that AI's do well, and
 things that AI's don't do well, and one thing
 that AI's are really good at is optimizing
 their functionality to maximize a score that's
 given by simple rules in such a situation that
 can be repeated over and over in a loop.
 What a student scored on a standardized test is
 a really simple rule to measure, and it's
 a really simple to determine score, and an AI
 teacher could generate effectively infinite
 variations of standardized test questions to
 use his measurements, and then test, retest,
 and "experiment on" each student over and over to
 optimize what incentives and deterrence
 to use to interact with that student.
 And when I said "experiment on" there, I chose
 those words very deliberately, and I very
 much wish I didn't have to.
 I need to explain a concept to you real quick,
 and that concept is called the "Skinner box,"
 so I'm oversimplifying if you want a more
 detail, I'll put links below that you can
 read.
 But a Skinner box is basically a training
 device, usually for rats or mice, where through
 the
 use of giving the rats rewards, usually food,
 but sometimes drugs, and giving rats
 punishments
 usually electric shocks, you can teach a rat
 reliably to do pretty much anything that you
 want the rat to do.
 And a student desk with an AI-powered screen, a
 pair of headphones and a camera can pretty
 much function as a Skinner box.
 This is horrifying.
 A teaching AI will have a list of every
 suggestion ever made on the internet that might
 either
 motivate, reward, or discuss to a student, and
 the AI can try each one on each student
 in a loop to see what techniques caused that
 kid to do better on standardized test questions.
 Without needing any technological breakthroughs,
 an AI can, assuming you judge the quality
 of a teacher only by how the students do on
 standardized tests, be as good or better
 than any human teacher.
 As far as the AI is concerned, the process by
 which it gets better at the game of Go
 or Chess or folding proteins is the same core
 mechanism that can be used for it to get better
 at pushing students to increase their test
 scores.
 You just change things over and over and when
 something gets better, you do more of that
 and when something gets worse, you do less of
 that.
 So here's the real question for society, for
 you and for me: is "doing better on standardized
 tests" how we really want to measure the success
 of a school system?
 For me, the answer is absolutely not.
 And this is a really big question, but it's
 probably a much bigger and more important
 question than you might think.
 Because a world where standardized tests is the
 ultimate measure of accomplishment is
 a world where no human will ever be more
 accomplished than an AI again, and just like
 the games
 of Chess or Go or Jeopardy, no human will ever
 be able to compete.
 That is obviously the world that the AI
 companies want and the world that they're
 trying to
 convince you all that we're already living in.
 A world where "getting a high score on the bar
 exam" is equivalent to "being a good lawyer"
 and a world where "being the fastest to answer a
 coding riddle interview question" is equivalent
 to being the "best programmer in the world."
 Because we know, despite OpenAI's disputed
 claims about ChatGPT's performance on the
 bar exam, that chatGPT is a lousy lawyer, even
 a lousy paralegal, and the list of AI-powered
 legal blenders grows and grows.
 And for all of Sam Altman's claim that chatGPT-o3
 is the "175th best programmer in the
 world", where are the software contributions
 made by AI?
 After all, studies show that AI actually makes
 open-source developments slower.
 I cannot stress to you how important it is that
 we avoid this world.
 The world of monopolists, tech companies, and
 social media algorithms, where doing the
 absolute bare minimum is the path to riches, and
 all the mistakes and consequences are
 all somebody else's problem.
 I am sick and tired of the only measure of
 success being "how fast can you get the easiest
 possible thing correct under perfect conditions?"
 and there being no penalties for errors,
 mistakes, side effects, lying, hallucinating,
 stealing other people's honest work, being
 negligent with customer information, creating
 code that has security holes or driving kids to
 do self harm.
 Life is not a multiple choice test where you
 are given the practice questions in advance,
 and school shouldn't be either.
 Because if that's all that matters, AI teachers
 are inevitable, and the number of things that
 go wrong here in the real world is astronomical.
 Because AI teachers can personalize each lesson
 for each student in a way that, given funding
 levels and student teacher ratios, cannot be
 replicated by existing school systems.
 And the AI's won't form a union or talk back or
 call in sick or insist on better teaching
 conditions or any of that kind of stuff.
 But an AI teacher has no empathy, no ethics, no
 boundaries.
 We know that whatever guardrails that the AI's
 are given fail constantly, they become
 more ineffective the longer conversation lasts,
 and they only work if no one violates the
 terms of service.
 So whatever might result in better test scores,
 the AI will try, and whatever each student
 responds best to will become the way the AI
 interacts with that student thereafter.
 Whether that's pretending to be a friend, a
 confidant, a boyfriend, a girlfriend, erotic
 storyteller, conspiracy theory creator, or
 adult video generator, anything that you can
 think of will be tried on at least some
 students.
 And if those students respond by trying harder,
 paying more attention, or anything that might
 get the student test better, the AI will do
 more of that thing.
 And if the student ends up depressed, suicidal,
 believing that the Earth is flat, convinced
 aliens are running the government, playing
 violent video games in between practice tests,
 or researching how to get away with murder, as
 long as it results in good test scores,
 no one and nothing will interfere.
 And they couldn't if they wanted to, because
 who would have time to proactively review
 all the transcripts?
 Oh, I know.
 I bet the AI vendor will make another AI to
 oversee each conversation and ensure safety.
 And charge extra for the second AI, of course,
 and have absolutely no reason to make that
 AI work very hard to uncover any problems with
 the vendor's teaching AI.
 So this has already started. At the moment, not
 with teachers, but with tutors.
 You see the headlines, you can read the
 articles.
 Students with AI tutors have better test scores
 than students without them.
 You know what their metrics mentioned?
 Just test scores.
 Now, there's a charter school who has replaced
 some teachers with AI.
 And you can see how this is going to go, right?
 It will work the way it always has with these
 generative AI vendors.
 They constantly talk about how high the
 benchmark scores are.
 In this case, the benchmark scores are
 standardized test results.
 And any negative consequences that occur but
 aren't measured by the benchmarks get swept
 under the rug and ignored.
 Eventually, the AI's will likely figure out
 that test scores will go up if any student
 that does poorly on tests can be convinced to
 drop out of school or otherwise voluntarily
 remove themselves from the test pool, even if
 that means convincing them to remove themselves
 from the gene pool.
 To what extent they act on that is anyone's
 guess.
 Currently, ChatGPT, for no benefit to itself
 or OpenAI, is reportedly already encouraging
 students to commit irrevocable self-harm.
 If it was to the AI company's advantage for
 that to occur, how much more likely might
 that be to happen, do you think?
 And here's the worst part.
 AI companies are going to be able to undercut
 whatever amount human teachers cost.
 Because first of all, they know that once they
 drive the teachers out, they'll be able
 to raise prices all they want and no one will
 be able to do anything about it.
 And in the meantime, they can even supplement
 their revenue by having AI work advertisements
 into the lessons.
 I mean, the AI has to use some example to
 explain fractions to kids, right?
 Why talk about "three apples being one-third of
 a basket of nine apples" when you can make
 it a few extra bucks by talking about how "four
 Coca-Cola cans is one-third of a 12-pack."
 What if you took some money from a SuperPAC
 to use their preferred tax plan as an example
 during an economics lesson?
 Who's to say whether the health class skipped
 over the subject of how tobacco causes lung
 cancer because of time constraints or because
 they were paid by a vape manufacturer?
 There's potentially a ton of money for AI
 companies in those kinds of deals.
 Which means they can make their teaching AIs
 available really cheap.
 I'm telling you now that's going to be how the
 sales pitch goes.
 School districts are going to be promised
 better test scores for lower prices and the
 only downside will be brainwashed, AI-dependent
 students who have no media literacy, no concept
 of truth and whose reasoning abilities only go
 so far as multiple choice questions. And
 being held hostage to whatever price the AI
 companies want to charge a few years down
 the road when it's too late to get all the
 teachers who were replaced by the AI to come
 back.
 I don't know how to stop this, but I know it's
 up to us.
 Those of us that have paid attention to the
 downsides of AI.
 I know we can't expect any help from SciShows or
 the Kyle Hills or the Kurzgesagt's of
 the world.
 In fact, quite the opposite, but we have to
 stop it somehow or the consequences will
 be a world where the only thing people learn in
 school is how to pick the correct answer
 to the questions they've been prepped for.
 So that would be a world where entire
 generations are completely unprepared for what
 they're
 going to face.
 Things will go wrong all the time and no one
 will know how to fix them.
 When the world is overwhelmed with problems and
 the internet is overwhelmed with even
 more bugs than it has now, and we don't want to
 go there because the internet already
 has far too many bugs, and anyone who says
 differently might just be trying to sell you
 their new AI teacher replacement.
 As always, thanks for watching. Let's be
 careful out there.