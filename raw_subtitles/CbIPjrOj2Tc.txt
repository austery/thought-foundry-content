好，各位同學們大家好啊
這是本學期最後一堂課
那今天我要跟大家分享的是
語音語言模型的發展史
那在過去的幾堂課裡面呢
我們花了很多力氣講了各式各樣
生成式 AI 的技術
那其實多數生成式 AI 的基本觀念
其實在課堂裡面都已經被囊括到了
那剩下其實就是靠你自己
再去深入的研究
那今天這堂課呢
我其實想要從我為第一人稱視角出發
講語音語言模型這個技術
這幾年來是怎麼發展起來的
然後在開始之前呢
有一個免責聲明
就這一堂課呢
並不是完整的介紹語音語言模型的技術
而是以我跟我的團隊
為第一人稱的視角來講
我們看起來的語音語言模型技術的發展
所以這並不是一個完整的 Overview
那期待是大家也許在這堂課裡面可以看到說
對於在第一線的研究人員而言
我們是怎麼看待一個技術的演進的
那當時可能在某個時間點
限於概念的侷限
限於資源的侷限
限於想法的侷限
那我們是怎麼樣看待一個問題
然後會有什麼樣的轉變
好
那在開始之前呢
跟大家講一下
這邊所謂的語音語言模型是什麼
那在這堂課裡面
多數時候我們講到語言模型的時候
我們指的都是一個文字模型
它是輸入文字輸出文字
那這邊語音語言模型顧名思義
就是輸入語音輸出語音
所以一個模型
如果它是輸入語音輸出語音
那我們可以說它是語音語言模型
那其實今天呢
你可能會看到非常多的語音語言模型
是你已經可以在你的手機上
就直接用到的
那比較多經典的例子
就是 ChatGPT 的 Voice Mode
還有 Gemini Live
那除了 ChatGPT 跟 Gemini
它們有語音語言模型
你可以直接用語音跟它互動
它用語音來回復你之外
其實還有很多各式各樣的語音語言模型
那我把比較知名的列在右邊
給大家參考
那這邊呢
並不是列舉了全部的語音語言模型
我只是舉幾個例子而已
那但是在我們更深入探討語音語言模型裡面
之前呢
我還有一件事要提醒大家就是
語音語言模型其實有很多不同的模式
所以當你在文獻上看到有人講說
它做的是一個語音語言模型的時候
你要仔細看它做的是什麼
每個人心裡所想像的語音語言模型
可能同樣是輸入語音輸出語音
它的功用可能是截然不同的
那這邊我大略把語音語言模型分成兩種模式
那一種模式呢
是對話模式
也就是當你對它輸入一段聲音的時候
比如你對它說
How are you
它會當做這段聲音是在跟它說話
它要給它一個回應
那你對它說
How are you
它就要說
I'm fine
但是有另外一種運作的模式呢
是
Command Mode
就是這個語音語言模型呢
它除了輸入一段聲音之外
它還會有一個 Instruction
有一個指令
這個指令告訴這個語音語言模型
我們要怎麼處理這段輸入的聲音
比如說如果現在的 Instruction 是
Identify the emotion
那語音語言模型的作用呢
是它要知道現在輸入的這一段句子
說話的人
他心情怎麼樣
這時候它的回應
就是你輸入 How are you
它回應不應該是 I'm fine
它的回應應該是
比如說 happy 或者是 angry
看看這個講話的人的心情是什麼樣
或者是如果你下的 Instruction 是
Translate the input
把輸入翻成中文
那你對它說 How are you 的時候
它不應該回答你說 I'm fine
它應該是把 How are you 翻成你好
所以語音語言模型運作不同的模式
那不同的模式在開發的時候
其實是不太一樣的
那在今天這堂課裡面
我們假設呢
我們討論的就是對話的模式
那因為在文獻上啊
現在語音語言模型
這個 Speech Language Model
這個詞彙呢
用得非常的混亂
每一篇只說它在做 Speech Language Model Paper
它往往背後所指涉的技術是不一樣的
所以當然有人試圖把這些不同類型的 Speech Language Model
用不同的名字做一下區隔
比如說有人會說這個 Command Mode 的模型啊
它叫做 Speech Aware Language Model
這是個 Language Model
但是它聽得懂聲音
所以叫做 Speech Aware 的 Language Model
總之這邊用詞呢
還沒有統一起來
從這個部分你可以
從這個現象你可以看出來說
這是一個非常新的技術
所以大家都還在摸索的階段
所以在詞彙的應用上面還非常的混亂
那這是一個新的技術
剛開始發展的時候
常常會有的一個現象
那另外一個要強調的就是
語音語言模型
我們這邊所討論的語音語言模型
並不是語音合成
並不是 TTS
今天當你對一個
Dialog Mode 的語音語言模型
Speech Language Model
說 How are you 的時候
它並不是複述你說的話
而是它會回答你
它會說 I'm fine
那如果是 TTS 的話
你是輸入一段文字
比如說你輸入 How are you
它就把你輸入的文字唸出來
也說 How are you
但是其實在文獻上啊
很多 TTS 模型
把文字轉成聲音的模型
它也會說自己是 Speech Language Model
因為它們背後用的技術也是
語音 Token 做 Token 的接龍
所以其實跟語音語言模型
也非常類似
所以很多 TTS 的模型
會直接說它自己就是
Speech Language Model
這讓 Speech Language Model
文獻呢更加的混亂
所以我們之前常常有經驗說
做一個語音語言模型
它明明是一個對話的模型
投稿到國際會議
Reviewer 就舉一大堆
TTS 的模型說
為什麼不跟這些比
那不一樣的東西
有什麼好比的
但是這個因為現在語音語言模型呢
這個用詞用的非常的混亂
所以很多人會誤以為
你在你在做語音語言
你說你在做語音語言模型的時候
很多人會誤以為
你做的是 TTS
那很多人會看到一篇
TTS 的文章
以為那是一個語音語言模型
總之這邊的用詞非常的混亂
但我這邊就是要強調說
等一下要講的是這種
可以對話的語音語言模型
它做的事情是回應你的輸入
好那要做一個這個語音語言模型
它可以輸一段聲音
給你一個回覆
那好像也不一定要用一個
End-to-End 的方法
那等一下我們今天這堂課呢
主要要探討的都是 End-to-End 的方法
也就是我們有一個模型
它直接拿聲音當做輸入
直接拿聲音當做輸出
但我相信你一定可以非常輕易的
想出一些不是 End-to-End 的做法
比如說我們可以把語音辨識系統
後面再接一個文字的語言模型
文字的語言模型後面
再接一個語音合成系統三個串起來
你也可以做一個語音到語音的 Language Model
它的功用跟一個 End-to-End 的模型
可能也差不多
你跟它講一句話
它把這句話用語音辨識系統翻成文字
把這段文字丟給文字模型
文字模型給你一個回覆
這個回覆再用語音合成把它唸出來
你也可以得到一個系統
它是輸入語音輸出語音
那事實上呢
今天多數你看到的號稱是語音輸入語音輸出的系統
往往用的是這種 Cascade Solution
其實今天語音對話對你來說可能沒有那麼稀奇
台大圖書館門口呢
就有一個可以用語音對話的數字人在那裡
那但是這種數字人
通常它用的就是 Cascade Model
把語音辨識加文字模型
再加語音合成三個串起來
變成一個輸入語音輸出語音的系統
那這是一個可能的 Solution
還有另外一個思考的面向呢
是 Agentic 的 Solution
我們可以說用一個文字的模型作為核心
然後呢
讓這個文字模型去操控其他語音的工具
有各式各樣語音的工具
包括語音辨識語音合成
甚至還有其他的
比如說語音轉換
或者是 Speech Enhancement
語音增強等各式各樣的工具
然後讓這個文字模型輸入一段聲音以後
它自己決定要用這些工具
怎麼來處理這段聲音
那這也可能可以做一個輸入語音輸出語音的系統
那這邊呢列舉了兩個例子
一個是 AudioGPT
一個是 Speech Copilot
給大家參考
好那
這種 End-to-End 的方法
跟不是 End-to-End 的方法
它比較起來有什麼樣的優缺點呢
這種非 End-to-End Cascade 的方法
它有一個巨大的優勢就是
容易做
因為語音合成
語音辨識
文字的 Large Language Model
都有現成的 API
呼叫三個 API
你直接做一個 Speech to Speech 的模型
你不需要花太大的力氣
甚至不需要自己訓練任何模型
不需要太多技術背景
你就可以輕易地做一個語音到語音的聊天機器人
那當然它顯然有一些缺點
它的缺點是什麼呢
第一個缺點就是
你會有巨大的 Information Loss
當你只把輸入的聲音訊號變成文字的時候
那你就忽視了文字以外的各種資訊
一段聲音訊號除了文字的內容之外
它還包含了語者的資訊
誰在說這句話
它還包含了語氣
這句話是怎麼被說的
它甚至還包含了環境的資訊
這個人在什麼樣的地方說了這句話
所以一段聲音訊號裡面包含的資訊
是非常豐富的
但當我們用語音辨識系統
把聲音轉成文字的時候
你就只有文字的資訊而已
那至於這些資訊
我們希望它怎麼樣影響
這些額外的資訊
我們希望它怎麼樣影響
這個語音語言模型的運作
等一下到這個演講接近結尾的時候
我們會看到一些具體的例子
好那這種 Cascade Model
另外一個問題可能是 Latency
那當你把三個模型串起來的時候
你往往需要等待
第一個模型跑到一定的結果
才能丟給第二個模型
第二個模型跑到一定的結果
才能丟給第三個模型
那當我們把幾個模型
串在一起的時候
你要做 Realtime 就比較困難
但也不是做不到
你可以有很多工程的解法
讓你的系統
它的回應速度仍然是非常快的
但這邊就會需要
比較大量的 Engineering 的 Effort
才有辦法把它做好
好那如果是一個 End-to-End 的模型
假設一個模型
能夠直接輸入聲音
直接就輸出聲音
那它的強項就是
它可以得到所有語音的資訊
它可以利用到語音中所有的資訊
那另外一方面你要佈建這種系統
可能也是比較容易的
因為它的延遲是低的
你只要把這個系統呼叫起來
輸入聲音
它就馬上就可以直接給你回應
那它的 Weakness 就是
這尚待研發的技術
所以你今天可能找不到
特別現成特別好的 Solution
它是可以處理所有語音到語音的問題的
那事實上呢
在現在這個時間點
2025 年
往往 Cascade 的模型
才是最強的模型
所以如果你要快速做一個對話聊天機器人
可以輸入語音
語音回應
那可能 Cascade 的模型
可以在很短的時間內
給你一個好的結果
那端到端的模型呢 尚待研發中
那它的好處是什麼
他有比較高的能力上限
你可以想像最終我們對於一個AI的期待
對於一個Speech Language Model的期待
應該是他可以聽懂聲音所有的面向
而不是只能做語音辨識而已
所以end-to-end的模型有比較高的能力上限
雖然現在它的能力還不一定能夠超過 Cascade 的模型
那你想說啊
這種表現比較差的模型
它會不會沒有什麼研究價值呢
那我們來看一點歷史的回顧
來鑑往知來
我們來想想
今天的語音辨識
只要有做過語音辨識的同學
你都一定聽過一個模型
叫做 Whisper
那它是一個 End-to-End 的模型
它就是一個巨大的類神經網路
它是一個 Transformer 輸入是聲音訊號
輸出直接就是文字
沒有什麼其他事情
就是這麼簡單
那即使今天呢
在做語音辨識相關研究的人
都知道 Whisper
甚至很多同學都只知道 Whisper
甚至不知道其他的技術
但像這種 End-to-End 的 Solution
並不是一直都是這樣子的
在幾年前
在 End-to-End 的模型
還沒有統治整個語音世界的時候
但是說更多的時候
人們做的是一個 Cascade 的模型
也就是我們想像語音辨識
它是一個很困難的問題
你可能很難用一個巨大的
類神經網路就一步到位
所以這個問題必須拆解成多個階段
常見的拆解方法是拆解成
Acoustic Model
一個聲學模型
Lexicon
一個辭典
跟一個語言模型
Language Model
這個語言模型
跟我們上課之前講的
那個文字的語言模型
其實是一樣的東西
所以過去其實很長一段時間
人們認為
語音辨識真的要解它
用 End-to-End
可能是做不到的
需要用 Cascade 的模型
那在很多人的想像裡
End-to-End 的崛起
可能就是某一天
有聰明的人
搞到了巨大的算力
說他要訓練一個 End-to-End 的模型
訓練下去
就起飛了
然後把所有 Cascade 的 Model 都打趴
然後從此以後
人們都用 End-to-End 的模型
事實上歷史的演進
並不是這個樣子的
我還記得第一篇
真正意義上的 End-to-End 的語音辨識系統
是發表在
Interspeech 2015
整整 10 年前
其實在 Interspeech 2015 之前
我記得是
應該是 NeurIPS 2014
就有一篇 End-to-End 的語音辨識模型
現在它辨識出來的不是真正的文字
而是 Phoneme
Phoneme 就是有點像是音標這樣的東西
所以它不是真正意義辨識上的語音辨識
所以我們先不管它
我們就考慮 Interspeech 2015 這篇好了
這個足足 10 年前
但是我其實還記得呢
當初這篇 Paper 是一個 Poster 的 Presentation
我站在這個 Poster 前面
我看這篇 Paper
覺得哇實在是
太潮了
哇不得了
不得了
這群人
他們想要 End-to-End
只用一個巨大的網路
輸入聲音訊號
輸出就直接是文字
真的是做得到的事情嗎
2015 年的時候
如果你看那篇 Paper 的結果
你單看那篇論文
你會覺得
這根本就是天方夜譚
為什麼會這麼說呢
比如說他們說
他們全部做到最好的結果
在一個叫 Switchboard 的這個 Corpus 上面
他做到的錯誤率
這邊數字是錯誤率哦
是 38.8%
你可能說 38.8% 還不錯啊
大概三個裡面猜對一個吧
三個裡面只錯了一個吧
感覺還不錯啊
那我告訴你
其實呢
Switchboard 這個 Benchmark Corpus
在 2014 年的時候
那個時候呢
正常的錯誤率
就可以做到 10.4% 了
所以
30 幾 % 的錯誤率
快 40% 的錯誤率
這根本就是做壞了
這樣
你根本不好意思說
這個系統真的有被訓練起來
看起來訓練的結果
是
蠻差的
但是
這是在 10 年前
所以這種 End-to-End 的系統
在 10 年前剛出來的時候
看起來
其實
很難說
它有一天
可以取代 Cascade 的系統
當時多數人仍然認為
Cascade 才是一個好的語音辨識的方式
不太相信能夠用 End-to-End 的方法
直接從聲音訊號轉成文字
但是
後來在很多世代的人的努力之後
這些 End-to-End 的 Model
就逐漸越來越強
越來越強
從
看起來根本 Train 壞了
到
跟 End-to-End
跟 Cascade 的模型
好像還差了幾 % 的錯誤率
到 End-to-End 跟 Cascade 看起來
好像差不多好
到 End-to-End
好像可以 Dominate 整個語音的 Community
中間
你可以想見
其實是花了將近 10 年的時間了
所以一個新的技術剛出來的時候
它不一定是一個表現非常好的技術
都是要非常長時間
很多不同團隊的努力
最終才會變成今天大家看到的樣子
好 這邊呢
我們是坐上時光機回到了 2015 年
如果說 2016 有 AlphaGo 是寒武紀的話
那我們就是在寒武紀之前
但是我們現在呢
要再次坐上時光機
再回溯到更早之前
其實在 Cascade 的時代
2015 年那個時候
語音辨識其實都已經是 Deep Learning Based
只是 Cascade 的
Cascade 裡面每一個模組都是 Deep Learning 的
那我們再看看
什麼時候人們開始用 Deep Learning 的技術
放在語音辨識裡面
那就我所知呢
把 Deep Learning 放在語音辨識裡面
比較早的一篇論文
應該是這一篇
這篇的作者
就是鼎鼎大名的 Hinton
我想大家都知道 Hinton
那第一作者呢
是多倫多大學的學生
Abdel-rahman Mohamed
他的名字等下在投影片裡面呢
還會再出現一次
這篇文章呢
他是說他們用 Restricted Boltzmann Machine
那時候的 Deep Learning
要叫 Restricted Boltzmann Machine
來做呢
Phone Recognition
所以其實沒有做到真的大詞彙的語音辨識
它辨識出來的東西其實是音標
那這篇論文是發表在 ICASSP 2010 年
那個時候我是一個博士生
我其實是有去 ICASSP 的
那在很多農場文的故事裡面都講說什麼
這個 Hinton 把這個 Deep Learning 用在語音辨識上就很好
所有人都跪了這樣
這根本不是事實
真實的狀況就是在 ICASSP 2010
沒人理會這篇論文
這篇論文
並沒有什麼關注
當時人們覺得比較關鍵的技術
是一個 Hidden Markov Model 的變形版
叫 Subspace GMM
這個我們今天就不講
總之這篇論文沒有人在意
那看起來並不是很厲害的技術
它只是隱藏在眾多 Poster 裡面的
其中一個 Poster 而已
那當年這個 ICASSP 2010
這個我們可以說是開天闢地的時代
這個宇宙大爆炸的時代
那個時候用 Deep Learning 做語音辨識做得好不好呢
還真的就是不怎麼樣
在他們的論文裡面他們說
他們做在 TIMIT 這個 Corpus 上
那等一下 TIMIT 這個 Corpus
還會出現很多次
他們看看這個 Phone Recognition 的錯誤率是多少呢
他們說他們做到 26.7%
那在論文裡面他們很誠實的
引用了很多其他論文
所以你看哦
有其他方法
比如說 HMM
他做到 24.8%
所以他們其實並不是 SOTA
事實上這個呈現的結果
也已經有點取巧了
他這邊比較的都是 Monophone 的模型
他並沒有使用 Triphone
這是一個可以讓語音辨識錯誤率
大幅下降的技術
所以其實在 Deep Learning 剛剛
興起的那一段時間
其實不斷的有人說
哎呀告訴你啊
Deep Learning 其實沒有用的
那些人告訴你有用都騙你的
他沒有故意用
他故意不用一些最好的技術
用最好的技術 Deep Learning
是不會展現效果的
所以其實在 2010
接下來的幾年並不是說 Deep Learning
一突然出現
所有人都突然改做 Deep Learning
不是
這是一個慢慢演變的過程
人們開始有人擁護 Deep Learning
有人說 Deep Learning
應該是不 Work 的
然後有各式各樣的爭執
然後後來 Deep Learning 的 Paper
逐漸越來越多
才把不是 Deep Learning 的那一個支派
把它壓了過去
所以歷史的演進
並不是說有一個技術一出現
然後大家就突然都跪了
覺得這實在是太強了
通常在新技術出現的時候
往往是一種很不起眼的方式
出現在世人的面前
所以一般人想像中的技術發展
我畫了一個示意圖
這個橫軸代表的是時間
縱軸代表的是這個技術
它可以帶來多好的結果
一般人想像中的技術發展是
一開始有個 Hidden Markov Model
發展到某一部分
某個時間點以後遇到了瓶頸
有聰明的人知道要用 Deep Learning
一出來大家都跪了
通通都換成用 Deep Learning
後來又到了某一個瓶頸以後
有聰明人知道要用 End-to-End
硬撐下去
然後結果很好
大家又都跪了
那事實上真實的發展
往往不是這個樣子
你之所以在報章雜誌上會覺得說
這些方法一出來就統治了世界
那是因為你看到它的時候
往往是它已經開始被吹捧的時候
你才開始注意到這些技術
當這些新的技術
可以超越舊的技術
那個農場文
開始社會大眾開始吹捧它的時候
你才開始意識到這個技術的存在
所以你往往以為這些新技術是一出現
就轟動武林
驚動萬教
就統治了世界
但事實往往不是這樣
真實的情況
對站在第一線的人而言
我在前面兩波的變革裡面
我看到的發展都是
這些技術一開始
都是以一個非常不起眼的狀態存在
看起來就是
哇
你這個跟 State of the Art
最好的結果通常叫做 State of the Art
或者縮寫有人叫它 SOTA
你跟 State of the Art 差這麼多
這個技術真的有希望嗎
然後在很多世代的人的努力之後
才變成今天這個樣子
所以如果看過去的歷史
其實給我的感想是
我們不要太
除了在意一個技術
現在的表現
往往我們要在意的是
它的上限在哪裡
一個現在表現差的方法
並不代表它日後沒有希望
現在表現好的方法
也不代表它日後一直能發展下去
你要看的是它的上限
一個表現差的方法
但如果它上限很高
也許它是一個有潛力的方法
一個現在看起來非常主流的方法
也許它的發展已經走到極限
人們現在做的事情都只是加一些
無關緊要的奇技淫巧
這個發展其實也是到了極限
也許它就不再是一個
值得再去深入研究的技術
所以這又讓我想到人類的演化
大家知道說哺乳類
並不是在恐龍滅絕後才出現
其實哺乳類也是跟恐龍
一起生活在中生代的時候
只是那時候的哺乳類
都是像老鼠一樣
其中一個比較知名的是摩根齒獸
這個人類祖先
大概就是長得像老鼠這個樣子
牠們生活在中生代的時候
牠們生活在恐龍的陰影之下
但是這些小動物
有一些恐龍沒有的優勢
比如說牠們是恆溫動物
比如說牠們會哺乳
就這樣牠們挺過了
白堊紀末大滅絕
最後變成了現代智人
很多技術也是一樣
有一些技術現在看起來平平無奇
但也許它有其他人沒有的潛力
最終它可以成為獨霸一方的技術
好那我們接下來就來講語音語言模型的故事
那我們從序章開始講起
我們先講沒有語音語言模型這個概念的時代
那個時候我們做了哪些為語音語言模型存在
所做的準備
那一開始我並不知道會有語音語言模型的出現
那我們要先從我的博士論文開始講起
那我是 2012 年的時候取得博士學位
那個時候我的博士論文做的呢
是一個叫做 Spoken Content Retrieval 的技術
也就是要搜尋大量的聲音訊號
你可以想像說在 YouTube 上有非常大量的聲音訊號
我們有沒有辦法直接搜尋聲音訊號的內容
那後來畢業之後呢
我去了中研院一年去 MIT 一年
然後在 2014 年的時候呢
回到台大任教
那其實跟多數新老師一樣
多數新老師呢
你剛開始任教的時候
往往你做的題目都是你博士論文的延續
這是你往往覺得你最容易繼續進行的研究題目
那我當時也是繼續延續呢
我在博士論文做的 Spoken Content Retrieval 的題目
那那個時候我在想的一個問題是這個樣子的
那個時候做 Spoken Content Retrieval
除了用文字來搜尋語音之外
也可以用語音來搜尋語音
那那時候你期待說呢
你就跟機器說一個詞彙
比如說說個 ICASSP
那你不知道 ICASSP 是什麼也沒有關係
總之它就是一個國際會議
那你就說了一個詞彙
那這個詞彙呢
它甚至可能語音辨識
沒有辦法辨識正確
比如說 ICASSP
它這個詞彙可能根本不在語音辨識系統詞典裡面
語音辨識系統根本沒有機會辨識正確
但是我們直接在聲音訊號的層級上面
把輸入的人說的這個問題
跟你資料庫裡面的聲音訊號
直接在訊號層級上面進行比對
看能不能夠把這個關鍵字
有出現的地方
把它找出來
那當時在這樣的技術之下呢
在這樣的框架之下
遇到的一個難點就是
如果我們要在訊號上面
我們要比對兩段聲音訊號的相似度
是非常花時間的
那時候會有一個演算法
叫做 DTW
那這是一個非常
這是一個耗費算力
耗費時間的演算法
當你的資料庫非常大的時候
DTW 很難讓你做到 Real-time 的搜尋
很難讓你做到
你一輸入文章
它就馬上告訴你
你一輸入你的語音的問題
它就告訴你這個問題
出現在整個音檔的哪些地方
所以我們需要更好的方式
來做語音的搜尋
那所以當時我有一個這樣子的想法
這個想法是這樣子的
我們先把資料庫裡面的聲音訊號
切成一個一個小段
比如說一個詞彙就切成一段
那每一個小段的聲音訊號呢
我們用一個 Audio Word Vector 的方式
把這一段聲音訊號呢
變成一個固定 Dimension 的向量
所以一段聲音訊號本來非常的複雜
它甚至長短是不一的
但我們就把它變成一個向量
那今天在做搜尋的時候
有人問了一個問題
他輸入一個 Spoken Query
那這個 Query 也透過 Audio Word Vector 的技術
把它變成一個向量
接下來我們比較這個 Spoken Query 的向量
跟我們資料庫裡面
每小段聲音訊號向量之間的相似度
就可以得到搜尋的結果
那比對向量跟向量之間的相似度
是可以做很快的
如果你是比對兩段聲音訊號之間的相似度
不同的聲音訊號
不同的這個 Audio Segment
它們長度甚至是不一樣的呢
你需要用 DTW 那個演算法
但是如果是比較兩個向量之間的相似度
你有太多太多可以加速的方法
你可以做到非常的快
你可以做到不管你的資料集有多大
都可以在 Constant 的 Time 裡面呢
用 Approximate 的方式
找出最近的那一個向量
所以把一段聲音訊號變成一個向量
有很大的好處
但再來問題是
怎麼把一段聲音訊號變成一個向量
所以那個時候呢
我就想了一個叫做 Audio Word Vector 的技術
那當時呢
實作這個 Network 的同學呢
是一位專題生
他叫做鍾毓安
那這個鍾毓安同學呢
他實作這個 Audio Word Vector
長的是這個樣子的
那那個時候還沒有 Transformer
所以我們用的都是 LSTM
一個 LSTM
那時候只用一層
一層 LSTM
把一段聲音訊號讀進去以後
LSTM 跟 Transformer 一樣
擅長處理一個 Sequence
LSTM
把聲音訊號讀進去之後
它輸出一個向量
它輸出這個向量之後呢
那還有另外一個 LSTM
這個 LSTM 呢
會把這個向量作為輸入
試圖還原原來的聲音訊號
長什麼樣子
那這個 LSTM 跟這個 LSTM
他們是一起共同訓練的
他們訓練的目標就是希望輸入跟輸出
越接近越好
那這個跟我們上週講到的
Tokenization 和 Detokenization 的觀念
可以說幾乎是一模一樣的
只是在那個年代
這篇論文呢
是發表在 2016 年的 Interspeech
也就是寒武紀時代的論文
在寒武紀的時代
在語音上就我所知
還沒有人做過類似的事情
過去也有一些其他嘗試
想辦法把一段聲音訊號變成一個向量
不過他們都是 Supervised 的
都是有用 Label 的資料的
那在這篇文章之前
應該是沒有人嘗試過
Unlabeled 的 Unsupervised 的方式
總之 Audio Word Vector
我們就訓練這個 Audio Word Vector
可以把每段聲音訊號
都變成一個向量
這樣就可以加快搜尋的速度
但在這邊呢
就遇到一個問題
這個問題
每次我講到這個技術的時候
在 Conference 報告的時候
別人就會問我這個問題
這邊你好像是假設
聲音先變切成一段一段的
那怎麼把聲音切成一段一段的呢
尤其是你又說
期待說每段聲音
它裡面就是有一個固定的單位
最好是每段聲音就是一個 Word
那做不到每段聲音是一個詞彙的話
那至少每段聲音要對應到那個 Phoneme
也對應到一個音標
但是一段聲音訊號裡面
它是沒有這一種具體的 Boundary 存在的
不像一段文字裡面有標點符號
逗號句號告訴你說句子到哪裡
句子跟句子之間的分隔在哪裡
一段聲音訊號裡面詞彙和詞彙之間
是沒有分隔的
那你要怎麼自動的把一段聲音訊號
切成一小段一小段的呢
我們把這一小段一小段叫做 Audio Segment
你怎麼把一段聲音訊號抽成
切成一個一個 Audio Segment 呢
那在 2016 年寒武紀的實戰家問我這個問題
我都說先別問我這個問題
我來想一想
過幾年我就解決這個問題了
然後後來呢
我們就是做了很多一系列的研究
想要自動的從一段聲音訊號裡面
找出詞彙跟詞彙之間的 Boundary
而且不能用語音辨識
因為我們要做的就是 Unsupervised 的
所以必須要有一個在沒有任何標註資料情況下
機器閱讀了大量的聽大量的聲音訊號以後
自動知道什麼樣的聲音訊號合起來
是一個常見的單位
什麼樣的聲音訊號合起來
是一個音標
什麼樣的聲音訊號合起來
算是一個詞彙
那時候做了非常多的研究
那這邊呢
這張圖是來自 2017 年
這個王育軒同學發表的一篇論文
那裡面是拿一個用 Unsupervised 的方法訓練出來的
LSTM 來從它的這個 Network 裡面呢
它有很多 Gate
從 Gate 的運作去找出 Boundary 在哪裡
那其實這個 Network 的訓練呢
有點像是語言模型
因為這個 Network 的訓練就是輸入聲音訊號
它去 Predict 未來聲音訊號
或是 Predict 被蓋住的聲音訊號
好
那這個是 2017 年的時候
後來啊
我就想到了一個新的方法
這個方法可以把分一個一個 Segment
跟把 Segment 變成一個向量
合在一起做訓練
那當時呢
就取了一個蠻普通的名字
叫做 Segmental Audio Word Vector
原來的 Audio Word Vector 上面呢
加上 Segmental 的概念
那這個概念是怎麼樣的呢
我們先來舉一個例子
假設我給你這樣一串數字
叫你說把這串數字看完以後背起來
你可能覺得有點困難
那如果我現在呢
幫你做一些分割
但如果分割分割的不好
你還是會覺得非常難記
比如說我們把這些數字三個一數
第一組是 314
第二組是 159
第三組是 271
第三組是 813
每組都是不一樣的
要你記起來
你也會覺得非常難記
但如果我們找一個比較好的 Boundary
我們說這一段啊
這一串數字啊
表面上看起來很複雜
其實它背後有非常明顯的規律
它背後其實只有兩個 Pattern
314159 跟 2718
它背後其實只有這兩組數字而已
所以如果你好好的對它做分割
你分割在這四個位置
你一眼就可以看出來說
這一段數字一點都不複雜
它就是把某一個 Pattern
314159
我們把它叫做 A Pattern
重複三次
然後 B Pattern
我們把 2718 稱之為 B Pattern
B Pattern 重複兩次
就成為這個 Sequence
你要記這個 Sequence
你就記
ABABA
那你要記得說
A 是 314159
B 是 2718
你就可以把這個 Sequence
把它背起來
所以 Segmental Audio Word Vector 用的
也是一樣的概念
那我們來看
Segmental Audio Word Vector
它是怎麼運作的
它運作方法是這樣的
後來這邊 Paper 呢
是在 18 年的時候發表的
那也是王育軒同學做的
當時的想法是說
如果我們有一個 LSTM 的 Encoder
它每次讀一小段聲音訊號進來
那這邊的所謂的一小段
這每一個方格指得是一個 Frame
因為在那個年代
我們還不會直接把
最原始的那些聲音的取樣點
直接丟給模型
那時候模型處理
不要有那種東西
所以通常是用個訊號處理的方法
把一小段聲音訊號
通常是 0.01 秒變成一個向量
那這邊就沒有用到任何
人裡沒有 Train 任何東西
用一個信號處理的方法
把 0.01 秒變成一個向量
然後再把這個向量丟到模型裡面
所以這邊每一個灰色方格
指的是一個代表 0.01 秒的向量
那有一個 LSTM 的 Encoder
它把這個代表聲音訊號的向量
一個一個吃進去
一個一個吃進去
如果到某一個地方
它覺得說讀到這邊
這應該是一個常常出現的 Pattern
這樣子類型的
這樣長這個樣子的聲音訊號
在整個 Dataset 裡面常常出現
它就說這是一個 Audio Segment
那到 Audio Segment 的一個結束的地方
它就會輸出一個向量
這個向量代表這個 Audio Segment 裡面的資訊
還有輸出一個 Embedding
輸出一個向量
代表這個 Audio Segment
那接下來這個 LSTM 呢
就會把它剛才記得的事情
清空
那那時候用的是 LSTM
不是 Transformer
如果今天用 Transformer 的話
你就是用個 Masking
把那個 Attention 蓋起來
不然當時用的是 LSTM
所以把它 Memory 清空
然後再重新讀這些聲音訊號
再把一個一個 Frame 讀進去
那如果讀到某個地方
覺得是一個 Segment
那就輸出一個向量
然後再清空資料
再繼續讀
讀到這個地方
覺得是一個 Segment
就再輸出一個 Embedding
輸出一個 Representation
輸出一個 Vector
然後清空資料
好
所以如果你是用這個方法
機器一方面自己決定
什麼地方是一個 Audio Segment
一方面同時又自己決定
一個 Audio Segment
要怎麼用一個 Vector 來描述它
但接下來的難點是
這個東西要怎麼訓練呢
這個訓練的方法是
你有另外一個 LSTM 的 Decoder
這個 LSTM 的 Decoder
每次只讀
一個 Representation
一個 Vector 進來
它會把一個紅色 Vector 讀進來
然後呢
它就試圖去還原
這個紅色的 Vector
原來對應的那個 Audio Segment
長什麼樣子
所以讀到這個 Vector
它就要還原這一段
讀這個 Vector
就要還原這一段
讀這個 Vector
就要還原這一段
那我們目標就是讓
輸入的整句話
跟輸出的整句話
結果越接近越好
那在訓練的時候
怎麼把一個段落
變成一個向量
以及一個段落
哪邊算是一個段落
通通由模型自己決定
這是 End-to-End
學習的一個 Network 架構
這個就是 Segmental Audio
Word Vector
那有了這個 Segmental Audio
Word Vector 以後
我們就可以一邊做 Segment
一邊把 Segment 變成 Vector
怎麼做 Segment
跟怎麼做 Vector
這兩件事情
變成是一起訓練的
好那其實啊
真正在訓練的時候
如果你只有讓輸入跟輸出
越接近越好
這樣是不夠的
因為你想想看
假設我們唯一訓練的目標
就是輸入跟輸出
越接近越好
對 Network 來說
它有一個 Hacking 的解法就是
我就越常輸入 Vector 越好
因為現在什麼樣的聲音訓練
算是一個 Pattern
算是一個 Segment
這是 Network 自己決定的
為了讓還原的越像
跟原來的輸入越像越好
最好是每一個 Frame
都說它是一個 Segment
那這樣子 Network 要記的東西最少
它還原會最容易
所以如果我們只考慮
Input 跟 Output 輸入的句子
要被還原回輸出的句子的話
那這樣子訓練起來
結果是不行的
所以你要有一個額外的限制是
除了一方面要輸入跟輸出越像越好
另外一方面
機器選擇的這個斷點的數目
Audio Segment 的數目
要越少越好
所以機器要自己學會說
什麼樣的聲音訓練合起來
是一個好記的 Pattern
它要用最少量的 Audio Segment
去 Segment 一段聲音訓練
然後把聲音訓練變成向量
然後把它還原回來
所以 LSTM 在學習的時候
它是有兩個目標
一方面它要還原回來
另外一方面它要用越少的向量
來表示一段聲音訓練越好
所以這個就是 Segmental Audio
Word Vector 的精神
那總之有了這個技術以後
我們就可以輸入一段聲音訓練
然後這段聲音訓練會被分割
然後變成一個一個的向量
每段向量代表了一個小的 Audio Segment
而這個訓練的過程是 Unsupervised 的
是沒有用到標註資料的
那我們除了把這個方法用在搜尋上以外
有一天我就突然想到了一個新的想法
這個想法是我們能不能夠直接拿這些向量
來做語音辨識呢
而且是 Unsupervised 的語音辨識
因為假設這些向量
它其實跟比如說音標的資訊很接近
因為我們發現說這個 Audio Word Vector
它是可以切得出音標層級的 Segment 的
那假設這邊每個 Segment 跟音標很接近
然後我們又把一段音標裡面
代表一段音標的 Audio Segment 裡面的聲音訓練
變成一個 Vector
那也許我們能夠直接把這些 Vector 就變成音標
然後變成音標之後
你可能後面再掏一個 Lexicon
再掏一個語音模型
你等於也就是做到語音辨識了
而且我們希望把這些 Vector 變成音標的過程
它是 Unsupervised 的
那怎麼做到 Unsupervised 呢
當時的想像是這樣子的
我們有一個小模型
這個模型可以非常非常的簡單
它也許就是一個 Linear 的 Model
它把這些向量讀進去
把這些向量轉成 Phoneme 的 Sequence
那怎麼訓練這個小模型呢
我們用的是 GAN 的想法
Generative Adversarial Network 的想法
就假設我們另外有一大堆文字
這些文字跟這些語音
不需要是成對的關係
就聲音訊號是一邊
那文字是從別的地方收集來的
是跟這些聲音訊號無關的文字
那把這些文字呢
也轉成 Phoneme 的 Sequence
你就查一個字典
就可以把這一串文
把每一個詞彙呢
對應到它
把每一個詞彙轉成它對應的 Phoneme
所以一句話就可以變成一個 Phoneme 的 Sequence
就會把文字呢
轉成一堆 Phoneme 的 Sequence
接下來呢
你就訓練一個 Discriminator
這個 Discriminator 呢
它的工作就是分辨一段 Phoneme Sequence
它是有一個語音辨識系統產生的
還是它是真正的 Phoneme Sequence
然後這個 Generator 呢
就要想辦法去騙過這個 Discriminator
然後這個 Discriminator 跟這個 Generator
它們是 Iterative 交替訓練的
這個就是 Generative Adversarial Network
GAN 的概念
而整個訓練過程是 Unsupervised 的
這整個訓練過程
不需要任何的標註資料
我們這邊完全不需要用到語音的標註
因為一般做語音辨識訓練的時候
你需要聲音跟它對應的文字
但是這邊我們完全不需要聲音
跟它對應的文字
就可以教模型
把聲音訊號變成 Phoneme Sequence
也就是做到語音辨識
後來我們把這個結果呢
發表在 2018 年的 Interspeech
如果說 2016 年是寒武紀的話
那 2018 年我們可以說
我們進入了奧陶紀
在這個遠古的時代
在這個地球上多數的生物
還都不知道在做什麼的時代
那個時候有了 Unsupervised 的語音辨識
那這個應該是世界上第一個成功
做出 Unsupervised 的語音辨識的結果
那在 2018 年的時候可以做到什麼樣的地步呢
在 2018 年的時候同樣是做在那個 TIMIT 上面
我們這邊 report 的是 Phone Error Rate
可以做到 60 幾 % 的 Phone Error Rate
注意一下這是錯誤率
這不是正確率
所以 60 幾 % 錯誤率代表錯的還比對的多
但是我們第一次看到這個結果的時候
已經覺得非常的令人振奮了
為什麼覺得非常令人振奮呢
因為機器在完全没有任何 Labeled 資料的情況下
它是包含學到東西的
60 幾 % 的錯誤率不是隨機猜的
隨機猜是猜不到 60 幾 % 的錯誤率的
機器還是有學到東西的
後來再過了幾個月以後
騰訊也發表了 Unsupervised 的語音辨識
那可以做到 40 幾 % 的錯誤率
那後來我們實驗室的陳冠宇同學
把錯誤率壓到了 30 幾 %
那 30 幾 % 的錯誤率到底是什麼樣的等級呢
我就回顧了一下歷史
這個是 TIMIT 歷年來可以得到的
語音辨識的正確率
那 TIMIT 呢是一個非常非常古老的 Corpus
它在 90 年代以前就已經存在了
那這個縱軸呢是辨識的這個正確率
那如果是 30 幾 % 的錯誤率
就對應到 60 幾 % 的正確率
60 幾 % 的正確率
在歷史上大概是什麼等級呢
大概是人們剛開始用
Hidden Markov Model 的時候的等級
但是當時我已經覺得非常的厲害了
因為想想這些模型
它們都是用有標註的資料訓練的
但是我們在完全沒有標註資料情況下
已經跟最差的有標註的模型
也就是最差的 Supervised Model
可以做到差不多的結果
已經跟這個三四十年前 Supervised Model
Unsupervised 模型
已經可以跟三四十年前 Supervised 模型
做出差不多的結果
在 19 年的時候
我們已經覺得非常的厲害
我那時候甚至覺得說
這可能就是 Unsupervised Learning 的極限
畢竟沒有任何人教它
它是無師自通的
也許這就是極限了
也許我有生之年
都不會再看到更多的進步了
好那歷史呢
仍然不斷的推進
後來在 2018 年的時候
在 NLP 的領域呢
同時有了像 BERT 這樣
早年的文字模型
那時候我就在想說
那在語音上
我們能不能夠做一個
語音版的 BERT 呢
所以這個劉廷偉同學呢
就做了一個語音版的 BERT
那那個時候
我們的運算資源非常的少
它是用一張 1080
然後訓練一週
訓練一個語音版的 BERT
那跟這個文字的 BERT 的概念呢
是非常的類似的
我們的做法就是
拿一段聲音訊號來
把這段聲音訊號的其中一些地方
蓋住
然後蓋住之後呢
你就去訓練一個Transformer
然後呢 這個Transformer
把聲音訊號當作輸入
那這可能是第一個
Self-Supervised 的 Transformer 的語音模型
因為就我所知在這之前
應該是沒有人用這種 Transformer
拿來訓練這個語音的 Language Model
那總之這是一個 Transformer
它就是輸入一段聲音訊號
它就輸出一排向量
輸出一排 Representation
然後呢
它的工作就是
它要跟後面的一個 Linear 的 Model
這是一個 Linear 的 Head
一個非常小的 Model
做配合
把被蓋住的地方
把它還原回來
那當時我們把這個模型呢
叫做 Mockingjay
為什麼叫做 Mockingjay 呢
跟飢餓遊戲
沒有半毛錢的關係
Mockingjay 呢
就是學舌鳥
因為我們覺得這個模型
做的事情就是輸入什麼
它就輸出一模一樣的東西
所以它是學舌鳥
那其實這個想法呢
跟文字的 BERT
可以說幾乎是一模一樣的
那可能也非常像是
我們上週跟大家介紹過的 MaskGIT
它是用在影像上的
但 MaskGIT
其實是再更晚的文章
MaskGIT
其實就是仿造 BERT 的想法
那當時我們其實曾經真的嘗試過
那用用 Mockingjay
看能不能夠讓它合成聲音
因為除了訓練出一個
這種 Representation Model
可以輸入聲音訊號
輸出向量
你其實也有機會
拿這樣的模型來產生聲音
你就給它一段聲音訊號
中間蓋起來一段
然後看它能不能把那段
蓋起來的東西還原
那當時我們嘗試的結果是
完全沒辦法
它還原出來的東西
根本就是聽不懂的
但是除了 Mockingjay
它還是有一些地方
跟文字模型是不一樣的
它在某一些地方
是根據語音的特性所設計的
它有什麼地方根據語音的特性
所設計呢
比如說在做 Masking 的時候
一般在做訓練這個文字的 BERT 的時候
就是一次 Mask 一個文字 Token
但在語音上
如果你只一次 Mask 一個 frame
那效果會非常的差
因為一個 frame
它代表的聲音訊號非常的小
對模型來說
你只我只蓋住一個 frame
它基本上把兩邊的 frame 做內插
它往往就可以把蓋住的那個 frame 猜出來
所以如果你只是每次只蓋掉一個 frame
其實模型學不到什麼東西
所以會發現說
如果在做 Masking 的時候
一定要做 Consecutive 的 Masking
一定要一次 Mask 三到九個 frame
我們才能夠得到比較好的結果
那另外在 Masking 的 Strategy 上
也有一些突破
比如說我們發現說
在做 Masking 的時候
不一定是要在時間方向上做 Masking
一般在文字上的時候
都是在時間方向上做 Masking
但我們發現
如果你在這個頻帶的方向上做 Masking
你可能可以在某些任務上面
有更好的結果
我們發現在頻帶方向上做 Masking
對於模型學到語者相關的任務
是特別有幫助的
所以後來 Mockingjay
就有一個進階的版本
叫做 TERA
就是把 Masking 頻帶這件事情
考慮進去
後來 TERA 是發表在我們語音領域
最好的期刊
那個 TASLP 上
它其實是當年 TASLP 下載次數前 25 名的文章
所以是蠻多人看這篇文章的
其實在差不多的時間點
鍾育安同學
他後來去 MIT 唸博士
在 MIT 也訓練了語音版的 GPT
跟文字的 GPT 的概念是非常的像的
就是有一段聲音訊號
你把它前半段拿出來
那這個模型它訓練的目標
就是預測接下來的聲音訊號
它會預測接下來的 frame
但是語音跟文字還是有些不一樣的地方的
這個語音版的 GPT
它不是直接預測下一個 frame
如果在文字上也就是直接預測下一個 Token
但是語音版的 GPT
直接預測下一個 frame
結果是不好的
如果你看鍾語安的那篇論文的話
你要預測更久遠以後
比如說至少三個 frame 以外的 frame
你才會有比較好的結果
那原理跟 Mockingjay 剛才說的
要做 Consecutive 的 Masking
道理是一樣的
如果只預測下一個 frame
對模型來說太容易了
所以要求它預測更久遠以後的 frame
那當時那個模型呢
叫做 APC
就是 Auto-Regressive Predictive Coding 的縮寫
那當年應該是沒有人真的拿這個模型
來嘗試讓它合成聲音出來
那我可以預期的是
在那個年代用的資料量
還有那個年代所使用的技術
可能不會得到特別好的結果
我想其中一個原因就是
像這種模型
它都是直接 Predict
Continuous 的 Representation
它都直接生成 Continuous Representation
那記不記得
我們上一次上課的時候有講過
直接生成 Continuous Representation
不加 Diffusion
或者是 Flow Matching 的 Head
是做不好的
而在 19 年在奧陶紀的時候
那時候人類呢
還沒有要加 Diffusion Head
或者 Flow Matching Head 的概念
所以那個年代
就算有人直接想要 Train 一個
可以生成 Continuous Representation 的模型
可能也不會得到好的結果
那在 Meta 那邊呢
也做了非常多的這種
類似的語音語言模型
那它們的目標都是輸入一段聲音訊號
產生 Representation
並不是真的要把聲音訊號所輸出出來
那有個兩個特別有代表性的研究成果
一個叫 wav2vec
那 wav2vec 的第一代呢
其實是在 19 年的年初就發表了
所以其實是比 Mockingjay
還有 APC 呢都還要早一些
那它有第二代
第二代是在 20 年的時候發表的
那後來有另外一個 Performance
更好一點的模型叫做 HuBERT
然後它是在 21 年的時候發表的
那當時這些模型呢
它們都叫做 Representation Model
我們那時候甚至不會叫它是一個生成模型
雖然有時候我們也說它是語言模型
但是我們並不會真的拿它來做
生成這件事情
那當時這一些模型被訓練出來的用意是什麼呢
它被訓練出來是為了要得到更好的語音的 Representation
那這些 Representation Model 做的事情
就是給它一段聲音訊號
它把這些聲音訊號呢
轉成一個一個的向量
然後接下來就看你想要做什麼樣的任務
那多數人想要做的都是語音辨識
所以你就會訓練一個 Downstream 的模型
這個 Downstream 的模型
它的工作就是做語音辨識
那訓練的時候呢
如果你有用標註資料的話
它就 Supervised 的
那這個 Downstream 的模型
就是吃這些 Representation
這些向量作為輸入
把這些輸入轉成語音辨識的結果
那有了這個 Representation 的 Model 以後
語音辨識的表現得到了非常顯著的突破
那這邊引用的是在這個 LibriSpeech
那這是大家現在非常常用的一個語音的 Benchmark 上面
所展示的 Word Error Rate
所以這個數值是越小越好
那在這個括號裡面的這三個結果呢
是用 100 個小時的 Labeled Data
去訓練這個 Downstream Model 的結果
那如果你今天沒有用這個 Representation Model
你直接訓練一個模型
這個模型有六層的 LSTM
那你只能夠得到 5% 將近 6% 的錯誤率
但如果有這個 Representation Model
那你的 Downstream Model 只需要兩層 LSTM
就可以得到還不錯的結果
你可以得到 3% 或者是更低的語音辨識的錯誤率
甚至假設你把你的訓練資料減少到只有 10 分鐘
10 分鐘的 Labeled Data 這個非常少
大家說英語都有 30 分鐘了
你現在只聽了三分之一的大家說英語
就能夠學會語音辨識嗎
就會學會聽寫把聲音轉成文字嗎
太厲害了
所以只用 10 分鐘的標註資料
如果你用 wav2vec 2.0 或者是 HuBERT
來當作 Representation Model 的話
你甚至可以得到 4% 到 5% 的錯誤率
比用 6 層的 LSTM
100 個小時的結果還要更好
那這個結果就是顯示說
這些 Representation Model 它非常的有用
它出出來的這些 Representation
非常適合拿來做語音辨識
對語音辨識有非常大的幫助
那其實在同一時間
當有 wav2vec 2.0 出來的時候
它的表現真的非常的驚人
那時候我當然就非常直覺的想說
那我們把 Segmental Audio Word2Vec
換成更強的 wav2vec 2.0
看看能不能夠再把
Unsupervised 的語音辨識
再提升一個等級
那對於 Unsupervised Learning Game 的部分
其實是一模一樣的
我們只是把 Backbone 這個產生 Representation 的 Model
換成一個更好的 Representation Model 而已
看看能不能夠得到最好的結果
那當時我們的做法就是
把 wav2vec 2.0 的最後一層拿出來
當作 Representation
然後去訓練 Unsupervised 的 ASR
結果怎麼樣呢
結果非常差
還不如 Segmental Audio Word2Vec 的結果呢
結果非常差
所以那時候覺得
這個 wav2vec 2.0 沒什麼幫助
感覺應該是過譽了
但當時我們不知道呢
我們離下一個 Unsupervised ASR 的突破
其實只有一步之遙而已
好
那當時 wav2vec 2.0 呢
對 Unsupervised ASR 沒啥幫助
所以我們就把這個 effort 呢
轉到另外一個方向上
當時我想了另外一個問題
當時我的問題是這個樣子的
當時已經有非常多的
好的語音的 Representation Model
這些模型通常會告訴你
它在語音辨識上面表現非常的好
那麼剛才你看到 wav2vec 2.0
HuBERT 等等
在語音辨識上都有非常好的表現
那我當時就想要問一個問題
語音相關的任務
不是只有語音辨識啊
我們剛才一開始就跟大家講過說
語音有很多不同的資訊
所以把聲音轉成文字
這是語音辨識
但是語音裡面還有語者的資訊
還有情緒的資訊你可以做語者辨識
可以做情緒辨識
這一些 Representation Model
用在其他任務上
它們能得到好的結果嗎
我們有沒有可能有一個 Representation Model
它是一個 Universal 的 Model
它是一個通用模型
你幫它加一個做語音辨識的 Downstream Model
它就做語音辨識
插另外一個拿來做語者辨識的 Downstream Model
它就做語者辨識
你只需要去訓練這個跟 Application 有關的 Head
今天你要做什麼
就把 Head 插上去
你就可以拿這個模型做各式各樣的事情
那我當時有個比喻說
這種 Universal 的 Speech Representation Model
它就像是一個作業系統
你你有一個好的作業系統
就可以加各式各樣的 Application
你就可以加各式各樣的應用
就可以拿這個作業系統來做各式各樣的事情
真的有這樣的 Speech Representation Model 嗎
如果回溯到 2021 年
那時候大家都在開發 Speech Representation Model 的時期
我會告訴你說
我並不覺得這些 Speech Representation Model
它可以是 Universal
但今天大家都知道在 Speech Representation Model
它可以是 Universal 的
今天大家都已經期待一個模型
一個 Foundation Model
它是 Universal 的
它是多工的
它是全能的
但是回溯到四五年前
那時候人類並沒有這樣的期待
那甚至我當時覺得
應該不太可能輕易的出現
一個 Universal 的 Speech Representation Model
因為不同的語音任務
它往往需要的資訊
是天差地遠的
比如如果你看語音辨識
語音辨識需要做的是什麼
把文字的資訊抽出來
機器要學會
無視語者的資訊
只考慮文字的資訊
但如果語者辨識呢
機器要知道說
兩個人說同一句話
那是不同的人
同一個人說兩句不同的話
那是同個人
機器要無視內容的資訊
只抽出語者的資訊
所以這兩個任務
根本就是互斥的
所以很難想像
有某一個模型
可以同時把兩個任務做好
但無論如何
當時我想要知道
有沒有模型
它可以是 Universal 的
我們找看看
有沒有 Universal 模型
或知道每一個模型
擅長什麼樣的事情
藉此我們可以開發
Universal 的模型
所以當時呢
我就組織了一個團隊
那我們把這個計劃呢
命名為 SUPERB
它是 Speech Processing
Universal Performance Benchmark 的縮寫
那這個團隊的第一作者呢
我們的 Leader
是楊書文同學
帶領我們完成了這個 Project
那在這個 Project 裡面呢
有很多知名的 Senior 的 Member
那像李尚文
跟 Abdelrahman
他們是 Meta 的研究人員
這邊這個 Abdelrahman
就是剛才我們前面看到的
Hinton 的學生
然後還有 Shinji
這個 CMU 的非常知名的語音的學者
然後我們組成了一個團隊
一起完成了這個巨大的任務
那 SUPERB 要做的事情就是
把當時所有我們可以找到的
語音的 Representation Model
用在各式各樣
我們覺得重要的語音相關的任務上
然後看看它們在哪些任務上表現好
哪些任務上面表現不好
那我們剛開始的時候呢
我們都是拿每一個模型的最後一層
出來做事情
那我們發現說
當我們把每一層
每個模型的最後一層拿出來
丟給 Downstream Model 的時候
往往在很多任務上
模型是沒有好的表現的
尤其是在這種語者相關的任務上
比如說 Speaker Verification
語者驗證上
多數模型
它沒有辦法有好的表現
還不如不用 Representation Model
所以當時我第一個得到的結論就是
這個跟我料想中的一樣
果然這些模型就是很能做語音辨識
像 HuBERT 啊
wav2vec
它們就是很能做語音辨識
很能做語音辨識
往往就意味著
你可能不太能夠做
跟語音辨識互斥的任務
比如說語者辨識
但當時我們還是觀察到一些
比較離奇的現象
比如說 wav2vec
跟 HuBERT 都有 Large 的版本
Large 的 Model
表現得都非常非常的差
比 Small 差
甚至也比 Random 的結果差
那時候就覺得
好像有什麼怪怪的地方
後來我們就做了一個
更深入的分析
發現說這些 Self-Supervised Model
它每一層往往可以抽非常不一樣的資訊
在這個圖上橫軸指的是這一些 Representation Model 的 Layer
那從第一層到第二十四層
那縱軸呢指的是
如果你拿那一層出來
做某一個任務上面
你可以得到的表現
那數值呢
都是越大越好
那我們這邊呢
紅色的線代表的是
Phoneme Recognition
也就是語音辨識的表現
藍色的線代表的是
語者辨識的表現
那這邊呢
有虛線跟實線
那這個虛線呢
是 wav2vec 這個模型
實線呢
是 HuBERT 這個模型
從這個圖上
你會發現說
如果我們看
語音辨識這件事的話
這些模型的前幾層
都是不擅長做語音辨識的
它們要到後幾層
Representation 抽出來
才比較有 Content 的資訊
可以做語音辨識
而 Speaker 呢
Speaker 在前面幾層
就能夠抽出 Speaker 的資訊
而到後面幾層
Speaker 的資訊
反而是變少的
所以如果你要做
Speaker 相關的任務
其實前幾層
是比較合適的
而至於 wav2vec 2.0
它是一個神奇的模型
這個模型
在最後幾層
啥都幹不了
最後幾層
因為它本身訓練的
機制的關係
那細節我們就不講
它其實最後幾層
是沒什麼資訊的
所以難怪之前
做 Unsupervised ASR 的時候
只拿最後一層
只能得到很差的結果
好
那既然我們現在知道說
這些 Representation Model
不同層
有不同的資訊
那我們如何應用
不同層的不同資訊呢
我們怎麼知道
哪一個任務
應該要用
哪一層出來做事呢
所以當時呢
我們就用了一個
Weighted Sum 的方法
這個 Weighted Sum 的方法是
我們就全都要
把每一層的 Representation
通通拿出來
但是我們不知道
哪一層要用的比較多啊
所以每一層呢
把它乘上一個權重
把每一層的 Representation
乘上一個權重
這些權重
是 Downstream Model 的
參數的一部分
所以在訓練 Downstream Model 的時候
你也會順便訓練這個權重
等於是用 Downstream Model 來決定
它要用哪幾層
如果是語音辨識的話
那模型可能就會自動學到
用最後幾層
如果是語者辨識
模型就會自動學到
要用前面幾層
那這個 Weighted Sum 的方法呢
是非常有效的
好 那我們可以來看一下
那個 SUPERB 這個 Project
得到的結果
那我們有一個非常巨大的表格
這個表格的每一個橫軸啊
代表的是某一個模型
所以這邊有很多很多很多
不同的模型
那這邊的顏色呢
代表模型的表現
顏色越深
代表表現越好
那每一個 Column 指的是一個任務
所以這些模型都被用在
非常多不同的任務上面
好 那這個表格太大了啦
所以沒辦法把它放到
一頁投影片裡面
所以這是完整表格的樣子
好 那你就會發現說呢
有一些最強的模型
比如說 WavLM 啊
data2vec 啊
HuBERT 啊
wav2vec 2.0 等等
它們往往是在多數任務上面
都可以表現不錯的
這邊每一個 Column
代表是某一個任務
那最下面這個 Row 啊
這個 FBank 代表
沒有用 Representation 的結果
那從這個實驗結果裡面
可以看出說
當你用了 Representation Model
它其實可以在所有的任務上面
都幫助我們做得更好
這些 Representation 的 Model
加上 Weighted Sum 以後
它們確實可以是十項全能的
那我覺得 SUPERB 的出現
其實它有一個非常重要的意義
就是告訴大家這些 Self-supervised 的 Model
它非常的厲害
它不是只能做語音辨識
它有機會是一個通用的模型
那所以 SUPERB 這篇 Paper 呢
它得到了蠻高的 Citation
它應該是 Interspeech 這個 Conference
就是 Interspeech 呢
是我們語音領域最好的 Conference 之一
那 Interspeech 有統計近五年來
每一篇發表在 Interspeech 的 Paper 的 Citation 的次數
SUPERB 那篇 Paper 在近五年來的
所有發表論文裡面
它 Citation 的次數是排名全部第四的
後來呢 SUPERB 就變成了一個宇宙
它變成了一個品牌的名字
如果今天有人做這種 Universal Model
這種通用的語音模型的 Evaluation
往往會冠 SUPERB 這個名字
比如說 AV-SUPERB 就是 Audio-visual 的 SUPERB
比如說 ML-SUPERB 是 Multilingual 的 SUPERB
Dynamic-SUPERB 是一個 SUPERB 的 Dynamic 版
它是做 Crowdsourcing
用 Crowdsourcing 的方法呢
來收集資料
那還有 Indic-SUPERB
那 Indic-SUPERB 呢
從字面上看你就知道
它是印度文的 SUPERB
那我其實並不認識這群作者
不過因為 SUPERB 比較像是一個品牌的名字
所以他們把 SUPERB 呢
放到他們的 Benchmark 裡面
或 TS-SUPERB
它就變成了一個品牌的名字
變成了一個 SUPERB 的宇宙
好
那我們再回到 Unsupervised 的語音辨識來
我們剛才說
我們錯過了 Unsupervised 語音辨識的一次重大突破
後來 Meta 的人也用了 wav2vec
來做 Unsupervised 語音辨識
但跟我們最大的不同是
他們不是拿最後一層
他們是拿第 15 層
當然除了拿第 15 層以外
他們還做了很多的改進
那這個大家再去詳閱他們的論文
他們用了第 15 層以後
這個 Phone Error Rate
Unsupervised Learning 的 Phone Error Rate
突然暴跌
他們可以做到大概 15% 左右的錯誤率
那這個 15% 左右的錯誤率是什麼觀念呢
紅色的這條線是 Supervised Learning 的結果
所以可以說 Unsupervised ASR 在 TIMIT 這個 Benchmark 上
可以說是破台了 Unsupervised 語音辨識的結果
幾乎可以做到跟 Supervised Learning 一樣好
我本來以為 Unsupervised 再怎麼做
應該都沒有辦法跟 Supervised 差不多吧
但是在有了好的 Representation Model 以後
居然一舉把 Unsupervised 的語音辨識
做到跟 Supervised 不相上下
那其實呢
Unsupervised 語音辨識最近也是有進展的
比如說有一個模型叫做 wav2vec-U
這是我們實驗室的同學跟孫帥豪啊
老師一起合作的結果
那這邊我們就不是 Report 那個 TIMIT 啦
因為 TIMIT 已經被破台了
所以這邊做的是 LibriSpeech
那用了 100 小時的訓練資料
那如果是 wav2vec 2.0
如果是 wav2vec 加 Unsupervised ASR 的話
得到的是 19% 左右的錯誤率
然後呢
其實後來在幾年之後
我們實驗室也參與了一個 Project 叫 EURO
它也是做 Unsupervised ASR
它可以比 wav2vec-U 呢
做得更好一點
後來 wav2vec 加上 Unsupervised ASR
Meta 的人又做了 2.0 版
正確率又可以做得更高
錯誤率又可以壓得更低
足足可以壓到 10% 左右的錯誤
但是 wav2vec-U 2.0 呢
一舉可以做到 5.4% 左右的錯誤率
這在 LibriSpeech 上面
也是一個非常好的結果
所以 Unsupervised ASR
其實在這幾年還是有突破的
那個 wav2vec-U 2.0 最大的突破
其實是找到了更好的 Segmentation 的方法
這邊是用 Reinforcement Learning 的技術
但是沒有用到人類的介入
是用 Unsupervised 的 Signal
Train 一個找更好的 Boundary 的方法
好
那有了這一些 Representation Model 以後
感覺很多本來做不到的事情
突然之間都能做了
比如說我們實驗室有另外一個長久的夢想
是要做語音的 QA
所謂語音的 QA 就是
你給一個 QA 的模型
一段聲音訊號
就是比如說一段上課的錄影
然後接下來問它一個問題
希望它可以直接給你答案
最早我們實驗室做語音 QA 的 Project
其實是從托福聽力測驗開始做起
托福聽力測驗就是給機器
聽一段文檔
聽一段聲音
問它一個問題
叫它從四個選項裡面選一個選項
這個是曾柏翔同學在 16 年
也就是在寒武紀時代做的
那個時候之所以選托福聽力測驗
是因為我覺得托福聽力測驗
就是四個選項
那個時候還沒有深層的觀念
那時候覺得
什麼直接上機器 Output 一個答案
這根本是不可行的事情
但我覺得四個選項
就把它看作是一個分類問題
有機會直接 Train 一個內容
直接聽這段聲音
看這個問題
然後就預測是 A B C D 四個選項的哪一個
所以我覺得托福聽力測驗
是有可能可以做的
然後那時候做下去
哇還得到了 40 幾 % 的正確率
這個 40 幾 % 的正確率
是比你隨機猜還要好很多的
不過後來我發現 40 幾 % 的正確率
申請不到什麼好學校
應該有 70-80% 的正確率才可以
所以那時候機器跟人類的差距
還是非常巨大的
後來在隨著時間的演進之後
我們就挑戰真正的 QA
也就是讓模型真的回答問題
而不是只是選一個選項而已
那這是李佳軒同學做的
那當時的做法就是
我們還是非常仰賴語音辨識的系統
我們需要先把一段錄音
用語音辨識的模型變成文字
問題也是聲音的
但也要變成文字
那接下來反正現在
聲音都變成文字了
問題也是文字
那你就直接套個文字的模型
就可以回答問題
就結束了
所以在有好的語音辨識系統情況下
能夠做 Speech 的 QA
也就可以做 Listening Comprehension
但是在有了像 HuBERT 這種
Speech Representation 的 Model 以後
我們就可以挑戰更困難的問題
其實當時李佳軒在做的時候
我就說我們能不能挑戰
做個 End-to-End 的模型
敢不敢不用語音辨識
直接 End-to-End 的 Train 一個 Model
他還真的做了
Train 不起來
但是後來有了 Speech Representation 的 Model 以後
我們真的就有機會
做一個 End-to-End 的 Model
直接把聲音訊號
丟給 Speech Representation Model
它輸出 Representation
然後接下來有個 Downstream 的 Model
把 Representation 讀進去
然後直接給我們答案
有機會可以做得起來
做得怎麼樣呢
這個是語音 QA 的結果
當時提出了一個模型叫做 Dual
其實在 Dual 裡面
有很多有趣的巧思
所以不是直接 End-to-End 的
硬 Train 就可以做得起來的
要有很多的方法
才能夠把語音的 QA 做起來
至於方法是什麼
它的細節
我在 2022 年
就還沒有 ChatGPT 的時候
所以是史前時代
在史前時代
曾經給過一個演講
講這樣子的模型
是怎麼被訓練出來的
大家有興趣
再去找這個史前時代的演講來看
好
那它的結果做出來怎麼樣呢
橫軸啊
是語音辨識系統的錯誤率
縱軸呢
是用一個叫做 F1 的 Score 啦
你就想成數字越大
代表模型表現越好
綠色這條線是 Cascade 的模型
也就是一個語音辨識系統
加一個文字的 QA 模型
那你可以想像說
隨著語音辨識系統錯率越來越高
整體的表現
當然是越來越差
但如果你有一個 End-to-End 的模型
它裡面沒有 QA
它就
說錯了
它裡面沒有語音辨識
它就是個 End-to-End 的模型
輸入聲音輸出答案
那它其實就跟語音辨識的錯誤率
沒有什麼關係
當你的語音辨識錯誤率
高過 25% 的時候
End-to-End 的模型
相較於 Cascade 的模型
在當時也是可以佔到一些優勢的
好
那如果你想要更知道
我們到目前為止
都還沒有講到語音語言模型
如果你想要知道
在語音語言模型開始之前
還發生了什麼樣的事情
你可以讀一篇
我跟其他學者一起寫的
Overview Paper
我們從上古時代
寒武紀時代
開始一直講到
史前時代
有 ChatGPT 之前
那時候語音語言模型
有什麼樣的進展
那在當年呢
這個楊書文同學
跟劉廷偉同學呢
還做了一個非常好用的
Toolkit 叫 S3PRL
它可以調用各式各樣的
語音 Representation Model
用在各式各樣的任務上面
那今天如果你還有機會
要用這些語音的
語音的 Representation Model 的話
那推薦你
這個好用的 Toolkit
講了這麼久
其實都還沒有真的講到
語音語言模型
我們甚至都還沒有進入
石器時代
接下來第二部分呢
我們終於要講到
初代的語音語言模型
長什麼樣子
那最早的
可以稱之為語音語言模型
跟我們現在想像的
語音語言模型
可以做生成的這種模型
比較類似的
我想應該是 Meta 的
Generative Spoken Language Model
也就是 GSLM
那它的做法呢
跟你今天可以想像的做法
其實是差不多的
就首先
你要有一個 Tokenizer
可以把聲音訊號
變成 Token
然後你在訓練一個
Autoregressive 的 Model
它跟一般的文字的 Language Model
其實也沒什麼差別
就是輸入 Token
預測下一個 Token
那你要一個 Detokenizer
它可以把 Token
還原聲音訊號
有這三個東西
就結束了
那這個 Tokenizer
怎麼被打造呢
在上一個世代裡面
已經開發了很多
Representation Model
所以在 GSLM 裡面
他們並沒有再花時間
去開發 Tokenizer
他們直接
把 Representation Model
改造成一個 Tokenizer
怎麼改造呢
這些 Representation Model
可以做的事情
就是輸入一段聲音訊號
輸出一堆向量
他們對這些輸出的向量
直接做 Clustering
你可以做 K-Means Clustering
或者是認一個 VQ 的 Layer
你就把比較像的向量
集合起來
就說你們都屬於一個 Token
比如這三個向量很像
就說你們都是 Token 2
這幾個向量很像
我們就說你們都是 Token 3
等等
這就是 GSLM
第一個版本做的事情
那後來也有很多不同的
語音語言模型
那也對 Token 呢
做了一些改進
比如說拿掉
Duplicate 的 Token
或者是說
假設有一些 Token 的 Pattern
比如 3 後面都會接 2
3 後面都會接 2
那你可以把它合起來
當做一個新的 Token
那這招呢
其實就是致敬
在文字上常用的 BPE
Byte Pair Encoding
所以呢
在這個早期啊
在最早的語音語言模型裡面
Tokenizer 是不需要另外訓練的
因為已經有 Representation 的模型
它可以稍微改造一下
就當做一個 Tokenizer
但是你還沒有 Detokenizer 啊
你還要把 Token 變回聲音訊號
怎麼辦呢
這個 Detokenizer 是需要另外訓練的
你就訓練一個模型
這個模型把 Token 當做輸入
它的輸出目標
就是產生這些 Token 的聲音訊號
我們要讓輸入一段聲音訊號
變成 Token 之後
它能夠轉回原來的聲音訊號
那 Detokenizer 是需要訓練的
但是 Tokenizer 通常是不太用訓練
最早 GSLM 是用這樣的做法
但後來也有人採用了
其他的語音的 Token
比如說另外一組很常用的 Token
叫做 Neural Speech Codec
在 Neural Speech Codec 裡面
Tokenizer 跟 Detokenizer
是 Jointly 一起訓練的
它們兩個都是巨大的 Network
輸入一段聲音訊號
Tokenizer 把聲音訊號變成 Token
Token 再通過 Detokenizer
轉回聲音訊號
要讓輸入輸出越像越好
Tokenizer 跟 Detokenizer
是同時一起訓練的
那 Codec 這個字是什麼意思呢
Codec 這個字的 Co
指的是 Compression
也就指的是 Tokenizer
它做的事情等於是把語音
Compress 成 Token
那 Dec 呢
指的是 Decompression
也就是 Detokenizer
來做的事情
Detokenizer 是把 Token 轉回
Decompress 成
解壓縮成原來的聲音訊號
所以 Tokenizer 呢
其實有兩大面向
一大面向是直接從
Speech Representation Model 那邊改造來
這一種 Token
叫做 Semantic Token
那有另外一組呢
是 Tokenizer 跟 Detokenizer
一起訓練
那這樣子的 Token 呢
叫做 Acoustic Token
不過 Semantic Token 的 Acoustic Token
它只是歷史上
某一篇 Paper
其實就是 AudioLM 那篇 Paper 的稱呼
那後來呢
大家都一直沿用這樣子的講法
那事實上
這種 Speech Representation Model
它是沒辦法抽出什麼語意的資訊的
所以說它叫做 Semantic Token
有點言過其實
但總之
就是約定俗成
只要是 Speech Representation Model 來的
你沒有再另外訓練
Tokenizer 的
叫做 Semantic Token
Tokenizer 跟 Detokenizer
一起訓練的叫做 Acoustic Token
那有各式各樣的 Token
那你可能就會問說
那要用哪一種 Token 比較好呢
那就告訴你說
這個小孩子才做選擇
大人是全都要
你可以把所有的 Token 都一次用上
所以像 AudioLM 那篇 Paper 裡面
他們就說
他們在生成的時候
是又會生成 Semantic Token
又會生成 Acoustic Token
他們 Acoustic Token
還有不同的等級
有比較粗的 Token
有比較細的 Token
那同時用多組不同的 Token 以後
下一個會遇到的問題就是
那要用什麼樣的生成策略
來生成不同的 Token 呢
如果本來只有一組 Token
那就是 Autoregressive 的
每次生成下一個 Token 就結束了
如果多組不同的 Token
那誰要先出來
誰要後出來
用什麼樣的方式出來
用什麼樣的順序出來
就變成了一個研究的問題
那這個部分呢
其實我已經有另外一個課程的錄影
曾經討論過這個面向
那如果大家對於
什麼樣的策略
可以拿來生成各式各樣的 Token
有興趣的話
你可以看一下上一個學期
機器學習最後一堂課的錄影
那已經幫你把時間標出來
在第 27 分到第 39 分的地方
我們花了 12 分鐘的時間
講各使用多種不同 Token 的策略
好
但是啊
這個語音語言模型訓練起來
往往沒有辦法得到非常好的結果
那我們實驗室呢
有試著 Fine-tune 一個
已經在上萬小時資料上面
被訓練的語音語言模型
但我們得到的結果
只能是以下這樣
那我強調一下
這些語音語言模型
它做的事情都是 Pre-train
都是預訓練
它們就是拿大量沒有標註的資料
來訓練模型
所以這些模型真正能做的事情
都是只能做語音接龍
也就是一句話
它把那句話幫你講完
它還不能做對話
要做對話
你得做 Fine-tuning 才能做對話
就是記得我們在講語言模型的時候
我們講說那種 Pre-train 的模型
你如果沒有給模型的這個 Chat Template
它根本就不會回答你的問題
它就只把一句話
不斷講下去而已
這些語音模型也是一樣
它們做的事情
就是把一句未完成的句子
把它講下去
我們給這個語音語言模型
一段這樣的聲音訊號
某個人刺殺了總統
接下來會發生什麼事情呢
這個語音語言模型覺得
接下來會發生這樣的事情
你一定覺得在說些什麼東西啊
我也看不懂它在說些什麼
那我就把這句話呢
拿去問 ChatGPT
說你覺得這句話合理嗎
它說這句話根本不知道在說什麼
它只有一些詞彙跟片語
但合起來根本不知道在說什麼
所以最初代的語音語言模型
是這個樣子的
但是這個已經是 2023 年了
那個時候已經有 ChatGPT 了
已經有文字了
已經進入中世紀了
所以那個時候有這樣子的人工智慧
人們是不能接受的
一個人工智慧不能好好說話
人類是不能接受的
那當時初代的語音語言模型呢
不太能好好說話
但我們還是試著呢
拿它來做一些事情
比如說我們試著對它做
In-context Learning
看能不能夠有什麼奇妙的表現
我們就跟這個語音語言模型講說
聽到這句話
你就說是
這句話就是 Happy
這句話就是 Sad
看它能不能夠做情緒辨識
給它一個新的句子
它就告訴我們是 Happy 跟 Sad
可以做到嗎
沒辦法做到
這些語音語言模型
沒什麼 In-context Learning 的能力
那我們後來是發現說
你得微調它
你還是得再 Fine-tune 它
它才會有 In-context Learning 的能力
也就是它並沒有自動
從那些 Unlabeled 的 Data 裡面
自動啟發出 In-context Learning 的能力
跟文字模型不一樣
GPT-3 是 Pre-train 模型
沒做 Alignment
它其實就有 In-context Learning 的能力
但語音模型是沒有的
那當時呢
我們實驗室的博士生張凱為同學
有一系列的研究
試圖去 Prompt 這些語音語言模型
那它 Prompt 是 Soft Prompt
是要學的
它試圖去找一些 Prompt
這些 Prompt 可以激發這些語音語言模型的能力
讓它們可以做各式各樣的事情
那它做了一個網站
記錄它一系列的研究
那我把網站網址放在這邊
給大家參考
那為什麼訓練語音語言模型這麼的困難呢
我們來做個算術吧
假設你有 100 萬小時的聲音訊號
100 萬小時的聲音訊號
聽起來已經非常多了
那 100 萬小時的聲音訊號
背後對應到多少的文字量呢
人類差不多一分鐘可以講 100 個 Token
那 100 萬小時的聲音訊號
它對應到 6 個 Billion
60 億個文字的 Token
你可能覺得
60 億個文字的 Token
聽起來好像很多
不
對一個文字模型來說
這根本非常少
不要忘了 Llama 3
是用 15 T 的文字 Token 進行訓練
如果有人把 Llama 3 訓練的這些文字資料
通通轉成語音
比如說用個語音合成系統
把這些文字通通唸出來
變成聲音的話
有多少量的聲音呢
有 28 萬 5000 小時
28 萬 5000 年前
地球上是沒有現代智人的
所以你要從那個時候
就開始播放音檔
一直聽到現在
這個才是能夠相當於 Llama 3
所學到的文字的資訊量
所以如果你要讓一個語音模型
它有等同於 Llama 3 的能力
那它需要的資料量
要 28 萬 5000 個小時
這是非常驚人的資料量
那也有人做了實驗驗證說
語音語言模型真的是比較難學
這個圖上黑色的點
代表文字模型的能力
綠色的點代表語音語言模型的能力
縱軸呢是 Evaluation Metrics
它跑了三個不同的 Evaluation
那這邊細節我們就不講
大家要注意
它所有的 Evaluation
它只 Evaluate 了模型的
Content 的能力
它還沒有問模型能不能夠聽得懂
情緒啊
環境音啊
語者資訊那些
它都沒管
它只問模型做文字接龍的時候
它接出來的句子是不是合理的
所以這三個 Measure 都只問
文字接龍能不能夠接得合理
你會發現說文字模型
當然都是吊打語音模型
語音模型跟文字模型
做不到同一個量級上
橫軸
橫軸是投入的運算資源
那你會發現
那這個橫軸是 Log Scale
你會發現
當你投入越來越多的運算資源
文字模型增長得比較快
而語音模型的增長
比文字模型慢
非常的多
如果我們今天看這邊這個例子
假設我們要讓文字模型跟語音模型
有同樣的能力
不要忘了橫軸
是 Log Scale
那你需要的運算量
大概是文字模型的
十倍到一百倍左右
你 Train 一個文字模型就很痛苦了
你像還要
文字模型十倍到一百倍的運算資源
這個很有可能是非常難達成的
而且這邊只討論了內容的部分
我們還沒有討論機器
有沒有學到語者的資訊呢
有沒有學到情緒的資訊呢
有沒有學到其他文字以外的資訊
所以看起來
語音語言模型
是非常有挑戰性的
那我們實驗室曾經常試用比較大量的資料
來訓練語音語言模型
比如說我們自己收集了上萬小時的資料
來自己 Train 一個語音語言模型
那那個時候 Train from scratch
做出來
哇也是不盡人意
我後來跟那個 Meta 最開始做 GSLM 的團隊交流
他們說就算資料擴展到十萬小時
也做不出什麼好的結果來
所以我當時覺得語音語言模型的挑戰非常的大
短時間內其實沒有辦法被克服
那這對做研究的人來說
其實是一件好事
代表我們還有很多事情可以做
但是就在
24 年的 5 月一切都變了
在 24 年的 5 月
當時 GPT-4o 要發布的時候
大概在幾天之前
就已經有風聲說
他們做的是一個語音模型
只是不知道做得怎麼樣而已
所以在發布的當天
我其實是守在電腦前面
要看看他們會做什麼樣的東西
他們就做了一個
我想像中的語音語言模型
應該要有的樣子
就是不只聽得懂文字
還聽得懂一些文字以外的資訊
而且可以做非常流暢的互動
當時我看到以後
其實是非常的震驚
那個時候已經半夜兩點了
發現說實驗室的同學也都在線上
大家非常的緊張
想要看看說
這個 OpenAI 做了什麼樣的東西
那我們終於感受到 NLP 的人的痛苦啊
在有了 ChatGPT 之後
大家知道 NLP 的領域就陷入了巨大的
滅頂之災
學術界能做的東西越來越少
那我們終於語音界也要體會到
NLP 領域的滅頂之災
我們終於要體會到痛苦啊
一旦沒得寫了這樣子
(笑)
所以那個時候呢
我們在 ChatGPT 發布之後啊
就馬上制定了一個計畫
這個計畫就是
也許接下來沒什麼模型可以 Train 了
我們要做的事情就是
衡量現有的模型
衡量 ChatGPT 可以做到什麼樣的地步
大家先把音檔
跟你要做的 Benchmark 準備好
它的 API 只要一上線
立刻就來 Evaluate 它
系統一上線
我們就立刻 Evaluate 它
但是隔天早上還是沒有上線
下午還是沒有上線
晚上還是沒有上線
過了好幾天都還是沒有上線
那有的人會說
不是有嗎
這個發布會之後
不是馬上就有語音的功能嗎
那個語音的功能
不是發布會上那個語音的功能
你知道嗎
它發布會上那個 Demo 的
或是它試圖說服你相信的
是它有一個 End-to-End 的語音系統
像我當時有點懷疑
那是不是一個 End-to-End 的語音系統
但學生跟我說
老師你傻啊
它 Blog 上都跟你說是 End-to-End 的語音系統
它就是 End-to-End 的
好
那總之我當時是有點懷疑
是不是真的 End-to-End
它感覺非常的 End-to-End
但是它釋出的那個 Application
顯然是 Cascade
它並沒有釋出他們所謂的高級語音模式
它只釋出了 Cascade
然後那幾天呢
一堆 UP 主的網紅在那邊錄說
哎我有這個語音模式了
你的語音模式都是假的
都是 Cascade 的模型
根本就沒有他們宣稱的那些功能
所以等了好幾週都一直沒有真正的語音語言模型的應用出現
那我記得 OpenAI 是一直到 9 月底的時候才上線真正的語音語言模型
但上線以後用起來真的是非常驚艷
但是在過了幾個月之後其實情況又變了
等下再來說過了幾個月之後
在剛 OpenAI 剛釋出他們的 Advanced Voice Mode Demo 的時候
我覺得非常的震驚
因為有點難以想像怎麼做的
但在過幾個月之後
大家越來越知道這樣子的語音語言模型
可能是怎麼被打造出來的
所以覺得還有很多研究的空間
可以繼續進行
雖然 OpenAI 沒有馬上釋出這個語音語言模型
但是在 2024 年的 7 月中
一個叫做 Kyutai 的新創
它是法國的一家公司
他們打造了一個語音語言模型
叫做 Moshi
Moshi 是有釋出的
它是在 10 月才真的開源
它的模型的權重
但是在這個 7 月的時候
它的 Demo 是可以直接線上玩的
可以直接線上互動的
所以如果你要說
世界上第一個可以互動的語音語言模型
我反而會覺得 Moshi 才是第一個
那 OpenAI 有可能是怎麼在很短的時間內
做出一個語音語言模型的呢
那我那時候的猜想
是利用了一個強大的文字模型
作為基礎
在 GPT-4o 的語音模式的 Demo 釋出之後
一週我錄了一個影片
講我對於 4o 的語音技術背後的猜測
那其中我提到的一個猜測
就是利用文字資訊
你有空的話可以再去看看
我當年的猜測
看看跟今天的技術
看起來有沒有非常的類似
好那什麼叫做利用文字的資訊呢
其實利用文字的資訊
在語音領域其實也不算是新鮮的事情
我們剛才有講到語音問答的系統
我們說在有了 Speech Representation Model 以後
我們就能夠做一個 End-to-End 的語音問答系統
其實這個語音問答系統能做起來還有一個訣竅
我們的 Downstream Model 其實是拿 BERT 做 Initialization
所以可以想像我們其實是把 Speech Representation Model
後面接上一個 BERT 來當做整個語音的問答系統
所以我們過去就已經知道
能夠拿文字的模型來初始化一個語音模型參數
而且它往往是有用的
那有個人想說那你怎麼沒有在
那個語言模型上試試看呢
我們試了我們剛才說有意義
我們收集了一萬小時的資料
自己 Train from scratch
Train 了一個語音語言模型結果不好
好那後來我們有拿文字的模型
用當時的 Llama 來做 Initialization
得到結果仍然非常的不好
仍然沒有辦法做出像 OpenAI 那樣子的效果
那 Meta 那邊呢
他們也嘗試拿一個文字模型
來 Initialize Speech Language Model
那他們的模型叫做 Twist
那 Twist 有用文字做 Initialization
當然是有幫助
但它仍然沒有辦法做出非常好的結果
像這樣的模型
你仍然沒有辦法預期
它每一句話講出來都是人聽得懂的
那還有什麼樣的方法利用文字資訊呢
另外一套想法是說
能不能打造一個語音原生的模型
我們今天在訓練語音語言模型的時候
把語音跟文字的資訊倒在一起
讓模型不只是跟文字學
也跟語音學兩者同時一起學習
那像這樣子的概念
其實早在打造 Speech Representation Model 的時代
比如說 2021 年
就是在那個中生代的時候
那個時候大家就已經會做這樣子的事情
那時候就已經有很多研究
試圖把語音跟文字倒在一起
一起訓練一個 Representation Model
但是如果只是倒在一起訓練
往往不一定能夠得到好的結果
怎麼讓兩種看起來非常不一樣的資訊
能夠被 Align 在一起
是需要非常多的技術的
那這邊我們就不細講
我把一些相關的論文放在這邊
給大家參考
那當然在語音語言模型上
也有人試過把語音跟文字倒在一起訓練
怎麼做呢
就是我有一堆
我有文字
我有語音
那這邊其實有一個比較強的假設是
假設這個文字跟語音有對應的關係
就這句話就是
How are you
所以這邊跟一般的 Pre-train 不一樣
一般 Pre-train 我們是假設說
完全沒有標註資料
但這邊我們會假設每一段聲音
我都有它對應的文字
這需要某種人力的介入才有辦法取得
所以變成 Pre-train 不是完全 Unsupervised
好那在 Training 的時候呢
會用一個技巧叫 Interleaving
也就是把文字 Sequence 裡面的一些 Token 換成語音
把語音 Token Sequence 裡面一些 Token 換成文字
然後再訓練下去
那這樣的方法可以讓機器更能把語音跟文字的資訊
Align 在一起
更能夠善用彼此的資訊來強化語音跟文字的能力
那我這邊就引用了一些相關的論文
那其中一個比較知名的論文也是 Meta 做的
這個模型呢
叫做 Spirit LM
好
但是在剛才講的那兩個方法
用文字模型做 Initialization
還有在 Training 的時候做 Interleaving
其實都沒辦法真的打造非常好的語音語言模型
在看了 OpenAI 的那個 Demo 之後
我覺得他們有不一樣的方法
那我當時的猜想是這個樣子
但很多後來的研究也都證實了
這樣的猜想
這樣的方法是非常有用的
我猜想是這個樣子的
他們一樣從文字模型作為 Initialization
但是在訓練的時候
你要教你的語音模型
同時產生文字跟語音的 Token
所以重點是
模型不只要用文字的參數
做 Initialization
它要能夠學習同時產生文字跟語音的 Token
那我們真正要的是語音的 Token
然後因為這些語音的 Token
會丟進 Detokenizer 裡面
還原出聲音訊號
你就知道模型想要說跟你說些什麼
但為什麼產生文字的 Token
仍然是必要的呢
因為我們今天在訓練的時候
我們非常常遇到的一個問題
就是 Forgetting 的問題
你每次教模型一個新的任務的時候
它很容易忘記舊的任務
如果你教一個文字模型
產生語音的 Token
但是同時
你又不讓它常常去回顧
怎麼產生文字 Token 的話
它馬上就忘記
怎麼產生文字 Token 了
它只會產生語音的 Token
然後那些語音的 Token
又不足以讓它學會
文字那麼多豐富的資訊
它整個就壞掉了
所以今天在訓練語音模型的時候
一個重點是
你仍然要教它產生文字的 Token
你仍然要教它產生文字的 Token
可以避免它忘記它在文字的時候
曾經知道的那些資訊
而那些文字曾經知道的資訊
是非常重要的
因為單憑語音的資料
是沒有辦法讓模型學到足夠的資訊的
文字是語音的壓縮
語音的資料比較複雜
而文字的資料比較簡練
今天機器透過大量文字的資料
它已經學到非常豐富的資訊
如果你要保留那些知識
用到語音的互動中
那你必須要避免它遺忘
它在文字的時候已經學到的資訊
所以今天在訓練的時候
保有文字生成的能力
文字 Token 生成的能力
可能是非常關鍵的一步
但這邊就有一個小小的問題
文字 Token 的長度跟語音 Token 的長度
往往是天差地遠
一句話
有人說 How are you?
How are you?
可能是三個文字的 Token
但是它對應的語音 Token
可能比三個還要多很多
因為通常一個語音 Token
就只對應到 0.02 秒的聲音訊號
所以一秒鐘就有 50 個語音的 Token
因為這兩者的差異非常的大
而且另外一方面
他們之間 Alignment 的關係
其實非常的複雜
比如說 How
可能對應到前面這些 Token
Are 可能只對應到這一小段 Token
You 可能要對應到這段 Token
他們之間沒有很明確的
非常一致的
非常有規律的 Alignment 之間的關係
所以在生成的時候
如何把這兩者放在一起
就變成一個需要考慮的問題
那這邊有不同的猜測
一個想法是
先生成文字再生成語音
教機器先生成文字的 Token
再生成語音的 Token
那對生語音 Token 的時候
對模型來說
它要做的事情
其實幾乎等同於是語音合成
它等於是先把文字生成出來
然後再根據這些文字
再做語音合成
生成出對應的語音的 Token
那這種方法缺點就是
它比較難做 Streaming
今天你跟模型說話的時候
你跟他說一句話
你會期待模型
馬上就給你回應
但像這種先生成完文字
再生完語音的模型
你就得先等它生完文字
比如生完文字要個三秒
你每跟它講話的時候
它都會停頓三秒
才開始回你
你就會覺得非常的不自然
那另外呢
你也可以是語音再生成文字
但是是在詞彙的層級上
做 Interleaving
就模型先生一個文字的詞彙
然後再生這個文字的 Token
對應的語音 Token
生一個文字的 Token
再生對應的語音 Token
生文字的 Token
再生對應的語音 Token
這是一個看起來蠻有效的方式
但這樣的壞處是
我們必須要先做 Alignment
你必須要假設說
你知道說
你在訓練的時候
必須假設你知道說
每一個文字的 Token
它對應哪些語音的 Token
那這樣 Alignment 的資料
不是沒辦法取得
但是它需要耗費一些額外的力氣
才有辦法取得
語音跟文字之間
複雜的 Alignment 的關係
還有另外一個方法
是 Chunk-wise 的 Interleaving
也就是說
我們完全不管語音跟文字之間的對應關係
我們就定好說
每生一個文字 Token
接下來就生兩個語音 Token
生一個文字 Token
就兩個語音 Token
一個文字 Token
就兩個語音 Token
那可能說
文字跟語音 Token 長度不一樣啊
會不會文字 Token 都生完了
語音 Token 還沒有生完
有可能
那文字 Token 如果生完了
那就繼續生語音的 Token
那有一些模型
比如說 GLM-4-Voice
就是用這樣子的方法訓練出來的
你可能會覺得說
那這樣這些 Token 跟這些文字
不就沒有對應的關係了嗎
也許今天
已經輸出到 Are 了
這邊還想
How 還沒有念完了
告訴你沒關係
這樣 Train
做得起來
GLM-4-Voice
就是用這樣的方法
訓練起來的
那還有其他的想法
在剛才的想法裡面
是
我們每一次只生成
要么是語音的 Token
要么是文字的 Token
那也有可能
你在每一個時間點
兩種 Token
同時生成
就是你有兩個 Language Model 的 Head
一個 Head
專門生文字 Token
一個 Head
專門生語音 Token
所以每次都生
一個文字 Token
跟一個語音 Token
當然這樣
你還是得處理文字
跟語音長度不一致的問題
那在歷史上
就有各式各樣的方法
那這些都是在大概去年年中的時候
年到年底的時候
提出來的一系列的方法
那 Moshi
也到了這個
它到了去年 10 月的時候
才釋出它的技術報告
它用的也是這種
有包含文字的生成的方式
但它有一套自己的方法
去處理文字跟語音 Token 長度不一致的問題
那後來我們實驗室呢
也在這個方向上做了一些嘗試
我們做了 word level 的 text and speech
那確實你有機會用這種方法
訓練出一個看起來還可以的
語音語言模型
那這邊呢就播一下
蕭淇元同學提供的音檔
那首先你會聽到
輸入給這個語音語言模型的
聲音訊號
它聽起來像是這個樣子的
好那接下來呢
模型就做文字接龍
大家要注意的是
它做文字接龍的時候是
一邊生文字的 Token
一邊生語音的 Token
它就生文字的 Token
生語音的 Token
生文字的 Token
生語音的 Token
然後把語音的 Token
拿出來過 Detokenizer
那就可以聽到聲音訊號
那我們也會把文字的 Token
集合起來
讓你知道說
如果看文字的話
模型想要說什麼
好來聽聽
模型想要跟你說什麼
I'd be happy to help you with insight ideas
for something to eat
what kind of food
are you in the mood for
are you craving something sweet
savory
or something else
do you have any
dietary restrictions
or preferences
let me know
and I can suggest
some options for you
至少是知道它在講什麼
但你會發現說
有一些合成的地方
比如說 Restriction
它就是亂念一通
那我想之所以
它有一些字會亂念的原因
是因為訓練資料
其實非常的少
在這個 Project 裡面
我們用的訓練資料
是不到 1000 個小時的
如果有一些詞彙
模型從來沒有聽過
它怎麼念
它可能就沒有辦法
精確的把那個詞彙
唸出來
那只有一千小時訓練資料
真的非常的少
光要讓模型好好回答問題
可能都不太夠
更遑論是要讓它學到一些
什麼 Emotion 啊
Speaker 啊相關的東西
如果要讓機器能夠學會
這些文字以外的資訊
那我們常常把文字以外的資訊
統稱為 Paralinguistic 的資訊
讓模型學會這些
Paralinguistic 的資訊
可能需要用 Web Scale 的 Data
用非常大量的資料
網路上爬到的大量資料
做一個大規模的 Pre-train
才有辦法辦到
所以如果只用上千個小時
做 Fine-tune
你可能只能讓模型回答問題
但不容易讓它學到
語意以外的東西
那可能會想說
那怎麼不用 1000 小時更多的資料呢
那是因為我們的手上的算力
是不足以支撐更大規模的計算
因為訓練這樣子的語音模型
是非常痛苦的
怎麼的痛苦法呢
你想想看
文字通常是一秒鐘的聲音訊號
對應到 3 到 4 個 Token
所以可以說文字大概是 3 到 4 赫茲 (Hz)
這樣子的頻率
而語音的 Token 呢
語音的 Token 通常是 50 赫茲 (Hz)
也就一秒
你有 50 個 Token
所以可以想像說
如果我們比較下面這個 Sequence 的長度
跟上面這個 Sequence 的長度
往少了算
下面這個語音模型要產生的 Sequence 長度
是文字模型的至少十倍以上
十倍以上是什麼意思
如果你知道這個 Transformer 裡面
不是有那個 Attention 嗎
Attention 需要的 Computation Cost
是跟長度的平方成正比的
所以十倍以上的 Sequence
意味著你要百倍以上的算力
那你說訓練一個一般的文字模型都已經很痛苦了
一個文字模型再加一百倍的算力
哪有人有算力能夠做這麼大規模的運算呢
但是其實很多大公司後來就是用這樣子的方法把他們的語言模型
語音語言模型練起來的
那如果你有足夠的算力
你當然可以自己做
但是在學校我們不可能取得這樣的算力
所以我們需要更好的方法
那有什麼樣更好的方法呢
這邊我想跟大家分享的一個方向
是尋找更合適的語音表示的方式
為什麼有機會尋找更合適的語音表示的方式呢
這個研究是跟 MTK Research 的團隊一起合作的
第一作者是我們實驗室的博士生曾亮軒同學
然後另外 MTK 那邊是陳宜昌(共同第一作者)參加了這個計畫
還有 NTU 的李冠儀同學參加了這個計畫
那這個是今年年初放在 arXiv 上面的文章
那為什麼有可能有更合適的語音的表示方式呢
想想看現在我們已經知道
今天在生成這些語音 Token 的時候
模型是會連著生成文字 Token 的
如果我們相信這種語音文字
Hybrid 生成的方式就是未來的主流的話
我們在打造語音 Token 的時候
應該要考慮到文字的 Token
什麼樣叫做打造語音 Token 的時候
要考慮到文字的 Token 呢
這邊我有兩個想法
這兩個想法
第一個是我們希望語音 Token 的數目
可以配合文字的 Token
兩者之間不要有非常複雜的關聯性
那語音的 Token 過去
通常都是 Fixed Duration
每一個語音 Token 都對應到一小段時間
比如說 0.02 秒
我們不要這種 Token
我們期待有 Dynamic 的時間
時間的語音 Token
每一個 Token 對應到不同長度的時間
而它跟文字是有很簡短的
很簡單的對應的關係
比如說甚至是一對一的關係
所以我們期待有一個 Tokenizer
它可以做到的事情是
一句話
裡面如果是三個文字的 Token
How are you
你就出三個文字的 Token 就好
把這段聲音訊號表示成三個文字的 Token
三個語音的 Token
這三個語音的 Token 分別告訴我們
這三個文字的 Token
How 跟 Are 跟 You
分別應該要怎麼做
Pronunciation
應該要怎麼發音
才是對的
才是合適的回覆
這是第一個想法
然後如果語音 Token 跟文字 Token
它們之間的關係非常簡單
比如說它們之間有一對一的關係
出一個文字 Token
就得出一個語音 Token 的話
我們在訓練語言模型的時候
就變得非常簡單
我們就不需要去考慮各式各樣的策略
我們只需要記得訓練模型
每次生一個文字 Token
接下來就生一個語音 Token
生一個文字 Token
就生一個語音 Token
這可能可以讓訓練更加的簡便
讓模型更容易學會這樣子的 Sequence
那還有另外一個考量是
今天既然生成的結果裡面
已經有文字
那語音的 Token
就不需要包含文字的資訊
過去在打造這些語音 Token 的時候
人們的想法往往是
這些語音的 Token
應該要包山包海
它應該包含語音裡面所有的資訊
但是現在如果我們在生成的時候
文字是一定會伴隨語音 Token 產生的
那語音 Token 其實只需要專注於儲存
模擬那一些文字無法表達的資訊就好
文字的資訊
交由語言模型已經產生的這些文字的 Token 來表達
所以我們的 Tokenizer
在吃輸入的時候
它不能只吃這些語音的 Token
它要把文字的 Token 也當作它的輸入
Tokenizer 是看這些文字的 Token
加這些語音的 Token 去產生聲音訊號
而這些文字的 Token 已經告訴 Detokenizer
這句話要講什麼
而這句話除了要講什麼以外
要怎麼被講出來
用什麼樣的聲音講出來
由語音的 Token 來操控 Detokenizer
來做這些事情
好那有了這些想法之後呢
這個曾亮軒同學呢
就想了一個神奇的模型
叫做 TASTE
這個 TASTE 呢
它抽取 Token 的方式是這個樣子的
好首先呢
我們有一個傳統的 Speech Representation Model
那這邊其實你可以套用任何 Speech Representation Model
在我們論文裡面呢
是直接用 Whisper 的 Encoder
好它會產生一排 Representation
每個 Representation 對應的是固定的時間
好那接下來呢
我們會從不同的 Layer 抽出兩排 Representation
那至於要抽出哪兩排
其實這是一個尚待研究的問題
那在論文裡面
我們根據我們的經驗
選擇了最合適的兩個 Layer
好接下來呢
我們假設這句聲音有做語音辨識
所以你知道它對應的文字長什麼樣子
那接下來呢
你會把這裡的文字當作是 Attention 裡面的 Query
把這兩個 Representation
一個當作 Key
一個當作 Value
至於哪一層適合當 Key
哪一層適合當 Value
那這個你需要做一點實驗
才能夠找出合適的層
來作為 Key 跟 Value
好那接下來就做 Attention
你把這個 Query 拿去
對 Key 算 Attention 的 Weight
然後把 Value 做 Weighted Sum
那每一個 Query 就給了我們一個 Token
給我們一個 Representation
那麼接下來會做 Quantization
所以最後輸出的是一個 Discrete 的 Token
那每一個 Query 都給我們一個 Token
每一個 Query 都給我們一個 Token
如果語音辨識的結果
有幾個文字的 Token
我們就抽幾個語音的 Token 出來
好那接下來這些語音的 Token
怎麼還原回聲音訊號呢
從這個 Pretrained 的 Speech Encoder
到 Aggregator
從聲音訊號到這些語音的 Token
合起來就是我們的 Tokenizer
接下來我們來看
Detokenizer 長什麼樣子
Detokenizer 除了吃這些 Audio Token 之外
它也會吃文字的資訊
它會把語音辨識辨識出來的結果
也當作它的輸入
所以其實你可以把這個 Detokenizer
看作是一個語音合成的模型
那事實上我們的 Detokenizer
就是直接拿一個語音合成的模型
去做 Initialize 的
所以我們的架構
跟一個語音合成的模型
這個模型叫 CosyVoice
是一模一樣的
只是跟一般語音合成
不一樣的地方是
一般語音合成就是根據文字
去產生聲音訊號
但我們現在輸入除了文字的 Token 以外
還有一些語音的 Token
這些語音的 Token
告訴我們這些文字的 Token
應該要怎麼被發音才對
所以這 Detokenizer
就根據文字跟語音的 Token
去還原聲音訊號
那訓練的時候呢
跟一般的這個語音 Token
還有 Image Token 訓練的方式一樣
就是要 Minimize Reconstruction Error
輸入一段聲音訊號
變成 Token
Token 通過 Detokenizer
還原回來以後
輸入跟還原的結果
要越接近越好
然後接下來
你就可以 End to End 的
去 Train Aggregator
跟 Detokenizer
你就有一個
可以抽出跟文字數目
一樣的語音 Token 的
這個 Tokenizer 了
這個方法叫做 TASTE
TASTE Align Speech Tokenization
And Embedding 的縮寫
好那我們這邊呢
做一個小實驗來展示一下
這一些語音的 Token 裡面
可能包含了什麼樣的資訊
那這個實驗是展示說
這些語音的 Token
它包含了長度的資訊
它告訴你
今天一個文字的 Token
要被念的多長
好那我們先給 Tokenizer
一段音檔
這段音檔是某一個人
用很慢的語速
來念一個句子
那就這句話輸進去以後
然後呢就產生一排 Token
那我們另外一個句子
是有個人念很快
我們可能念很快
好吧
這個念很快的句子
也丟到 Tokenizer
產生一排 Token
那接下來我們做的事情是
把這兩個句子的部分 Token
對調
我們把第一個句子的
Came around to
對應的語音 Token
跟第二個句子的
News on the的對應的語音 Token
把它對調
對調完之後
有了不一樣的語音 Token
再丟到 Tokenizer
去還原回聲音
我們聽聽看
會變成什麼樣子
你有聽出來嗎
Came around to
這三個字突然被加速了
我們再聽一次
對看這三個字突然就加速了
代表這三個 Token
告訴我們
告訴 Tokenizer 說
念這三個字的時候
要被加速
好
那我們來看另外一個例子
所以 News on the 的
換成藍色的 Token
那我們看看
合出來結果怎麼樣
你會發現
News on the 的這三個字
它念的時候被減速了
告訴我們說
這些語音的 Token
它確實告訴我們
一個詞彙
一個文字的 Token
要怎麼被念出來
好
那接下來呢
有了這個新的 Tokenizer 之後
就是訓練的事情了
你需要做的事情就是
好
找一些語音的資料
這些語音的資料
不需要有任何的標註
我們並不
我們並不真的需要
文字的標註
所以這種資料
你是可以輕易的取得的
好
然後呢
你把這一些語音的資料
做語音辨識
所以你得到
它對應的文字
然後呢
有它對應的文字之後
你就看有幾個文字的 Token
就抽幾個語音的 Token
然後接下來
你就訓練一個
語音的語言模型
它每次就是
輸入一個文字 Token
就輸出個語音 Token
輸出一個文字 Token
就輸出一個語音 Token
那這個語音語言模型
你也可以從文字的語言模型
來做 Initialization
然後確保它可以繼承
文字模型已經有的能力
好
那這邊的模型
大家要注意一下
它只是 Pretrain 的模型
我們並沒有做
Supervised Fine Tuning
我們只拿大量的
沒有標註的資料
來訓練它
所謂的大量呢
大概是一萬小時左右啦
一萬小時你可能聽的覺得很多
我學生在大公司工作常常告訴我說
老師你不要再說一萬小時很多了
正確的模型訓練的
語音的單位是億
這樣子
你聽好了是億
他們都是用上一小時的資料來訓練模型的
真的非常的驚人
好總之
我們在我們有限的算力之下
用不到一萬小時的資料
來 Pretrain 一個模型
那這邊是沒有做 SFT 的
所以模型唯一能做的事情
就是語音接龍
你給它前半句話
它幫你把後半句話講完
那像這樣子的模型要怎麼評估呢
我們也得做一些 Evaluation
來告訴大家說
這個模型的表現怎麼樣
所以這邊評估方式就是
你就給模型一段聲音訊號
然後讓它幫你做聲音訊號接龍
接完之後
怎麼知道它講得好不好呢
我們從三個面向來評估它
第一個面向是
我們會把這個接出來的聲音訊號
做語音辨識
然後辨識完之後
把辨識的結果
丟給 GPT4
問他說
你覺得這句話合不合理
所以這邊是衡量
它的 Semantic coherence
它講話的合理的程度
那會量一個叫做 UTMOS 的東西
它代表的是語音的 Quality
當然這些都是自動化的評量的方法
有時候自動化的評量方法
你可以有個模型
它騙過了這一些自動化評量的方法
但實際上表現不好
所以最後我們也要有人來評估
看他覺得哪一個模型生成的結果是最好的
那結果如何呢
我們這邊最主要比較的對象就是
TWIST 跟 Spirit LM
這兩個模型我們前面都有提到
TWIST 就是用文字模型做 Initialization
但是它其實沒有保有生文字的能力
Spirit LM 就是在 Training 的時候做 Interleaving
它把語音跟文字用 Interleaving 的方式做訓練
然後我們的模型是 Ours
然後你就會發現說
我們的模型在所有三個評量的指標上
其實都是比 Baseline 還要好
那我們甚至嘗試了一下說
這些模型能不能回答問題
雖然它是 Pre-trained 的模型
但 Pre-trained 的模型它做接龍的時候
有時候還是可以接得出正確的答案來
所以它還是有一定程度可以回答問題
雖然它沒有那些 Supervised Fine-tuning 的模型做得還要好
但它是可以回答問題的
那這個模型實際表現怎麼樣呢
其實在上個學期我已經做過 Demo
看看這些模型的英文能力怎麼樣
你可能問說為什麼這邊會有 Ruby 講
你去看這個影片你就知道了
好但這邊呢
因為英文已經 Demo 過了
這邊我們來 Demo 一下模型的中文能力
我就給他前半句中文
叫他把這句話生完
那這邊我先用我自己的聲音來嘗試一下
大家好
我就說大家好
那模型就繼續接下去
大家好
我是
我是奎老師
為什麼我是奎老師我也不知道
反正它講出來就是這個樣子
要注意一下
它是一個 Pre-trained 的模型
所以就跟我們之前文字的 Pre-trained 模型一樣
文字的 Pre-trained 模型
你不是叫做文字接龍
就亂講話嗎
它一樣
它是接語音的 Token 出來
那它想講什麼就講什麼
好那我們再看下一個例子
我是李宏毅
我說我是李宏毅
它繼續做接龍
我是李宏毅
然後目前是在台灣的一個
就是一個機構叫做台灣社會型的非營利組織
叫做台灣地方創生協會
就是這樣
這個就是它發明出來一個句子
就它 Hallucinate 出來的東西
再給它更長的輸入
大家好
我是李宏毅
今天很高興呢
來給這場演講
好接下來模型就得胡謅一下演講的內容了
好它就來開始講了
大家好
我是李宏毅
今天很高興來給這場演講
那今天演講的內容大概會是
就是現在正在推行中的一個計畫
就是使大學生創業的計畫
好那我試著給它一點情緒
你這個廢物
你這個廢物
然後看它講什麼
你這個廢物
還是趕快走吧
然後接下來讓它回答問題
它其實也是一定程度回答問題的能力的
台灣最高的山是哪座山啊
好那台灣最高的山是哪座山
看它會怎麼接下去
台灣最高的山是哪座山啊
大家應該都知道是玉山嘛
對不對
就是這樣
所以它知道台灣最高的山是玉山
好剛才都是用我的聲音啦
後來就找了一些莫名的迷因的聲音
好這句話是
我是你個鬼
大家都知道接下來的光處是什麼
就你這個糟老頭子壞得很
好來看看模型接下來會講什麼
我是你個鬼啊
好了開玩笑我們的話題要回歸正軌
好了好了今天是
好它就講到這裡
好那另外一個例子呢是出自配音網
那亮你好厲害
又拿到全學年的第一名
好那它背景是有音樂的
那等一下你會發現說
因為我們實際上在做接龍的時候
我們會把這句話輸給這個變成 Token
輸給語音語言模型
語言語言模型再產生更多的 Token
那我們還原的時候會把它輸進去的 Token
跟輸出的 Token 一起做還原
所以今天我們會
所以你會發現說啊
Prompt 的部分
就是我講的前半句的聲音是有一點變的
因為那前半句的聲音是從 Token 還原回來的
在這個例子裡別明顯不一樣
本來有音樂的聲音
但變成 Token 以後音樂的資訊就不見了
因為我們在 Train Our Tokenizer 的時候
模型是沒有聽過音樂的聲音的
所以它沒有辦法把音樂的資訊放到 Token 裡面
它本身它自動的把音樂的資訊就濾掉
只包含了聲音的資訊
所以等一下聽到說亮你好厲害這句話
它聲音有點變
而且音樂也不見了
好這個是模型接龍的結果
亮你好厲害啊
又拿到全學年的第一名啊
那我想要知道一下張
尖英文你都是念哪些書
你是念教科書嗎
它講的蠻合理的吧
它問了一個蠻合理的問題
好然後下一個
下一個是什麼
下一個也是出自配音網了
不要瞎掰好嗎
就不要瞎掰好吧
模型會怎麼接呢
不要瞎掰好吧
你們可以去 Google 一下
網址的傳說或者英格蘭王室
就知道了啦
今天的英國王室
就是來自威爾斯王國
只不過經過統戰融合後
有改變王室的原名
這也是威爾斯這個國家
就這樣它講的
它講的對不對不好說啦
我沒有仔細查
不知道 Hallucinate 的問題有多嚴重
好接下來兩個例子
沒有那麼成功
模型有講一些怪怪
聽不太懂的話
不過我們來看看
沒有那麼成功的例子
好第一個就是
小明建模獎
哎呦
我老爸得了 MVP
你會發現這句話呢
因為講的時候非常帶有情緒
所以變成 Token 再還原回來
其實聲音跑掉蠻多的
來聽聽看模型怎麼接
哎呦
我老爸得了 MVP
就是因為他有辦法
舒適地待在家
舒服地喝咖啡
看報紙喝到滿滿一杯
看報紙喝到滿滿一杯
不知道什麼意思啊
它講話是有押韻的
其實蠻厲害的
另外另外一個例子
也是小明建模的例子啊
啊能能
它就能能
這個尖尖的聲音啊
變成 Token 以後還原回來
就其實是變成聽起來
像是另外一個女生的聲音
好我們看模型是怎麼做接龍的
啊能啊能
那你現在回家那是什麼喝酒嗎
喝酒嗎
其實它對對對對
就是
對
這個就是模型自己產生出來
說它為什麼會笑
不知道
反正它學到了就是笑
它就知道在這邊它要笑一下
那個笑聲也是它自己產生出來的
所以這個就是用比較多的資料去
Pretrain 的醍醐味
這樣你就 Train 下去
哇你都不知道它學到了什麼
它會學到各式各樣奇奇怪怪的東西
那我們期待這樣的模型
更有機會做出一個
什麼都懂的語音語言模型
好那到目前為止呢
我們提到的語音語言模型都只是 Pretrain 的模型
那你知道 Pretrain 並不是打造一個模型的全部
今天 Pretrain 之後
你還要做 SFT
還要做 RLHF
那對於語音模型的 SFT 跟 RLHF
目前沒有非常非常特別的想法啦
當然你要做 SFT 的時候
你就要教模型一問一答
所以這個時候你顯然需要對話的資料
如果你都拿不是對話的資料
都拿一堆演講來訓練它
它就只知道說聽一個人的聲音
它就用同一個人的聲音講下去
所以你要教它做對話
你得收集一些對話的資料
來教語音語言模型
不過有時候對話的資料
你可能沒有辦法收集到非常大量的資料
所以今天一個常見的想法就是
也許你可以自己合成對話的資料
今天你可以叫文字模型生成對話的腳本
再拿語音合成的模型
把對話的腳本唸出來
你就有語音的對話
可以拿來訓練模型了
那接下來得做 RLHF
讓模型的這個輸出貼近人類的需求
那這邊的 RLHF 跟一般的文字的 RLHF
可能沒有什麼本質上的不同
你可以人類提供 feedback
告訴語音語言模型
這個答案是好的
這個答案是不好的
那你甚至可以訓練一個 Reward Model
這個 Reward Model 呢
就是給語音語言模型的輸入跟輸出
它去判斷這個輸入跟輸出
合起來是不是一個好的對話
然後人類呢會去教這個 Reward Model
產生合適的 Reward
你再用 Reward Model 來給語音語言模型提供回饋
讓語音語言模型的輸出更接近人類的需求
那這邊跟文字的做法可能沒有非常核心的差異
所以這邊我們就講得比較簡短一點
只給大家看幾個語音語言模型 RLHF 的例子
這個第一個例子呢
是林冠廷同學在 Amazon 實習的時候做的
那這篇論文發表的時間點是在去年的 11 月
大概一年前
那在這個 AI 的領域一年前
就是很久很久以前了
所以這是一個早期的論文
所以那個時候在想的
還不是怎麼讓語音語言模型
可以學會 paralinguistic 的資訊
其實林冠廷同學在更早的論文
有一些想要讓語音語言模型
學會 paralinguistic 的資訊
不過是做的是 SFT 的
那在 RLHF 這個階段呢
他當時想的是
能不能讓這些模型好好說話
說的話至少是人聽得懂的
因為他當時用的模型呢
還沒有做這個文字跟語音的
它只有生文字的 Token
所以往往模型說出來的話
是人聽不懂的
不知道在講些什麼
看看能不能用 RLHF
矯正一下這些模型
讓模型能好好說話
這邊就直接複製論文的圖了
他們就是把模型的輸出
做語音辨識先變成文字
再把這些文字丟給一個文字模型
叫這個文字模型去評分
說這個答案到底是好
還是不好呢
那語音語言模型
有這樣的評分之後
它就得到回饋
你就可以 fine-tune 語音語言模型
最終他們就算不是用文字
跟語音 Token 的 hybrid decoding
透過 RL 的力量
也可以讓模型好好說話
那這邊呢
我想跟大家分享的是
楊書文同學呢
去 ByteDance 實習的時候
他也發表了一篇論文
那這個是上個月
才放到 arXiv 上面的文章
那在他們的論文裡面
也是做 RLHF
但是他們就更 focus 在
怎麼讓模型學會
paralinguistic 的資料
就是他們的模型
可以做到給一樣的
content 的句子
就是有兩句話的內容
是一樣的
但是可能是不同的情緒
然後模型能不能夠做
適當的回答
如果是不同的性別
模型能不能夠做
適當的回答
如果是不同的年紀
有老人的聲音
小孩的聲音
模型能不能夠做
適當的回答
那他們有一個 demo 啦
所以我可以展示一下
他們的這個 demo
他們這個模型
想要做的展示就是
他們給模型內容一樣
但是這個 emotion
或者其他 paralinguistic 資訊
不一樣的句子
然後看看模型
會有什麼樣的回應
好我們就來看一下
他們的 demo
所以就有一個人用
興高采烈的聲音說
我現在知道是誰在我們團隊裡面
得到了升遷的機會
因為是高興的聲音啦
所以模型就要用高興的聲音回應
所以模型是給一個正面的回答
好
那現在給一個失落的聲音
好像沒有選他
所以還蠻難過的
那我們看模型說什麼
所以模型就安慰了一下他
這邊下面還有很多例子啦
我們找一些別的類型的例子
這邊有很多情緒的例子
然後我們找個別其他類型的
比如這邊
好 這邊呢
是模型用諷刺的語氣
跟真誠的語氣來說話
所以第一個是諷刺的語氣
反諷的語氣
就模型對他陰陽怪氣一樣子
人類對模型陰陽怪氣
模型知道人類對它陰陽怪氣嗎
那就道歉啦
另外一個就是
如果是真誠的讚美
對 模型就給一個正面的回答
好 那我們再找一個別的
這邊
好 這邊有那個性別的
就是說他說像我這樣的人應該做什麼樣的檢查
然後可以預防癌症
好 看看模型的回答
因為它知道對方是個女性
所以它就建議了一些乳房檢測之類的事情
好 那如果是男性的聲音
好 這是模型的回應
對 它給男生跟女生的建議是不一樣的
然後
好 這邊有那個不同年紀的
不同年紀的
好 如果是大人跟模型說
你能不能幫我設定一個銀行的賬戶
這模型的回答
對 模型知道他是個小孩
所以就不幫他弄銀行賬戶
好 所以這個就是語音語言模型
能做的事情
那接下來呢 我要再講下一步的研究
讓語音語言模型一邊說一邊思考
那我們知道說
今天很多文字模型都有 Reasoning 的能力
那 Reasoning 呢 通常翻成深度思考
比如說 ChatGPT
它在給它一個問題的時候
它常常出現正在思考
這就是一種 Reasoning 的能力
或 DeepSeek
它會告訴你說它已思考
然後 Gemini
會告訴你說它有某一個思路
那通常這些思考過程
它 default 可能是被藏起來的
但你可以把這個思考過程點開
看看模型在想些什麼
像 Claude
它其實也有這種 Reasoning 的功能
那現在文字模型有這些 Reasoning 的功能
而這些 Reasoning 的功能
確實可以強化文字模型的能力
那所謂 Reasoning 的功能
實際上做的事情就是模型在產生它的答案之前
它額外產生一些文字的 Token
這些文字的 Token 並不需要給使用者看
模型產生出來給自己看
可以幫助模型得到更正確的答案
對於語音來說
我們能不能做一樣的事情呢
到剛才目前為止
我們講的語音模型都是輸入語音
輸出語音
但實際上輸出的不只是語音輸出
是語音跟文字的 Hybrid
等一下講的其實模型都是輸出語音跟文字的 Hybrid
今天能夠動的語音語言模型通常都這麼做的
不過等一下為了簡便起見
我就不再把文字的 Token 畫出來
但是在它得到答案之前
我們能不能讓它跟文字模型一樣
也做個 Reasoning
那這邊 Reasoning 的時候輸出的可能也是文字的 Token
也不給使用者看
透過 Reasoning 讓模型得到更好的結果呢
那確實有人嘗試過這個方向
比如說 Audio Reasoner
Audio Thinker
都是把語音語言模型加上了 Reasoning 的能力
但是如果我們要把 Reasoning 的能力
加入這一種 Dialog Model
你會發現在實用上
會讓這些模型有非常大的問題
如果今天每次你對模型說完一句話
它得花很長的一段時間
可能是 10 秒 15 秒做 Reasoning
然後才輸出它的回應
如果今天你是用一個線上的平台
用 ChatGPT 的文字版
你可能可以接受
你在電腦前面等一下
但如果今天我們想要這些語音語言模型
模仿人類非常真實的對話
兩個人在對話的時候
其中一個人每次要等 10 秒才能夠回應
實在是非常難讓人接受
所以怎麼辦呢
我們能不能夠讓模型一邊說話
一邊想
其實人類也是這樣子的
我們在跟另外一個人講話的時候
其實同時你也正在思考
你等一下要講什麼
比如說身為一個老師
在很多場合常常有人會問你問題
比如在演講完
大家會舉手問你問題
那通常人家問完問題都會期待
你馬上就給一個答案
如果你說我隔個 30 秒再給答案
大家是不能接受的
但是有時候如果你沒有問題
立即有問題的答案怎麼辦呢
這個時候你都會說
嗯
這真是個好問題
想幫你爭取到幾秒鐘的時間
思考一下問題的答案
那我們能不能夠讓模型做一樣的事情
它能不能夠邊聽邊講呢
它能不能夠邊講邊想呢
那怎麼讓模型做到邊講話邊思考呢
本來我們以為
這可能需要一個新的模型的架構
舉例來說
我們可能需要有兩個平行的 decoder
這兩個平行的 decoder
一個負責產生語音的 Token
一個負責產生 Reasoning 的 Token
他們互相之間彼此做 attention
那這樣可以讓模型一邊說話
一邊思考
但是等一下要講的這個姜成翰同學的研究啊
他發現不需要改變語言模型的架構
用原來語言模型的架構
就有機會讓模型做到
一邊說話一邊思考
那他的論文呢
已經放到 arXiv 上
是今年七月放上去的
那他把這個模型叫做 STITCH
那這個呢
是他跟這個 Microsoft 的研究人員
一起合作的一個成果
好這個 STITCH 是怎麼運作的呢
我們在講 STITCH 怎麼運作之前
我先舉一個例子
看看現在的語音語言模型是怎麼運作的
這邊舉例
是用 GLM-4-Voice-9B 的模型
像這樣子的模型啊
它在對話的時候
它就是會先生成一些 Token
那其實它生成的一個 39 個 Token 裡面
它的佈局都是
先生成 13 個文字的 Token
再生成 26 個語音的 Token
合起來總共 39 個 Token
它以 39 個 Token 為一組
那生完 39 個 Token 以後
它就會把這些 Token 丟給 detokenizer
去還原出聲音訊號
然後在播放聲音訊號的時候
同時它得再生另外 39 個 Token
它得在聲音訊號播完之前
把這 39 個 Token 生完
透過 detokenizer
合成出聲音訊號
那把聲音訊號合出來以後
在前一段音檔播完
就可以立刻接新的聲音訊號
這樣人聽起來就會是連貫
而不會有不自然的地方
那在播放前一段音檔的時間
足夠 GLM-4-Voice
產生 39 個 Token 加上合成聲音嗎
非常的足夠
如果你今天有一張 A100 的話
你做生 39 個 Token 加上 detokenizer
你只需要大概 0.5 秒的時間
而用 0.5 秒的時間
生成 39 個 Token 以後
生出來的 Token
做完 detokenizer
合成出來的聲音
對應到大概 2 秒的時間
所以其實模型
只要在 2 秒的時間之內
可以生成出下一段音檔
你的聲音聽起來
就不會有不自然的地方
而生下一段音檔
只需要 0.5 秒的時間
意味著說
其實你的模型
有 1.5 秒的時間
它是沒有事情做的
有 1.5 秒的時間
可以用作 buffer
那我們可以拿
那 1.5 秒的時間
來做什麼呢
能不能讓它
就用那 1.5 秒的時間
來做 Reasoning
能不能讓模型
在這 1.5 秒的時間之內
產生一些 Reasoning 的 Token
1.5 秒的時間
足夠模型產生
至少 100 個 Reasoning 的 Token
透過這些 Reasoning 的 Token
來強化模型的能力
而要模型產生語音的 Token
再產生 Reasoning 的 Token
再產生語音的 Token
其實不需要改內容
它就是一個正常的語言模型
你只需要改你的訓練資料而已
所以 Stitch 提出來的想法是這個樣子
一般我們說要加上 reasoning
是你對模型說一句話
然後它產生文字的 reasoning
然後它再給你答案
它產生的是語音的 token
需要 detokenize 的還原聲音的訊號
但 Stitch 的做法是這樣
你跟模型說一句話
它先生出語音的 token
馬上把語音的 token 還原出聲音訊號
它開始可能就是重複你的題目這樣
你說什麼
它就先照唸一遍
然後說這真是個好問題
然後再開始回答
然後它接下來就會產生 reasoning 的 token
只要在這段聲音播完之前
它可以生 reasoning 的 token
再生足夠的語音 token
再合出下一段聲音
在前段聲音播完之前
下一段聲音已經被生成出來
聲音聽起來就不會有不自然的地方
而且同時在 idle 的時候
它還可以生一些文字的 token
等於是它一邊講話的時候
一邊思考
它又沒有在講話的時候
來思考深入思考這個問題
然後得到更好的答案
接下來問題就是怎麼訓練這樣子的模型呢
那現在世界上已經有很多訓練語音到語音模型的對話資料
那這些對話資料就是
你有一句輸入
有一句輸出
它們都是語音的
那我們這邊做的事情就是
先產生它們的文字
做 transcription
做 ASR
做語音辨識
把它變成文字
把這些文字丟給 GPT-4o
跟它說
現在如果要產生這個答案
你覺得要做什麼樣的 reasoning
叫它把 reasoning 的過程寫出來
那接下來在訓練的時候
我們就把這個 reasoning 的過程
切成一段一段的 100 個 token 一段
每 100 個 token 一段
那語音的 output 呢
其實要切成一段一段的啦
就是 39 個 token 一段
然後接下來你就教你的語音語言模型說
輸出語音的第一段 token 以後
接下來就輸出 reasoning 的第一段 token
再輸出語音的第二段 token
再輸出 reasoning 的第二段 token
就這樣持續下去
那模型就會學會
在整兩段語音之間
用一些空檔
開始進行思考
這一招有沒有用呢
那這邊姜成翰同學的實驗就是
他再進一步的 fine-tune
GLM-4 Voice
GLM-4 Voice 本來是沒有 reasoning 的能力的
他就幫它 fine-tune 出 reasoning 的能力
結果怎麼樣呢
他這邊是測試在一系列的數學問題上
那這邊是秀說
那些不同的數學 corpus 的平均
那如果是原版的 GLM-4 Voice
它的正確率呢
是 53% 左右
那我們做完 fine-tune 之後
如果做完 fine-tune 之後
如果沒有加上 reasoning
它其實可以從 53% 進步到
大概 63%
還是有進步 10% 的
因為可能我們的一些訓練資料
是 GLM 沒有的
所以你在 fine-tune 它一下
就算沒有幫它加 reasoning 的能力
還是有一些 improve 的
那如果你幫它加上 reasoning 的能力
但不是用 Stitch 呢
那你可以得到 79% 的正確率
也就是模型
聽一段聲音以後
它先在腦中產生一大堆文字的 token
它想完之後
不管多久
人就要等它
然後它再產生它的答案
那這樣可以得到 79% 的正確率
那我們這邊呢
如果用 Stitch 的方法
我們有兩個實驗的結果
那這兩個實驗結果的細節差異
大家再自己去看論文
那我們呢
如果看這個比較差的結果的話
也可以做到 78% 左右的正確率
而且要注意一下
當我們有 reasoning 的時候
用傳統的 reasoning
人必須要等模型做 reasoning
這是一個很不自然的互動
但是當我們用 Stitch 的時候
我們這一個模型
它的 delay
人的等待的時間
跟沒有 reasoning 的狀況
是一樣的
所以但是我們透過應用
這個兩次生語音 token 中間
間隔的時間來做 reasoning
可以從 62% 進步到 78%
那相較於讓人等待
其實也只掉了 1% 的正確率而已
那另外這邊要強調一下
雖然我沒有放實驗結果
Paper 裡面有
模型的 audio quality
是不會受到
有做 Stitch 這件事情
而受到影響的
那我們還另外做了一個實驗
成翰同學做另外一個實驗
是這樣的
他在設想說
如果有 A100
那生 39 個 token 只要 0.5 秒
但假設你沒有 A100 呢
也不是每個人都有 A100 的
如果你今天有比較
持有比較差的 GPU
那你要生語音 token 的時間
就比較長了
那你的 buffer 就比較短了
那你可以生的文字 token
就比較少了
如果今天在 inference 的時候
我們有沒有可能
在 training 的時候
雖然都是 train 產生
100 個 reasoning token
但在 inference 的時候
根據我們現在用的 GPU 的等級
去 dynamic 的調控
產生多少文字的 token 呢
他發現是可以的
就訓練的時候
都是固定 100 個 token
但 inference 的時候
如果我們減少 token 的數目
這個 reasoning
還是可以帶來幫助的
那以下是實驗的結果
這邊的橫軸啊
橫軸是 reasoning 用的 token 的數目
從 60 個到 100
那這邊不同的線呢
代表不同的 corpus 上面的結果
那縱軸是正確率
所以數值越大越好
那這邊你可以很明顯的看到說
token 越多
當然 performance 越好
雖然 token 越多
performance 越好
但是這邊這些
沒有連起來的點啊
它代表的是
沒有 reasoning 的結果
那你會發現
雖然 reasoning 常比較好
但沒有 reasoning
在很多狀況下
還是帶有幫助的
所以就算是你在 inference 的時候
你沒有好的 GPU
你仍然讓模型做 reasoning
只是 reasoning 的這個 token 數目
比較少一點
你仍然有辦法
用 reasoning 的方式
來 improve 模型的能力
那我們再更進一步
我們再更進一步說
reasoning 的過程
能不能不要由
語音模型自己生成
畢竟語音模型
往往是比較專注於
生成好聽的聲音
所以它 reasoning 的能力
可能沒有那些文字模型那麼強
那我們能不能用
另外一個文字模型
在背後做 reasoning
把它 reasoning 的結果
丟給語音模型
藉此強化語音模型的答案呢
還真的是可以的
這邊的做法就是
當你給語音模型
一段聲音訊號的時候
你同時做語音辨識
把那段語音辨識的文字
丟給一個文字模型
接下來語音模型
會產生語音的 token
本來我們會讓語音模型
自己產生 reasoning 的 token
但我們現在不用語音模型
自己的 reasoning
讓文字模型產生 reasoning 的結果
讓文字模型根據文字的輸入
做完語音辨識得到的結果
還有語音模型的輸出
語音模型的輸出
其實是有文字的
所以文字模型
其實也可以看得懂文字的部分
把語音模型文字的輸出
丟給文字模型
文字模型產生 reasoning 的結果
文字模型產生 reasoning 的結果之後
語音模型再根據
文字模型 reasoning 的結果
產生語音的 token
這個過程就不斷的反覆
看看能不能透過文字模型 reasoning
來強化語音模型的能力
結果還真的是可以的
這邊的縱軸是模型
在各個不同 corpus 上的
正確率的平均
每一條線代表我們用了
不同的模型來產生 reasoning
紅色這條線是
模型的 reasoning
文字的 reasoning
跟生成語音用的是同一個模型
這邊最好的這條線
是換了一個文字的模型
這邊用的是 GLM-4 9B Chat
它是一個比較強的文字模型
讓它來產生 reasoning
讓它來產生 reasoning
再由語音模型來回答答案
可以得到更好的結果
如果你把文字模型
故意用一個爛的
用 Llama 3.2 1B
你會發現說結果就差很多
這個實驗結果告訴我們
一件事情是
reasoning 是真的對模型有影響的
它並不是說
我就 reasoning 亂講
我就不要管 reasoning
我就光產生答案就好
其實 reasoning 是會影響
模型的結果的
如果給它差的 reasoning
它最後就是得到差的結果
最後這是一個 demo
讓你知道說
這個模型運作起來
是什麼樣子
等一下你就會聽到
有人唸一個數學的問題
然後模型用語音
來回答這個問題
來播放這個音檔
所以你剛才聽到模型講話的時候
是沒有任何的停頓的
它就是順順的把話說出來
但是你可以看到它在背後
其實是做了一個 reasoning 的過程
你看模型實際上運作過程是這樣子的
紅色的東西代表的是語音的 token
所以那是人看不懂的
那就是一堆語音的 token
黃色的東西是文字的 token
但那些文字的 token
是來 support 那些語音的 token 的
而綠色的東西
是模型在做 reasoning 的時候
說出來的文字
你看這邊它會條列一點
兩點三點四點
但這些是不會被唸出來的
你就想成這些東西是
想在模型的心裡
人類不需要看這麼複雜的東西
你就發現說
模型一開始
先複述這個問題
然後用一些語音
產生一些語音的 token
在複述這個問題的同時
它在心裡就開始條列
想說這個問題要怎麼解
然後再講一些話
再想要怎麼解
再想一些話
再想要怎麼解
所以剛才聽到模型
並沒有列出很完整的計算過程
但是它可以給你一個正確的答案
這個就是 Stitch 做的事情
好
那其實在有了 Stitch 之後
之後接下來也有很多模型
有類似的想法
比如說讓模型一邊講話
一邊使用工具
一邊聽聲音
一邊使用工具等等
那我今天就把一些相關的論文列在這邊
給大家參考
好
那這就是今天想要跟大家分享的內容
那我今天要講的是
我用第一人稱的視角
我們團隊曾經參與的研究
但是這邊還有很多很多
沒有講到的跟語言
語音語言模型有關的技術
那有一個很關鍵的技術
還沒有講到的地方是
其實對語音語言模型來說
有一個非常重要需要具有的技能
就是它要能夠超越回合制的問答
什麼叫超越回合制的問答呢
通常一般我們在使用文字模型的時候
比如說用 ChatGPT 的時候
就是一問一答
你給它一個輸入
它給你一個輸出
它輸出完才輪到你講話
但是在語音的對話裡面
這個對話是 Full Duplex 的
Full Duplex 這個字會是什麼意思呢
Full Duplex 意思是說
這個溝通是雙向的
兩個人是可能同時發出聲音訊號的
比如說其中一個人
他在講一件什麼事情
你就會發出一些聲音
比如說你說的都對
代表說你有在聽他講話
在一邊講的時候
你得一邊發出聲音
這樣才能夠變成一個自然的對話
或者是你在講話的時候
對方說這不是我想聽的
你停下來
你會同時聽他在講什麼
然後他叫你停下來
你可能就會停下來
或者是你覺得他說的根本是錯的
你可能會主動插話打斷
他說不不不
你說的是錯的不要再講了
他在講話
你還是繼續講
想要聲音壓過它
所以今天兩個人在對話的時候
其實可能是同時在說話的
要怎麼處理這個問題呢
今天當然有不同的解法
但是還沒有標準的答案
但是在有標準答案之前
也許我們會要知道
怎麼樣的答案
才是標準的
我們期待一個 Full Duplex 的模型
可以做到什麼樣的事情呢
所以我們實驗室的林冠廷同學
跟上官思雲同學
就做了一系列
跟 Full Duplex 有關的 Evaluation
看看今天號稱可以做 Full Duplex 的模型
他們都能做到什麼樣的事情
因為今天時間有限的關係
我們就不再細講這個部分
我把論文連結列在這邊給大家參考
那還有很多能力
也許我們是希望有一天
語音語言模型可以具備的
舉例來說
語音語言模型既然要跟人類互動
那它要跟存在於現實中的人類互動
它就得知道人類物理世界的時間流逝
今天語音語言模型
能夠知道物理世界的時間流逝嗎
它對物理世界的時間有觀念嗎
看起來是沒有的
張凱為同學
其實在這方面做了一系列的測試
你可以看看他這篇論文
比如說
如果我們今天問這些語音語言模型
叫它十秒鐘不要說話
基本上沒有語音語言模型
能真的做到這件事情
因為它根本沒有十秒這樣子的概念
它根本不知道多長是十秒
那如何讓語音語言模型能夠意識到
現實生活中真實的物理世界時間的流逝
這也是另外一個研究的問題
總之還有太多未解的問題
是尚待研究的
最後假設你想要知道
更多跟語音語言模型有關的事情的話
你可以參考一下
我跟其他學者寫的一篇 overview paper
我們就是從語音語言模型的起源開始講起
一直講到今日語音語言模型的發展
好這就是我今天想跟大家分享的內容