---
author: Best Partners TV
date: '2026-02-03'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=baJ72mP_fKo
speaker: Best Partners TV
tags:
  - long-term-agent
  - autonomous-planning
  - harness-architecture
  - ai-development
  - future-of-ai
title: 长程Agent：AI的未来行动者与自主规划新时代
summary: 本次访谈深入探讨了长程Agent的核心概念、技术演进（从Framework到Harness）、关键技术（压缩、文件系统、调度）及其在编程、AI SRE、科研等领域的爆发式应用。访谈揭示了Agent的自主规划、长时间运行能力，并展望了记忆、混合交互模式、状态可视化UI及代码沙箱等未来发展方向，预示着AI行动者纪元的到来。
insight: ''
draft: true
series: ''
category: ai-ml
area: tech-engineering
project: []
people: []
companies_orgs:
  - LangChain
  - OpenAI
  - Anthropic
  - Google
products_models:
  - GPT-5.2
  - Claude Opus 4.5
  - Gemini 3
  - Claude Code
media_books:
  - 二零二六:这是AGI
status: evergreen
---
### 引言：AI新范式

**主持人**: 大家好，这里是最佳拍档。二零二六年的AI，可能不再是聊天框里的问答机器人了。红杉资本在《二零二六:这是AGI》的文章中给出了一个定义：AGI的核心不是能说会道，而是能把事情搞定的能力。而承载这种能力的关键载体，就是今天我们要聊到的**长程Agent**。二零二三年，**AutoGPT**的爆火让我们第一次窥见了AI自主做事的可能，但是那时候的技术，还只能够停留在概念验证上。而到了二零二六年，情况彻底不同了。模型能力的跃迁、配套工具链的成熟，让**长程Agent**开始真正从实验室走向产业落地。作为这场变革的核心推动者，**LangChain**的创始人**哈里森·蔡斯**，站在Agent基础设施的最前沿，和**红杉资本**的索尼娅·黄、帕特·格雷迪，展开了一场深度对话，揭秘了**长程Agent**爆发的底层逻辑、技术架构，以及未来十年的产业走向。今天，我们就借着这场访谈，聊一聊**长程Agent**的来龙去脉、核心细节和应用前景。

<details>
<summary>Original English</summary>

**Host**: Hello everyone, this is Best Partners. AI in 2026 may no longer be just a chatbot Q&A robot. Sequoia Capital, in its article "2026: This is AGI," provided a definition: the core of AGI is not eloquence, but the ability to get things done. And the key carrier of this capability is what we will discuss today: **long-term agents**. In 2023, the explosion of **AutoGPT** gave us the first glimpse of AI's potential for autonomous action, but the technology at the time could only stay at the proof-of-concept stage. By 2026, the situation is completely different. The leap in model capabilities and the maturity of supporting toolchains have allowed **long-term agents** to truly move from the lab to industrial application. As a core driver of this transformation, **Harrison Chase**, founder of **LangChain**, stands at the forefront of agent infrastructure, engaging in a deep dialogue with **Sequoia Capital**'s Sonya Huang and Pat Grady, revealing the underlying logic, technical architecture, and future industry trends of the **long-term agent** explosion. Today, we will use this interview to discuss the origins, core details, and application prospects of **long-term agents**.

</details>

### 长程Agent的本质

**主持人**: 首先我们得先搞懂一个核心问题：**长程Agent**和我们之前用的AI有什么本质上的区别呢？过去的AI，无论是ChatGPT还是各类垂直领域的问答工具，本质上都是一个即时响应者。你输入一个问题，它基于上下文给出一个答案，对话结束，任务也就终止了。它们就像健谈者一样，擅长解释、描述，但是很难独立完成需要多步骤、长时间推进的复杂任务。而**长程Agent**则完全不同。它的核心特征是自主规划、长时间运行、目标导向。你不需要一步步的指导它，只需要给出一个明确的目标，它就能在后台自主循环，从思考到规划、到行动、再到观察的整个流程，跨越几个小时、几天甚至更长的时间，逐步推进任务，最终产出一个成型的结果。

<details>
<summary>Original English</summary>

**Host**: First, we need to understand a core question: What is the essential difference between **long-term agents** and the AI we used before? Past AI, whether it was ChatGPT or various vertical Q&A tools, was essentially an instant responder. You input a question, it gives an answer based on the context, the conversation ends, and the task terminates. They are like talkative individuals, good at explaining and describing, but find it difficult to independently complete complex tasks that require multiple steps and long-term progression. **Long-term agents**, however, are completely different. Their core characteristics are autonomous planning, long-term operation, and goal orientation. You don't need to guide them step-by-step; just provide a clear goal, and they can autonomously loop in the background, going through the entire process from thinking to planning, to action, and to observation, spanning hours, days, or even longer, gradually advancing the task to ultimately produce a finished result.

</details>

### 爆发的两大关键

**主持人**: **蔡斯**在访谈中明确的表示，**长程Agent**终于开始真正能工作了。这背后是两大关键因素的共同作用。第一个关键因素是模型能力的质变。二零二三年**AutoGPT**之所以没能普及，核心问题就是当时的模型还不够聪明，推理能力不足，无法在复杂任务中做出正确决策，也难以处理长程任务中的上下文管理。而经过两年的迭代，无论是**OpenAI**的**GPT-5.2**系列、**Anthropic**的**Claude Opus 4.5**，还是**Google**的**Gemini 3**系列，都在推理、工具使用和长上下文理解上实现了飞跃。尤其是**Claude Code**，针对代码编辑、文件处理等场景做了专门优化，让模型具备了自主操作计算机的基础能力。第二个关键因素是配套设施的成熟。这里的核心就是**Harness**和**Scaffolding**，也就是软件外壳和辅助框架的完善。**蔡斯**解释道，早期的Agent开发，我们更多依赖**Scaffolding**，也就是围绕着模型搭建的辅助代码结构，它能帮我们引导模型的输出、管理简单的流程，但是缺乏复杂的自主规划能力。而现在，我们进入了**Harness**时代。这种软件环境不仅包裹着模型，还内置了规划工具、文件系统交互能力以及最佳实践，形成了一个开箱即用的强预设系统。简单来说，以前的**Scaffolding**是给模型搭个架子让它站着，现在的**Harness**是给模型配了个工具箱，让它干活。

<details>
<summary>Original English</summary>

**Host**: **Chase** explicitly stated in the interview that **long-term agents** are finally starting to truly work. Behind this are two key factors working together. The first key factor is the qualitative change in model capabilities. The core reason **AutoGPT** failed to gain widespread adoption in 2023 was that the models at the time were not smart enough, lacked sufficient reasoning ability, could not make correct decisions in complex tasks, and struggled with context management in long-term tasks. After two years of iteration, whether it's **OpenAI**'s **GPT-5.2** series, **Anthropic**'s **Claude Opus 4.5**, or **Google**'s **Gemini 3** series, they have all achieved breakthroughs in reasoning, tool usage, and long-context understanding. **Claude Code**, in particular, has been specifically optimized for scenarios like code editing and file handling, giving models the fundamental ability to operate computers autonomously. The second key factor is the maturity of supporting infrastructure. The core here is **Harness** and **Scaffolding**, referring to the refinement of software shells and auxiliary frameworks. **Chase** explained that in early agent development, we relied more on **Scaffolding**, which is the auxiliary code structure built around the model. It could help guide the model's output and manage simple workflows, but lacked complex autonomous planning capabilities. Now, we have entered the **Harness** era. This software environment not only encapsulates the model but also integrates planning tools, file system interaction capabilities, and best practices, forming a ready-to-use, strongly pre-set system. Simply put, **Scaffolding** used to be about building a frame for the model to stand on, while **Harness** now provides the model with a toolbox to get work done.

</details>

### 典型应用场景

**主持人**: 这两大因素的叠加，让**长程Agent**的应用场景快速爆发。而最典型的爆发领域就是编程。**蔡斯**作为每天都在使用这类工具的开发者，直言编程领域的**长程Agent**案例最多，也最成熟。现在提交一个开发需求，比如搭建一个用户登录系统，需要包含短信验证和密码加密的功能，**长程Agent**可以自主规划开发的步骤：先创建项目的结构、编写核心代码、调试依赖包、生成测试用例，最后提交一个完整的代码合并请求，等待人类开发者的审核。这种产出初稿加人类优化的模式，已经成为了很多科技公司的日常。除了编程，还有两个场景同样值得关注。第一个是AI SRE。比如红杉投资的**Traversal**公司，他们的AI SRE能够自主处理长时程的系统运维任务，深入挖掘服务器日志、诊断潜在故障、甚至自动修复简单的软件漏洞，把原本需要工程师花费几小时的排查工作，压缩到几十分钟内完成。第二个是科研。**长程Agent**可以自主检索文献、整理实验数据、分析研究趋势，最终生成一份结构化的科研报告初稿。科研人员只需要在此基础上补充细节、验证结论即可。**蔡斯**特别强调，**长程Agent**的核心价值，就是为复杂任务提供高质量的初稿。它不需要达到百分之九十九点九的可靠性，毕竟复杂任务的最终把关还需要人类，但是它能够承担百分之九十以上的重复性、流程性工作，把人类从繁琐的劳动中解放出来。比如金融领域的**Klarna**，这家先买后付的支付公司，已经落地了人机协作模式。一线AI无法处理的复杂用户咨询，不会直接丢给人工客服，而是由后台运行的**长程Agent**，先生成一份包含前因后果的总结报告，包括用户诉求、之前的沟通记录、可能的解决方案等等内容。人工客服拿到报告后，能够快速的进行对接，从而大幅提升处理的效率。

<details>
<summary>Original English</summary>

**Host**: The combination of these two factors has led to a rapid explosion in the application scenarios for **long-term agents**. The most typical area of explosion is programming. **Chase**, as a developer who uses these tools daily, directly states that programming is where **long-term agents** have the most cases and are the most mature. Now, if you submit a development request, such as building a user login system with SMS verification and password encryption, a **long-term agent** can autonomously plan the development steps: first create the project structure, write the core code, debug dependencies, generate test cases, and finally submit a complete code merge request for human developer review. This model of producing a draft followed by human optimization has become the daily routine for many tech companies. Besides programming, two other scenarios are equally noteworthy. The first is AI SRE. For instance, **Traversal**, a company invested in by Sequoia, has AI SREs that can autonomously handle long-term system operations tasks, delve into server logs, diagnose potential failures, and even automatically fix simple software vulnerabilities, compressing hours of troubleshooting work by engineers into tens of minutes. The second is scientific research. **Long-term agents** can autonomously search literature, organize experimental data, analyze research trends, and ultimately generate a structured draft of a research report. Researchers only need to supplement details and verify conclusions based on this draft. **Chase** specifically emphasizes that the core value of **long-term agents** is providing high-quality drafts for complex tasks. They don't need to achieve 99.9% reliability, as human oversight is still required for final checks on complex tasks, but they can handle over 90% of repetitive, procedural work, freeing humans from tedious labor. For example, **Klarna** in the financial sector, a buy-now-pay-later payment company, has implemented a human-AI collaboration model. Complex user inquiries that frontline AI cannot handle are not directly escalated to human customer service. Instead, a background **long-term agent** first generates a summary report detailing the cause and effect, including user requests, previous communication records, potential solutions, and more. Human customer service representatives can then quickly follow up after receiving the report, significantly improving handling efficiency.

</details>

### Framework到Harness

**主持人**: 要理解**长程Agent**的爆发，我们还必须搞懂它的底层架构，尤其是**蔡斯**反复强调的，从**Framework**到**Harness**的演进。这不仅是**LangChain**的发展路径，更是整个Agent行业的技术趋势。首先，我们需要明确三个核心概念的区别，这也是很多从业者容易混淆的点。第一个是模型，也就是我们常说的大语言模型。它的本质很简单：输入Token，输出Token。无论是**GPT-5.2**、**Claude Opus 4.5**还是**Gemini 3 Pro**，核心功能都是理解文本加生成文本。这是**长程Agent**的大脑，但是光有大脑还不够。第二个是**Framework**。这是**LangChain**早期的核心定位，围绕模型建立起来的一个抽象层。它的特点是无预设，核心价值是灵活，让开发者可以轻松切换不同的模型、添加工具、对接向量数据库和记忆模块。比如你用**LangChain**的框架，今天可以对接**OpenAI**的模型，明天可以换成**Anthropic**的模型，不需要大幅修改代码。但是框架的问题也很明显，它只提供了零件，你需要自己组装成一个完整的系统，对于复杂任务来说，开发成本很高。第三个是**Harness**。这是**长程Agent**时代的核心架构。正如**LangChain**推出的**Deep Agents**，就是基于**Harness**的典型案例。它的特点是强预设，认为这样做事才是正确的，所以内置了规划工具、文件系统的交互能力、子Agent的调度机制等等，是一个开箱即用的完整解决方案。如果说**Framework**是一堆高质量的乐高零件，那么**Harness**就是已经拼好的乐高模型，你只需要根据自己的需求微调一下，就能直接使用。

<details>
<summary>Original English</summary>

**Host**: To understand the explosion of **long-term agents**, we must also grasp their underlying architecture, especially the evolution from **Framework** to **Harness**, which **Chase** repeatedly emphasizes. This is not only **LangChain**'s development path but also a technical trend for the entire agent industry. First, we need to clarify the distinction between three core concepts, which often confuse many practitioners. The first is the model, commonly known as the large language model. Its essence is simple: input tokens, output tokens. Whether it's **GPT-5.2**, **Claude Opus 4.5**, or **Gemini 3 Pro**, the core function is understanding text and generating text. This is the brain of a **long-term agent**, but a brain alone is not enough. The second is **Framework**. This was **LangChain**'s early core positioning, an abstraction layer built around the model. Its characteristic is being un-pre-set, with its core value being flexibility, allowing developers to easily switch between different models, add tools, and connect to vector databases and memory modules. For example, using **LangChain**'s framework, you can connect to **OpenAI**'s model today and switch to **Anthropic**'s model tomorrow without significant code modifications. However, the framework's problem is also evident: it only provides parts, and you need to assemble them into a complete system, leading to high development costs for complex tasks. The third is **Harness**. This is the core architecture of the **long-term agent** era. For instance, **Deep Agents** launched by **LangChain** is a typical example based on **Harness**. Its characteristic is strong pre-setting, believing this is the correct way to do things, thus integrating planning tools, file system interaction capabilities, sub-agent scheduling mechanisms, etc., making it a ready-to-use complete solution. If **Framework** is a pile of high-quality Lego bricks, then **Harness** is a pre-assembled Lego model that you only need to fine-tune according to your needs to use directly.

</details>

### Harness核心技术

**主持人**: **蔡斯**在访谈中透露，**Harness**的核心竞争力来自三个关键技术。第一个是压缩。**长程Agent**的运行时间很长，哪怕是现在最大的模型，上下文窗口也是有限的。当任务推进到一定的阶段，上下文会被占满，这时候就需要取舍：哪些信息要保留在上下文里，哪些信息需要暂时存储起来。比如，可能需要把之前的对话记录、工具调用结果，压缩成摘要保留在上下文里，而原始数据存储到文件系统中，模型需要的时侯再去查阅。这听起来简单，但是怎么判断哪些信息重要，怎么平衡压缩效率和信息的完整性，是**Harness**设计的核心难点，也是目前前沿研究的一个重点。第二个是文件系统的交互。**蔡斯**是文件系统优先的坚定支持者，他认为文件系统权限将成为所有Agent的标配。为什么呢？因为文件系统是管理长程任务数据的最佳载体。模型可以把中间结果、原始数据、规划方案都存储在文件里，避免上下文过载。同时，文件系统的结构化存储方式，也让模型更容易追溯历史操作，复用之前的成果。比如，一个科研Agent可以把检索到的文献存储为PDF文件，把数据分析代码存储为Python脚本，以及把报告初稿存储为Markdown文件。这样在后续推进任务时，就可以直接调用这些文件。甚至**LangChain**还推出了虚拟文件系统的概念，底层由Redis等数据库支持，稳定性更高，避免了真实文件系统的安全风险。第三个是子Agent和技能模块的调度。一个复杂任务往往需要多种能力，比如生成一份行业的研究报告，需要文献检索、数据统计、文本撰写、格式排版等多种技能。**Harness**的作用就是把这些技能拆分成不同的子Agent，由主模型统一调度。主模型负责规划整体流程，然后把文献检索的任务交给检索子Agent，把数据统计的任务交给数据分析的子Agent，最后汇总所有子Agent的结果，生成最终报告。这里的关键是协同，主模型和子Agent之间要传递完整的信息，避免出现子Agent做了一堆的工作，主模型却只收到一句“见上文”的情况。现在的开源**Harness**，系统提示词动辄几百行，核心就是为了解决这种协同问题。

<details>
<summary>Original English</summary>

**Host**: **Chase** revealed in the interview that the core competitiveness of **Harness** comes from three key technologies. The first is compression. **Long-term agents** run for a long time, and even the largest models today have limited context windows. When a task progresses to a certain stage, the context will be full, requiring trade-offs: which information to keep in context and which to temporarily store. For example, previous conversation logs and tool call results might need to be compressed into summaries for the context, while raw data is stored in the file system for retrieval when needed by the model. This sounds simple, but determining the importance of information and balancing compression efficiency with information completeness are core challenges in **Harness** design and a focus of current cutting-edge research. The second is file system interaction. **Chase** is a staunch advocate for file system priority, believing that file system permissions will become standard for all agents. Why? Because the file system is the best medium for managing long-term task data. Models can store intermediate results, raw data, and planning proposals in files, avoiding context overload. Furthermore, the structured storage of the file system makes it easier for models to trace historical operations and reuse previous achievements. For instance, a research agent can store retrieved literature as PDF files, data analysis code as Python scripts, and draft reports as Markdown files. These files can then be directly invoked when advancing the task later. **LangChain** has even introduced the concept of a virtual file system, supported by databases like Redis at the backend, offering higher stability and avoiding the security risks of real file systems. The third is the scheduling of sub-agents and skill modules. A complex task often requires multiple capabilities, such as generating an industry research report, which requires literature retrieval, data statistics, text writing, and formatting skills. The role of **Harness** is to break down these skills into different sub-agents, orchestrated by the main model. The main model is responsible for planning the overall workflow, then assigning the literature retrieval task to the retrieval sub-agent, the data statistics task to the data analysis sub-agent, and finally consolidating the results from all sub-agents to generate the final report. The key here is collaboration: complete information must be passed between the main model and sub-agents to avoid situations where a sub-agent does a lot of work, but the main model only receives a response like "see above." Current open-source **Harness** systems often feature system prompts hundreds of lines long, primarily aimed at solving these collaboration issues.

</details>

### 榜单与第三方优势

**主持人**: 这三大技术的结合，让**Harness**具备了支撑**长程Agent**的能力。而行业实践也证明了**Harness**的重要性。目前最权威的AI Agent终端能力榜单**Terminal-Bench 2.0**显示，同一个模型在不同的**Harness**加持下，性能差异巨大。比如，**OpenAI**的**GPT-5.2**，在**Factory**公司的**Droid Agent Harness**中，准确率达到了百分之六十四点九，排名第一。而在其他公司的**Harness**中，准确率可能只有百分之六十左右。更值得注意的是，榜单头部的玩家不仅有模型厂商，比如**OpenAI**、**Anthropic**，还有第三方创业公司，比如**Factory**、**JetBrains**等等。这说明，只要深刻理解了模型的特性，第三方开发者完全能够在**Harness**层面挖掘出巨大的性能提升。在这个领域，模型厂商并不具备天生的优势。**Factory**公司的**Droid Agent**就是一个典型的案例。它基于**GPT-5.2**和**Claude Opus 4.5**等模型，在**Terminal-Bench 2.0**的八十九个真实终端任务（包括编译代码、训练模型、搭建服务器、生物信息分析等等）中，准确率达到了百分之六十四点九，位居榜首。它的成功，核心就在于**Harness**的设计，充分利用了**GPT-5.2**在命令行上的训练优势，同时优化了上下文压缩和子Agent的调度机制，让模型能够高效处理长时程、多步骤的终端任务。**蔡斯**还提到，**Harness**的演进和模型的进化，是一个共同进化的关系。现在的模型之所以能够很好的适配文件系统、命令行工具，是因为它们在训练数据中包含了大量的代码、命令行操作数据。而**Harness**的发展，又会产生更多的交互数据，反过来促进模型的优化。两年前，我们不可能预知基于文件系统的**Harness**会成为主流，因为当时的模型还没有针对这些场景进行充分训练。而现在，模型和**Harness**的相互成就，终于让**长程Agent**实现了规模化落地。

<details>
<summary>Original English</summary>

**Host**: The combination of these three technologies enables **Harness** to support **long-term agents**. Industry practices also demonstrate the importance of **Harness**. The current most authoritative AI Agent terminal capability leaderboard, **Terminal-Bench 2.0**, shows significant performance differences for the same model when powered by different **Harness** systems. For example, **OpenAI**'s **GPT-5.2**, within **Factory**'s **Droid Agent Harness**, achieved an accuracy of 64.9%, ranking first. In other companies' **Harness** systems, the accuracy might be around 60%. Notably, top players on the leaderboard include not only model providers like **OpenAI** and **Anthropic**, but also third-party startups like **Factory** and **JetBrains**. This indicates that by deeply understanding model characteristics, third-party developers can significantly enhance performance at the **Harness** level. In this domain, model vendors do not possess an inherent advantage. **Factory**'s **Droid Agent** is a prime example. Based on models like **GPT-5.2** and **Claude Opus 4.5**, it ranked first on **Terminal-Bench 2.0** with an accuracy of 64.9% across 89 real-world terminal tasks, including code compilation, model training, server setup, and bioinformatics analysis. Its success lies primarily in its **Harness** design, which fully leverages **GPT-5.2**'s training advantages in command-line operations while optimizing context compression and sub-agent scheduling mechanisms, enabling the model to efficiently handle long-term, multi-step terminal tasks. **Chase** also mentioned that the evolution of **Harness** and model evolution are in a co-evolutionary relationship. Current models are well-adapted to file systems and command-line tools because their training data includes extensive code and command-line operation data. The development of **Harness**, in turn, generates more interaction data, which further promotes model optimization. Two years ago, we couldn't have predicted that file system-based **Harness** would become mainstream, as models at the time were not sufficiently trained for these scenarios. Now, the mutual advancement of models and **Harness** has finally enabled the large-scale deployment of **long-term agents**.

</details>

### Coding Agent的未来

**主持人**: 在访谈中，帕特·格雷迪抛出了一个很有前瞻性的问题：他问，“**Coding Agent**到底是一个子类别，还是说所有的Agent本质上都应该是**Coding Agent**呢？”这个问题的核心，其实是在探讨通用AI的实现路径。如果AI的目标是让计算机干活，而代码是控制计算机最精准、最通用的方式，那么**Coding Agent**是不是就是通用AI的终极形态呢？**蔡斯**对这个问题的思考显然十分深入。他首先明确了一个前提，那就是构建**长程Agent**必须给它文件系统的权限，而编程能力其实是文件系统权限的延伸。文件系统能存储数据，而代码能操作数据、运行任务、实现复杂的逻辑。比如，一个金融Agent可以用代码编写脚本，自动抓取股票数据、计算均线、生成可视化图表。一个科研Agent可以用代码编写数据分析程序，处理实验数据、拟合模型、输出统计结果。这些操作，用自然语言指令是很难精准实现的，但是用代码可以做到零歧义。但是他也指出，**编程Agent**和具备文件系统权限的Agent并不是完全等同的。比如，**LangChain**的虚拟文件系统，不需要模型写代码，也能实现数据的存储和检索，满足一部分简单场景的需求。但是对于长尾的复杂用例，编程能力是无可替代的。你无法用虚拟文件系统来运行一个机器学习模型，也无法用虚拟文件系统搭建出一个Web服务器，而这些都需要代码才能实现。更重要的是，编程本身就是一种通用工具。无论你是在金融、科研、医疗还是教育领域，只要涉及到复杂的流程自动化、数据处理、系统搭建等任务，编程都是核心的手段之一。**蔡斯**甚至提到，即使是非编程的任务，你也可以通过编写代码来实现。所以代码本身就是很好的通用手段。这就引出了一个关键的结论：**Coding Agent**可能是目前最接近通用AI的形态，因为它不局限于某个垂直领域，而是通过代码这个通用语言，对接所有的计算机系统和工具，实现跨领域的复杂任务。当然，这并不意味着所有的Agent都必须会写代码。对于一些简单的、标准化的任务，自然语言指令加文件系统已经足够了。但是对于长时程、高复杂度、高精准度的任务，编程能力将成为**长程Agent**的核心竞争力。目前，行业的发展趋势也在印证这一点。**Terminal-Bench 2.0**榜单上的头部玩家几乎都是编程类的Agent，**Claude Code**、**Amp Code**等产品的爆火，也说明市场对AI编程能力的需求非常旺盛。更重要的是，**Coding Agent**的能力边界还在不断的扩展，从单一文件的代码生成，到多文件的项目开发，再到完整SaaS应用的构建，**Coding Agent**正在逐步具备端到端解决复杂工程问题的能力。当然，**蔡斯**也承认，这只是目前的阶段性判断。未来是不是会出现比代码更通用、更高效的计算机控制语言，还是一个未知数。但是就目前来看，**Coding Agent**是**长程Agent**发展的一个核心方向，也是目前最有可能实现通用AI的路径。

<details>
<summary>Original English</summary>

**Host**: During the interview, Pat Grady posed a forward-looking question: "Is a **Coding Agent** a subcategory, or should all agents essentially be **Coding Agents**?" The core of this question lies in exploring the path to achieving general AI. If the goal of AI is to make computers work, and code is the most precise and universal way to control computers, then is a **Coding Agent** the ultimate form of general AI? **Chase**'s thoughts on this are clearly profound. He first clarifies a prerequisite: building **long-term agents** requires granting them file system permissions, and programming ability is essentially an extension of file system permissions. File systems can store data, while code can operate on data, run tasks, and implement complex logic. For example, a finance agent can write scripts using code to automatically fetch stock data, calculate moving averages, and generate visualizations. A research agent can write data analysis programs to process experimental data, fit models, and output statistical results. These operations are difficult to achieve precisely with natural language instructions, but code allows for zero ambiguity. However, he also points out that **programming agents** and agents with file system permissions are not entirely equivalent. For instance, **LangChain**'s virtual file system can achieve data storage and retrieval without the model writing code, meeting the needs of some simpler scenarios. But for complex long-tail use cases, programming ability is irreplaceable. You cannot run a machine learning model with a virtual file system, nor can you build a web server with it; these require code. More importantly, programming itself is a universal tool. Whether in finance, research, healthcare, or education, any task involving complex process automation, data processing, or system construction relies heavily on programming. **Chase** even suggests that non-programming tasks can also be accomplished through writing code, making code a good universal means. This leads to a key conclusion: **Coding Agents** may be the closest form to general AI currently, as they are not limited to a specific vertical domain but use code as a universal language to interface with all computer systems and tools, achieving complex cross-domain tasks. Of course, this does not mean all agents must write code. For simple, standardized tasks, natural language instructions combined with file systems are sufficient. However, for long-term, highly complex, and precise tasks, programming ability will become the core competitiveness of **long-term agents**. Currently, industry trends support this view. Top players on the **Terminal-Bench 2.0** leaderboard are almost all programming-based agents, and the popularity of products like **Claude Code** and **Amp Code** indicates strong market demand for AI programming capabilities. More importantly, the capability boundaries of **Coding Agents** are continuously expanding, from generating single-file code to developing multi-file projects, and even building complete SaaS applications. **Coding Agents** are gradually gaining the ability to solve complex engineering problems end-to-end. **Chase** admits this is a current, phased judgment. Whether a more universal and efficient computer control language than code will emerge in the future remains unknown. But for now, **Coding Agents** represent a core direction for **long-term agent** development and the most likely path to achieving general AI.

</details>

### 与传统软件的区别

**主持人**: 很多传统软件的开发者在接触**长程Agent**的时候，都会有一个困惑：觉得这不就是自动化脚本的升级版吗？但是**蔡斯**明确指出，构建**长程Agent**和构建传统软件存在着本质上的区别。这种区别不仅体现在技术层面，更体现在开发理念、迭代模式和核心目标等方面。第一个核心区别，逻辑的来源不同。传统软件的逻辑是完全可见、完全可控的，所有的流程、判断、操作都写在代码里，开发者能够精准预测软件在任何场景下的行为。而**长程Agent**的逻辑，很大一部分来自于只提供了框架的模型代码，比如**Harness**的结构、工具的对接方式，而具体的决策、规划、操作步骤，都是模型根据上下文和目标自主生成的。这意味着，你无法只看代码就推断出Agent在特定场景下的行为，必须实际运行它，然后观察它的表现。这种非确定性黑盒系统的特性，给开发带来了巨大的挑战。比如，同一个需求，Agent在不同上下文环境下可能会生成完全不同的操作步骤，甚至在同一个任务中，前十三步的操作都很顺利，第十四步却因为上下文变化而出现偏差。这也是为什么**LangSmith**的**Tracing**功能如此受欢迎的原因之一。它能够复现Agent内部的每一步操作：模型的思考过程、工具的调用结果、上下文的变化、文件的修改记录。开发者可以通过**Tracing**功能精准定位Agent的决策偏差，而不是像传统软件那样，一边看着代码一边查bug。第二个核心区别，**Source of Truth**，也就是单一事实的来源不同。传统软件的事实来源是代码，软件的行为由代码唯一决定，只要代码不变，行为就不变。而**长程Agent**的事实来源是代码加**Tracing**。代码定义了基础的规则，而**Tracing**记录了Agent的实际行为和上下文变化。这意味着，Agent的开发和测试不能只依赖离线的单元测试，而必须依赖在线测试。只有让Agent接触真实世界的输入，观察它的行为涌现，才能发现问题、优化模型。更重要的是，**Tracing**正在成为团队协作的核心支点。在传统软件团队中，遇到问题大家会说，“去GitHub看代码吧”。而在Agent开发团队中，大家会说，“去看看**Tracing**吧”。比如，用户如果反馈某个Agent处理邮件时分类错误，开发者不需要去检查代码，而是应该通过**Tracing**来查看Agent的思考过程：它提取了哪些邮件的关键词？基于什么规则做的分类呢？是不是遗漏了什么关键信息呢？通过这些信息，能够快速定位问题的根源，比如可能是提示词的引导不够清晰，也可能是模型对某些关键词的理解有偏差，而不是代码本身的问题。第三个核心区别，是迭代的模式不同。传统软件的迭代是目标明确、行为可预测的。开发者先设定好功能目标，然后编写代码实现，测试通过后发布，发布前就能明确知道软件的行为。而**长程Agent**的迭代是目标模糊、行为涌现的。你可能知道要做一个邮件分类的Agent，但是你无法预知它在面对各种复杂邮件（比如带附件的、多语言的、内容模糊的邮件）时的表现。因此，Agent的开发需要更多的迭代反馈。发布后，通过收集用户的反馈、分析**Tracing**的数据、优化提示词和**Harness**，再发布测试，循环往复，直到Agent的表现能够达到预期为止。而这一切的核心支撑，就是**记忆**（**Memory**）。**蔡斯**在访谈中反复强调**记忆**的重要性。如果系统能够自我学习，就减少了开发者手动修改系统提示词的频率。Agent的**记忆**，本质上是长周期的上下文工程，它记录了之前的交互历史、任务经验、错误教训等等，让Agent在后续的任务中能够举一反三。比如，一个邮件Agent如果记住了用户把工作汇报类邮件都归类到重要文件夹，下次遇到类似邮件时就会自动分类。如果记住了用户曾经纠正过会议通知和培训通知的分类错误，下次就会更加精准的判断。**蔡斯**还分享了一个亲身经历：他之前用**LangChain**构建了一个邮件Agent，积累了很多记忆。后来他想把这个Agent迁移到**LangSmith Agent Builder**中，结果因为丢失了旧的记忆，哪怕提示词和工具跟之前完全相同，新Agent的体验也远远不如旧的那个。这个案例充分说明，**记忆**是Agent的核心护城河。一个经过了长时间磨合、内化了特定任务模式和背景记忆的Agent，它的竞争力是从零开始构建的Agent所无法比拟的。

<details>
<summary>Original English</summary>

**Host**: Many traditional software developers are puzzled when they encounter **long-term agents**, thinking, "Isn't this just an upgraded version of automation scripts?" However, **Chase** explicitly points out that there are fundamental differences between building **long-term agents** and traditional software. These differences are not only technical but also lie in development philosophy, iteration models, and core objectives. The first core difference is the origin of logic. The logic in traditional software is fully visible and controllable; all processes, judgments, and operations are written in code, allowing developers to precisely predict the software's behavior in any scenario. In contrast, a significant portion of **long-term agent** logic comes from the model code provided by the framework, such as the structure of **Harness** and how tools are integrated. The specific decisions, plans, and operational steps are autonomously generated by the model based on context and goals. This means you cannot infer an agent's behavior in a specific scenario just by looking at the code; you must run it and observe its performance. This non-deterministic black-box system characteristic poses significant development challenges. For example, for the same requirement, an agent might generate completely different operational steps in different contexts, or even within the same task, the first thirteen steps might go smoothly, but the fourteenth step deviates due to context changes. This is also why **LangSmith**'s **Tracing** feature is so popular. It can reproduce every step within the agent: the model's thought process, tool call results, context changes, and file modification records. Developers can use **Tracing** to pinpoint decision deviations precisely, rather than debugging traditional software by looking at code. The second core difference is the **Source of Truth**. In traditional software, the source of truth is the code; the software's behavior is solely determined by the code. As long as the code remains unchanged, the behavior remains unchanged. For **long-term agents**, the source of truth is code plus **Tracing**. Code defines the basic rules, while **Tracing** records the agent's actual behavior and context changes. This implies that agent development and testing cannot rely solely on offline unit tests but must involve online testing. Only by exposing the agent to real-world inputs and observing emergent behavior can problems be identified and models optimized. More importantly, **Tracing** is becoming a core pillar of team collaboration. In traditional software teams, when encountering issues, people say, "Check the code on GitHub." In agent development teams, they say, "Let's look at the **Tracing**." For instance, if a user reports that an agent misclassified an email, developers don't need to inspect the code; they should use **Tracing** to examine the agent's thought process: what keywords did it extract? Based on what rules did it classify? Did it miss any crucial information? This information can quickly pinpoint the root cause, which might be unclear prompt guidance or the model's misinterpretation of certain keywords, rather than an issue with the code itself. The third core difference is the iteration model. Traditional software iteration is goal-oriented and behavior-predictable. Developers set functional goals, write code to implement them, test, and release. The software's behavior is known before release. **Long-term agent** iteration, however, is goal-ambiguous and behavior-emergent. You might know you want to build an email classification agent, but you cannot predict its performance when faced with various complex emails (e.g., those with attachments, in multiple languages, or with ambiguous content). Therefore, agent development requires more iterative feedback. After release, through collecting user feedback, analyzing **Tracing** data, and optimizing prompts and **Harness**, followed by further testing, this cycle repeats until the agent's performance meets expectations. The core support for all of this is **Memory**. **Chase** repeatedly emphasizes the importance of **memory** in the interview. If a system can learn autonomously, it reduces the frequency of developers manually modifying system prompts. Agent **memory** is essentially long-term context engineering; it records past interaction history, task experiences, lessons learned from mistakes, etc., enabling agents to generalize from past experiences in subsequent tasks. For example, if an email agent remembers that the user always categorizes work report emails into the important folder, it will automatically classify similar emails next time. If it remembers that the user previously corrected the classification of meeting notices and training notices, it will make more accurate judgments in the future. **Chase** also shared a personal experience: he previously built an email agent using **LangChain**, accumulating significant memory. Later, he wanted to migrate this agent to **LangSmith Agent Builder**. Due to the loss of old memories, even with identical prompts and tools, the new agent's experience was far inferior to the old one. This case fully illustrates that **memory** is the agent's core moat. An agent that has undergone long-term refinement and internalized specific task patterns and background memories possesses a competitiveness that cannot be matched by agents built from scratch.

</details>

### 记忆与评估体系

**主持人**: 如果说**记忆**是Agent的经验库，那么评估体系就是Agent的自我修正机制。传统软件的评估很简单，通过程序化断言，比如输入A，输出是不是B，来判断功能是不是达标。但是**长程Agent**做的是人做的事，比如生成报告、编写代码、处理运维问题等等。对这些任务的评估，很难用非黑即白的规则来定义，必须引入人类判断或者类人的判断才行。这就引出了评估体系的两种核心方式。第一种是人类直接参与评估。比如，在**LangSmith**中，开发者可以通过**Annotation Queues**直接给Agent的行为标注反馈，比如“这一步做得好”、“这里分类错误了”、“应该优先处理这个任务”等等。这些自然语言反馈会被记录到**Tracing**中，成为优化Agent的核心依据。对于一些高价值、高风险的场景，比如生成金融报告、处理医疗数据等等，人类评估是不可或缺的，毕竟Agent的初稿需要人类把关，避免出现严重的错误。第二种是让大模型来当裁判。这种方式的核心是用一个更强大的大模型来评估Agent的行为是不是达标。比如，用**GPT-5.2**来评估一个编程Agent生成的代码是否正确、高效，用**Claude Opus 4.5**来评估一个科研Agent生成的报告是否严谨、全面。但是这里有一个关键的前提是，这个当裁判的模型必须和人类判断是对齐的。如果大模型的评判标准和人类的不一致，那么评估结果就是毫无意义的。为了解决这个问题，**LangSmith**推出了对齐评估的功能，先让人类标注一部分的**Tracing**数据，建立正确行为的样本库，然后用这些样本库来训练裁判模型，让它学习人类的评判标准。比如，人类标注了“代码必须包含错误处理逻辑才能达标”，那么裁判模型就会以这个为标准，评估Agent生成的代码是否符合要求。这种人类标注校准加大模型自动评估的模式，既保证了评估的准确性，又提高了评估的效率，非常适合大规模的Agent迭代。更重要的是，评估体系不仅是给开发者的反馈，更是Agent自我改进的依据。**蔡斯**把这称为迭代式的自我改进。通过查看**Tracing**数据，反思自己之前的行为，然后自动修改提示词、调整规划逻辑、优化工具的调用方式，实现Agent越用越聪明的效果。**LangSmith Agent Builder**就是一个典型的案例。这个工具允许用户用自然语言来描述需求，比如“帮我创建一个能读取Gmail、自动分类邮件并且草拟回复的助手”，它就能够自动生成一个完整的Agent。更酷的是，它具备自我改进的能力。当你和Agent交互时，如果你说“应该把会议通知归类到紧急文件夹，而不是普通文件夹”，Agent就会自动修改自己的指令文件，下次就会按照新的规则来分类邮件。**LangChain**还计划增加休眠期计算的功能，让Agent在每晚自动运行，查看当天的**Tracing**数据，总结经验和教训，更新自身状态，实现无人干预的自我优化。当然，这种自我改进并不是完全无人工干预的。**蔡斯**强调，最理想的状态是Agent产出初稿（比如修改了提示词），然后人类进行审核，确保它不跑偏。毕竟，Agent的自我改进可能会出现方向错误，比如它可能会过度优化某个功能导致其他功能失效，或者误解了用户的反馈做出不符合需求的修改。因此，**Human in the loop**是Agent自我改进的必要保障，也是目前最现实、最安全的模式。

<details>
<summary>Original English</summary>

**Host**: If **memory** is the agent's experience repository, then the evaluation system is the agent's self-correction mechanism. Evaluating traditional software is simple, using programmatic assertions like "if input A, is output B?" to determine if functionality meets standards. However, **long-term agents** perform human-like tasks, such as generating reports, writing code, and handling operational issues. Evaluating these tasks is difficult with simple binary rules; human or human-like judgment is necessary. This leads to two core methods for evaluation systems. The first is direct human evaluation. For instance, in **LangSmith**, developers can provide feedback on agent behavior through **Annotation Queues**, such as "this step was done well," "classification error here," or "prioritize this task." These natural language feedbacks are recorded in **Tracing** and become the basis for optimizing the agent. For high-value, high-risk scenarios like generating financial reports or processing medical data, human evaluation is indispensable, as agent drafts require human oversight to prevent serious errors. The second method is using large models as judges. The core idea here is to use a more powerful large model to assess whether the agent's behavior meets standards. For example, using **GPT-5.2** to evaluate the correctness and efficiency of code generated by a programming agent, or **Claude Opus 4.5** to assess the rigor and completeness of a research report generated by a research agent. However, a crucial prerequisite is that the judging model must be aligned with human judgment. If the large model's evaluation criteria differ from human standards, the results are meaningless. To address this, **LangSmith** introduced an alignment evaluation feature. Humans first annotate a portion of **Tracing** data to create a sample library of correct behaviors. This library is then used to train the judging model, teaching it human evaluation standards. For example, if humans annotate that "code must include error handling logic to be compliant," the judging model will use this standard to evaluate the agent's generated code. This approach of human-annotated calibration combined with automated model evaluation ensures accuracy and improves efficiency, making it highly suitable for large-scale agent iteration. More importantly, the evaluation system serves not only as feedback for developers but also as a basis for agent self-improvement. **Chase** calls this iterative self-improvement. By reviewing **Tracing** data, reflecting on past actions, and then automatically modifying prompts, adjusting planning logic, or optimizing tool usage, agents become smarter with use. **LangSmith Agent Builder** is a prime example. This tool allows users to describe requirements in natural language, such as "Create an assistant that can read Gmail, automatically classify emails, and draft replies," and it can automatically generate a complete agent. Even cooler, it possesses self-improvement capabilities. When you interact with the agent and say, "Meeting notifications should be classified as urgent, not regular," the agent automatically modifies its instruction file and will classify emails according to the new rule next time. **LangChain** also plans to add a "dormant period computation" feature, allowing agents to run automatically each night, review the day's **Tracing** data, summarize lessons learned, update their status, and achieve unsupervised self-optimization. Of course, this self-improvement is not entirely without human intervention. **Chase** emphasizes that the ideal state is for the agent to produce a draft (e.g., modify prompts), which humans then review to ensure it doesn't go off track. After all, an agent's self-improvement might lead it in the wrong direction; it might over-optimize one function, causing others to fail, or misunderstand user feedback and make inappropriate modifications. Therefore, **Human in the loop** is a necessary safeguard for agent self-improvement and is currently the most realistic and secure model.

</details>

### 未来交互形态

**主持人**: 聊完了技术架构和自我改进，我们最关心的问题来了：未来的**长程Agent**会以什么样的形态出现在我们的工作和生活中呢？我们会怎么和它们交互呢？它们的核心竞争力又是什么呢？**蔡斯**给出的第一个答案是**记忆**。他在访谈中直言说，“**记忆**是真正的护城河”。这个观点的核心在于长时程磨合的价值。一个Agent的价值，不仅在于它的初始能力，更在于它越用越懂你的过程，让Agent逐渐成为你的专属助手，而这种专属感是新构建的Agent所无法替代的。当然，**记忆**的价值也是有边界的。比如，ChatGPT虽然也加了**记忆**功能，但是很多用户并不常用，因为大家用ChatGPT都是做一次性任务，比如问一个代码问题、查一个旅游攻略等等，任务之间没有关联，**记忆**的作用自然有限。但是对于长时程、高频次、个性化的任务，比如科研、办公、金融分析来说，**记忆**的价值就会被无限放大。**蔡斯**的判断是，在垂直场景下，**记忆**绝对是大势所趋。除此以外，未来的**长程Agent**不会是纯同步或者纯异步的，而是同步模式加异步模式的混合形态。为什么需要异步模式呢？因为**长程Agent**的运行时间很长，可能需要几个小时才能生成一份科研报告，或者可能需要几天才能完成一个软件项目的开发。在这个过程中，用户不可能一直在等待，所以默认应该是异步的。你下达任务后，Agent在后台运行，完成后再通知你，你再进行后续的操作。像一些项目管理工具和电子邮件都是异步管理的很好参考，它们能够让你清晰的看到任务进度、状态，而不需要实时的关注。那为什么还需要同步模式呢？因为当Agent产出结果后，你需要即时反馈。比如，Agent生成的报告有错误，你需要马上指出；Agent编写的代码有漏洞，你需要实时调试。这时候，同步沟通模式就必不可少了。你可以直接和Agent聊天，指出问题在哪里，Agent实时进行修改，从而形成从反馈到修正的闭环。**蔡斯**还提到了一个关键的细节：现在的Agent不只是在说话，实际上它们是在修改状态，比如文件系统里的文件。这意味着，未来的交互界面必须能够是可视化状态的。就像程序员离不开用IDE来看代码一样，你需要知道Agent修改了哪些文件、调整了哪些参数、运行了哪些任务。这种状态的可视化，才是交互体验的核心。**Anthropic**的**Cowork**已经给出了一个很好的范式，它可以设置一个目录作为共享工作区，Agent和用户都能访问这个工作区，查看、修改文件。这种模式建立了一种清晰的心理模型，那就是我们在一个共享状态上进行协作。无论是本地文件、Google Drive还是Notion，共享状态的可视化都是交互的核心。基于混合交互模式和状态可视化的需求，未来的**长程Agent** UI会彻底告别现在的纯对话框形态，转向状态管理界面。这种界面会包含三个核心部分：第一，任务管理模块，参考Jira、Kanban等工具，显示Agent正在处理的任务、进度和优先级；第二，共享状态的可视化模块，显示文件系统、代码仓库、数据表格等等，让你能够直观看到Agent的操作结果；第三，同步沟通模块，内置聊天功能，方便你在需要的时候和Agent进行实时交互，给出反馈。简单说，未来的**长程Agent** UI会更像项目管理工具加IDE加聊天软件的结合体。它不再只是一个沟通的窗口，更是任务管理和状态监控的平台。只有这种界面设计，才能够完美适配异步管理加同步反馈的混合交互模式，让用户既能高效管理多个长时程任务，又能及时解决Agent遇到的问题。

<details>
<summary>Original English</summary>

**Host**: After discussing technical architecture and self-improvement, we come to our most pressing questions: In what form will future **long-term agents** appear in our work and lives? How will we interact with them? What will be their core competitiveness? **Chase**'s first answer is **memory**. He stated directly in the interview, "**Memory** is the true moat." The core of this viewpoint lies in the value of long-term refinement. An agent's value is not only in its initial capabilities but also in its process of understanding you better with use, gradually becoming your exclusive assistant, a sense of exclusivity that newly built agents cannot replicate. Of course, the value of **memory** has its limits. For example, although ChatGPT has added a **memory** function, many users don't use it often because they use ChatGPT for one-off tasks, like asking a coding question or looking up travel advice, where tasks are unrelated, limiting the role of **memory**. However, for long-term, high-frequency, personalized tasks like research, office work, or financial analysis, the value of **memory** is immensely amplified. **Chase**'s judgment is that in vertical scenarios, **memory** is definitely a major trend. Beyond that, future **long-term agents** will not be purely synchronous or purely asynchronous but a hybrid form of synchronous and asynchronous modes. Why is an asynchronous mode needed? Because **long-term agents** run for a long time; it might take hours to generate a research report, or days to complete a software project. During this process, users cannot wait indefinitely, so the default should be asynchronous. After you issue a task, the agent runs in the background and notifies you upon completion, after which you proceed with further operations. Project management tools and emails serve as good references for asynchronous management; they allow you to clearly see task progress and status without constant real-time attention. Then why is a synchronous mode still necessary? Because when the agent produces results, you need immediate feedback. For instance, if the agent's generated report contains errors, you need to point them out immediately; if the agent's code has vulnerabilities, you need real-time debugging. In such cases, synchronous communication becomes essential. You can chat directly with the agent, identify the problem, and the agent can make real-time modifications, forming a feedback-to-correction loop. **Chase** also mentioned a key detail: current agents are not just talking; they are actually modifying states, such as files in the file system. This implies that future interaction interfaces must be visually state-aware. Just as programmers rely on IDEs to view code, you need to know which files the agent has modified, which parameters it has adjusted, and which tasks it has executed. This visualization of state is the core of the interaction experience. **Anthropic**'s **Cowork** has provided a good paradigm: it allows setting up a directory as a shared workspace, accessible by both the agent and the user for viewing and modifying files. This model establishes a clear mental model: we are collaborating on a shared state. Whether it's local files, Google Drive, or Notion, the visualization of shared state is central to interaction. Based on the needs for hybrid interaction modes and state visualization, the UI for future **long-term agents** will completely abandon the current pure dialog box format and shift towards a state management interface. This interface will comprise three core components: First, a task management module, referencing tools like Jira and Kanban, displaying tasks the agent is currently handling, their progress, and priorities. Second, a shared state visualization module, showing file systems, code repositories, data tables, etc., allowing you to intuitively see the agent's operational results. Third, a synchronous communication module with built-in chat functionality, enabling real-time interaction and feedback when needed. In simple terms, the future **long-term agent** UI will be more like a combination of a project management tool, an IDE, and a chat application. It will no longer be just a communication window but a platform for task management and state monitoring. Only this interface design can perfectly accommodate the hybrid interaction model of asynchronous management and synchronous feedback, allowing users to efficiently manage multiple long-term tasks and promptly resolve issues encountered by the agent.

</details>

### 代码沙箱与安全

**主持人**: 最后，**蔡斯**明确指出，**代码沙箱**绝对是一个未来的核心组件。**代码沙箱**的作用是为Agent提供一个安全、隔离的运行环境。Agent可以在沙箱里编写代码、运行脚本、测试程序，而不会影响外部系统的安全。他还给出了三个判断：第一，文件系统访问是所有Agent的标配；第二，编程能力是**长程Agent**的核心竞争力；第三，浏览器访问目前还不够成熟。虽然有一些尝试，比如让Agent通过命令行操作浏览器，但是模型的准确性和稳定性还需要提升。这意味着，未来的**长程Agent**都会内置一个**代码沙箱**，具备文件系统的访问和编程能力，能够在隔离环境中安全的运行复杂任务。而浏览器访问，可能会在模型进一步优化后，成为一个补充能力。**红杉资本**说，**AGI**就是把事情搞定的能力，而**长程Agent**正是实现这种能力的关键一步。二零二六年，也许会是AI行动者纪元的起点。我们有理由相信，在未来几年，**长程Agent**将会彻底重构我们的工作和生活，让AI真正成为大家身边的最佳拍档。感谢收看本期视频，我们下期再见。

<details>
<summary>Original English</summary>

**Host**: Finally, **Chase** explicitly states that a **code sandbox** will absolutely be a core component of the future. The purpose of a **code sandbox** is to provide a secure, isolated execution environment for agents. Agents can write code, run scripts, and test programs within the sandbox without affecting the security of external systems. He also offered three judgments: First, file system access will be standard for all agents; second, programming ability is the core competitiveness of **long-term agents**; third, browser access is currently not mature enough. Although there have been attempts, such as allowing agents to control browsers via command line, the model's accuracy and stability still need improvement. This implies that future **long-term agents** will all come with a built-in **code sandbox**, possessing file system access and programming capabilities, allowing them to safely execute complex tasks in an isolated environment. Browser access, however, may become a supplementary capability after further model optimization. **Sequoia Capital** states that **AGI** is the ability to get things done, and **long-term agents** are a crucial step towards achieving this capability. 2026 may mark the beginning of the AI actor era. We have reason to believe that in the coming years, **long-term agents** will completely reshape our work and lives, truly making AI our best partner. Thank you for watching this episode; see you next time.

</details>