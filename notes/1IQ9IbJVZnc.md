---
area: society-systems
category: general
companies_orgs:
- OpenAI
- Anthropic
- Google DeepMind
date: '2025-11-25'
draft: true
guest: ''
insight: ''
layout: post.njk
people:
- Carl
- Hank Green
- Geoffrey Hinton
- John Hopfield
- Sam Altman
products_models:
- ChatGPT
- Claude Opus 4
project:
- ai-impact-analysis
- systems-thinking
- historical-insights
series: ''
source: https://www.youtube.com/watch?v=1IQ9IbJVZnc
speaker: Internet of Bugs
status: evergreen
summary: 本文深入剖析了YouTube频道SciShow关于人工智能的节目，指出其在AI发展速度、潜在风险评估以及行业宣传方面存在严重误导和信息缺失。作者通过对比AI与核能的发展历程，揭示了AI发展速度被夸大的事实，并强调了当前AI带来的虚假信息、操纵、失业和人权侵犯等实际问题，而非仅关注遥远的“灭绝”风险，呼吁警惕AI行业的话语权及其对公众认知的影响。
tags:
- ai-development-speed
- ai-misinformation
- industry
- media
- risk
title: 揭露SciShow对AI的误导：发展速度、真实风险与行业宣传
---

### 对SciShow AI节目的批判

SciShow 最近发布了一期关于人工智能的节目，尽管我喜欢 SciShow 并且讨厌 YouTuber 之间的争执，但我真的希望这一切没有发生。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">SciShow recently published an AI episode, and while I love SciShow and hate YouTuber drama, I really wish this wasn't happening.</p>
</details>

然而，他们在视频中说的一些内容实在太糟糕了，而且当前关于 AI 的错误信息正在引发诸多问题，我觉得自己不得不采取行动。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">However, what they said in that video was so bad, and misinformation about AI is causing so many problems now, that I feel compelled to do something.</p>
</details>

视频中的一些错误信息可能只是失误，有些只是遗漏，而有些则是行业宣传，但我坦白说，对于所有这些内容，我无法避免使用“谎言”这个词。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Some of that video's misinformation might just be mistakes, some are merely omissions, and some is industry propaganda; however, I honestly cannot avoid using the word "lie" for all of it.</p>
</details>

例如，以下这段话：
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">For example, consider this:</p>
</details>

[Hank] “但即使与飞机、抗生素和核能相比，我们开发人工智能的速度也超越了所有这些。”
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">"But even compared to aircraft, antibiotics, and nuclear power, the speed at which we are developing artificial intelligence beats them all."</p>
</details>

这简直是公然的谎言。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">That is just blatantly false.</p>
</details>

真诚的人们可能会对 AI 的生命周期所处阶段及其发展速度持有不同意见，我将在几分钟后对此进行更详细的阐述。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Sincere people might disagree about where we are in the lifecycle of AI and how fast it has been moving, and I will elaborate on that in a few minutes.</p>
</details>

但 AI 的发展速度绝不可能比原子能更快。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But there is NO WAY that AI is happening faster than atomic power.</p>
</details>

两者根本无法相提并论，这一点非常重要，因为在该视频的后面部分，AI 的假想危险被等同于原子武器的实际危险。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">It's not even close, and it really matters because later in that video, the hypothetical danger of AI is equated with the actual danger of atomic weapons.</p>
</details>

要让我相信这并非谎言而仅仅是失误，我就必须相信 SciShow 和 Hank 对此并不了解，然而早在 2013 年：
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">For me to believe it's not a lie but just a mistake, I would have to believe that SciShow and Hank didn't know better, except that back in 2013:</p>
</details>

[Hank] “但**曼哈顿计划**（Manhattan Project: 二战期间美国主导的研发原子弹的秘密计划）使人类迅速获得了大量关于原子的信息。”
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">"But the Manhattan Project is responsible for humanity gaining a lot of information about the atom and really quickly."</p>
</details>

因此，今天我将详细剖析 SciShow 那期 AI 视频中最令人震惊的问题。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So today, I'm going to break down the most egregious problems with their AI video.</p>
</details>

我将非常清楚地告诉大家哪些是观点、哪些是解读、哪些是遗漏、哪些是事实错误，并且会提供大量链接，供大家深入学习并自行判断。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I'm going to be very clear with you about what's opinion, what's interpretation, what's omission, and what's factually incorrect, and I'm going to provide you with a bunch of links where you can learn more and make your own decisions.</p>
</details>

因为我希望尽可能多的人明白，如今许多来自 AI 行业的信息，以及被记者、网红和像 SciShow 这样的教育者不加批判地重复和放大传播的行业论调，很多时候——并非总是，但很多时候——都只是一派胡言。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Because I want as many people as possible to understand that a lot of what comes out of the AI industry these days, and industry talking points that are uncritically repeated and amplified by journalists, influencers, and educators like SciShow, is very often—not always, but very often—just a load of nonsense.</p>
</details>

这里是 The Internet of Bugs，我叫 Carl，我从事软件专业工作已经超过 35 年了，我正在尽自己的一份力，让互联网成为一个更安全、更少“漏洞”的地方。如今，这主要意味着努力教育人们认识 AI 的“糟粕”和错误信息，以免情况进一步失控。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">This is The Internet of Bugs; my name is Carl. I've been a software professional for more than 35 years now, and I'm trying to do my part to make the internet a safer, less buggy place, which these days largely involves trying to educate people about AI "slop" and misinformation before it gets any more out of hand.</p>
</details>

### AI发展速度的误导性叙述

好的，回到 SciShow 节目的第一句话，Hank Green 再次谈到了 AI 的发展速度：
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Okay, so back to the first line in the SciShow piece, here's Hank Green again discussing how fast AI has developed:</p>
</details>

[Hank] “仅仅在 **ChatGPT**（Generative Pre-trained Transformer: 一种大型语言模型）推出三年后，AI 代理现在就能在国际数学奥林匹克竞赛中赢得金牌。”
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">"Just three years after the launch of ChatGPT, AI agents can now win gold at the International Math Olympiad."</p>
</details>

好的，关于数学奖牌那部分，这甚至不是真的，但我们稍后再讨论。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Okay, that bit about the math medal isn't even true, but we'll come back to that later.</p>
</details>

现在，我想谈谈“三年”这个说法。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Right now, I want to talk about the "three years" part.</p>
</details>

那么，在三年时间里，我们从 ChatGPT 的高调亮相，走到了 ChatGPT 5 的失败。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So in three years, we've gone from ChatGPT's very public splash to the flop that was ChatGPT 5.</p>
</details>

相比之下，在 1942 年 8 月到 1945 年 8 月的三年间，人类从曼哈顿计划的建立，发展到广岛和长崎核弹袭击中超过 10 万人丧生。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">By comparison, in three years, from August 1942 to August 1945, humanity went from the founding of the Manhattan Project to more than 100,000 people being killed in the nuclear bomb strikes on Hiroshima and Nagasaki.</p>
</details>

认为这两件事在任何方面具有可比性，都令人感到愤怒。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The idea that these two things are in any way comparable is infuriating.</p>
</details>

从统计学上看，你们中的大多数人可能不会对此有如此强烈的情绪反应，但我是在冷战时期长大的，那时我会在学校里躲在课桌下，听着警报声响起，不知道世界是否即将毁灭，只能祈祷那只是一次演习。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Statistically, most of you won't have as much of an emotional reaction to it, but I grew up during the Cold War, hiding under my desk at school, listening to the sirens going off, not knowing if the world was about to end, and praying it was just another drill.</p>
</details>

我知道什么是**生存威胁**（Existential Threat: 指可能导致人类文明或物种永久性毁灭的风险）。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I know what an existential threat looks like.</p>
</details>

AI 并非如此，无论是现在、不久的将来，还是很可能永远都不会。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">AI is not that, not currently, not soon, and quite possibly not ever.</p>
</details>

如今，用核武器在几小时内杀死地球上几乎所有人类所需的一切都已就绪，只待一声发射指令。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Everything required to kill almost all humans on Earth within a few hours using nuclear weapons is in place right now, just waiting on an order to launch.</p>
</details>

人类用了 20 年时间，从 1939 年发现**核裂变**（Nuclear Fission: 原子核分裂并释放巨大能量的过程）到 1959 年拥有装备核弹头并随时可发射的**洲际弹道导弹**（ICBMs: Intercontinental Ballistic Missiles: 射程超过5500公里的远程导弹）。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">It took humanity 20 years, from 1939 to 1959, to go from the discovery of nuclear fission to ICBMs armed with nuclear explosives and ready to launch at any time.</p>
</details>

猜猜 20 年前的 2005 年，AI 领域发生了什么？
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Guess what was happening in AI 20 years ago in 2005?</p>
</details>

那时有一本书出版，向世界宣称**超级智能**（Superintelligence: 远超人类智能的假想智能）即将到来！而且，尽管 Hank 在视频中暗示，AI 的历史远比 ChatGPT 要久远。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">This book was published announcing to the world that superintelligence was near! And, despite what Hank was implying there, AI is a lot older than ChatGPT.</p>
</details>

这里有一本 1995 年的 AI 教科书，还有一本是 1983 年的。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So here is an AI textbook from 1995, and this one is from 1983.</p>
</details>

尽管事实证明 2005 年超级智能并未临近，我们现在却被要求相信它“更近了”。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Despite superintelligence turning out not to be near in 2005, we're now supposed to believe it's even nearer.</p>
</details>

这种胡言乱语本应是 AI 骗子所为，但 SciShow 理应做得更好。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So this kind of nonsense should be expected from AI grifters, but SciShow should be better than that.</p>
</details>

所以，明确地说，没有任何时间段内 AI 的发展速度超过了原子能。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So, to be clear, there is no time interval where AI has progressed faster than atomic power.</p>
</details>

从核裂变发现到长崎原子弹爆炸用了 7 年，而驱动 ChatGPT 的 **Transformer架构**（Transformer Architecture: 一种深度学习模型，广泛应用于自然语言处理，是ChatGPT等大型语言模型的基础）的发现则是在 8 年前。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The time from the discovery of fission to Nagasaki is 7 years, while the discovery of the transformer architecture that powers ChatGPT was 8 years ago.</p>
</details>

1932 年中子的发现比长崎原子弹爆炸早了 13 年，而作为机器学习基本构建块的第一个**神经网络**（Neural Networks: 模拟人脑神经元连接方式的计算模型，是机器学习的基础），则可以追溯到 40 多年前。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The discovery of the neutron in 1932 was 13 years before Nagasaki, but the first neural networks, the basic building blocks of machine learning, go back more than 40 years.</p>
</details>

希望对所有人来说，都显而易见的是，AI 领域发生的任何事情，都无法与长崎和广岛的事件相提并论。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And hopefully, it's obvious to absolutely everyone that nothing whatsoever has happened related to AI that's in any way equivalent to Nagasaki and Hiroshima.</p>
</details>

声称 AI 发展更快，这纯粹是一个谎言，没有更温和的词语可以形容它。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The claim that AI is happening faster is just a lie; there's no milder word to describe it.</p>
</details>

SciShow 的观众理应获得更好的信息。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">SciShow viewers deserve better.</p>
</details>

### 最大的遗漏：行业宣传与真实风险

好的，既然我已经谈到了视频中最大的谎言，现在我将讨论最大的遗漏以及它为何如此重要。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Okay, so now that I've talked about the biggest lie in the video, I'm going to discuss the biggest omission and why it's so important.</p>
</details>

跳到视频的结尾，这里有一个声明，其重要性可能比你最初想象的要大得多。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Jumping to the end of the video, here's a disclosure that's a lot more important than you might initially think.</p>
</details>

[Hank] “感谢 Control AI 赞助本视频。”
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">"Thanks to Control AI for sponsoring this video."</p>
</details>

Control AI 曾联系我商谈合作，所以我对他们做了一些研究。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Control AI has reached out to me about collaborating, so I've done some research on them.</p>
</details>

我真的不喜欢我所看到的一切，因为在我看来，他们似乎正在充当 AI 行业的宣传机构。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And I really don't like what I saw, because they feel to me as if they are acting as a propaganda arm of the AI industry.</p>
</details>

让我解释一下我为什么会有这种感觉。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Let me explain why I feel that way.</p>
</details>

注意：我并不是说他们是 OpenAI 及其友商的宣传机构，只是在我看来，他们表现得像是在这样做。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Note: I am not saying they are a propaganda arm of OpenAI and friends; it just feels to me like they are acting like it.</p>
</details>

让我告诉你原因。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Let me tell you why.</p>
</details>

好的，Hank 即将宣读屏幕左上方的一份声明，这份声明由许多 AI 高管、科学家和知名人士签署，但 Hank 不会告诉你的是，实际上存在两份不同的此类声明。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So, Hank's about to read a statement that's on the upper left of your screen, signed by a bunch of AI executives, scientists, and famous people, but what Hank isn't going to tell you is that there are actually two different such statements.</p>
</details>

[Hank] “在由诺贝尔奖得主、计算机科学家乃至 AI 公司首席执行官签署的声明中，这些人警告称，应对 AI 风险‘应与流行病和核战争等其他社会规模风险一样，成为全球优先事项’。”
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">"In the statement signed by Nobel Prize winners, computer scientists, and even AI company CEOs, these people warned that addressing the risk of AI 'should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.'"</p>
</details>

现在，在屏幕左下方，我展示的是 Control AI 的网站。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Now, in the lower left, I'm showing Control AI's website.</p>
</details>

你会注意到 Control AI 提到了 Hank 在视频中宣读的“灭绝声明”，然后立即转移话题，声称 AI 促进增长和创新，甚至提到了医学。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">You'll notice that Control AI mentions the extinction statement that Hank read in the video and then immediately changes the subject to claims that AI boosts growth and innovation, even mentioning medicine.</p>
</details>

话题转变相当大。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Quite the subject changed there.</p>
</details>

现在，在屏幕右侧，我展示的是 red-lines.ai 网站上的另一份声明（顺便说一下，所有这些链接都在下方）。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Now, on the right, I'm putting up the site red-lines.ai with the other statement (all these links are provided below, by the way).</p>
</details>

请注意，这份声明起源于第 80 届**联合国大会**（UN General Assembly: 联合国的主要审议、决策和代表性机构）。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Note that this statement originated during the 80th UN General Assembly.</p>
</details>

现在，让我们看看 Hank 宣读的声明中遗漏了什么。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Now, let's look at what was omitted from the statement that Hank read.</p>
</details>

red-line 声明提到了“广泛的**虚假信息**”（Disinformation: 故意散布的错误信息），而 AI 造成的广泛虚假信息已经是一个真实存在的问题（链接在下方），但 SciShow 的声明却对此置若罔闻。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The red-line statement talks about "widespread disinformation," and widespread disinformation from AI is already a real problem (links below), but the SciShow statement ignores it.</p>
</details>

red-line 声明提到了“对个人（包括儿童）的大规模操纵”，这同样是一个真实存在的问题（链接在下方），但 SciShow 的声明再次对此不予理会。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The red-line statement talks about "large-scale manipulation of individuals, including children," which is again already a real problem (links below), but the SciShow statement again ignores it.</p>
</details>

同样，源自联合国的声明警告了“大规模失业”和“系统性人权侵犯”。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Likewise, the statement that originated from the UN warns about "mass unemployment" and "systemic human rights violations."</p>
</details>

这些都是已经真实存在且正在发生的问题，但 Control AI 和 SciShow 再次选择不向你提及这些。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">These are already real problems and they're actually occurring, but again, Control AI and SciShow don't want to tell you about those.</p>
</details>

同样，我已在下方提供了这些问题目前正在发生的参考资料。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And again, I've provided references for those problems actually happening now, down below.</p>
</details>

这份声明最接近谈到“灭绝”的地方是提到了“流行病”，虽然流行病很糟糕，但它们并非灭绝级别的威胁，正如我们在 2020 年、1918 年、中世纪黑死病时期以及其他许多时候所发现的那样。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The closest this statement gets to talking about extinction is mentioning "pandemics," which, while bad, are not extinction-level threats, as we found out in 2020, 1918, during the Black Death in the Middle Ages, and at a bunch of other times.</p>
</details>

但这份声明中遗漏的最具说服力的一点是，引用原文：“确保所有先进 AI 供应商都负有责任。”
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But the most telling thing omitted from this statement is, quote, "Ensuring that all advanced AI providers are accountable."</p>
</details>

我想知道谁可能不希望人们谈论这一点。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I wonder who might not want people talking about that one.</p>
</details>

Control AI 的网页以及 SciShow 在这个视频中，非常非常希望让你思考并担忧人类可能在未来的某个时候，也许会因为 AI 而灭绝的潜在、假想风险。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The Control AI webpage, and SciShow in this video, really, really want to make you think about and worry about the potential, hypothetical, someday, maybe sometime in the future, maybe risk of humanity possibly going extinct because of AI.</p>
</details>

然而我——我为此制作了一个完整的视频，链接在下方——认为我们的注意力需要集中在右侧声明所强调的**当前问题**上，例如虚假信息、操纵、就业、人权侵犯，以及追究 OpenAI 和其他供应商的责任。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Whereas I, and I made a whole video about this that I linked below, think that our attention needs to be focused on the *now* problems as emphasized by the statement on the right, like disinformation, manipulation, employment, human rights violations, and holding OpenAI and other providers accountable.</p>
</details>

我不是唯一一个持这种观点的人。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And I'm not the only one.</p>
</details>

### 声明签署者的立场差异

让我们来看看这些声明的签署者。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Let's take a look at the signatories of these statements.</p>
</details>

Hank 说有多位诺贝尔奖得主签署了“灭绝声明”。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Hank said that Nobel Prize winners (plural) had signed the extinction statement.</p>
</details>

我查阅了名单，只找到了一位同时签署了两份声明的人，那就是**杰弗里·辛顿**（Geoffrey Hinton: 著名的计算机科学家，被誉为“深度学习教父”）。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I went through the list and I can only find one, Geoffrey Hinton, who signed both statements.</p>
</details>

值得注意的是，与辛顿共同获得 2024 年诺贝尔物理学奖的**约翰·霍普菲尔德**（John Hopfield: 著名的物理学家和计算机科学家，与辛顿共同获得2024年诺贝尔物理学奖），只签署了右侧的声明。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Notably, John Hopfield, who shared the 2024 Nobel Prize in Physics with Hinton, only signed the statement on the right.</p>
</details>

同样，只签署右侧声明的还包括至少五位诺贝尔和平奖得主、三位诺贝尔经济学奖得主、一位诺贝尔化学奖得主，以及至少三位前国家元首和一名前联合国主席。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Also, only signing the statement on the right are at least five Nobel Peace Prize recipients, three recipients of the Nobel Prize for Economics, one for Chemistry, and at least three former heads of state and a former president of the United Nations.</p>
</details>

现在，让我们看看谁只签署了“灭绝声明”：OpenAI 的首席执行官**萨姆·奥特曼**（Sam Altman: OpenAI 首席执行官），以及 Anthropic 和 Google DeepMind 的首席执行官。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Now, let's look at who only signed the extinction statement: Sam Altman, the CEO of OpenAI, as well as the CEOs of Anthropic and Google DeepMind.</p>
</details>

想象一下。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Imagine that.</p>
</details>

三家最大 AI 公司的首席执行官认为我们应该担忧 AI 导致的灭绝风险，却不认为我们应该担忧 AI 造成的虚假信息、操纵或人权侵犯，他们也绝对不希望我们担忧如何确保所有先进 AI 供应商都负有责任。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The CEOs of three of the largest AI companies think we should worry about extinction from AI, but don't think we should worry about AI causing disinformation, manipulation, or human rights violations, and they definitely don't want us to worry about ensuring that all advanced AI providers are held accountable.</p>
</details>

右边是五位诺贝尔和平奖得主，左边是三位大型 AI 公司的首席执行官，你站在哪一边？
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Five Nobel Peace Prize recipients on the right, and three CEOs of huge AI companies on the left—which side are you on?</p>
</details>

你希望 SciShow 会站在哪一边？
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Which side would you hope that SciShow would be on?</p>
</details>

这就是为什么我说 Control AI 以及 SciShow 在这个视频中，在我看来像是在推销亲 AI 行业的宣传，因为他们试图让你去思考那些你无能为力的、假想的未来问题。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">This is why I say that Control AI, and SciShow in this video, feel to me like they are pushing pro-AI industry propaganda because they are trying to get you to think about hypothetical future problems that you can't do anything about.</p>
</details>

当他们只推行这种叙事时，其副作用就是让你分心，无法关注那些真正发生的问题——那些 AI 公司据称正在造成的问题，以及 AI 公司至少在某些情况下已经因此被起诉的问题。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And when they push that narrative, and only that narrative, it has the side effect of distracting you from the real problems that are actually happening, that AI companies are allegedly causing now, and that AI companies are, at least in some cases, already being sued for.</p>
</details>

再次强调，链接在下方。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Again, links below.</p>
</details>

### 其他次要问题与总结

好的。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Okay.</p>
</details>

以上是 SciShow 视频中两个最重要的问题，但还有其他一些 SciShow 本应注意到的次要问题，我现在将开始讨论其中一些。但请理解，本视频的其余部分远不如以下两点重要：一是关于 AI 发展速度快于原子能的谎言，二是推销关于 AI 假想未来风险的声明，同时却避免讨论与 AI 当前风险更相关的声明——而这些风险，AI 公司可能更不希望你思考。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So those are the two most important problems with the SciShow video, but there are other less important problems that SciShow should have caught, and I'm going to start talking about some of those now. However, understand that the rest of this video is much less important than the lie about AI happening faster than atomic power, and pushing the statement about AI's hypothetical future risks while avoiding discussion of the much more relevant statement about AI's current risks that the AI companies would probably rather you not think about.</p>
</details>

所以，如果你从这个视频中什么都没记住，我希望你能记住这两点。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So if you take nothing else away from this video, I hope you remember those two things.</p>
</details>

好的。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Okay.</p>
</details>

接下来是 Hank 关于 AI 成就的说法。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Next up is Hank's claim about what AI has done.</p>
</details>

我说过我会再谈到它。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I told you I'd come back to it.</p>
</details>

他说：“AI 代理现在可以在国际数学奥林匹克竞赛中赢得金牌。”
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">He says, "AI agents can now win gold at the International Math Olympiad."</p>
</details>

然而，这并非事实，至少如果你相信**国际数学奥林匹克竞赛**（International Math Olympiad: 每年一度面向高中生的世界性数学竞赛）本身的话。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Now, that's not true, at least if you believe the International Math Olympiad itself.</p>
</details>

有相关文章证明。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">There's that article.</p>
</details>

就像 AI 行业多年来提出的许多主张一样，这充其量只是一种被专家驳斥的夸大其词，然后又被那些不屑于花 30 秒进行谷歌搜索来核实就重复传播的人不负责任地散布。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Like so many claims the AI industry has made over the years, it is, at best, an exaggeration contradicted by experts, and then irresponsibly repeated by people who didn't bother to do a 30-second Google search to investigate the claim before repeating it.</p>
</details>

下一个片段，直接剪辑掉了关于一首海歌的连续笑话，这并非谎言，但它确实具有误导性。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">This next clip, directly edited to remove a running joke about a sea shanty, isn't a lie, but it's really misleading.</p>
</details>

在其中，Hank 将谈论**对齐问题**（Alignment Problem: 确保人工智能系统行为与人类价值观和意图保持一致的挑战），即我们如何努力阻止 AI 做它们不应该做的事情。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">In it, Hank is going to talk about the alignment problem, which is when we try to keep AIs from doing things that they aren't supposed to do.</p>
</details>

[Hank] “另一方面，AI 不应该屈服于如何制造**生物武器**（Bioweapon: 利用生物毒素或微生物（如细菌、病毒）来伤害或杀死人类、动物或植物的武器）的请求。”
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">"On the other hand, AI's shouldn't give in to a request for how to build a bioweapon."</p>
</details>

不幸的是，这种事情已经发生过。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Unfortunately, that kind of thing has happened.</p>
</details>

当 Anthropic 测试其 AI 模型 **Claude Opus 4**（一种大型语言模型）的早期版本时，他们发现它帮助非专业人士制造生物武器的成功率，比那些只使用互联网尝试相同操作的人高出 2.5 倍。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">When Anthropic was testing an early version of their AI Claude Opus 4, they found that it helped non-experts build bioweapons 2.5 times more successfully than people who tried the same thing with just the internet at their disposal.</p>
</details>

Hank 在那里谈论的内容听起来很可怕，因为 Hank 让它听起来像是 Claude 在不应该的情况下，帮助测试者制造生物武器的成功率比他们仅凭互联网高出 2.5 倍。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">What Hank is talking about there sounds scary because Hank made it sound like Claude had, when it wasn't supposed to, helped testers build a bioweapon 2.5 times more successfully than they could have with just the internet.</p>
</details>

但事实并非如此。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But that's not at all what happened.</p>
</details>

这是 Hank 提到的报告。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Here's the report that Hank is talking about.</p>
</details>

看到那个高亮部分了吗？
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">See that highlighted part?</p>
</details>

在那次测试中，协助制造生物武器的 AI 被故意移除了安全防护措施。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The AI that helped with the bioweapon had deliberately had its safeguards removed during that test.</p>
</details>

Hank 谈论的是 AI 在不应该的情况下“屈服”于制造生物武器的请求，但这个例子根本不相关。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Hank is talking about AIs "giving in" to requests for how to build a bioweapon when they're not supposed to, but this example is not relevant at all.</p>
</details>

因为 Hank 在视频这部分谈论的安全防护措施被明确地关闭了。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Because the safeguards that Hank is talking about in this part of the video were explicitly turned off.</p>
</details>

现在请理解，对齐问题确实是一个真实存在的问题，SciShow 本可以挑选许多例子来阐明这个对齐问题。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Now, understand, the alignment problem is a real problem, and there are lots of examples that SciShow could have picked to illustrate the alignment issue.</p>
</details>

但这个例子并没有展示他所声称的内容，而且这个例子通过谈论生物武器，进一步推动了“人类灭绝是每个人在思考 AI 风险时都应该考虑的问题”这一叙事。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But this illustration isn't showing what he's telling you it's showing, and this illustration, by talking about bioweapons, further pushes the narrative that human extinction is what everyone should be thinking about when they think about AI risks.</p>
</details>

那么，这里还有另一部分：
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So here's another part:</p>
</details>

[Hank] “2025 年 4 月，OpenAI 不得不回滚其旗舰 ChatGPT 模型的一项更新，因为它变得过于**谄媚**（Sycophantic: 指过分奉承或讨好他人）。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">"In April 2025, OpenAI had to roll back an update to its flagship ChatGPT model for being too sycophantic.</p>
</details>

其中一个最实际和明显的问题是，它开始支持人们在未咨询医生的情况下停止服药的决定。”
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">One of the most practical and visible problems was that it had started endorsing people's decision to stop taking their medications without consulting their doctors."</p>
</details>

尽管这是事实，但我认为这真的、真的低估了问题的严重性。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Now, while that's true, I'd argue it really, really downplays the issue.</p>
</details>

我认为，这里有更多更重要的“实际和明显”问题。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Here, I think, are much more important "practical and visible" problems.</p>
</details>

这个视频还有更多问题，但为了不深入细节，我将开始尝试总结。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">There are more issues with this video, but because I don't want to get too far into the weeds, I'm going to start trying to wrap it up.</p>
</details>

我要在这里指出，Hank 是如何不加批判地引用了一位 AI 首席执行官关于人类不理解 AI 如何运作的论文。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I'm going to point out here how Hank uncritically quoted an essay from an AI CEO about humanity not understanding how AIs work.</p>
</details>

在我的专业看来，这段引文纯属垃圾，我为此制作了一个完整的视频，链接在下方。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">This quote is, in my professional opinion, a load of garbage, and I made a whole video about it that I'll link below.</p>
</details>

如果这还不够，Hank 还在他自己的频道上制作了一个关于 AI 的视频，比这个视频还要糟糕。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And if that wasn't enough, Hank also did a video on his own channel about AI that is even worse than this one.</p>
</details>

我觉得我应该对此做些什么，但那个视频长达一个小时十五分钟。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I feel like I ought to do something about it, but it goes on for like an hour and 15 minutes.</p>
</details>

逐行审阅、提取所有相关引文、查找参考资料和支持证据，并区分哪些是公众普遍感兴趣的、哪些只对关心 AI 内部运作的技术人员有意义、哪些抱怨只是吹毛求疵，这将耗费我大量时间。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">It would take me so long to go through it line by line, pull out all the relevant quotes, dig up the references and supporting evidence, and figure out what is of general public interest versus what would only be interesting to technical people who care about AI "under the covers," and what complaints would just be nitpicking.</p>
</details>

所以我不确定何时或是否有精力来处理那个视频。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So I'm not sure when or if I'm going to have the bandwidth to tackle that.</p>
</details>

如此多的 AI 错误信息以如此快的速度涌现，让我感到抓狂，我甚至无法全部跟上，更不用说为所有这些制作视频了。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">It drives me crazy that there's so much misinformation about AI coming out so fast that I can't even keep up with all of it, much less make videos about it all.</p>
</details>

制作一个包含事实和参考资料的视频，比充当 AI 行业的代言人要费力得多，也耗时得多。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Putting together a video with facts and references is so much more work and so much more time-consuming than being an AI industry mouthpiece.</p>
</details>

与 SciShow 团队不同，我只是一个人，而像 SciShow 这样拥有资源可以做得更好的频道，却选择把事情搞砸，这无疑没有帮助。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Unlike the SciShow team, I'm just one guy, and it certainly doesn't help when channels like SciShow, which do have the resources to make things better, choose to make things worse instead.</p>
</details>

但我将尽我所能继续努力提供帮助。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But I'm going to keep trying to help to the extent that I can.</p>
</details>

祝我好运，如果你愿意，请随时订阅。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Wish me luck, and feel free to subscribe if you're so inclined.</p>
</details>

我本想请求大家在评论中友善一些，但这不会有什么用，所以我也不费事了。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I would ask for people to be nice in the comments, but it wouldn't do any good, so I'm not going to bother.</p>
</details>

请永远记住，互联网充满了“漏洞”——以及错误信息——任何持不同意见的人可能只是在鹦鹉学舌地重复行业论调，并试图分散你的注意力，让你无法关注这个行业据称正在伤害真实人群的所有方式，即使你正在观看此视频时也是如此。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Just always remember that the Internet is full of "Bugs"—and misinformation—and anyone who says different might just be parroting industry talking points and trying to distract you from all the ways the industry is allegedly harming real people, even as you are watching this.</p>
</details>

让我们在外保持警惕。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Let's be careful out there.</p>
</details>

感谢观看。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Thanks for watching.</p>
</details>