---
area: tech-insights
category: technology
companies_orgs: []
date: '2025-09-05'
draft: true
guest: ''
insight: ''
layout: post.njk
media_books:
- best-partners-tv
products_models: []
project:
- ai-impact-analysis
series: ''
source: https://www.youtube.com/watch?v=fgoQYlLT_IY
speaker: Best Partners TV
status: evergreen
summary: Hot Chips 2025大会深入探讨了芯片行业的未来走向，涵盖AI计算、网络、光I/O、CPU、图形、安全及散热等七大关键技术领域，揭示了从存储突破到精细散热的最新进展。
tags:
- ai-chip
- data-center-infrastructure
- performance
- science
- technology
title: Hot Chips 2025 七大技术领域深度解析：AI计算、网络、光I/O、CPU、图形、安全与散热创新
---

### Hot Chips 2025 大会概览与核心突破

国际顶级芯片会议Hot Chips已正式落下帷幕，作为芯片及系统设计领域的年度风向标，本届大会揭示了未来科技的底层逻辑。大会议程设计巧妙，首日聚焦CPU、安全、图形与网络等基础支柱，第二天则转向光学、散热与机器学习等AI时代的核心痛点。本篇文章将沿着大会的技术脉络，逐领域拆解本届Hot Chips的核心突破，旨在以通俗易懂的方式阐释藏匿于技术文档中的参数、架构与创新，从而清晰展现未来1-2年芯片行业的真实走向。

### 光I/O技术进展

光I/O的优势在于更高的速率、更低的功耗、更强的抗干扰能力以及更远的传输距离。本届Hot Chips大会上，光I/O领域的分享主要围绕“如何将光技术落地”展开，从共封装光学到3D光中介层，方向明确地指向逐步用光连接替代电连接，尤其是在AI芯片和集群互连场景中。

#### Celestial AI 的 Beach Front 光结构模组

**Celestial AI** 公司参与了台积电5nm和4nm的早期创新客户计划，已完成四次流片，技术成熟度高。他们目前的重点是“带中介层的**HBM**（High Bandwidth Memory: 高带宽内存）”，利用光连接优化HBM的数据流，能够直接提升AI计算效率。其**PFLink**技术包含一个硅光子层，集成了无源和有源元件，关键在于实现了“**SerDes**（Serializer/Deserializer: 串行器/解串器）与通道匹配”，从而达到超高能效。他们还在构建**光MAC**（Optical Media Access Control: 光媒体访问控制层），确保光连接的可靠性（**RAS**功能，Reliability, Availability, Serviceability: 可靠性、可用性、可服务性），避免光链路故障影响系统。

Celestial AI还提出了一个关键观点，即“电**fabric**（互连网络）与光fabric的扩展定律不同”。随着多芯片封装尺寸的增大，电fabric的带宽会受限于物理接口的数量，而光fabric的带宽能持续增长，因为光可以并行传输更多通道且不受电磁干扰。这意味着在未来更大规模的多芯片封装中，光fabric将成为必然选择。此外，他们还展示了**CoWoS-L**（Chip-on-Wafer-on-Substrate with Local silicon interposer: 台积电的一种先进封装技术）芯片组的**光学多芯片互连桥OIMB**（Optical Inter-Module Bridge），解决了光学接口的安全问题。由于光信号容易被窃听，OIMB能够确保光传输的安全性，这对金融、政务等敏感场景至关重要。

#### Ayar Labs 的 TeraPHY 光I/O芯片

**Ayar Labs** 公司的**TeraPHY**光I/O芯片目标是用光学技术实现AI系统的横向扩展。大规模AI系统需要将数百万个芯片连接成一个集群，传输距离从机架内的大概3米到多机架的15米。如果采用电I/O，每机架的功耗会暴涨，难以承受。Ayar Labs的解决方案是采用**UCIe**（Universal Chiplet Interconnect Express: 通用芯粒互连标准）光I/O重定时器。Ayar Labs将光I/O做成了UCIe **Chiplet**（小芯片: 预先制造的、可互连的小型功能芯片），能够轻松集成到AI芯片的封装中，无需修改现有芯片设计，兼容性强。

这款UCIe Chiplet的速率达到了8Tbps，能提供大量封装外带宽，解决AI芯片“对外传不动数据”的问题。其核心创新是解耦光信号和电信号。传统光连接中，光信号和电信号的传输路径是绑定的，一旦其中一个出问题，整个链路就会失效。而Ayar Labs的设计中，UCIe接收器会先将电信号重定时，再转换成光信号传输，实现光电解耦，各自的优化空间更大，也更容易排查故障。Ayar Labs的TeraPHY芯片已进入设计验证测试阶段，即将量产。他们还进行了长期链路稳定性测试，如热循环测试。通过优化封装材料，确保光链路在芯片加热和冷却导致材料膨胀收缩时仍能稳定传输。他们还展示了一个共封装的500W设备，证明光I/O能够适配高功率AI芯片的散热需求，不再局限于低功耗场景。

#### Lightmatter 的 3D 光中介层

**Lightmatter** 公司提出了“3D光中介层”的概念，推出了**Passage M1000**平台。其核心是在光中介层上封装计算和内存芯片，通过3D堆叠实现紧凑结构，同时提供超高带宽。当前芯片互连的一大痛点是芯片外围物理区域有限，I/O接口数量受限。要实现100倍以上的带宽，必须改变现有范式，而3D光中介层正是这一新范式。Passage M1000的预期速率高达114Tbps，按照Lightmatter的说法，这是迈向200Tbps **XPU**（eXtreme Processing Unit: 极限处理单元）和400Tbps交换机的第一步，且已做好生产准备。

设计上，他们解决了一个关键问题，即光学元件与电SerDes的物理尺寸匹配问题。光学元件通常比电SerDes小，直接集成会浪费空间。Lightmatter采用硅微环谐振器来调节光信号，实现了非常紧凑的光I/O，使光学元件和电SerDes的尺寸刚好匹配，不浪费封装空间。选择硅微环谐振器的原因有三：一是尺寸小，能在有限空间内集成更多通道；二是功耗低，调节光信号不需要太多能量；三是响应速度快，能跟上AI芯片的高频数据传输需求。他们还打造了光引擎**Lightmatter Guide**，负责光信号的生成、传输和接收。Passage M1000还具有一定的可重构性，能够根据不同工作负载调整光链路，灵活性强。此外，其Tile设计有16条水平总线，通过十字形金属缝线实现电气连接，光路交换功能还能提供冗余，即使一条光链路损坏，其他链路仍能工作，提升了可靠性。

#### 英伟达的光I/O创新

**英伟达** 在光I/O领域的重点是“跨区域扩展”，推出了**Spectrum-XGS**以太网技术，目标是将多个分布式数据中心组合成一个“十亿瓦级AI超级工厂”。这不仅需要硬件支持，还需要“距离感知算法”，因为不同数据中心之间的距离可能长达几十、上百公里，光信号传输会有延迟。该算法需要根据距离动态调整数据传输策略，确保整体性能。根据英伟达的数据，采用Spectrum-XGS技术，多站点**NCCL**（NVIDIA Collective Communications Library: 英伟达集合通信库）的横向扩展性能是传统**OTS**（Optical Transport System: 光传输系统）以太网的1.9倍，能大幅加速多GPU和多节点的通信，使AI训练不再受单个数据中心资源限制。

硬件方面，英伟达推出了“200G/SerDes共封装光学”技术。传统可插拔光学引擎需要额外电力和空间，而共封装光学直接将光学元件和交换机芯片封装在一起，无需额外供电，能够节省大量电力。**NVIDIA Photonics**硅光**CPO**（Co-Packaged Optics: 共封装光学）芯片速率达到了1.6T，采用了新型微环调制器，进一步提升了能效。英伟达的**Spectrum-6 102T**集成硅光交换机也备受关注，实现了翻倍的吞吐量、更高的可靠性和更低的功耗。搭配之前的**Spectrum-X**和**Quantum-X**交换机，形成了完整的光网络产品线。他们还透露即将推出CPO网络交换机，鉴于共封装技术是未来光网络的主流方向，英伟达的布局显然旨在抢占该赛道的先机。

### CPU 技术突破

CPU目前面临的最大挑战是“摩尔定律触顶”，晶体管密度的提升速度越来越慢，单芯片性能难以再像以前那样“每18个月翻一番”。本届Hot Chips上，CPU领域的共识是需要依靠“先进制程+Chiplet+3D堆叠”来突破性能瓶颈，将不同功能的芯片裸片通过先进封装技术集成在一起，从而在提升性能的同时，控制成本和功耗，并提高良率。

#### 英特尔的下一代至强处理器 Clearwater Forest

英特尔的下一代至强处理器**Clearwater Forest**拥有288核，采用**Intel 18A**制程和3D封装技术。其核心思路是利用3D堆叠来提升缓存和内存带宽。该处理器的Chiplet设计精细：12个能效核CPU Chiplet采用Intel 18A工艺，3个基础Chiplet采用**Intel 3**工艺，2个I/O Chiplet沿用了上一代**Sierra Forest**的**Intel 7**工艺。芯片间通过**EMIB**（Embedded Multi-die Interconnect Bridge: 嵌入式多芯片互连桥）连接，EMIB是英特尔一项成熟的封装技术，能够提供高带宽、低延迟的Chiplet互连。

缓存方面，Clearwater Forest的末级缓存**LLC**（Last Level Cache: 末级缓存）达到1152MB，意味着每个插槽有576MB的LLC。具体到芯片上，每个144核的Tile有108MB的LLC，两个Tile即216MB，加上其他缓存，总缓存容量非常大。大缓存的优势在于减少内存访问次数，CPU无需频繁从内存读取数据，大幅降低延迟，这对AI推理、数据库等场景非常有利。前端设计上，Clearwater Forest通过3个3-wide指令解码器将指令宽度提升了50%，即一次能读取更多指令，提升指令执行效率。分支预测器也进行了优化，能够更准确地预测程序下一步要执行的指令，减少“预测错误”带来的性能损失。后端的乱序执行引擎也进行了升级，从每时钟周期调度5个操作提升到8个操作，执行端口数量增加到26个，整数和向量执行能力都翻了一倍，这意味着CPU能够同时处理更多任务，多线程性能将有明显提升。

内存子系统也有改进，L2未命中缓冲区的大小翻倍，能存储128个未命中数据，减少了CPU等待时间。单个Clearwater Forest模块有4个核心，共享4MB的统一L2缓存，L2缓存带宽也比上一代翻倍，达到400GB/s。双插槽系统中，每个芯片有12个**DDR5**-8000（Double Data Rate 5: 第五代双倍数据速率内存）内存通道，总内存带宽达到1300GB/s，能够满足大规模数据处理需求。根据英特尔的数据，Clearwater Forest机架的每瓦性能是上一代Sierra Forest的3.5倍，能效提升显著。

#### IBM 的 Power11 处理器

**IBM** 的**Power11**处理器采用三星7nm工艺，设计理念是“按需增加核心数”，不盲目堆叠核心，而是根据实际需求优化架构。他们计划在后代Power处理器中聚焦几个重点：一是每个插槽上集成的硅片数是上一代的3倍；二是利用良率协同效应；三是保持跨Chiplet的高带宽连接；四是优化**OMI**（Open Memory Interface: 开放内存接口）内存的效率；五是减少延迟，提升拓扑协同性；六是保证长期发展的效率和灵活性。

Power11的核心升级在于内存子系统，IBM称之为“OMI内存架构”，这是一种分层内存架构。一块芯片最多可支持32个DDR5内存端口，传输速度最高可达38.4Gbps，最终将推出定制化的内存规格**OMI D-DIMM**。IBM对HBM并不看好，因为HBM容量较低，无法满足大规模数据处理需求。他们的目标是8TB **DRAM**（Dynamic Random-Access Memory: 动态随机存取存储器）和1TB/s以上的内存带宽，而OMI架构基于DDR5内存即可实现这一目标，成本远低于HBM。不过OMI也有一个小缺点，即缓冲区会增加6到8ns的延迟，但IBM认为该延迟在可接受范围内，且换来的容量和带宽优势更具价值。此外，Power11还优化了对外部**PCIe**（Peripheral Component Interconnect Express: 高速串行计算机扩展总线标准）加速器的支持。IBM拥有自己的**Spyre**加速器，能够与Power11配合提升AI计算性能，例如在机器学习场景中，Spyre加速器能够卸载Power11的计算任务，使CPU专注于数据调度，整体性能提升明显。

#### Condor Computing 的 Cuzco RISC-V CPU IP

晶心科技的子公司**Condor Computing**展示了首款高性能**RISC-V CPU IP**（Reduced Instruction Set Computer - Five Central Processing Unit Intellectual Property: 第五代精简指令集计算机中央处理器知识产权）。这款产品由一支仅有50名工程师的团队完成，却实现了很高的性能。据介绍，与其他有相近功耗的高性能授权CPU相比，**Cuzco**的性能更加出色。Cuzco的优势明确：一是降低成本，RISC-V是开源架构，无需支付授权费；二是提高能效，针对低功耗场景进行了优化；三是扩展性强，每个集群有8个高性能计算CPU核心，能按需扩展核心数量；四是兼容性好，符合面向高性能RISC-V计算的最新**RVA23**规范，软件生态可复用；五是灵活定制，完全支持**ISA**（Instruction Set Architecture: 指令集架构）的定制，能够根据客户需求添加特殊指令。

Cuzco的设计与多数高性能处理器类似，提供了完整的IP方案，除了CPU核心以外，还有缓存和一致性管理功能，能直接接入内存和I/O总线，客户无需进行额外的集成开发。其核心创新是基于时间的微架构。传统乱序执行CPU需要复杂的调度逻辑，晶体管多、功耗高；而Cuzco采用硬件编译进行指令排序，通过“寄存器计分板”和“时间资源矩阵（TRM）”，精确预测指令的执行时间，提前安排好执行顺序。这种设计的好处是调度的确定性降低了逻辑复杂性，消除了复杂的运行时每周期调度，减少了动态功率，用更少的晶体管实现了更高的能效。Cuzco采用基于**slice**的CPU设计，最多支持8个核心，每个核心有私有的L2缓存，多个核心共享L3缓存。这种分层缓存设计能平衡延迟和容量。根据Condor Computing的数据，Cuzco在**SPECint2006**测试中，每时钟周期的性能几乎是晶心科技当前**AX65**核心的两倍，证明了这款RISC-V CPU的高性能潜力。

#### PEZY Computing 的第四代 MIMD 多核处理器 PEZY-SC4s

日本**PEZY Computing**公司展示了第四代**MIMD**（Multiple Instruction, Multiple Data: 多指令多数据）多核处理器**PEZY-SC4s**。其特点是每个处理器核心能够执行不同的指令、处理不同的数据，适合具有高度独立线程的应用程序，如科学计算、基因组分析等。PEZY认为，对于这类应用，MIMD比**SIMD**（Single Instruction, Multiple Data: 单指令多数据）更加有效，因为能充分利用线程的独立性。

PEZY-SC4s采用台积电5nm **FinFET**（Fin Field-Effect Transistor: 鳍式场效应晶体管）工艺，芯片尺寸为18.4mm×30.2mm，总面积约556mm²，集成了48亿颗晶体管。**SRAM**（Static Random-Access Memory: 静态随机存取存储器）容量为1.6Gb，内部总线读带宽达到12TB/s，写带宽达到6TB/s，能满足大规模数据传输需求。其计算资源丰富，有2048个处理单元**PE**，支持16384个线程，还有PE和缓存的分层缓存，能够减少数据访问延迟，提升线程并行效率。外部内存方面，PEZY-SC4s采用了**HBM3**，有4个设备，带宽达到3.2TB/s，容量达到96GB；外部接口是PCIe Gen5，有16个lane，带宽达到64GB/s，能够快速连接主机和其他设备。

系统部署上，PEZY设计了“主机CPU+PEZY-SC4s”的节点，每个节点包含1张**AMD EPYC 9555P CPU**、4张PEZY-SC4s和**NDR InfiniBand**网卡。规划的系统配置有90个节点，总共737280个PE，双精度下峰值算力达到8.6 **PFLOPS**（Peta Floating-point Operations Per Second: 每秒千万亿次浮点运算），能够满足大型科学计算的需求，如气候模拟、量子化学等。PEZY还对PEZY-SC4s进行了仿真测试，在执行双精度通用矩阵乘法**DGEMM**工作负载时，功率效率是上一代的2倍以上。在基因组序列比对算法**Smith-Waterman**测试中，性能可达到359 **GCUPS**（Giga Cell Updates Per Second: 每秒十亿次单元更新），是上一代**PEZY-SC3**的3.86倍，侧面证明了MIMD架构在特定场景下的性能优势。此外，PEZY还在开发第五代**PEZY-SC5**，计划采用3nm或更小工艺，预计2027年发布。他们还在开发一种新的硬件描述语言**Veryl**，作为**SystemVerilog**的开源替代方案，PEZY-SC5的核心组件也将用Veryl开发，目标是降低芯片设计门槛。

### 图形技术创新

图形领域目前的趋势明确，即利用AI提升渲染效率。无论是游戏、创作，还是**AR/VR**（Augmented Reality/Virtual Reality: 增强现实/虚拟现实），都需要更逼真的画面。然而，传统渲染方式功耗高、速度慢，AI的加入能大幅优化这一过程。本届Hot Chips上，AMD和英伟达两大GPU巨头都分享了图形架构的优化，尤其强调对光线追踪、AI性能和神经渲染的支持。Meta则带来了AI眼镜专用芯片的设计，聚焦空间和功耗受限场景下的渲染优化。

#### AMD 的 RDNA 4 架构

**AMD** 的**RDNA 4**架构专为下一代游戏和创作设计，核心升级是“AI算力+光线追踪优化”，能够支持严苛的游戏应用、先进的视频编码和流媒体能力的生产力场景。RDNA 4的**SoC**（System-on-a-Chip: 系统级芯片）架构设计灵活，高度可扩展，能根据市场需求调整配置，打造从入门到旗舰的多种产品**SKU**（Stock Keeping Unit: 库存量单位），且无需重新设计整个架构，降低开发成本。

RDNA 4针对高端游戏工作负载做了大量优化：一是栅格化和计算效率，通过优化着色器引擎的执行逻辑，提升多边形渲染和通用计算的速度；二是光线追踪性能，这是本次升级的重点。RDNA 4的光线求交性能比上一代翻了一倍，还新增了专用的硬件实例转换器，将“实例转换”这一任务从着色器程序中转移出来，使着色器专注于渲染，进一步提升光追速度；**边界体积层次结构BVH**（Bounding Volume Hierarchy: 边界体积层次结构）也从4列加宽到8列，能够更高效地组织场景数据，减少光线求交的计算量。新采用的节点压缩技术还能减少BVH的尺寸，降低内存占用。光线追踪的另一项优化是“定向边界框”。传统边界框是轴对齐的，对不规则物体的包裹性差，会导致许多无效的求交测试。而定向边界框能根据物体形状调整方向，更精确地包裹物体，大幅提高光线相交测试的效率。此外，乱序内存访问也做了优化，某些高优先级的请求能优先处理，无需等待其他延迟高的工作，这对光追这种“频繁访问内存”的场景非常重要。

着色器引擎方面，RDNA 4通过动态寄存器分配，增加了“传播波数”。传播波是着色器中的并行执行单元，数量越多，并行处理能力越强，能够同时处理更多像素或顶点数据。针对机器学习和AI的工作负载，RDNA 4还增加了**FP8**（8-bit Floating Point: 8位浮点数）精度支持和稀疏化功能。FP8能在保证画质的前提下提升算力密度，稀疏化则能跳过无效数据的计算，减少功耗和延迟。AI在图形中的应用也很具体，例如用神经辐射缓存来存储场景的辐射信息，用神经超采样和去噪技术填补因使用过少光线造成的画面空白，既能提升渲染速度，又能保证画质。存储方面，RDNA 4的SoC架构中，数据在着色器引擎、各种缓存和内存控制器之间的流动高效，**Infinity Fabric**的带宽高达1KB每时钟频率，能够快速传输大量渲染数据。RDNA 4的结构是模块化的，例如**Navi 48 GPU**能切成两半，制造出更小的GPU，这不仅减少了开发GPU变体的工作量，同时能提升芯片的可靠性，即使某个模块出问题，其他模块仍能工作。此外，RDNA 4还有新的内存压缩和解压缩功能，对软件完全透明，全部由硬件处理。根据AMD的数据，某些栅格工作负载的性能能提升大约15%，fabric带宽占用率降低大约25%，且无需软件修改，兼容性很好。

#### 英伟达的 Blackwell 架构图形产品

英伟达的**Blackwell**架构图形产品重点是“神经渲染”，称**RTX Blackwell**为“神经渲染的新时代奠定基础”。神经渲染的核心是“融合传统图形与AI”，即用AI来生成画面，而不是完全依靠传统的光栅化或光追，这样既能提供更好的视觉保真度和沉浸式体验，又能节省电力，还能支持游戏中的AI **agent**（智能体），例如动态**NPC**（Non-Player Character: 非玩家角色）。为了提升AI性能，英伟达在Blackwell架构中大量使用了**FP4**（4-bit Floating Point: 4位浮点数）计算。FP4精度虽然比FP8更低，但算力密度更高，对于AI渲染这种“对精度要求不高”的场景来说完全足够，且能大幅降低内存占用和功耗。此外，英伟达还大量使用了“着色器执行重排序”技术，根据着色器的执行状态动态调整任务顺序，保持GPU核心计算单元**SM**（Streaming Multiprocessor: 流式多处理器）的满载，从而发挥最大性能，避免资源浪费。

内存方面，Blackwell增加了对**GDDR7**（Graphics Double Data Rate 7: 第七代图形双倍数据速率内存）的支持。GDDR7采用**PAM3**（Pulse Amplitude Modulation 3: 3电平脉冲幅度调制）调制技术，与**GDDR6X**的**PAM4**相比，PAM3每时钟周期的位数更少，但**信噪比SNR**（Signal-to-Noise Ratio: 信号噪声比）更高，能支持更高的时钟速度，整体带宽反而更高，还能支持更低的电压，减少功耗。英伟达还特别关注“首token执行时间”，因为在混合图形/机器学习工作负载中，首token时间越短，AI响应越及时，交互体验越好。因此，Blackwell通过优化AI计算的调度逻辑，大幅缩短了首token时间。此外，Blackwell图形GPU还集成了一整套AI管理处理器，专门协调图形和机器学习的交错工作，例如在游戏渲染的间隙调度AI任务执行，确保数据传输和SM的高效运行，避免出现图形任务等待AI或AI任务等待图形的情况。

帧生成技术也是一大亮点，通过AI生成中间帧，能够在保证帧率的前提下降低GPU功耗。根据英伟达的数据，帧生成能将GPU功耗减半，这对移动设备和笔记本电脑来说非常实用。Blackwell还支持通用多实例GPU **MIG**（Multi-Instance GPU: 多实例GPU），即将一张GPU分成多个独立的虚拟GPU，同时运行多个工作负载。例如**RTX Pro 6000**，单个1080p客户端的工作负载太小，无法完全利用GPU的算力，将其拆分成多个小的虚拟GPU后，能通过并行执行多个工作负载保持GPU满载。4个MIG实例比传统的时间切片**timeslicing**性能提升60%，资源利用率更高。

#### Meta 的 Orion AI 眼镜专用芯片

**Meta** 的**Orion** AI眼镜专用芯片，其眼镜原型特点是普通眼镜外观结合AR沉浸式功能，正在突破AI眼镜在空间和功耗方面的极限。由于眼镜体积小，功耗预算极其有限，通常只有几瓦，但又需要实时处理眼动追踪、手势识别、**世界锁定渲染WRL**（World-Locked Rendering: 世界锁定渲染）等任务，传统芯片根本无法满足。因此Meta专门设计了一套芯片方案。

Meta的核心挑战是WRL，这是AR/MR应用中的关键技术，指将虚拟物体固定在现实世界的特定位置上，使其与物理环境保持相对静止，从而在用户移动时虚拟物体不会“飘”，体验更加沉浸。WRL对延迟和功耗要求极高，延迟超过20ms用户就会有“眩晕感”；功耗太高则眼镜续航短。因此Meta必须用专用芯片来加速WRL。为了平衡性能和功耗，Meta采用了多种前沿技术：一是先进工艺节点，Orion构思之初就定下5nm工艺，能在小面积内集成大量晶体管同时控制功耗；二是减少DRAM的使用，DRAM功耗高、体积大，Meta尽量用片上SRAM存储数据，减少DRAM访问；三是**Vmin Fmax**优化（Minimum Operating Voltage, Maximum Operating Frequency: 最小工作电压，最大工作频率），在保证性能的前提下尽可能降低最小工作电压，减少静态功耗；四是积极的电源管理与数据压缩，根据任务负载动态调整芯片电压和频率，同时压缩数据传输量，减少功耗；五是创意封装和减线数，采用更紧凑的封装技术，例如**SiP**（System in Package: 系统级封装）来减少体积，同时优化引脚设计，减少线数，降低信号干扰和功耗。

Orion的计算任务拆分也很巧妙，它将重负载任务，例如复杂AI推理，放到外部的**Puck**设备中，眼镜本地只处理低延迟任务，例如WRL、眼动追踪等。这样既能降低眼镜的功耗和体积，又能保证实时性。Puck设备中有三个主要处理芯片，分别是显示处理器、眼镜处理器和计算协处理器，分工明确。眼镜处理器负责处理所有的眼动、手部追踪以及摄像头输入，采用SiP封装，5nm工艺，集成了24亿颗晶体管，这个晶体管数量在眼镜芯片中算多的了。为了安全，Meta还在芯片中植入了“安全信任根”，确保所有进出芯片的数据都经过加密，防止隐私泄露。来自Puck的图像是**HEVC**编码的（High Efficiency Video Coding: 高效率视频编码），眼镜处理器需要先解码，再重新编码为显示处理器的专有格式，这个过程要在几毫秒内完成，对解码性能要求很高。显示处理器有两个，每只眼睛对应一个，负责重新投影，或称时间扭曲。这是由于摄像头采集图像和屏幕显示有延迟，显示处理器需要根据用户的实时位置调整图像视角，确保虚拟物体和现实世界对齐。显示处理器没有外部存储器，所有数据都存储在片上SRAM中，因此SRAM容量很大，能避免DRAM带来的延迟和功耗。计算协处理器是Orion中性能最强、功耗最高的芯片，同样采用5nm工艺，配备**LPDDR4X**内存，集成了57亿颗晶体管，负责计算机视觉处理、机器学习执行、音频渲染、HEVC编码等重负载任务，例如手势识别的AI模型推理就由它来完成。它也有相对较大的片上SRAM缓存，减少内存访问延迟，确保任务实时执行。

### 网络安全挑战与硬件防护

随着AI和云计算的发展，网络安全的挑战也日益严峻。微软在大会上公布了一组令人震惊的数据：2024年网络犯罪的“GDP”高于9万亿美元，预计2025年将超过10万亿美元，排名介于中国和德国之间，已成为“全球第三大GDP实体”。面对如此严峻的安全形势，软件防护已不足够，必须从硬件层面构建安全屏障。本届Hot Chips上，微软分享了Azure的硬件安全方案，核心是将安全集成到每台服务器，而不是依赖中心化的**HSM**（Hardware Security Module: 硬件安全模块）。

HSM是一种专门用来保护加密密钥的硬件设备，能提供高强度的加密运算和密钥管理，防止密钥被窃取。传统的HSM部署采用“中心化模型”，例如一个数据中心放置几台HSM服务器，所有需要加密的任务都需与这些服务器通信。这种方式存在明显问题：一是延迟高，所有请求需排队；二是单点故障，HSM服务器故障会导致整个数据中心的加密任务停滞；三是扩展性差，随着服务器数量增加，HSM会成为瓶颈。

微软的解决方案是“**Azure Integrated HSM**”，这是一款专用的安全**ASIC**（Application-Specific Integrated Circuit: 专用集成电路）芯片，直接集成到每台服务器中，无需再依赖中心化HSM。这种设计的好处是：一是延迟低，加密任务可在本地服务器完成，无需与中心化服务器进行**TLS**（Transport Layer Security: 传输层安全协议）握手；二是扩展性好，服务器越多，安全算力也越多，不会出现瓶颈；三是可靠性高，单台服务器的HSM故障不影响其他服务器，避免单点故障。这款ASIC的设计也很有针对性，它集成了HSM优化硬件，包括**AES**（Advanced Encryption Standard: 高级加密标准）和**PKE**（Public Key Encryption: 公钥加密）操作的硬件加速引擎，这些都是最常用的加密算法，硬件加速能大幅提升效率；还有用于控制逻辑的实时核心，确保加密任务的实时处理；接口和安全标准也做了加固，能够检测入侵和篡改行为，例如有人试图物理拆解芯片，或用侧信道攻击窃取密钥，ASIC能立即触发防护机制，销毁密钥或停止工作。

微软还进入了“机密计算”领域，其目标是“保护正在使用的数据”，尤其是在多租户云环境中。用户数据在云服务器上运行时，即使是云服务商也无法看到原始数据。Azure Integrated HSM能够为机密计算提供硬件根信任，确保数据在内存中运行时也是加密的，只有授权的应用程序才能解密，防止数据被窃取或篡改。为了证明设计的合理性，微软还详细分析了ASIC的门数分布，在ASIC的芯片面积中，硬件密码模块占了62%，这足以说明加密功能是这款芯片的核心重心。

微软之所以决定将这款定制ASIC开源，背后有四个关键考量：第一是安全透明度，开源能让全球安全专家参与审计，找出潜在漏洞，相比闭源设计，“众包式”安全验证更能抵御高级攻击；第二是一致性，开源方案能确保微软全球数据中心设施安全与操作标准统一，避免不同闭源组件间的兼容性问题；第三是密码学标准化，加密算法本身是高度标准化的技术，开源能更好地贴合行业标准，减少自定义闭源算法的风险；第四是层层防御，开源并非“裸奔”，而是在硬件级防护的基础上，再叠加软件、协议层面的安全措施，形成多维度的安全屏障，让攻击者“突破一层还有一层”。

### 散热技术的演进

散热是芯片领域的“隐形杀手”。随着AI芯片算力越来越强，功耗也随之飙升，例如之前提到的**AMD MI355X**液冷版本总板功耗达到1400W，谷歌**Ironwood TPU**的**SuperPod**更是要支撑百万级芯片的散热。传统的风冷、甚至普通液冷已经难以承受。本届Hot Chips上，散热领域的创新点明确，即利用更精细的结构设计与生成式AI优化，使散热效率追上算力的增长。其中**Fabric8Labs**的方案最具代表性。

Fabric8Labs的核心技术是“**电化学增材制造ECAM**（Electrochemical Additive Manufacturing: 电化学增材制造）”，其原理可简单理解为用电荷替代光，以像素级精度沉积铜。传统3D打印是利用激光或喷嘴进行“堆料”，而ECAM是利用电化学原理，使铜离子在电场作用下精准附着在基底上，从而制造出传统工艺无法实现的复杂3D结构。铜是绝佳的导热材料，这为散热设计开辟了新的空间。

他们展示的散热方案有三个关键方向：第一个是3D结构优化，通过ECAM制造出多孔、镂空的铜制散热结构，例如类似“蜂窝”或“树枝”的形态。这种结构的表面积比传统扁平式散热片大几十倍，能够大幅提升热交换效率；第二个是生成式AI驱动设计，用AI模拟不同芯片的发热分布，例如GPU的SM单元、CPU的核心区域等发热热点，然后自动生成最适配的散热结构。例如在发热最严重的区域设计更密集的铜制通道，在低热区域减少材料用量，既保证散热效果又避免浪费；第三个是直接硅基沉积，这是一种更激进的思路，将铜直接沉积在硅片上，使散热结构与芯片“零距离接触”，热量无需经过封装层传导，直接通过铜结构导出。这种方式能将热阻降到最低，尤其适合高功率密度的AI芯片。

Fabric8Labs还展示了一款“两相液冷浸入式蒸发板”，其核心是“优化流体蒸发过程”。传统液冷依靠液体流动带走热量，而两相液冷是让液体在散热板内蒸发，利用“蒸发吸热”的物理原理高效降温，比单相液冷的散热效率高3到5倍。他们通过ECAM在蒸发板内部制造出复杂的微通道结构，增加液体与散热面的接触面积，同时引导蒸汽快速排出，避免“蒸汽堵塞”影响散热。这种设计能使散热板在相同体积下比传统产品多带走40%的热量。更长远的设想是“封装级冷板”和“直接硅基散热”，即未来的芯片封装不再是“先做芯片再套散热壳”，而是将散热结构纳入封装设计的第一步，例如在Chiplet之间预留铜制散热通道，在HBM内存旁边集成微型蒸发管，甚至用ECAM在芯片裸片上直接制造散热鳍片。Fabric8Labs还提到，他们正与**EDA**（Electronic Design Automation: 电子设计自动化）软件厂商合作，将ECAM散热设计工具集成到芯片设计流程中，使芯片设计师在设计电路时就能同步优化散热结构，实现“算力与散热的协同设计”，而非事后补救。

### AI 计算领域的存储突破

AI计算当前最大的痛点不是“算力不够”，而是“数据传不动”。当模型参数增长到千亿、万亿级时，内存带宽和容量就成为瓶颈。很多时候芯片算力未跑满，就受限于数据的读取。本届Hot Chips上，几乎所有AI计算相关的分享都围绕“如何突破存储瓶颈”展开，方向非常清晰：要么优化内存架构，要么支持更低精度的数据格式，要么实现超大规模芯片互连，同时还要兼顾能效。毕竟现在数据中心的电费已成为许多企业的沉重负担。

#### Marvell 的存储优化方案

**Marvell** 公司提出了一个观点：“存储是唯一重要的东西”，并拿出了三项针对性创新：定制SRAM、定制HBM和**CXL**（Compute Express Link: 计算快速链接）控制器。这三者层层配合，从“近内存”到“远内存”全面优化带宽和延迟。

首先是定制SRAM。SRAM是离AI加速器最近的内存，速度最快但容量小，因此优化其关键在于如何在有限空间内榨取更多带宽。Marvell展示了业界首款2nm定制SRAM设计，能够提供6Gb的高速内存。最关键的是其“性价比”，在相同工艺尺寸下，其带宽密度是标准SRAM的17倍，所需面积减少50%，待机功耗还能减少66%。实现这一目标的核心思路有三：一是使SRAM运行速度更快；二是将SRAM单元做得更宽，能一次传输更多数据；三是增加更多端口，使同时读写的通道更多。这样一来，即使是1Mb的大型SRAM阵列也能保持高带宽密度，这对需要高频读取数据的AI推理场景来说是一场“及时雨”。

其次是定制HBM。HBM是高端AI芯片的“标配”，但其接口会占用大量片上空间，挤占计算单元的位置。Marvell的解决方案是与**SK海力士**、**三星**、**美光**这三大HBM供应商合作，优化HBM的基片和接口。具体来说，即减少I/O接口的面积，腾出芯片边缘的空间来支持高速信号的传输，从而提升带宽。他们采用标准DRAM芯片，搭配为加速器定制的基片，还用上了速率达每秒每毫米30Tbps的下一代**D2D IP**（Die-to-Die Interconnect Intellectual Property: 芯片间互连知识产权）。这样一来，不仅能缓解物理和散热限制，还能大幅减少功耗，节省下来的空间可用于安装更多计算单元或增加新的功能，形成性能和功耗之间的正向循环。

最后是CXL控制器。它针对更大规模的内存扩展。Marvell打造了**Structera CXL**产品线，核心理念是“不绕路”。传统内存扩展需经过CPU和PCIe交换机，延迟高、带宽损耗大。而Marvell的高容量内存扩展设备能够直接连接，延迟更低、带宽更高。其中**Structera A CXL**近内存加速器很有代表性，它集成了16个**Arm Neoverse v2 CPU**核心、4通道DDR5，内存带宽能达到200GB/s，容量达4TB，功耗却不到100W。这刚好能够分担AI推理这类带宽密集型任务。举例而言，一台64核的高端x86 CPU服务器，加一颗Structera A芯片，就能增加25%的核心数、50%的内存带宽，还能多4TB内存，但总功耗只增加100W。算下来，每GB/s的传输功耗反而下降了，这对于需要扩容但又不想更换整机的企业来说，成本优势非常明显。

#### d-Matrix 的存内计算方案

如果说Marvell是“优化现有内存的架构”，那么AI芯片公司**d-Matrix**的思路就是“重构内存与计算的关系”，用“存内计算”来突破瓶颈。当前AI推理有一个新趋势，即小参数模型的表现已能超过大语言模型，但在生成更多**token**（令牌：语言模型处理的最小信息单元）时仍会被内存限制。像实时语音、AI agent这类场景对延迟要求又极高，传统架构根本无法满足。因此d-Matrix的解决方案是将内存和计算紧密集成，重新设计内存结构。他们的AI推理芯片**Corsair**就是基于这个思路研发的。

Corsair采用数字存内计算架构，搭配自定义的矩阵乘法电路和块浮点数据格式，能效可达38 **TOPS/W**（Tera Operations Per Second Per Watt: 每瓦万亿次运算），在FP8精度下算力达2400 TOPS，FP4精度下更是能达到9600 TOPS。更关键的是延迟，用它运行**Llama3-70B**模型，单token生成时间仅需2ms，这对实时交互场景至关重要。

硬件设计方面，每张Corsair PCIe卡有2个封装，每个封装含4个Chiplet，采用台积电6nm工艺，总共提供2GB SRAM，内存带宽高达150TB/s，远超传统HBM的带宽，峰值功耗600W。在800MHz时功耗275W，1.2GHz时550W，属于“高性能且功耗可控”的水平。为了支持更大规模的扩展，Corsair的设计也非常灵活。其PCIe卡顶部有桥接连接器，两张卡能够通过**DMX Bridge**连接成16个Chiplet，实现“All-to-All”连接。标准服务器中能安装8张卡，还能通过PCIe或以太网横向扩展到多台服务器。芯片内部架构也很讲究，每个Chiplet由4个**Quad**组成，每个Quad含有4个**Slice**、1个RISC-V控制核心和1个调度引擎。每个Slice又有**DIMC**核心（Digital In-Memory Compute: 数字存内计算）、**SIMD**核心（Single Instruction, Multiple Data: 单指令多数据）和数据重塑引擎。这种分层设计能使计算和内存访问更加高效。此外，它支持**MicroScaling**的“块浮点格式”，即一个块（Block）内所有数据会用相同的缩放因子进行运算，这样既能利用整数运算的高效性，又能实现浮点的高动态范围。

#### 华为的超节点网络方案

接下来是**华为**的分享，他们的重点是“超节点网络”。现在十亿瓦级AI数据中心越来越多，超节点就是将大量设备紧密连接成一个大型计算系统，目标是将芯片数量扩展到100万，带宽提升到10Tbps。数据传输模式也从“异步**DMA**（Direct Memory Access: 直接内存访问）”变为“同步加载/存储”，而且要能连接CPU、GPU、内存池、SSD、网卡、交换机等所有设备。华为提出的解决方案是“**统一总线网格UB-Mesh**（Unified Bus Mesh: 统一总线网格）”，其核心思路是“用统一协议+混合拓扑来平衡性能与成本”。

传统网络为何不行？因为随着节点规模扩大，传统网络的成本会“超线性增长”，要实现100倍带宽，成本可能要增长1000倍，这是难以承受的。UB-Mesh的优势是成本的“亚线性增长”，节点越多，成本增长越慢。在具体的实现方式上，华为研究了三种拓扑技术：一是**CLOS**拓扑，适合低带宽的顶级网络，例如100万个节点的规模，特点是多功能、高可靠；二是**nD mesh**拓扑，适合机架（约64个节点）或大Pod（范围从128到8192个节点），特点是本地带宽高，远程带宽可按需减少；三是**nD sparse mesh**拓扑，适合更小的本地部署（例如16到128个节点），特点是成本低且带宽高。他们还发现一个关键规律，即大语言模型训练的流量是“两两分层”的，基于此规律设计拓扑能进一步优化带宽利用效率。

此外，UB-Mesh还解决了“可靠性”问题。传统光纤链路如果出现故障，整个超节点都会受影响。华为的解决方案有两个：一是链路级重试，在同一个模块上对其他光纤链路重试，避免再次走故障路径；二是**MAC交叉连接**，将MAC模块交叉连接到多个光学模块上，即使一个模块损坏，另一个仍能工作。他们的目标是将平均无故障时间**MTBF**（Mean Time Between Failures: 平均故障间隔时间）提升100倍。具体做法是设置“热备机架”，故障机架下线后，热备机架立刻接管；等故障机架修复后，再作为新的热备机架回归。如果机架本身带有额外芯片，还能作为“弱热备机架”来提供部分算力，进一步提升可靠性。

#### 英伟达的 GB10 SoC 超算

英伟达在AI计算领域的分享走的是“小型化”路线，他们将AI超算搬到了桌面，推出了**GB10 SoC**（System-on-a-Chip: 系统级芯片）。这款芯片是英伟达**DGX Spark**小型工作站的“心脏”，目标是让中小企业也能使用高性能AI计算。GB10的架构很有特点，它集成了英伟达**Blackwell GPU**和**联发科**（MediaTek）打造的20核**Arm CPU**，采用台积电3nm工艺和2.5D先进封装，继承了Blackwell架构的所有核心功能，同时将功耗控制得很好。

从参数来看，GB10采用了128GB **LPDDR5x**高带宽统一内存，FP32精度下AI性能可达到31 **TFLOPS**（Tera Floating-point Operations Per Second: 每秒万亿次浮点运算），FP4精度下更是高达1000 TFLOPS，额定热设计功耗**TDP**（Thermal Design Power: 散热设计功耗）仅140W。这个功耗水平在桌面工作站中完全可控。内存子系统由联发科负责，他们还实现了部分英伟达**IP**（Intellectual Property: 知识产权）功能，例如显示控制器和**C2C**（Chip-to-Chip: 芯片到芯片）链接。特别值得一提的是GB10中的24MB L2缓存，它实现了CPU和GPU的缓存一致性，这能够大幅降低数据传输的性能开销，还能简化软件开发。以前CPU和GPU需要频繁同步数据，现在有了缓存一致性，开发难度直接下降一个档次。

搭载GB10的DGX Spark工作站现在开始支持更大规模的扩展。单机能运行2000亿参数的AI大模型，或700亿参数的微调模型。如果用**ConnectX-7**网卡将两台DGX Spark连接起来，还能支持更大的模型，例如参数超过2000亿的场景。对于许多需要本地部署AI模型的企业来说，这种“小而强”的工作站比动辄上千万的大型集群更为实用。

#### AMD 的 MI350 系列 AI 芯片

AMD带来了全新的**MI350**系列AI芯片，基于**CDNA 4**架构，核心亮点是“3D堆叠+低精度支持”，专门为生成式AI设计。MI350系列的硬件架构很有诚意，采用3D芯片堆叠技术，在两个6nm I/O基片上堆叠了8个3nm **XCD**（Compute Die: 计算裸片）芯片，总共集成了1850亿颗晶体管，这个数量在AI芯片中可算是第一梯队。封装方面，MI350系列支持标准**OAM**（OCP Accelerator Module: 开放计算项目加速器模块）封装，分为两个版本：MI350X采用风冷系统，MI355X采用液冷系统。液冷系统的总板功耗达到1400W，适合高密度部署。虽然风冷和液冷的内存容量、带宽相同，但液冷版本的计算性能更高，因为它能承载更高的功率。

相比上一代产品，MI350系列的两个I/O **die**（裸片）提供了更宽、更低时钟频率的**D2D**连接（Die-to-Die Interconnect: 裸片间互连），这样也能提升能效。毕竟高频运行虽然快，但功耗也高，平衡频率和带宽才是关键。在内存和缓存方面，MI350系列的HBM带宽比上一代多2TB/s，内存容量也更大。这意味着运行相同的模型，所需的GPU数量会减少，间接降低了部署成本。本地数据共享**LDS**（Local Data Share: 本地数据共享）的容量比上一代**MI300**翻了一倍，XCD芯片的峰值引擎时钟频率达到了2.4GHz，每个XCD还有4MB L2缓存，这些都能提升数据的访问效率。

精度支持上，MI350系列不仅支持FP8，还支持行业标准的**MXFP6**和**MXFP4**数据格式。更低的精度意味着更高的算力密度和更低的内存占用，对生成式AI推理场景非常友好。软件方面，AMD搭配了**ROCm 7**软件栈。根据官方数据，使用MI355X运行**DeepSeek R1**模型，推理速度是上一代**MI300X**的3倍，在FP4精度下性能超过英伟达**B200**。预训练**Llama 3 70B**模型时，性能也能达到上一代的两三倍，这个提升幅度非常可观。此外，AMD还预告了明年发布的MI400系列将搭载432GB的HBM4，性能提升会更显著。看来他们在AI芯片领域的追赶速度已越来越快。

#### 谷歌的 Ironwood TPU

最后是**谷歌**的压轴分享，他们带来了代号为**IRONWOOD**的新一代**TPU**（Tensor Processing Unit: 张量处理单元）。这是谷歌首款专为大规模AI推理设计的TPU，突破点非常多，几乎覆盖了“性能、扩展、能效、可靠性”所有维度。

核心参数方面，Ironwood单SuperPod最多可容纳9216颗芯片，采用**光电路交换机OCS**（Optical Circuit Switch: 光路交换机）共享内存，可直接寻址的共享HBM内存容量达到1.77PB。FP8精度下单SuperPod性能可扩展到42.5 **EFLOPS**（Exa Floating-point Operations Per Second: 每秒百亿亿次浮点运算）。能效方面，每瓦性能是上一代谷歌**TPU Trillium**的2倍，还支持机密计算，集成了更多可靠性和安全性功能。

硬件架构上，Ironwood是谷歌首款双计算**die TPU**，采用8层HBM3e内存，提供192GB容量和7.3TB/s带宽，能够满足超大规模模型的实时数据读取需求。值得一提的是，Ironwood的设计还用到了AI。谷歌和**AlphaChip**团队合作，用AI来设计**ALU**（Arithmetic Logic Unit: 算术逻辑单元）电路和优化芯片布局，大幅提升了设计效率和芯片性能，堪称“用AI造AI芯片”的一个典型案例。

互连方面，Ironwood支持单SuperPod扩展到9216个芯片，还能横向扩展到数十个SuperPod，满足不同规模的推理需求。硬件形态上，每个**Ironwood Tray**包含4个TPU，采用液冷设计；16个Tray装入一个机架，每个机架有64个TPU，同时连接16个CPU主机机架。机架内所有互连采用铜缆，OCS负责连接其他机架。这样既能保证近距离传输的低延迟，又能实现远距离的高带宽扩展。与之前采用OCS的**TPUv4**相比，Ironwood将一个Pod内的芯片数量增加了1倍，而且OCS支持将Pod配置成不同大小的“矩形棱柱体”，如果有节点失效，能直接丢弃，通过从检查点恢复，重新配置切片来使用其他的机架，这对于超大规模集群的可靠性至关重要。

此外，Ironwood还搭配了谷歌第三代液冷系统，采用多重循环设计，确保进入冷却板的水足够干净，避免堵塞冷却板。它还集成了第四代**SparseCore**，用于嵌入和集体卸载任务，提升推理效率。电力方面，它可以通过软硬件功能平滑电力消耗波动，避免因功耗骤升骤降影响系统稳定。根据谷歌的说法，Ironwood创下了“共享内存多处理器”的新纪录，1.77PB HBM实现了低开销的高带宽数据共享，不仅能有效支持超大型模型，同时将每瓦性能提升到TPUv4的近6倍、Trillium的2倍，这对于需要长期运行推理任务的企业来说，能节省一大笔电费。

### 网络技术创新

AI集群的运行离不开网络这条“生命线”。如果网络延迟高、丢包多，即使芯片算力再强，整体性能也会被拉垮。本届Hot Chips上，网络领域的分享主要围绕“减负、提速、保可靠”三个方向展开。例如英特尔用**IPU**（Infrastructure Processing Unit: 基础设施处理单元）帮助CPU卸载工作，AMD和英伟达推出新一代高速网卡，博通则聚焦高性能交换机芯片，目标都是让大规模数据传输“又快又稳”。

#### 英特尔的 IPU E2200 400G

**英特尔**的IPU **E2200 400G**芯片采用台积电**N5**工艺，核心定位是“卸载并加速网络传输的基础设施工作负载”，简单来说就是“帮助CPU分担工作”。当前数据中心中，CPU需要处理计算、网络、存储等一系列任务，很容易被网络传输所拖累。因此IPU的作用就是接管网络相关的工作，让CPU专注于计算。

IPU E2200的网络子系统全面，包含PCIe Gen5 x32域、400G以太网MAC、Arm Neoverse N2核心计算单元，还提供自定义的可编程卸载选项，支持**P4**可编程数据包处理（Programming Protocol-independent Packet Processors: 一种用于编程网络数据平面的语言）、高性能内联加密等功能。它还支持三种工作模式，包括多主机模式、无头模式和融合模式，兼容性强，能够适配不同的数据中心场景。实际应用方面，IPU E2200已在数据中心落地，例如在云环境中，它能卸载虚拟机的网络转发任务，降低主机的CPU占用率；在存储场景中，能加速分布式存储的数据流传输，提升存储访问速度；在AI集群中，还能优化跨节点的数据同步，减少训练延迟。可以说，IPU正成为数据中心网络的“新基建”。

#### AMD 的 Pensando Pollara 400 AI 网卡

AMD的**Pensando Pollara 400 AI**网卡有一个特别的标签，即“业界首款**超以太网联盟UEC**（Ultra Ethernet Consortium: 超以太网联盟）就绪的AI网卡”。超以太网联盟UEC旨在利用以太网解决AI横向扩展网络的痛点，例如**ECMP**负载平衡（Equal-Cost Multi-Path: 等价多路径）的链路利用率低、网络和节点拥塞、丢包等问题。这些问题在AI训练集群中很常见，一旦出现丢包，训练任务可能需要重新开始，损失巨大。

Pollara 400的核心优势是“可编程”，它采用P4架构来构建数据包流程。P4是一种面向数据平面的编程语言，能够灵活定义数据包的处理逻辑，比传统固定功能的网卡更适应AI网络的动态需求。它能根据AI训练的流量特点动态调整路由策略，避免拥塞，还能优化虚拟地址到物理地址的转换，减少数据传输的延迟。它的原子内存操作，例如对共享内存的读写，也设计在SRAM相邻位置，进一步降低延迟。此外，它还增强了“管线缓存一致性”，确保多节点之间的数据同步更高效。根据AMD的数据，搭配AMD的分布式深度学习通信库**RCCL**（Radeon Collective Communications Library: Radeon集合通信库）使用时，Pollara 400能将AI的训练性能提升40%，这一提升非常实在，因为对于AI训练来说，网络延迟每降低一点，整体训练时间就能缩短一截。

#### 英伟达的 ConnectX-8 SuperNIC

英伟达的网络产品则更加激进，他们带来了**ConnectX-8 SuperNIC**，这是一款**PCIe Gen6**网卡，最高速率达到800Gb/s，有48个PCIe Gen6通道，是目前速率最高的网卡之一。ConnectX-8的设计非常灵活，既支持Spectrum-X以太网，又支持Quantum-X Infiniband。以太网成本低、兼容性强；Infiniband延迟低、性能高。用户可根据需求选择，无需更换网卡。

为何需要如此高的速率？因为现在数据中心已从“单服务器计算”变为“集群计算”，GPU需要与集群中的其他GPU、CPU、存储快速通信，低速率根本无法满足。ConnectX-8的首个部署是**GB300 NVL72**，其连接方式非常讲究。由于英伟达**Grace**超级芯片以PCIe Gen5速度运行，因此采用Gen5 x16链路连接Grace CPU，然后用Gen6 x16链路连接**B300 GPU**，确保高带宽传输。它还预留了一个Gen5 x4链路连接**SSD**（Solid State Drive: 固态硬盘），满足存储访问需求。此外，英伟达的**MGX PCIe**交换机板卡也使用了这款网卡，这样既能支持博通的交换机芯片，又能为未来的B300 PCIe GPU提供Gen6到NIC的高速连接，兼容性极佳。

为了提升网络效率，ConnectX-8还集成了**PSA**数据包处理器（Packet Switching Architecture: 数据包交换架构）和**数据路径加速器DPA**（Data Path Accelerator: 数据路径加速器）。DPA是一个RISC-V事件处理器，能够实时处理网络事件，减少CPU干预。Spectrum-X以太网的拥塞控制和路由功能也能与DPA配合，进一步降低延迟。根据英伟达的数据，Spectrum-X以太网在训练时间步长和尾部延迟上的表现远优于传统以太网，能有效提升AI训练的稳定性。

#### 博通的 Tomahawk Ultra 网络芯片

最后是**博通**（Broadcom）的**Tomahawk Ultra**网络芯片，这款芯片专为高性能计算和AI扩展设计，目标是改变“以太网不适合高性能工作负载”的偏见。博通的交换机阵容中，**Tomahawk 6**是102.4Tbps吞吐量的专用芯片，而Tomahawk Ultra则更侧重“平衡性能与成本”。

其packet转发管线设计很有特点，支持多项关键功能：一是**链路层重传**（Link Layer Retry），作为以太网**FEC**（Forward Error Correction: 前向纠错）的补充，能提高突发错误或次优链路的健壮性，减少对高延迟的端到端重传的需求，这对AI训练非常重要，因为端到端重传会大幅增加延迟；二是**基于信用的流量控制CBFC**（Credit-Based Flow Control: 基于信用的流量控制），确保交换机缓冲区不会溢出，避免丢包；三是**AI Fabric Header**，它覆盖在以太网MAC header上，只保留最有用的字段，既能优化传输效率，又能保持完整的以太网MAC兼容性，无需修改现有设备；四是网络计算支持**集体操作**，例如AI训练中的**All-Reduce**操作，能在交换机层面加速，减少数据在节点间的传输次数。此外，Tomahawk Ultra还支持**拓扑感知自适应路由**，能根据网络拓扑动态调整路由，避免某个链路过载。拥塞控制功能也能确保流量均匀分布，不会出现“一条链路堵死，其他链路空闲”的情况。延迟方面，Tomahawk Ultra所有接口在以64B数据包大小测试时，延迟不到250ns，这一延迟水平已非常接近Infiniband，再加上以太网的成本优势，对许多企业来说是“性价比之选”。

### 2025 全球AI芯片峰会展望

本届Hot Chips大会七大技术领域的完整解析，从AI计算的存储突破到散热的精细设计，每一个技术细节都在推动芯片行业向“更高算力、更低能耗、更安全可靠”迈进。在大会的尾声，主办方还预告了“2025全球AI芯片峰会”，这场峰会将聚焦AI芯片的“落地挑战”，例如如何降低AI芯片的部署成本、如何构建统一的软件生态、如何平衡算力与能耗等。预计届时会有更多企业展示量产级的AI芯片产品，而非仅仅是实验室原型。对于关注AI芯片行业的朋友来说，这场峰会值得重点关注，后续也将第一时间带来报道。