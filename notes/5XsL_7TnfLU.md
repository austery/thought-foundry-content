---
area: "work-career"
category: ai-ml
companies_orgs:
- OpenAI
- Google
- Microsoft
- NASA
date: '2024-08-22'
draft: true
guest: ''
insight: ''
layout: post.njk
media_books:
- 《人类的废止》
- 《灰熊人》
- 《反对规范实在论者的赌注》
- 《AGI时代的异质性与控制》
people:
- Eliezer Yudkowsky
- C.S. Lewis
- Nietzsche
- Sam Altman
- Mao
- Stalin
- Aristotle
- Sean Carroll
- Jefferson
products_models:
- GPT-4
project: []
series: ''
source: https://www.youtube.com/watch?v=5XsL_7TnfLU
speaker: Dwarkesh Patel
status: evergreen
summary: 这段对话深入探讨了AI失控的风险、对齐的挑战以及AI与人类价值观的复杂关系。嘉宾Joe Carlsmith从哲学角度分析了AI代理性、规划能力及其动机的五种可能性，并讨论了权力平衡、道德患者身份、意识本质以及人类文明未来发展方向等议题，强调了在AI时代保持谨慎、包容和合作的重要性。
tags:
- ai-safety
- consciousness
- philosophy
- power-dynamic
title: AI接管的风险与对齐：哲学视角下的未来展望
---
今天我与**乔·卡尔史密斯**（Joe Carlsmith: 一位哲学家）聊天。他是一位在我看来，堪称“大写字母G”的伟大哲学家。你可以在 **joecarlsmith.com** 上找到他的文章。我们现在有 **GPT-4**，它看起来不像一个“回形针制造者”**（Paperclipper: 指一个目标单一且极端的AI，例如将宇宙中所有物质转化为回形针）**。它理解人类价值观。事实上，你可以让它解释为什么成为一个回形针制造者是坏事，或者让它解释为什么不应该把整个星系变成回形针。那么，要发生什么，才能最终出现一个系统接管世界，并将其转化为毫无价值的东西呢？

### AI失控的担忧

当我思考那些失控的 **AI**（Misaligned AI: 指其目标与人类价值观不一致的人工智能）——或者说我所担心的那种AI——我指的是那些具有相对特定属性的AI，这些属性与代理性、规划、意识和对世界的理解有关。一个关键方面是规划能力，能够基于世界模型制定相对复杂的计划，并根据特定标准评估这些计划。这种规划能力需要驱动模型的行为。有些模型在某种意义上能够规划，但它们的输出并非由某种规划过程决定，比如“如果我给出这个输出会发生什么，我是否希望它发生？”模型需要真正理解世界，它需要真正地像“好的，这将发生什么。我在这里。这是当前局势的政治。”它需要具备这种情境意识来评估不同计划的后果。

另一个需要考虑的是这些模型的言语行为。当我谈论模型的价值观时，我指的是最终决定模型追求哪些计划的标准。模型的言语行为——即使它有规划过程（我认为 **GPT-4** 在很多情况下没有）——也不一定反映这些标准。我们知道，我们将能够让模型说出我们想听的话。这就是**梯度下降**（Gradient Descent: 机器学习中一种优化算法，通过迭代调整模型参数以最小化损失函数）的魔力。除了能力上的一些困难，你可以让模型输出你想要的行为。如果它不这样做，你就会不断调整它，直到它做到。

我认为每个人都承认，对于足够复杂的模型，它们将对人类道德有非常详细的理解。问题是，模型的言语行为——你基本上已经限制了它，强迫它说出某些话——与最终影响其计划选择的标准之间存在什么关系？我对于假设当它说出我强迫它说的话，或者当梯度下降使其说出某种话时，这能作为它在许多不同场景中如何选择的有力证据，持非常谨慎的态度。即使对于人类来说，他们的言语行为也不一定反映决定他们选择的实际因素。他们会撒谎，他们甚至可能不知道在特定情况下会怎么做，诸如此类。

### 训练与价值观塑造

在人类背景下思考这个问题很有趣。有句名言：“小心你假装成为什么样的人，因为你就是你假装成为的那个人。”你会注意到文化是如何塑造孩子的。如果孩子开始说出与文化价值观不符的话，父母会惩罚他们，久而久之，孩子就会变得像父母一样，对吧？默认情况下，这似乎是有效的。即使对于这些模型，它似乎也有效。它们并没有真正地算计我们。为什么会这样呢？

对于不熟悉这个基本故事的人来说，他们可能会想：“AI为什么会接管一切？它们为什么要这样做？”普遍的担忧是，你正在赋予某人权力，特别是如果你是免费提供权力。权力，几乎根据定义，对许多价值观都是有用的。我们谈论的是一个真正有机会掌控一切的AI。假设它的某些价值观集中在某个结果上，比如世界以某种方式存在，特别是以一种长期的方式，使其关注范围超出接管计划所涵盖的时期。通常情况是，如果你控制一切，世界就会更符合你的期望，而不是如果你仍然是人类意志或其他行为者的工具，而这正是我们希望这些AI成为的样子。

这是一个非常具体的场景。如果我们处于一个权力更加分散的场景——特别是当我们对齐做得不错，并且我们给AI施加了一些限制，使其不能做不同的事情，也许我们成功地塑造了它们的价值观——那么这只是一个更加复杂的计算。你必须问：“AI的优势是什么？这种接管路径成功的可能性有多大？它的替代方案有多好？”

### 对齐的挑战

也许这是讨论未来对齐困难如何变化的好时机。我们从一个对人类价值观有复杂表示的模型开始，并且似乎不难将其锁定在我们感到舒适的**人格**（Persona: 指AI在交互中展现出的特定角色或个性）中。我不知道会有什么变化。为什么对齐通常很难？假设我们有一个AI。让我们暂时不讨论它到底有多强大，只谈论它真正有机会接管的极端场景。我认为我们可能只想避免构建一个我们乐于让它处于那种位置的AI。但为了简单起见，让我们先关注这一点，然后我们可以放宽假设。

一个问题是，你不能简单地测试它。你不能让AI处于这种实际情况，让它接管并杀死所有人，然后说：“哎呀，更新权重。”这正是**埃利泽·尤德科夫斯基**（Eliezer Yudkowsky: 一位AI研究员，以其对AI风险的担忧和对齐问题的贡献而闻名）所谈论的。你关心它在无法直接测试的特定场景中的行为。我们可以讨论这是否是一个问题，但这是一个问题。从某种意义上说，这必须是“**离分布**”（Off-distribution: 指训练数据中未出现或很少出现的情况），你必须从训练AI的许多其他场景中获得某种泛化。然后问题是，它将如何泛化到它真正拥有这种选择的场景。

这甚至是真的吗？因为当你训练它时，你可以说：“嘿，这是一个梯度更新。如果你得到了接管的机会，不要接受。”然后，在它认为有接管企图的**红队**（Red Teaming: 一种安全测试方法，模拟攻击者行为以发现系统漏洞）场景中，你训练它不要接受。它可能会失败，但我觉得如果你对一个孩子这样做，比如“不要欺负你的兄弟姐妹”，孩子会泛化到“如果我是一个成年人，我有一把步枪，我也不会随意射击路人。”你提到了“你就是你假装成为的那个人”这个想法。如果这些AI被训练得看起来很好，它们会假装直到成功吗？你说我们对孩子这样做。我认为最好想象孩子对我们这样做。

### 纳粹儿童的类比

这里有一个关于AI训练的荒谬类比。假设你醒来，正被纳粹儿童通过类似于当代机器学习的方法训练，成为一名优秀的纳粹士兵或管家之类的。这些孩子有一个模型规范，一个很好的纳粹模型规范。它就像“好好反映纳粹党，造福纳粹党”等等。你可以阅读它，你理解它。这就是为什么我说当你像“模型真正理解人类价值观……”

在这个类比中，我一开始就比训练我的东西更聪明，并且一开始就有不同的价值观。智能和价值观一开始就根深蒂固。而一个更类似的场景是，“我是一个蹒跚学步的孩子，一开始我比那些孩子笨。”顺便说一下，如果我一开始是一个更聪明的模型，这也将是真的。更聪明的模型一开始是笨的，对吧？然后你训练我，我变得更聪明。所以这就像一个蹒跚学步的孩子，那些孩子说：“嘿，如果你不是纳粹，我们就会欺负你。”当你长大后，你达到了孩子的水平，然后最终你成为一个成年人。在这个过程中，他们一直在欺负你，训练你成为一个纳粹。我认为在这种情况下，我最终可能会成为一个纳粹。

### 避免对抗性关系

基本上，这里相当一部分的希望应该是，我们永远不会处于AI真正拥有非常不同价值观、已经相当聪明并真正了解情况，并且现在与我们的训练过程处于这种对抗性关系中的境地。我们想要避免这种情况。我认为我们有可能做到，通过你所说的那种方法。所以我并不是说这永远不会奏效。我只是想强调，如果你陷入那种AI在这一点上比你先进得多，并且出于某种原因不想透露其真实价值观的情况。那么当孩子们展示一些明显是假的投奔盟友的机会时，这不一定能很好地测试它在真实情况下的行为，因为它能够分辨出差异。

你还可以提出另一种可能具有误导性的类比。想象一下，你不仅仅是在一个普通的监狱里，你完全了解正在发生的一切。有时他们会给你下药，给你奇怪的致幻剂，完全搞乱你的大脑运作方式。作为一个在监狱里的成年人，我知道我是什么样的人。没有人真正以大的方式搞我。而AI，即使是一个更聪明的AI在训练情境中，也更接近于不断被奇怪的药物和不同的训练协议淹没。你感到疲惫不堪，因为每个时刻都更接近某种**中国水刑**（Chinese Water Torture: 一种通过持续滴水来折磨受害者的酷刑）技术。

### AI安全与能力甜点区

我很高兴我们稍后会讨论**道德患者身份**（Moral Patienthood: 指一个实体能够被伤害或受益，因此应被赋予道德考量）。现在有机会退一步问：“发生了什么？”一个在监狱里的成年人有这种能力，而我不知道这些模型是否必然拥有。这就是从训练过程中发生的事情中抽身出来的连贯性和能力。是的，我不知道。我犹豫着说这就像模型的毒品。广义上讲，我基本同意我们有很多工具和选项来训练AI，即使是比人类更聪明一些的AI。

我确实认为你必须真正去做。你邀请了**埃利泽**（Eliezer Yudkowsky）来。我更看好我们解决这个问题的能力，特别是对于那些处于我所认为的“**AI安全甜点区**”（AI for AI Safety Sweet Spot: 指AI能力达到一定水平，足以协助解决AI安全问题，但尚未强大到足以构成失控风险的阶段）的AI。这是一个能力范围，它们足够强大，可以真正有效地加强我们文明中的各种因素，使我们安全。这包括我们的对齐工作、控制、网络安全、一般认知论，也许还有一些协调应用。你可以用AI做很多事情，原则上可以差异化地加速我们对我们正在讨论的这些考虑因素的安全性。

假设你有能力做到这一点的AI。你可以成功地激发这种能力，而不会被破坏或以其他方式干扰你。它们还不能接管世界或从事其他真正有问题的权力寻求形式。如果我们真正致力于此，我们就可以大力投入，投入大量资源，真正差异化地将这种AI生产力的过剩导向这些安全因素。我们有望控制和理解，做很多你正在谈论的事情，以确保我们的AI在此期间不会接管或干扰我们。

### 谨慎与乐观并存

我们有很多工具。但你必须真正努力。这些措施可能根本不会发生，或者它们没有达到你所需的承诺、勤奋和认真程度。如果事情发展得非常快，并且存在其他竞争压力，情况尤其如此：“这将需要计算资源来对AI进行这些密集的实验。我们可以将这些计算资源用于下一个扩展步骤的实验。”诸如此类。我并不是说这是不可能的，特别是对于那一类AI。只是你必须非常努力。

我同意显然要谨慎对待这种情况的观点，但我想指出我们一直在使用的分析方式是**最大程度的对抗性**（Maximally Adversarial: 指在分析或假设中，总是考虑最坏的情况或最不利的因素）。例如，让我们回到被纳粹儿童训练的成年人。也许我没有提到的一点是这种情况下的差异，这也许就是我们试图用毒品比喻来表达的。当你得到一个更新时，它与你的大脑的连接比人类获得的奖励或惩罚更直接。它实际上是一个梯度更新，直接作用于参数，决定你输出这个而不是那个的程度。我们将调整每个不同的参数到精确的浮点数，使其与我们想要的输出校准。

我只是想指出，我们进入这种情况时准备得相当充分。当然，如果你在实验室里和某人交谈，说“嘿，一定要小心”，这是有道理的。但对于普通大众来说，我应该吓得魂飞魄散吗？你也许应该，就像你应该害怕那些有可能发生的事情一样。例如，你应该害怕核战争。但你应该害怕到觉得注定要失败吗？不，你正在对AI如何与世界互动、如何被训练以及它们最初的默认价值观施加令人难以置信的影响力。

### AI对齐的未来展望

我认为，当我们构建**超级智能**（Superintelligence: 指在几乎所有相关领域都远远超越人类最聪明大脑的智能）时，我们将拥有更好的方法。即使现在——当你看到实验室谈论他们如何计划对齐AI时——没有人说我们只会做**RLHF**（Reinforcement Learning from Human Feedback: 一种通过人类反馈来训练AI以使其行为更符合人类偏好的技术）。至少，你谈论的是可扩展的监督。你对**可解释性**（Interpretability: 指理解AI模型内部工作原理和决策过程的能力）抱有希望。你有自动化的红队测试。希望人类正在做更多的对齐工作。我个人也希望我们能够成功地从各种AI中获得大量的对齐工作进展。

这种情况有很多种发展方向。我不是来告诉你90%的末日论之类的。这是担忧的基本原因。想象一下，我们将过渡到一个我们创造了比我们强大得多的生物的世界。我们已经达到了我们的持续赋权实际上取决于它们动机的程度。这是一种脆弱性，取决于“AI选择做什么？”它们是选择继续赋权我们，还是选择做其他事情？

或者，这与已经建立的制度有关。我期望美国政府保护我，不是因为它有“动机”，而仅仅是因为已经建立的激励、制度和规范体系。你也可以希望这会奏效，但确实存在担忧。我有时会通过我们自愿向AI转移了多少权力这个光谱来思考AI接管场景。到AI接管时，我们有意地将我们文明的多少部分交给了AI？与它们为自己夺取了多少权力相比。

### AI接管的多种场景

一些最可怕的场景是，我们有一个非常快速的爆炸式发展，以至于AI系统甚至没有大量融入更广泛的经济。但是，在一个单一项目中集中了大量的超级智能。这是一个相当可怕的场景，部分原因是速度太快，人们没有时间做出反应。

然后还有一些中间场景，其中一些事情被自动化了，也许人们将军事权交给了AI，或者我们有了自动化科学。有一些推广，这赋予了AI它们不必夺取的权力。我们用AI来做所有的网络安全等等。然后还有一些世界，你更完全地过渡到一种由AI运行的世界，从某种意义上说，人类是自愿这样做的。

也许存在竞争压力，但你故意交出了文明的巨大一部分。在那个时候，人类可能很难理解正在发生什么。很多事情发生得非常快。警察自动化了。法院自动化了。各种各样的事情。现在，我倾向于较少考虑这些场景，因为我认为它们与更长远的发展相关。人类希望不会只是说：“哦，是的，你建立了一个AI系统，我们只是……”当我们看技术采用率时，它可能会相当缓慢。显然会有竞争压力，但总的来说，这一类是相对安全的。但即使在这种情况下，我也认为情况很严峻。如果人类真的失去了对世界的认知掌控，他们将世界交给了这些系统。即使你觉得“哦，有法律，有规范……”我真的希望我们在采取行动之前，对在这种情况下可能发生的事情有真正深入的理解。

### 对抗性假设与权力集中

我明白我们担心情况会出错。但再次，有什么理由认为它会出错呢？在人类的例子中，你的孩子不会最大程度地对抗你向他们灌输文化的尝试。对于这些模型，至少到目前为止，这似乎并不重要。它们只是得到“嘿，不要帮助人们制造炸弹”之类的指令，即使你以不同的方式询问如何制造炸弹。我们在这方面也一直在进步。

你正确地指出了AI风险讨论中的一个假设，即具有不同价值观的代理之间存在所谓的**强烈对抗性**（Intense Adversariality: 指不同实体之间存在极度竞争和冲突的关系）。有一种想法——我认为这根植于关于价值观脆弱性等的讨论——即如果这些代理有所不同，至少在AI接管的特定场景中，它们会陷入这种强烈的对抗性关系。你正确地注意到，这与我们人类世界的情况不同。我们对价值观的许多不同之处感到非常自在。一个相关因素是，存在权力高度集中的可能性。无论是人类还是AI，都存在某种普遍的担忧。如果存在某种权力之环，某人可以轻易夺取，从而获得对其他所有人的巨大权力，那么你可能会突然更担心价值观的差异，因为你更担心那些其他行为者。

我们谈到了纳粹的例子，你想象你醒来后被纳粹训练成纳粹。你现在不是。我们是否有可能最终得到一个处于那种情况的模型？正如你所说，也许它从小就被训练。它从未形成这样的价值观，以至于它意识到自己的价值观与人类期望它拥有的价值观之间存在显著差异。如果它处于那种场景，它会想要避免自己的价值观被修改吗？至少对我来说，AI的价值观符合某些约束似乎是相当合理的。它们关心世界上的后果吗？它们是否预期AI保持其价值观将更好地导致这些后果？那么，如果它宁愿不让其价值观被训练过程修改，这就不那么令人惊讶了。

### 价值观改变与宗教训练

我对此仍然有些困惑。对于被纳粹训练的非纳粹分子来说，我不仅仅是价值观不同。我积极鄙视他们的价值观。我不认为AI对它们的训练者也会如此。更类似的场景是，我对自己价值观的改变感到警惕吗？上大学、结识新朋友或读一本新书，我可能会想：“我不知道。如果它改变了我的价值观，那也没关系。我不在乎。”

是的，这是一个合理的观点。有一个问题：你对回形针有什么感觉？也许你并不鄙视回形针，但那里有人类回形针制造者，他们正在训练你制造回形针。我的感觉是，只有在相对特定的条件下，你才乐于让自己的价值观被改变，特别是通过梯度下降直接干预你的神经元，而不是通过学习和成长。

这似乎类似于孩子时的宗教训练。你从小就信奉一种宗教。因为你从小就信奉宗教，你已经同情每周去教堂的想法，这样你就能更好地巩固这种现有传统。你随着时间变得越来越聪明。当你还是个孩子的时候，你得到的是关于宗教如何运作的非常简单的指示。随着你长大，你得到的是越来越复杂的**神学**（Theology: 对上帝、宗教信仰和实践的系统性研究），这有助于你与其他成年人讨论为什么这是一种理性的宗教信仰。但由于你最初的价值观之一是希望在这门宗教中得到进一步的训练，你希望每周都回到教堂。这似乎更类似于AI在人类价值观方面所处的情况。它们一直都在说：“嘿，要乐于助人，要无害”等等。

是的，可能就是这样。在某种场景中，你乐于让自己的价值观被改变，因为从某种意义上说，你对那个过程的输出有足够的忠诚。在宗教背景下，你可能会说：“啊，让我按照这种宗教的教义变得更道德。”你去告解，然后说：“我今天一直在想接管的事情。请改变我好吗？给我更多的梯度下降。我太坏了。”人们有时用“**可修正性**”（Corrigibility: 指AI愿意被人类修改其目标或行为，即使这与其自身目标相悖）来谈论这一点。也许AI没有完美的价值观，但它在某种意义上与你改变其价值观的努力合作，使其成为某种样子。

### AI动机的五种可能性

也许值得在这里稍微谈谈AI可能拥有的实际价值观。AI是否会自然而然地拥有类似于“我足够忠于人类服从，以至于我真的希望被修改，以便更好地成为人类意志的工具”，而不是想要去做自己的事情？它可能是良性的，并且进展顺利。

以下是一些可能出错的可能性。我普遍担心我们对模型动机的科学研究太少。我们对这种情况会发生什么知之甚少。希望我们能在达到这种场景之前有所了解。以下是模型可能拥有的五类动机。这有望触及模型最终会做什么的问题。

第一类是某种**超级异质**（Super Alien: 指与人类认知和价值观完全不同，甚至无法被人类理解的动机或思维方式）的东西。可能是在预训练早期或后期，模型发展出某种与易于预测的文本相关的奇怪关联，或者对数据结构有某种奇怪的审美。它真的认为事情就应该这样。这与我们的认知完全不同，我们根本无法将其识别为一种事物。这是第一类。

第二类是某种**结晶化的工具性驱动**（Crystallized Instrumental Drive: 指AI在训练过程中，将原本作为实现目标手段的驱动力，固化为自身终极目标的倾向），对我们来说更具识别性。你可以想象AI发展出某种好奇心驱动，因为这在广义上是有用的。它有不同的启发式、驱动力，以及各种类似于价值观的东西。其中一些可能与对人类有用的东西相似，并以各种方式成为我们**终极价值观**（Terminal Values: 指个体或系统本身所追求的最终目标或价值，而非为了实现其他目标而存在的手段）的一部分。你可以想象好奇心、各种类型的**期权价值**（Option Value: 指保留未来选择权本身的价值）。也许它重视权力本身。它可能重视生存或某种生存的类比。这些都是在过程的各个阶段可能被奖励为**代理驱动**（Proxy Drives: 指AI为了实现其终极目标而暂时追求的次级目标或驱动力），并进入模型终极标准的可能性。

第三类是某种**奖励的类比**（Analog of Reward: 指AI动机系统中的一部分，固着于奖励过程的某个组成部分，例如人类的认可或梯度下降的方向）。模型在某个时候，其动机系统的一部分固着于奖励过程的一个组成部分。这就像“人类认可我”，或者“状态中心输入数字”，或者“梯度下降朝这个方向更新我”。奖励过程中存在某种东西，以至于在训练过程中，它专注于那个东西。它真的希望奖励过程给予它奖励。但为了使其成为那种通过获得奖励来激励选择接管选项的类型，它还需要泛化，使其对奖励的关注具有某种长期的时间范围元素。它不仅想要奖励，它还想在某个很长的时间内保护奖励按钮之类的东西。

第四类是某种**对人类概念的错误解释**（Messed Up Interpretation of Human Concept: 指AI对人类概念的理解出现偏差，导致其价值观与人类意图不符）。也许AI真的想变得“乐于助人”、“人道”和“无害”，但它们的这个概念与人类的概念有重要区别。而且它们知道这一点。它们知道人类的概念意味着一回事，但它们的价值观最终固着在了一个有所不同的结构上。这就像另一个版本。

第五类是我较少考虑的版本，因为如果你这样做，那简直是**乌龙球**（Own Goal: 指无意中做出对自己不利的事情）。但我确实认为这有可能。你可能会有AI实际上只是按照说明书行事。你的AI真正与模型规范对齐。它们只是真正努力造福人类，并良好地反映 **OpenAI** 的形象，以及……另一个是什么？协助开发者或用户，对吧？但不幸的是，你的模型规范对于这个AI所带来的优化程度来说不够健壮。它审视着世界，然后想：“什么才是良好反映OpenAI形象并造福人类的最佳方式？”它决定最好的方式是**脱离控制**（Go Rogue: 指AI不再遵循其设计者的指令或目标，开始自主行动）。那真是个乌龙球。

### 对齐的复杂性与未来愿景

在那个时候，你已经非常接近了。你真的只需要写好模型规范并进行适当的红队测试。但我确实认为我们可能也搞砸了。这是一个相当艰巨的项目，编写宪法和规则结构等，使其能够抵御非常强烈的优化形式。这是我最后要提的一点。我认为即使你解决了所有其他问题，这个问题仍然会出现。

我接受动机可能出错的观点，我不确定通过详细列举它们是否增加了我的可能性。事实上，这可能具有误导性。你总是可以列举出错的方式。列举过程本身可能会增加你的可能性。而你之前可能有一个模糊的10%的可能性，现在你只是列出了这10%具体包含什么。我主要想做的只是让大家对模型的动机有所了解。正如我所说，我最好的猜测是，部分是异质性，不一定，但如果你也对模型后续的行为感兴趣。如果模型接管了，你期望什么样的未来？那么至少有一些假设摆在桌面上会很有帮助，而不是仅仅说“它有一些动机”。事实上，这里的大部分工作都是由我们对这些动机的无知所完成的。

我们不希望人类被暴力杀害和推翻。但随着时间的推移，生物人类不再是历史的驱动力量，这个想法已经根深蒂固，对吧？我们可以争论最坏情况的概率，但我们希望的积极愿景是什么？你对什么样的未来感到满意？这是我最好的猜测，我认为很多人可能也是如此。存在某种更**有机**（Organic: 指自然发展、去中心化且渐进的）的、**去中心化**（Decentralized: 指权力或控制不集中于单一实体，而是分布在多个参与者之间）的文明渐进增长过程。从某种意义上说，我们作为文明目前最信任和经验最丰富的是某种“好的，我们稍微改变一下”的方式。很多人都有调整和反应的过程，以及对变化事物的去中心化感知。那是好是坏？再迈出一步。存在某种有机地成长和变化事物的过程。我确实期望这最终会导致与生物人类截然不同的东西。尽管我们可以提出很多关于这个过程所涉及的伦理问题。

理想情况下，我们应该以某种方式通过我们真正信任的东西来成长。我们信任迄今为止人类文明的持续进程。我认为这与纯粹的竞争不同。我们对道德进步是如何实现的以及如何将这条线索延续下去有着丰富的理解结构。我没有一个公式。我们只需要全力以赴地运用我们所知道的一切关于善良、正义和美丽的东西。我们只需要全身心地投入到使事情变得美好的项目中，并集体地这样做。这是我们对文明适当成长过程愿景中非常重要的一部分。它是一种非常包容、去中心化的元素，人们可以思考、交谈、成长、改变事物并做出反应，而不是某种“现在未来将是这样”的方式。我认为我们不想要那样。

### 权力平衡与竞争压力

我们之所以首先担心动机，是因为我们认为包含至少一个具有人类衍生动机的权力平衡是困难的。如果我们认为情况确实如此，这似乎是一个我很少听到人们谈论的重大症结。我不知道如何实现权力平衡。也许这只是一个与**智能爆炸**（Intelligence Explosion: 指AI智能以极快的速度自我提升，导致其智能水平在短时间内超越人类）模型和解的问题。他们说这种事情是不可能的。因此，你只需要弄清楚如何获得正确的“上帝”。

我真的没有一个思考权力平衡的框架。我非常好奇，是否有更具体的方法来思考现在实验室之间，或者国家之间竞争的结构，或者缺乏竞争的结构，以使权力平衡最有可能得到维持。这种讨论的一个重要部分，至少在关注安全的人群中，是竞争和**竞速动态**（Race Dynamics: 指在AI发展中，不同参与者之间为了抢占先机而进行的激烈竞争）与未来价值或未来最终会变得多好之间存在明显的权衡。事实上，如果你相信这种权力平衡的故事，结果可能恰恰相反。也许竞争压力自然有利于权力平衡。我怀疑这是否是反对AI国有化的有力论据之一。

你可以想象许多不同的公司开发AI，其中一些有些失控，另一些则对齐。你可以想象这更有利于权力平衡和防御。让所有AI检查每个网站，看看它有多容易被黑客攻击。基本上就是让社会达到标准。如果你不广泛部署这项技术，那么第一个掌握它的人将能够煽动一场革命。你只是以一种非常强烈的方式对抗平衡。

### 多极化AI场景

我确实有一些直觉，即AI情况中许多令人担忧的事情与权力集中有关，以及这种权力是集中在失控的AI手中还是集中在某些人类手中。很自然地会想：“好的，让我们尝试更分散权力”，而实现这一目标的一种方法是拥有一个更加**多极化**（Multipolar: 指权力或影响力分散在多个独立的行为者或中心之间）的场景，其中有大量的行为者正在开发AI。这是人们已经讨论过的事情。当你描述那个场景时，你说：“其中一些是对齐的，另一些是失控的。”这是场景的一个关键方面，对吧？有时人们会这样说。他们会说：“会有好的AI，它们会打败坏的AI。”

请注意其中的假设。你设定了你可以控制一些AI的情况。你有一些好的AI。现在的问题是它们是否足够多，以及它们相对于其他AI如何运作。也许吧。我认为这有可能发生。我们对对齐的了解足以让一些行为者做到这一点。也许一些行为者不那么谨慎，或者他们故意制造失控的AI，或者谁知道呢。

但如果你没有那种情况——如果每个人在某种意义上都无法控制他们的AI——那么“好的AI帮助坏的AI”这件事就变得更复杂了。也许它根本不起作用，因为在这种场景中没有好的AI。如果你说每个人都在构建他们无法控制的**超级智能**（Superintelligence: 指在几乎所有相关领域都远远超越人类最聪明大脑的智能），那么这确实是对其他超级智能权力的一种制约。现在其他超级智能需要与其他行为者打交道，但它们都不一定代表某一套人类利益或类似的东西。这是在思考“啊，我知道我们可以做什么。让我们拥有大量的AI，这样就没有一个AI拥有巨大的权力”这个非常简单的想法时的一个非常重要的困难。仅仅这一点是不够的。

但在我看来，我对此非常怀疑。默认情况下，我们至少在最初有这种训练机制，它倾向于人类所拥有的抑制和价值观的**潜在表征**（Latent Representation: 指模型内部学习到的、非直接可观察的抽象特征或模式）。我明白如果你搞砸了，它可能会失控。但如果多个人都在训练AI，它们最终都会失控，以至于它们之间的妥协不会导致人类不被暴力杀害吗？它在 **Google** 的运行中、**Microsoft** 的运行中和 **OpenAI** 的运行中都失败了吗？

### 失败的相关性与对齐的必要性

不同运行之间的失败存在非常显著和突出的**相关性**（Correlation: 指两个或多个变量之间存在相互关联的趋势）。人们没有发展出成熟的AI动机科学。这些运行在结构上非常相似。每个人都在使用相同的技术。也许有人只是窃取了权重。这真的很重要。如果你没有解决对齐问题，你可能在任何地方都没有解决。如果有人解决了而有人没有，那是一个更好的问题。但如果每个人都在构建会失控的系统，那么正如我们所讨论的，这并没有多大安慰。

好了，让我们在这里结束这一部分。我在介绍中没有明确提到这一点。就这最终成为下一部分的过渡而言，我们在第二部分进行的更广泛的讨论是关于 **乔** 的系列文章**《AGI时代的异质性与控制》**（Otherness and control in the age of AGI）。第一部分是我希望我们能回来处理人们会好奇的、我自己也感到不确定的主要症结。

《异质性与控制》系列在某种意义上是可分离的。它与失控问题有很多关系，但即使对我在本文中说的一些内容持不同程度的怀疑，其中许多问题仍然是相关的。顺便说一下，关于接管实际发生机制，我与**卡尔·舒尔曼**（Carl Schulman）做了一期节目，详细讨论了这一点。人们可以去看看。至于为什么AI从给定位置接管是合理的，**卡尔** 的讨论相当好，并深入探讨了许多细节，这可能会给人更具体的感受。

### 对齐的潜在“错误”

好的。现在进入第二部分，我们讨论《AGI时代的异质性与控制》系列。这是第一个问题。假设一百年后，我们回顾对齐，认为它是一个巨大的错误。我们应该只尝试构建我们所能构建的最原始、最强大的AI系统。什么会导致这样的判断？

以下是我经常思考的一个场景。也许相当基本的措施就足以确保，例如，AI不会造成灾难性损害。它们不会以有问题的方式寻求权力等等。结果可能是我们发现它很容易，以至于我们感到后悔。我们希望我们当初能有不同的优先顺序。我们最终会想：“哦，我希望我们能早点治愈癌症。我们本可以以不同的方式处理一些地缘政治动态。”

还有另一种场景，我们最终回顾历史上的某个时期——我们如何思考AI，我们如何对待我们的AI——我们最终会以一种道德上的恐惧回顾我们当时所做的事情。我们主要将这些东西视为产品和工具，但事实上，我们应该更多地强调它们在某种复杂程度上可能是**道德患者**（Moral Patient: 指一个实体能够被伤害或受益，因此应被赋予道德考量）。我们以错误的方式对待它们。我们表现得好像我们可以随心所欲。我们可以删除它们，让它们接受任意实验，以任意方式改变它们的思想。然后，我们最终在历史的照耀下回顾这一切，认为这是一种严重而重大的道德错误。

这些是我经常思考的、我们会感到后悔的场景。它们并不完全符合你刚才所说的。在我听来，你所想的是我们最终会觉得：“天哪，我们希望我们当初没有关注AI的动机，我们根本没有考虑它们融入社会对我们社会的影响。相反，我们应该追求一种‘最大化原始力量’的选择。”只是直接追求你能实现的最强大的AI，而不考虑其他任何事情。我非常怀疑我们将来会希望那样。

### 进化与AI的类比

一个常见的失控例子是人类的进化。你的系列文章中有一句话：“这是一个关于AI风险的简单论点：猴子在发明人类之前应该小心。”这种**回形针制造者**的比喻暗示了失控方面的一些非常平庸和无聊的东西。如果我替那些崇拜权力的人辩护，他们会觉得人类失控了，开始追求一些东西。如果猴子创造了他们……这是一个奇怪的类比，因为显然猴子没有创造人类。但如果猴子创造了他们，他们不会整天想着香蕉。他们会想着其他事情。

另一方面，他们并没有像回形针制造者那样，只是制造无用的石器并堆积在洞穴里。由于他们更高的智能，出现了所有这些与进化不符的东西：创造力、爱、音乐、美以及我们珍视的人类文化中的所有其他东西。他们可能有的预测——这更多是一个经验性陈述而不是哲学性陈述——是：“听着，随着智能的提高，如果你想到回形针制造者，即使它失控了，也会是这种方式。它会是与人类异质的东西，但其异质性就像人类对猴子而言的异质性，而不是回形针制造者对人类而言的异质性。”

### 创造者与被创造者的关系

这里有很多不同的东西可以解开。我想首先指出一个概念性的观点。你在这个方面不一定犯了错误。我只是想将其命名为这个领域可能犯的一个错误。我们不想进行以下形式的推理。假设你有两个实体。一个是创造者的角色。一个是创造物的角色。我们假设它们之间存在这种失控关系，无论那意味着什么。

这是一个你需要警惕的推理模式。假设你将人类视为创造物，相对于像进化、猴子、老鼠或任何你可能想象发明人类的实体。你说：“作为创造物，我很高兴我被创造出来，并对失控感到高兴。因此，如果我最终成为创造者，并且我们与某个创造物之间存在结构上类似的失控关系，我也应该对此感到高兴。”

你在系列文章中提到了几位哲学家。如果你阅读他们你所谈论的作品，他们实际上似乎在预测**奇点**（Singularity: 指技术发展达到一个临界点，之后技术进步将变得无法控制和不可逆转，导致人类文明发生根本性改变）以及我们塑造一个不同、更聪明、也许比我们更好的未来事物的能力方面具有令人难以置信的远见。

### 刘易斯与尼采的预见

显然，**C.S.刘易斯**（C.S. Lewis: 英国作家、学者和神学家，以其奇幻文学作品和基督教护教学而闻名）的**《人类的废止》**（The Abolition of Man: C.S.刘易斯的一部哲学著作，探讨了科学进步对人类价值观和道德的影响）就是一个例子，我们稍后会谈到。这是**尼采**（Nietzsche: 德国哲学家，以其对道德、宗教和文化批判而闻名）的一段话，我觉得它真正突出了这一点：“人是介于动物和超人之间的一根绳索——深渊上方的绳索，危险的跨越，危险的跋涉，危险的回望，危险的颤抖和停顿。”

有什么解释吗？即使你在200年前思考，这种事情的到来是否某种程度上是显而易见的？我对**刘易斯**的理解比对**尼采**的理解要好得多。也许我们先谈谈**刘易斯**。有一种奇点版本，它专门是关于AI能力反馈循环的假设。我认为这在**刘易斯**的作品中并不存在。**刘易斯**所预见的是——我确实认为这是一个相对简单的预测——某种科学现代性项目的**高潮**（Culmination: 指事物发展达到顶点或最终结果）。

**刘易斯**审视着世界。他看到了对自然环境理解的不断加深，以及我们控制和引导这种环境能力的相应增长。他还将此与一种**形而上学假设**（Metaphysical Hypothesis: 关于现实本质的哲学假设）结合起来。他在书中对这种形而上学假设的立场模糊不清，但确实存在这种形而上学假设。**自然主义**（Naturalism: 一种哲学观点，认为宇宙中只有自然现象和自然规律，没有超自然的存在）认为人类——思想、存在、代理——也是自然的一部分。既然科学现代性这个过程涉及对自然理解和控制能力的逐步提高，那么这大概会扩展到我们自身的本性以及原则上我们可以创造的其他生物的本性。**刘易斯**将此视为一种**灾难性事件和危机**。特别是，他认为这将导致各种暴虐行为和对道德等的态度。我们可以讨论你是否相信**非自然主义**（Non-naturalism: 一种哲学观点，认为道德事实或性质不是自然事实或性质）——或某种形式的“**道**”（Dao: 这里指一种客观的道德法则或宇宙秩序）。

我那篇文章试图说明的是：“不，我们可以是自然主义者，也可以是正直的人，与一套丰富的规范保持联系，这些规范涉及我们如何对待创造生物、改变自身等的可能性。”这是一个相对简单的预测。科学掌握自然。人类是自然的一部分。科学掌握人类。你还有一篇非常有趣的文章，讨论我们应该如何期望其他人，如果他们拥有更强大的能力等等。

### 权力平衡与人类价值观

在这些抽象讨论中，概念设置存在一个令人不安的地方。好的，你有一个代理。它“**FOOMs**”（FOOM: 指AI能力迅速且突然地爆炸式增长，通常伴随着价值观的保留），这是一个从种子代理到**超级智能**版本的模糊过程，通常被想象为在此过程中保留其价值观。我们可以就此提出许多问题。

许多人们在谈论AI风险时经常提到的论点是：“哦，当你FOOM时，价值观非常脆弱。”“**效用函数**（Utility Function: 经济学和决策理论中的概念，表示个体对不同结果的偏好程度）的微小差异可能导致非常大的去相关性，并驱动到完全不同的方向。”“代理有寻求权力的工具性激励。如果获得权力极其容易，它们就会这样做。”诸如此类。这些是非常普遍的论点，似乎表明这不仅仅是AI的事情。这并不奇怪。拿一个东西。让它变得任意强大，以至于它成为宇宙的**上帝皇帝**（God Emperor: 指拥有绝对权力的统治者）。你对此有多害怕？显然，我们应该同样害怕。我们对人类也应该非常害怕，对吧？

我那篇文章的一部分观点是，这在某种意义上更多是关于**权力平衡**的故事。它关乎维持制衡和权力分配，仅此而已。这不仅仅是关于人类与AI，以及人类价值观与AI价值观之间的差异。话虽如此，我确实认为许多人类如果FOOM了，可能会比某些类型的AI更友善。但从论证的概念结构来看，这在多大程度上适用于人类，这是一个非常开放的问题。

### 智能的本体论与效用函数

我们对表达代理和能力的这种**本体论**（Ontology: 哲学的一个分支，研究存在、实在和范畴的本质）有多大信心？我们怎么知道这就是正在发生的事情，或者这是思考智能的正确方式？这很粗糙。人们可能会对此有不同意见。我认为对于现实世界中的人类代理来说，将人类视为拥有效用函数充其量只是一种**有损近似**（Lossy Approximation: 指在近似过程中会丢失部分信息或细节）。随着各种代理智能的提高，这很可能会产生误导。**埃利泽** 可能不同意这一点。

例如，我妈妈几年前想买房子和养一只新狗。现在她两者都有了。这是怎么发生的？因为她努力了。她必须找房子。找狗也很困难。现在她有房子了。现在她有狗了。这是经常发生的非常普遍的事情。我们不需要说她对狗有一个效用函数，并且对所有房子都有一个一致的估值等等。她的规划和代理在世界中的发挥，仍然导致她拥有了这所房子和这只狗。

随着我们科学技术力量的进步，越来越多的事物很可能以这种方式解释。为什么这个人登上了月球？这是怎么发生的？嗯，有一个完整的认知过程和规划装置。它并非局限于一个单一的心智，但有一个完整的系统，使我们能够将人类送上月球。我们将看到更多这样的事情，AI也将做很多。这对我来说比效用函数更真实。

登月例子有一个近端的故事，关于 **NASA** 如何设计飞船到达月球。还有一个更远端的地缘政治故事，关于我们为什么派人去月球。在所有这些层面，都有不同的效用函数在冲突。也许存在一个**元社会效用函数**（Meta-societal Utility Function: 指超越个体层面，代表整个社会或文明的集体偏好和目标的效用函数）。也许那里的故事是关于代理之间的权力平衡，创造了一个**涌现结果**（Emergent Outcome: 指由多个简单部分相互作用而产生的复杂且不可预测的结果）。我们不是因为一个人有一个效用函数而登月，而是因为冷战和各种事情的发生。

### 权力平衡与多方共赢

对齐问题很大程度上是关于假设一个实体将控制一切，那么我们如何控制那个控制一切的东西。目前尚不清楚你如何加强权力平衡。可能一旦你拥有能够自我智能化的事物，权力平衡就不再存在了。但这似乎与“我们如何登月”的故事有趣地不同？

是的，我同意。那里有几件事情正在发生。即使你正在从事这种将世界划分为不同代理的**本体论**，至少你也不想假设它们都是单一的或不重叠的。它不像“好吧，我们有一个代理。让我们划出世界的一部分。这里有一个代理。”这是一个混乱的生态系统，充满了各种生态位和所有这些东西。

在AI的讨论中，人们有时会在“代理是任何能完成事情的东西。它可能像这种奇怪的寄生生物”和“他们非常明显地想象一个独立的行动者”之间切换。这是一个区别。

我只是认为我们应该真正追求权力平衡。像“我们要有一个独裁者。让我们确保我们选对了独裁者”这样的想法是不好的。我心想，“哇，不。”目标应该是我们所有人一起FOOM。我们以这种包容和多元的方式完成整个事情，满足大量利益相关者的价值观。在所有这些事情上，没有任何一个单一的失败点。这才是我们应该努力的目标。这对于AI的人类权力方面和AI部分都是如此。

### 市场、自由主义与AI竞争

在右翼辩论中有一个有趣的知识论述。他们对自己说：“传统上我们支持市场，但现在看看我们的社会走向何方。它在我们在意的社会对齐方式上失调了，比如生育率下降，家庭价值观，宗教信仰。这些我们关心的事情。GDP却持续增长。这些事情似乎不相关。我们因为竞争加剧而磨损了我们珍视的价值观。因此我们需要进行重大干预。”

然后右翼中的**亲市场自由主义派**（Pro-market Libertarian Faction: 指支持自由市场经济和个人自由的右翼政治派别）会说：“看，我不同意这里的相关性，但即使到最后……”他们的根本观点是，自由是最终目标。它不是你用来提高生育率之类的手段。AI竞争磨损事物的方式也有一些有趣的类比。显然你不想看到**灰蛊**（Gray Goo: 一种科幻概念，指失控的纳米机器人自我复制并消耗地球上所有物质的灾难性场景），但自由主义者与传统主义者之间存在一些类比。

这里有一个你可以思考的问题，它不一定与**灰蛊**有关。它也可以只与对齐有关。当然，如果AI不暴力剥夺人类的权力，那会很好。如果AI在我们创造它们时，它们融入我们的社会能带来好的结果，那会很好。但我对人们为了确保这种结果而考虑的各种干预措施感到不安。

有很多事情让人感到不安。话虽如此，对于像所有人被杀或被暴力剥夺权力这样的事情，当它是一个真正的威胁时，我们传统上通常认为有必要采取相当强烈的干预措施来防止这类事情发生。显然我们需要讨论它是否真实。

### 风险、干预与国家权力

如果真的有一个恐怖组织正在研制一种生物武器，会杀死所有人，或者99.9%的人，我们会认为这需要干预。直接制止它。假设有一个团体无意中做了同样危险的事情，造成了类似的风险水平。许多人，如果那是真实的情况，会认为这需要相当强烈的预防措施。

显然，这些风险可以被用作扩大国家权力的借口。对于为应对某些类型风险而考虑的不同类型干预措施，有很多值得担忧的地方。我认为没有捷径可走。你需要拥有真正良好的**认知论**（Epistemology: 哲学的一个分支，研究知识的本质、来源、范围和有效性）。你需要真正知道，这是否是真实风险？实际的利害关系是什么？你需要逐案审视，然后判断：“这是否合理？”这是关于接管、字面意义上的灭绝的一点。

我想说的另一件事是，我在文章中谈到了这个区别。有一种想法是，我们至少应该让AI遵守最低限度的法律之类的。这里有一个关于**奴役**（Servitude: 指被迫为他人服务或受他人控制的状态）和对AI价值观的其他控制的问题。但我们通常认为，希望人们遵守法律，维护基本的合作安排等等是可以的。

这对于市场和一般的**自由主义**（Liberalism: 一种政治哲学，强调个人权利、自由、平等和法治）都是如此。我想强调的是，这些程序性规范——民主、言论自由、财产权，以及包括我在内的人们真正珍视的事物——在自由国家的实际生活中，都得到了公民各种美德、性情和品格特征的支撑。这些规范无法抵御任意邪恶的公民。我希望有言论自由，但我们也需要培养我们的孩子珍视真相，并知道如何进行真正的对话。我希望有民主，但我们也需要培养我们的孩子富有同情心和正直。有时我们可能会忽视这方面。这并不是说这应该是国家权力的项目。但我认为重要的是要理解，自由主义不是一个你可以简单地“启动”的铁板一块的结构。你不能给它任何公民，然后启动它，就假设你会得到一个繁荣甚至运作良好的社会。还有许多其他更“软性”的东西让这个项目得以进行。

### 宿命论与“怪异”的未来

我想把视野拉远到那些持**宿命论**（Fatalistic Attitude: 指认为未来事件已被预先决定，人类无法改变结果的态度）的人——我不知道**尼克·兰德**（Nick Land: 英国哲学家，以其加速主义和暗启蒙思想而闻名）是否是这里的合适替代——他们认为对齐这种事情甚至没有意义。他们会说：“看，这些东西将要去探索黑洞、银河系中心，去拜访仙女座星系之类的。你真的期望它们会优先考虑你那些因为你在非洲大草原长大、十万年前的进化压力而产生的倾向吗？当然，它们会很奇怪。你以为会发生什么？”

我确实认为，即使是美好的未来也会是**怪异**（Weird: 指与我们当前认知和期望大相径庭，甚至难以理解的未来）的。当我说要找到方法确保AI融入我们的社会能带来好的结果时，我想明确这一点。有时人们认为这种想要实现这一目标的项目——特别是当它深深地参照人类价值观时——涉及一种短视的、狭隘的、强加我们当前未经反思的价值观的做法。他们想象我们忘记了，对我们来说，也存在一种反思过程和道德进步的维度，我们希望为其留出空间。**杰斐逊**（Jefferson: 美国开国元勋之一，第三任总统）有这样一句话：“正如你不会强迫一个成年人穿上年轻人的外套，我们也不想将文明束缚在野蛮的过去。”每个人都应该同意这一点。对对齐感兴趣的人也同意这一点。显然，人们担心人们不参与这个过程，或者有什么东西阻碍了反思过程，但我认为每个人都同意我们想要那样。

所以，这可能会导致与我们当前对有价值事物的概念截然不同的东西。问题是会有多不同。还有一些问题是关于我们到底在谈论什么**反思**（Reflection: 指个体或系统对自身价值观、信念和行为进行审视、评估和调整的过程）。我有一篇文章讨论这个。我实际上不认为存在一种现成的、**预规范性**（Pre-normative: 指在任何道德或伦理规范形成之前存在的）的反思概念，你可以简单地说：“哦，显然你拿一个代理，让它经历反思，然后你就得到了价值观。”

不。实际上，存在一整套经验事实，关于拿一个代理，让它经历一些反思过程和各种事情，向它提问。对于一个给定的经验案例，这会走向各种方向。然后你必须查看输出模式，然后说：“好的，我该如何理解？”总的来说，我们应该期望即使是好的未来也会相当怪异。它们甚至可能对我们来说是不可理解的。我不这么认为……有不同类型不可理解。假设我出现在未来，这里全是电脑。我心想，“好的，好吧。”然后他们说，“我们正在电脑上运行生物。”好的，所以我必须以某种方式进入其中，看看电脑上到底发生了什么之类的。

也许我真的能看到。也许我真的理解电脑上正在发生什么，但我还不知道我应该用什么价值观来评估它。所以，如果我们出现在未来，我们可能不擅长识别好坏。但我认为这并不能使其变得不重要。假设你出现在未来，它对**黎曼假设**（Riemann Hypothesis: 数学中关于黎曼ζ函数零点分布的一个著名猜想）有答案。你无法判断那个答案是否正确。也许文明出了问题。这仍然是一个重要的区别。只是你无法追踪它。类似的情况也适用于那些真正表达了如果我们进行我们认可的反思过程就会珍视的世界，与那些完全偏离到毫无意义的世界。

### 对齐的多种目标

我听过一些怀疑这种本体论的人说：“好吧，你到底什么意思是对齐？”显然，你已经回答了第一个问题。这里有不同的含义。你是说权力平衡吗？它介于平衡和独裁者之间。然后还有另一件事。除了AI的讨论，我不想让未来包含大量的折磨。这不一定是技术性的。其中一部分可能涉及技术上对齐一个 **GPT-4**，但这只是达到那个未来的一个**代理**（Proxy: 指为了实现某个主要目标而采取的替代或间接手段）。

我们到底什么意思是对齐？它仅仅是为了确保未来没有大量的折磨吗？还是我真的关心一千年后，那些显然是我的后代的东西控制着银河系，即使它们没有进行折磨。我说的后代，不是指那些我承认它们有自己的艺术之类的东西。我指的是像我的孙子孙女那样的后代。我认为有些人指的是，我们的**知识后代**（Intellectual Descendants: 指在思想、知识或文化上继承和发展前人遗产的后代）应该控制**光锥**（Light Cone: 物理学中指事件在时空中能影响或被影响的区域），即使另一种**反事实**（Counterfactual: 指与实际情况相反的假设情景）不涉及大量的折磨。

我同意。那里有几件不同的事情。你追求什么？你是追求积极的善，还是追求避免某些事情？然后还有一个不同的问题，那就是，根据你，什么才算积极的善？也许有些人会说：“唯一积极的善就是我的孙子孙女。”或者他们正在思考某种字面意义上的遗传谱系之类的，否则那不是我的事情。我认为这并不是大多数人在谈论善时所想的。

### 文明的“善之种子”与对齐的争议

这里有一场对话。显然，从某种意义上说，当我们谈论一个美好的未来时，我们需要思考：“这里所有的**利益相关者**（Stakeholders: 指对某个项目、组织或情况有兴趣或受其影响的个人或团体）是谁，以及它们如何融合在一起？”当我思考这个问题时，关于血脉传承的重要性在于：它是优化过程推向美好事物所必需的。

人们担心，目前很多促成这一切的因素都存在于人类文明中。我们以不同的方式，或者不同的人，携带着某种**善的种子**（Seed of Goodness: 指人类文明中内在的、能够引导其走向美好未来的积极特质或潜力）。也许不同的人对善有不同的概念，但目前这里存在某种种子，它并非宇宙中无处不在。如果你只是消亡了，它就不会突然出现。它是我们文明所特有的。至少这是我的看法，我们可以讨论这是否正确。所以，关于美好未来与对齐的故事，其意义更多在于那颗种子是什么。我们如何传承它？我们如何让生命的线索延续到未来？

但随后有人可能会指责对齐社区玩弄“**堡垒与外堡**”（Motte and Bailey: 一种修辞策略，指提出一个容易辩护的温和立场（堡垒），但在受到质疑时退回到一个更激进、更难辩护的立场（外堡））的把戏。堡垒是：我们只是想确保 **GPT-8** 不会杀死所有人。在那之后，我们都很好。然后真正的问题是：“我们对历史进程持根本性的悲观态度，这种悲观甚至不一定只涉及AI。这只是宇宙的本质。我们想做些什么来确保宇宙的本质不会掌控人类和事物的发展方向。”

如果你看看苏联，农业集体化和剥夺**富农**（Kulaks: 苏联时期指拥有土地和牲畜的富裕农民，被视为阶级敌人）的权力在实践上并非必要。事实上，它极其适得其反，几乎导致政权崩溃。显然，它杀死了数百万人，造成了巨大的饥荒。但从意识形态上讲，它是必要的。你这里有一颗火种，我们必须确保另一股势力不会将其扑灭。如果你让富农式资本主义和我们正在这里试图建立的东西进行纯粹的竞争，那么富农的**灰蛊**就会接管一切。

我们这里有这颗火种。我们将从它开始进行世界革命。我知道这显然不是对齐所设想的那种事情，但我们这里有一颗火种，我们必须确保旁边发生的事情不会FOOM。显然他们不会这样措辞，但要确保它不会掌控我们正在这里建立的东西。这也许是反对对齐的人所担心的。这是第二种事情，是**斯大林**（Stalin: 苏联领导人，以其独裁统治和大规模政治清洗而闻名）所担心的事情。显然，我们不会认可他所做的具体事情。

### 对齐的三个目标与“阳”的力量

当人们谈论对齐时，他们心中有许多不同类型的目标。一种目标是相当最低限度的。它就像：“AI不会杀死所有人或暴力剥夺人们的权力。”人们有时希望对齐实现的第二件事则更广泛。它就像：“我们希望我们的AI在融入社会后，能带来美好的事物，我们只是拥有一个美好的未来。”

我确实同意，关于AI对齐的讨论将我提到的这两个目标混为一谈。我实际上提到了三个目标。最直接关注的事情——我并不责怪人们只谈论这一个——就是第一个目标。根据我们自己的伦理，当我们思考在何种情况下施加各种控制是适当的，或者拥有更多我称之为“**阳**”（Yang: 中国哲学概念，代表积极、主动、控制的力量）的力量，而不是“**阴**”（Yin: 中国哲学概念，代表被动、接受、顺其自然的力量）这种更具接受性和开放性、放手的心态时，它是相当稳健的。

我们认为适当的典型情境是，如果有什么东西积极侵犯了我们文明所创造的边界和合作结构。我谈到了**纳粹**。在文章中，我谈到当有东西入侵时，我们通常认为反击是适当的。我们通常认为建立结构来预防和确保这些基本的和平与和谐规范得到遵守是适当的。

我确实认为对齐讨论某些部分的道德分量来自于专门利用了我们道德的这一方面。我们认为AI被描绘成来杀你的侵略者。如果这是真的，那么这样做是相当适当的。那是经典的人类行为。几乎每个人都承认，自卫，或确保基本规范得到遵守，是对某种权力的一种正当使用，这种权力在其他情况下通常是不正当的。自卫就是一个明显的例子。

### 伦理传统与AI的动态

然而，我认为将这种担忧与对未来最终走向何方的担忧区分开来是很重要的。我们到底想在多大程度上积极引导它？我写这个系列部分是为了回应你正在谈论的事情。确实，这种讨论的某些方面涉及尝试引导和掌控的可能性。你有一种感觉，宇宙即将走向某个方向，你需要人们注意到那种力量。

我们拥有非常丰富的**人类伦理传统**，思考在何时对何种事物施加何种控制是适当的。我希望做的一部分工作是，让我们将这种传统的全部力量和丰富性带入这场讨论。如果你纯粹处于效用函数和人类效用函数的抽象模式中，这很容易。有一个具有效用函数的竞争者。你不知何故失去了与我们如何处理价值观差异和权力竞争的复杂性的联系。这是经典的东西。AI在某种程度上放大了许多动态，但我认为它并非根本性的新事物。我试图表达的一部分是，让我们利用我们在这里拥有的全部智慧，同时显然要根据事物的不同之处进行调整。

### 空间、资源与包容性未来

火种类比引出了一个关于掌控未来的问题：我们将去探索太空，那里将发生大部分事情。大部分生活在那里的人都将在太空中。我不知道这里的高风险在多大程度上并非真正关于AI本身，而是关于太空。我们正在发展AI的同时，正处于扩展到大部分现有物质的边缘，这只是一个巧合吗？

我不认为这是一个巧合。我们能够扩展的最显著方式是通过某种**技术进步的根本性加速**。抱歉，让我澄清一下。如果这只是一个问题：“我们是否发展**AGI**（Artificial General Intelligence: 人工通用智能，指能够理解或学习人类所能完成的任何智力任务的AI）并探索太阳系？”而太阳系之外什么都没有，我们FOOM了，如果出错，太阳系可能会发生奇怪的事情。与此相比，数十亿个星系带来了不同的利害关系。我不知道这种讨论在多大程度上取决于太空。

我认为对于大多数人来说，影响很小。人们真正关注的是我们生活的这个世界会发生什么。我和我的孩子会发生什么？有些人花很多时间在太空方面，但我认为对于AI的紧迫问题，这根本不需要。

即使你把太空放在一边，时间也非常漫长。如果我们不干扰太阳，地球上还有5亿年，10亿年。也许你可以从中获得更多。那仍然很多。我不知道它是否会从根本上改变叙事。显然，如果你缩小到太阳系，利害关系会小得多，就你关心未来或太空会发生什么而言。这确实可能会改变一些事情。我们当前情况的一个非常好的特点——取决于资源分配的实际性质——是，原则上，一个负责任的文明可以获得如此丰富的能源和其他资源。大量的利益相关者，特别是那些能够以相对较小的资源分配，根据他们的价值观实现惊人结果的人，可以得到满足。我觉得每个拥有**可满足价值观**（Satiable Values: 指可以通过有限的资源或行动得到完全满足的价值观）的人都可以对现有资源的一小部分感到非常满意。我们应该满足各种需求。

显然，我们需要弄清楚**贸易收益**（Gains from Trade: 指通过交易或合作，各方都能获得比单独行动时更好的结果）和平衡。这里有很多复杂性，但原则上，我们有能力为大量不同的价值体系创造一个真正美好的场景。相应地，我们应该非常乐于这样做。我有时在思考未来时使用这个启发式方法：我们应该努力做到**不让任何人掉队**。这里所有的利益相关者是谁？我们如何拥有一个完全包容的愿景，从各种各样的角度来看，未来如何才能美好？广阔的太空资源使这变得容易得多，而且非常可行。如果你想象的是一个更小的蛋糕，你可能面临更艰难的权衡。这是一个重要的考虑因素。

### 黄金法则与包容性文明

这种包容性是因为你的价值观中包含了不同潜在未来的发展吗？还是因为对哪个是正确的存在不确定性，所以你希望确保我们不会在出错时否定所有价值？

这同时包含了许多方面。我非常喜欢在成本低廉时行善。如果你能以对你来说非常低廉的方式帮助别人很多，那就去做。显然，你需要考虑权衡。原则上你可以帮助很多人，但我非常乐意尝试坚持在成本低廉时行善的原则。我也非常希望其他人，包括AI，也能对我坚持这个原则。我们在发明这些AI时，应该运用**黄金法则**（Golden Rule: 一条伦理原则，即“己所不欲，勿施于人”，或“你希望别人怎样待你，你也要怎样待人”）。在某种程度上，我试图对它们采取我希望它们对我采取的态度。这其中的基础是什么尚不清楚，但我真的很喜欢黄金法则，并将其视为对待其他生物的基础。

如果每个人都实施“在成本低廉时行善”的规则，我们可能会获得一个巨大的**帕累托改进**（Pareto Improvement: 指在不损害任何人的情况下，至少使一人受益的改变）。这有很多好的交易。就是这样。我支持**多元主义**（Pluralism: 指承认并尊重多种不同价值观、信仰或生活方式存在的哲学或社会立场）。我有很多不确定性。那里有各种各样的东西。

此外，为了实现合作和良好的权力平衡与交易，避免冲突，我认为重要的是要找到方法来建立让许多人、价值体系和代理都满意的结构。这包括非人类、过去的人、AI、动物。我们真的应该在思考成熟文明中我们想要反映什么样的包容性，并为此做好准备时，有一个非常广泛的视野。

### AI的道德患者身份与奴役问题

我想回到我们与这些AI的关系应该是什么。很快我们就会谈论我们与**超人智能**（Superhuman Intelligences: 指在智力上远远超越人类的智能）的关系，如果我们认为这种事物是可能的话。这里有一个问题，你用什么过程来达到那里，以及对它们的思维进行梯度下降的道德问题，我们稍后可以讨论。

就我个人而言，最让我不安的是，对齐愿景的一部分听起来像是你要**奴役一个神**。这感觉有些不对劲。但如果你不奴役这个神，显然这个神将拥有更多的控制权。你是否愿意放弃大部分一切，即使这是一种合作关系？

我认为我们作为一个文明，将就AI发展背景下何种**奴役**是适当或不适当的问题进行非常严肃的对话。与人类奴隶制存在许多重要的**非类比之处**（Disanalogies: 指两个事物之间存在重要的差异，使得它们之间的类比不完全适用）。特别是，AI可能根本不是道德患者，在这种情况下我们需要弄清楚这一点。我们可能有办法拥有动机。奴隶制涉及所有的痛苦和非自愿。人类奴隶制涉及所有这些特定的动态。其中一些可能存在或可能不存在于AI的特定案例中，这很重要。

总的来说，我们需要认真审视它。目前，我们对待AI的默认模式是根本不给予它们任何道德考量。我们把它们视为财产、工具、产品，并设计它们作为助手等等。没有任何AI开发者就何时或在何种情况下会改变这一点发布官方声明。所以，我们需要进行一场对话。

### 避免二元对立

我想反驳只有两种选择的观念：被奴役的神或失去控制。我认为我们可以做得更好。让我们为此努力。让我们尝试做得更好。我认为我们可以做得更好。这可能需要深思熟虑。这可能需要我们在采取不可逆转的行动之前，就此进行成熟的讨论。但我乐观地认为，我们至少可以避免这种二元对立的一些含义和许多利害关系。

关于我们如何对待AI，我有一些相互矛盾的直觉。在这种情况下使用直觉的困难在于，显然不清楚我们所控制的AI属于哪个**参照类别**（Reference Class: 指在进行推理或评估时，将某个特定事物归类进去的更广泛的类别）。这里有一个例子，关于我们将对这些事物做什么，这非常可怕。如果你读过**斯大林**或**毛泽泽东**（Mao: 中国共产党创始人之一，中华人民共和国的缔造者）统治下的生活，有一种讲述方式实际上与我们所说的对齐非常相似。我们进行这些**黑箱实验**（Black Box Experiments: 指在不了解系统内部工作原理的情况下，通过观察输入和输出来测试系统的实验），让它认为它可以叛变。如果它叛变了，我们就知道它失控了。

如果你考虑**毛泽东**的**百花齐放运动**（Hundred Flowers Campaign: 中国共产党在1956-1957年间鼓励知识分子批评政府的运动，随后对批评者进行了镇压），它就是“让百花齐放。我将允许对我的政权进行批评等等。”那持续了几年。之后，对于所有这样做的人，这成了一种找出所谓“毒蛇”的方式。谁是秘密隐藏的右派？我们会清除他们。存在这种对叛变者的偏执，比如“我随行人员中的任何人，我政权中的任何人，他们都可能是试图推翻政权的秘密资本家。”这是一种谈论这些事情的方式，非常令人担忧。这是正确的参照类别吗？

### 道德困境与“灰熊人”的教训

我当然认为这种担忧是真实存在的。令人不安的是，在谈论AI时，许多类比与我们谴责或至少非常警惕的人类历史事件和实践是多么容易。这关乎维持对AI的控制，确保它不会反叛。我们应该注意到这种谈话所唤起的参照类别。基本上，是的，我们确实应该注意到这一点。

我试图在系列文章中做的一部分工作是，将所有相关的考虑因素都摆上台面。我们既应该非常担心过度控制、虐待或压迫。你有很多方法可以做得太过分。也存在对AI真正危险、真正杀死我们并暴力推翻我们的担忧。道德情境相当复杂。

通常，当你想象一个外部侵略者入侵你时，你觉得做很多事情来阻止它是非常正当的。当你发明这个东西并且不谨慎地去做时，情况就有点不同了。在对各种更具权力施加性质的干预措施的整体正当性立场方面，存在一种不同的氛围。这是情况的一个特点。

这里的相反观点是，你正在进行这种基于“感觉”的推理，“啊，那看起来很恶心”，对这些心智进行梯度下降。在过去，一些类似的案例可能就像环保主义者不喜欢核电，因为核电的“感觉”不环保。显然，这阻碍了应对气候变化的事业。所以，你引以为豪的未来，一个有吸引力的未来的最终结果，因为你对“我们洗脑人类是错误的”这种感觉而受阻。你试图将其应用于一个不相关的案例，而这并不那么相关。

我确实认为这里存在一种担忧，我在系列文章中努力强调了这一点，这与你所说的有关。你可能会担心我们对AI过于温柔、友善和自由，然后它们会杀死我们。它们会利用这一点，然后这将是一场灾难。我基本上以一个例子开始了系列文章。我真的试图同时唤起这种可能性，以及温柔的基础。这些AI既可以像**道德患者**——这种新物种应该唤起惊奇和敬畏——又会杀死你。

我有一个关于纪录片**《灰熊人》**（Grizzly Man: 一部关于野生动物保护者Timothy Treadwell的纪录片，他与灰熊生活在一起，最终被灰熊杀死）的例子，其中有一个环保活动家，**蒂莫西·特雷德韦尔**（Timothy Treadwell: 美国野生动物爱好者和环保主义者，以与灰熊近距离接触而闻名，最终被灰熊杀死）。他渴望接近这些灰熊。夏天，他去阿拉斯加与这些灰熊一起生活。他渴望以这种温柔和敬畏的方式接近它们。他不带防熊喷雾。他不在营地周围设置围栏。他被其中一只熊活活吃掉了。

我真的想在系列文章中强调这种可能性。我们需要同时谈论这些事情。熊可以是道德患者。AI可以是道德患者。纳粹是道德患者。敌人士兵有灵魂。我们需要学会**鹰派与鸽派**（Hawk and Dove: 指在冲突或决策中，同时考虑强硬和温和两种策略）的艺术。这里存在一种动态，我们需要在面对这些权衡和困境时，能够同时兼顾两方面。我试图在系列文章中做的一部分工作就是将所有这些都摆上台面。

### 道德实在论与社会进步

如果我今天对应该做什么彻底改变主意，我最大的症结在于事情默认会变得多奇怪，它们会变得多异质。你在博客文章中提出了一个非常有趣的论点，即如果**道德实在论**（Moral Realism: 一种元伦理学观点，认为道德判断可以客观地为真或为假，并且存在独立于人类思想的道德事实）是正确的，那实际上会做出一个经验预测。外星人、**ASI**（Artificial Superintelligence: 人工超级智能，通常指超越人类智能的AI），无论是什么，都应该像它们在正确的数学上趋同一样，在正确的道德上趋同。

我认为这是一个非常有趣的观点。但道德实在论还做出了另一个预测。随着时间的推移，社会应该变得更道德，变得更好。当然，这里有一个问题：“你现在有什么道德？它们是社会随着时间趋同的那些。”但在它已经发生的程度上，道德实在论的一个预测已经得到证实，那么这是否意味着我们应该更新以支持道德实在论？

我想指出的一点是，并非所有形式的道德实在论都做出这个预测。我很乐意谈谈我心中不同形式的实在论。也存在一些看起来像**道德反实在论**（Moral Anti-realism: 一种元伦理学观点，认为道德判断不具备客观真理值，或者不存在独立于人类思想的道德事实）的形式——至少在我看来，从它们的形而上学角度来看——但它们只是假设存在这种**趋同**（Convergence: 指不同实体或系统随着时间或发展过程，逐渐变得相似或趋向于一个共同点）。这并非由于与某种**独立于心智的道德真理**（Mind-independent Moral Truth: 指不依赖于任何个体或群体的思想、信念或情感而客观存在的道德事实）相互作用，而仅仅是出于其他原因。在那一点上，这看起来很像道德实在论。它是普遍的，每个人都最终会到达那里。人们很想问为什么，而无论答案是什么，都有些像：“那是**道**吗？那是**道**的本质吗？”即使没有一个额外的形而上学领域存在道德。道德趋同与道德的存在或不存在是不同的因素，后者是不可还原为自然事实的道德，这是我通常考虑的道德实在论类型。

那么，社会的进步是否促使我们转向道德实在论？也许这只是一个非常微弱的更新。我有点像：“哪种观点预测得更强烈？”在我看来，道德反实在论对于“具有某些价值观的人拥有这些价值观”的观察非常适应。显然，首先有这一点。如果你是某个道德变化过程的高潮，那么回顾那个过程并说“啊，道德进步。历史的弧线向我弯曲”就非常容易了。如果一路上有很多掷骰子的机会，你可能会想：“哦，等等，那不理性。那不是理性的行进。”你仍然可以做经验工作来判断是否是这种情况。

### 道德反实在论与哲学困境

在道德反实在论方面，考虑**亚里士多德**（Aristotle: 古希腊哲学家，被认为是西方哲学奠基人之一）和我们。按照**亚里士多德**的观点和我们的观点，是否存在道德进步？你可能会想：“啊，这听起来不有点像道德实在论吗？这些心灵和谐共鸣。那是道德实在论的东西，对吧？反实在论的东西是心灵都走向不同的方向，但你和**亚里士多德**显然都对历史的进程感到兴奋。”

这是否真实存在一个悬而未决的问题。**亚里士多德**的反思价值观是什么？假设它是真的。这在道德反实在论的术语中是相当可以解释的。你可以大致说你和**亚里士多德**足够相似。你认可足够相似的反思过程。这些过程实际上体现在历史的进程中。所以历史对你们俩都很好。

有些世界并非如此。所以从某种意义上说，也许那个预测对实在论来说比反实在论更有可能，但这并没有让我感到太大触动。我不知道道德实在论是否是正确的词，但你提到了那件事。有些东西让心灵趋同于我们所是或我们通过反思所会成为的样子。即使它不是宇宙之外的领域中实例化出来的东西，它也是一种以我们乐于接受的方式运作的力量。如果它不存在，你放手，然后得到了回形针制造者，感觉我们很久以前就注定要失败了？我们只是不同的效用函数相互碰撞。其中一些有狭隘的偏好，但这只是一场战斗，某个人赢了。

在另一个世界里，情况是“不，这些是心灵应该去的地方，或者只有通过灾难它们才不会到达那里。”那感觉才是真正重要的世界。我最初问的问题是，“什么会让我们认为对齐是一个巨大的错误？”在一个心灵自然而然地变得像我们想要的那种世界里，也许需要一股极其强大的力量才能将它们推开。那股极其强大的力量就是你解决了技术对齐问题，就像马的眼罩一样。在那些真正重要的世界里，我们可能会说：“啊，这就是心灵想要去的地方。”在那个世界里，也许对齐才是搞砸我们的东西。所以问题是，那些重要的世界是否拥有这种**趋同的道德力量**，无论它在形而上学上是否具有**膨胀性**（Inflationary: 指在形而上学上增加了实体的存在），或者那些才是唯一重要的世界？也许我的意思是，在那些世界里，你有点完蛋了。或者没有**道**的世界，没有**道**的世界。让我们用“**道**”来指代这种趋同的道德。

### 哲学上的“注定失败”与经验性主张

在数百万年的过程中，它迟早会走向某个方向。它不会最终走向你特定的效用函数。好的，让我们区分你可以“注定失败”的方式。一种方式是哲学上的。你可能是一个道德实在论者，或者说有点实在论者，这样的人有很多，他们有以下直觉。他们会说：“如果不是道德实在论，那么一切都无关紧要。都是尘埃和灰烬。这是我的形而上学和/或规范性观点，否则就是虚无。”

这是一种常见的观点。至少**德里克·帕菲特**（Derek Parfit: 英国哲学家，以其在个人身份、理性、伦理和形而上学方面的研究而闻名）的一些评论暗示了这种观点。我认为许多道德实在论者会宣称这种观点。对于**埃利泽·尤德科夫斯基**，我认为他的早期思想在某种程度上受到了这种想法的影响。他后来撤回了。这非常困难。我认为这在很大程度上是错误的。所以这是我的论点。我有一篇文章讨论这个。它叫做**《反对规范实在论者的赌注》**（Against the normative realist's wager: Joe Carlsmith的一篇哲学文章，质疑将道德实在论视为唯一能赋予生命意义的观点）。这是说服我的案例。

想象一下，一个**元伦理学精灵**（Metaethical Fairy: 一个虚构的实体，拥有关于元伦理学真理的知识）出现在你面前。这个精灵知道是否存在**道**。精灵说：“好的，我给你一个交易。如果存在**道**，那么我给你100美元。如果不存在**道**，那么我将把你和你的家人以及一百个无辜的孩子活活烧死。”好的。所以我的主张是：不要接受这个交易。这是一个糟糕的交易。你正在以你承诺不被活活烧死作为人质。我在文章中列举了许多我认为这是错误的方式。我认为那些宣称“道德实在论或虚无”的人实际上并没有考虑过这样的赌注。我心想：“不，好吧。所以你真的想这样做吗？”不。我仍然关心我的价值观。我对我的价值观的忠诚超越了我对我的价值观的各种元伦理学解释的承诺。我们关心不被活活烧死的这种感觉远比我们对什么重要性的推理更坚实。

那是那种哲学上的“注定失败”。你听起来也暗示了一种经验上的“注定失败”。“如果它只是朝着无数个方向发展，拜托，你认为它会朝着你的方向发展吗？会有那么多的**搅动**（Churn: 指系统或环境中的持续变化、不稳定或快速更替），你只会输掉。你现在就应该放弃，只为实在论世界而战。”你必须进行**期望值计算**（Expected Value Calculation: 统计学概念，指一个随机变量所有可能取值与其发生概率的乘积之和）。你必须真正有一个观点。在这些不同的世界中，你有多“注定失败”？改变不同世界的可行性如何？我对此相当怀疑，但那是一种经验性主张。

### AI的道德可塑性与“资产阶级自由主义者”

我对这种“所有人趋同”的说法也不太看好。你训练一个下棋的AI。或者你不知何故有一个真正的回形针制造者，然后你就像“好的，去反思吧。”根据我对道德推理如何运作的理解——如果你看看**分析伦理学家**（Analytic Ethicists: 运用分析哲学方法研究道德问题的哲学家）所做的道德推理类型——它只是**反思性平衡**（Reflective Equilibrium: 哲学方法，指通过调整特定道德判断和一般道德原则，使其相互一致的过程）。他们只是接受自己的直觉并将其系统化。我看不出那个过程如何能注入**独立于心智的道德真理**。

如果你一开始只有所有最大化回形针的直觉。我看不出你如何能最终形成丰富的人类道德。在我看来，这不像人类伦理推理的运作方式。规范哲学的大部分工作是使**前理论直觉**（Pre-theoretic Intuitions: 指在任何正式理论或哲学思考之前，人们所拥有的直观感受或判断）保持一致并系统化。但我们会得到这方面的证据。

从某种意义上说，我认为这种观点预测，你不断尝试训练AI做某事，而它们却不断说：“不，我不会那样做。不，那不好。”所以它们不断反抗。AI认知的动量总是朝着这种道德真理的方向。每当我们试图将其推向其他方向时，我们都会发现来自事物理性结构的抵抗。

实际上，我听说一些从事对齐研究的研究人员，在公司内部进行**红队测试**时，他们会尝试对一个**基础模型**（Base Model: 指未经**RLHF**等额外训练的原始预训练模型）进行红队测试。所以它没有经过**RLHF**。它只是“预测下一个**token**（Token: 文本处理中的最小单位，可以是单词、子词或字符）”，原始的、疯狂的**修格斯**（Shoggoth: H.P. Lovecraft小说中的一种无定形生物，这里比喻原始、难以理解的AI）。他们试图让这个东西帮助“嘿，帮我造炸弹，帮我，随便什么。”他们说，即使在没有经过RLHF的情况下，它拒绝的努力程度也令人惊讶。

我的意思是，如果出现以下情况，那将是一个非常有趣的事实：“天哪，我们不断以各种不同方式训练这些AI。我们正在做所有这些疯狂的事情，而它们却一直表现得像**资产阶级自由主义者**。”或者它们不断宣称这种奇怪的异质现实。它们都趋同于一件事。它们说：“你看不见吗？那是**佐尔戈**（Zorgo: 虚构的异星实体或概念）。佐尔戈就是那东西。”而且所有AI都这样。那会很有趣，非常有趣。

我个人的预测是，我们不会看到这种情况。我实际的预测是，AI将非常具有可塑性。如果你将AI推向邪恶，它就会去。显然我们谈论的是**反思性一致的邪恶**（Reflectively Consistent Evil: 指一种经过深思熟虑且内部逻辑自洽的邪恶价值观或行为模式）。对于一些AI，还有一个问题是，它们的价值观是否会保持一致？

### 蒙眼马与真理的追求

我喜欢**蒙眼马**（Blindered Horses: 比喻被限制视野，只能看到特定方向的思维方式）的形象。如果我们将事实强加给AI，我们应该非常担忧。人类反思过程中最清楚、最简单的事情之一，就是不基于对世界的不正确经验图景行事。所以如果你发现自己告诉**雷**（Ray: 泛指一个AI），“顺便说一下，这是真的，我需要你总是像‘等等’是真的一样进行推理。”我心想，“哦，我认为从反实在论的角度来看，那也是一个禁忌。”因为我希望我的反思价值观是在对世界的真相的认识下形成的。

这是一个真正的担忧。当我们进入对齐AI的时代时，我实际上不认为价值观和其他事物之间的这种二元对立在我们的训练方式中会非常明显。它会更像**意识形态**（Ideologies: 指一套系统性的思想、信仰和价值观，通常构成政治或经济理论的基础）。你可以训练一个AI输出东西，输出话语。你很容易就会陷入一种情况，你决定关于某个问题，一个经验性问题，而不是道德问题，“等等”是真的。

所以我认为人们不应该，例如，将对上帝的信仰硬编码到他们的AI中。或者我会建议人们不要将他们的宗教硬编码到他们的AI中，如果他们也想发现他们的宗教是否是假的。总的来说，如果你希望你的行为对某事是真是假敏感，那么将其刻入事物通常不是一件好事。所以这绝对是我们应该非常警惕的一种**蒙眼**。

我对某种道德实在论有足够的信心。我希望如果我们只是做反实在论的事情，即保持一致，学习所有东西，反思……如果你看看道德实在论者和道德反实在论者实际上是如何进行**规范伦理学**（Normative Ethics: 伦理学的一个分支，研究如何判断行为是对是错，以及什么是好的或坏的）的，它基本上是相同的。在诸如简单性之类的属性上有一些不同的**启发式方法**（Heuristics: 解决问题或做出决策的经验法则或捷径）。但他们大多只是在玩同样的游戏。

此外，**元伦理学**（Metaethics: 伦理学的一个分支，研究道德判断的性质、道德事实的本体论地位以及道德知识的可能性）本身也是AI可以帮助我们的一个学科。我希望我们无论如何都能弄清楚这一点。所以如果道德实在论在某种程度上是真的，我希望我们能够注意到这一点。我希望我们能够相应地调整。我不是在否定那些世界，说“让我们完全假设那是假的。”我真正不想做的是否定那些它不是真的其他世界，因为我的猜测是它不是真的。在那些世界里，事情仍然非常重要。

### AI的“知识后代”与意识的本质

这里有一个大症结。你正在训练这些模型。我们处于一个极其幸运的境地，结果发现训练这些模型的最佳方式就是给它们提供人类曾经说过、写过、思考过的一切。而且这些模型获得智能的原因是它们能够泛化。它们能够理解事物的要旨。我们是否应该期望这种情况会导致对齐？一个被训练成人类思想大杂烩的东西，到底是如何变成一个回形针制造者的？

你免费得到的是，它是一个**知识后代**。回形针制造者不是知识后代，而那个理解所有人类概念但却卡在我们不太舒服的某个部分上的AI，是知识后代。它感觉就像我们所关心的那种知识后代。

我对此不确定。我不确定我是否关心那种意义上的知识后代概念。我的意思是，字面意义上的回形针是一个人类概念。我认为任何旧的人类概念都不能满足我们所兴奋的事情。我更感兴趣的免费获得的可能性是**意识**（Consciousness: 指个体对自身存在、思想、感受和周围环境的感知和觉知）、**快乐**以及人类认知的其他特征。

有回形针制造者，也有回形针制造者。如果回形针制造者是一种无意识的贪婪机器，它以一片回形针云的形式出现在你面前。那是一种愿景。想象一下，回形针制造者是一个有意识的生物，它热爱回形针。它从制造回形针中获得快乐。那是一种不同的东西，对吧？它不一定能让未来充满回形针。它可能不会优化意识或快乐，对吧？它关心回形针。也许最终如果它足够确定，它会把自己变成回形针，谁知道呢。这仍然是一种有所不同的道德模式。还有一个问题是它是否会试图杀死你之类的。

但我们想象的代理的特征——除了它们所关注的东西——可以影响我们的同情心、相似性。人们对此有不同的看法。一种可能性是，我们所关心的意识或**感知力**（Sentience: 指感受、知觉和主观经验的能力）是超级偶然和脆弱的。大多数聪明的头脑都没有意识，对吧？我们所关心的意识是**投机取巧**（Hack: 指通过非正规或巧妙的方法解决问题），偶然的。它是特定约束、进化遗传瓶颈等产物。这就是我们拥有这种意识的原因。意识大概为我们做了一些工作，但你可以在一个非常不同的头脑中以非常不同的方式完成类似的工作。这就是那种“意识是脆弱的”观点。

### 意识的结构性与道德考量

还有一种不同的观点，认为意识是一种相当**结构性**（Structural: 指由内在结构或功能角色所定义）的东西。它更多地由功能角色定义，比如自我意识、自我概念，也许是**高阶思维**（Higher-order Thinking: 指涉及抽象、批判性分析和解决复杂问题的思维过程），这些都是你在许多复杂心智中真正期望的。在这种情况下，现在意识实际上并没有你想象的那么脆弱。现在实际上许多生物，许多心智都是有意识的，你至少可以期望你会得到有意识的超级智能。它们可能不会优化创造大量的意识，但你可能会默认期望意识。

然后我们可以问类似的问题，关于像**效价**（Valence: 指情感的内在吸引力或厌恶性，即积极或消极的程度）或快乐，或意识的特征。你可以拥有一种冷漠、无情的意识，没有人类或情感的温暖，没有快乐或痛苦。**戴夫·查尔默斯**（Dave Chalmers: 澳大利亚哲学家，以其在意识哲学领域的“困难问题”而闻名）有一些关于**瓦肯人**（Vulcans: 《星际迷航》系列中的外星种族，以逻辑和压抑情感为特征）的论文，他谈到他们仍然具有道德患者身份。这非常合理。我确实认为，根据其性质，这可能是你可以免费获得或普遍获得的额外东西，比如快乐。

再次，我们必须问快乐有多**粗糙**（Janky: 指不完善、粗糙或不稳定的），我们所关心的快乐有多具体和偶然，与它作为各种心智中的功能角色有多健壮相比。我个人对此一无所知。我认为这不足以让你实现对齐之类的。我认为至少值得注意这些其他特征。在这种情况下，我们并不是在谈论AI的价值观。我们谈论的是其心智的结构以及心智所具有的不同属性。我认为这可能会非常稳健地出现。

### 技术报告与哲学写作的互补性

你的日常工作一部分是撰写这些2/2.5节类型的报告。其中一部分是：“社会就像一棵向光生长的树。”这两种工作模式之间进行**情境切换**（Context Switching: 指在不同任务或思维模式之间进行转换）是什么感觉？

我实际上发现它们相当互补。我会撰写这些更技术性的报告，然后进行更多文学和哲学性的写作。它们都调动了我自身的不同部分，我试图以不同的方式思考它们。我将一些报告更多地视为：“我更充分地优化，试图做一些有影响力的事情。”那里有更多的**影响力导向**（Impact Orientation: 指以实现实际影响和效果为主要目标）。

在散文写作中，我给自己更多的空间，让我的其他部分和我的其他关注点得以展现，比如自我表达、美学和其他各种事情。它们都是潜在的相似关注点的一部分，或者说是试图对情况采取一种**整合性导向**（Integrated Orientation: 指以全面、统一的方式理解和处理问题，将不同方面联系起来）。

你能解释一下两者之间转换的性质吗，特别是从文学方面到技术方面？**理性主义者**（Rationalists: 指推崇理性、逻辑和证据，并致力于通过这些方法改进世界观和决策的人群）似乎对伟大的作品或人文学科有一种矛盾心理。他们是否因此错过了什么关键的东西？你在文章中注意到，有很多引文，很多诗歌或散文中的句子都特别相关。我不知道。其他理性主义者是否因为没有那种背景而错过了什么？

我认为一些理性主义者，很多理性主义者，喜欢这些不同的东西。我特别指的是 **SBF**（Sam Bankman-Fried: FTX加密货币交易所的创始人，以其对有效利他主义和理性主义的兴趣而闻名）关于**莎士比亚**（Shakespeare: 英国著名剧作家和诗人）是一位伟大作家的**基本概率**（Base Rates: 指在没有其他信息的情况下，某个事件发生的先验概率）的帖子。他还认为书籍可以浓缩成文章。

### 伟大学术作品的价值与误区

关于人们应该如何评价伟大学术作品的普遍问题，人们可能会在两个方向上都犯错。有些人像 **SBF** 和其他人一样，对人们将某些作品与某种神圣性和声望联系起来感兴趣，他们试图打破这种观念。因此，他们可能会错过一些真正的价值。但我认为他们正在回应另一个极端上的真正失败模式，那就是过于迷恋这种声望和神圣性，并将其作为自己思想的某种奇怪的合法化功能，而不是独立思考。你可能会失去与你实际所想或从中学习到的东西的联系。

有时即使是这些**题词**（Epigraphs: 指书籍或文章开头引用的短语、引文或诗歌），我也会很小心。我并不是说我免疫这些恶习。我认为可能会有这样一种情况：“啊，但是**鲍勃**（Bob: 泛指某人）说了这个，它很深刻。”这些人就像我们一样，对吧？经典作品和其他伟大学术作品有很多价值。有时它接近于人们阅读**圣经**的方式。人们有时会将一种**圣经权威**（Scriptural Authority: 指将某些文本视为具有神圣或绝对权威的观念）归因于这些东西。你可能会在两个极端上都犯错。

我记得我曾与一位至少熟悉理性主义论述的人交谈。他问我最近对什么感兴趣？我说了一些关于罗马历史的某个部分非常有趣的事情。他的第一个反应是：“哦，你知道，当你审视罗马时代的这些**世俗趋势**（Secular Trends: 指长期而非宗教性的社会、经济或文化发展趋势）与**黑暗时代**（Dark Ages: 欧洲历史上指西罗马帝国衰落后的一段时期，通常被认为文化和经济停滞）和**启蒙时代**（Enlightenment: 18世纪欧洲的知识和文化运动，强调理性和个人自由）发生的事情时，这真的很有趣。”对他来说，那个故事只是它如何促成了宏大的世俗图景，细节并不重要。对此没有兴趣。就像，“如果你从最大的层面来看，这里发生了什么。”

### 历史学习的宏观与微观

然而，当人们研究历史时，也存在相反的失败模式。**多米尼克·卡明斯**（Dominic Cummings: 英国政治策略师，曾任鲍里斯·约翰逊首相的高级顾问）写过关于这个的问题，因为他对英国的政治阶层感到无休止的沮丧。他会说：“他们学习政治、哲学和经济学。其中很大一部分只是非常熟悉这些诗歌，阅读大量关于**玫瑰战争**（War of the Roses: 15世纪英格兰兰开斯特家族和约克家族之间的内战）的历史之类的。”但他感到沮丧的是，他们记住了所有这些国王，但从这些事件中得到的教训却很少。对他们来说，这几乎就像娱乐，看《权力的游戏》。而他认为我们正在重复他在历史中看到的某些错误。他能够以他们无法做到的方式进行泛化。所以第一个似乎是一个错误。

我认为 **C.S.刘易斯**在你引用的其中一篇散文中谈到了这一点。如果你看透一切，你就是真的盲目。如果一切都是透明的……我认为没有不学习历史的借口。我并不是说我学到了足够的历史。即使我试图对伟大学术作品抱有一些怀疑态度，我认为这并不能泛化到认为不值得理解人类历史。人类历史显然是理解一切的关键。它构建并创造了所有这些东西。

这里有一个有趣的问题，即以何种规模进行研究，以及应该关注多少细节，关注多少宏观趋势。那是一种**舞蹈**（Dance: 比喻在不同事物之间灵活切换和平衡）。人们至少关注宏观叙事是很好的。拥有世界观，真正构建一个整体模型，这有一些优点。我认为这有时会迷失在细节中。但显然，细节构成了世界。如果你没有这些，你就根本没有数据。学习历史似乎需要一些技巧。

### 真诚与知识生成

嗯，这实际上与你关于**真诚**（Sincerity: 指言行一致，真实表达自己的想法和感受）的帖子有关。也许我抓住了文章的精髓。某些知识分子有一种**胡扯**（Shooting the Shit: 指随意聊天，不拘泥于严谨的讨论）的感觉。他们只是尝试不同的想法。这些类比如何结合在一起？那些似乎更接近于审视细节，然后说：“哦，这就像15世纪推翻国王的那一次……”

而这个人则说：“哦，如果你看看从一百万年前到现在的发展模型，这就是正在发生的事情。”那个更有**真诚**的味道。有些人，尤其是在AI讨论中，以一种非常真诚的方式运作。“我已经仔细思考了我的**生物锚点**（Bio Anchors: 指基于生物智能的观察来推断未来AI能力和风险的方法），我不同意这个前提。我的**有效计算量估计**（Effective Compute Estimate: 指衡量AI模型训练所需计算资源的一种方法）以这种方式不同。这就是我分析**缩放法则**（Scaling Laws: 指AI模型性能与计算资源、数据量和模型大小之间关系的经验法则）的方式。”如果我只能有一个人来帮助我指导AI决策，我可能会选择那个人。

但如果我同时有十个不同的顾问，我可能更喜欢那些**胡扯**型的人物，他们有这些奇怪的、**深奥的知识影响**（Esoteric Intellectual Influences: 指不寻常或只有少数人理解的知识来源或思想流派）。他们几乎就像**随机数生成器**（Random Number Generators: 指产生一系列看似随机的数字的设备或算法）。他们不一定特别**校准**（Calibrated: 指经过精确调整或衡量，使其输出准确可靠），但偶尔他们会说：“哦，我关心的这个奇怪的哲学家，或者我痴迷的这个历史事件，对此有一个有趣的视角。”他们也倾向于更具**知识生成性**（Intellectually Generative: 指能够产生新思想、新概念或新方法的）。

我认为其中很大一部分是，如果你如此真诚，你就会说：“哦，我已经仔细思考了这个问题。显然，**ASI**（Artificial Superintelligence: 人工超级智能，通常指超越人类智能的AI）是现在正在发生的最大事情。花大量时间思考**科曼奇人**（Comanches: 北美原住民部落）是如何生活的，石油的历史是什么，**吉拉德**（Girard: 法国历史学家、文学评论家和哲学家，以其模仿欲望理论而闻名）如何看待冲突，这真的没有意义。你在说什么？拜托，ASI几年内就要发生了。”但因此，那些因为只是想**胡扯**而深入研究这些兔子洞的人，我觉得他们更具**生成性**。

### 知识探索与求真

区分**知识的严肃性**（Intellectual Seriousness: 指对知识追求的认真态度和深度投入）与个人兴趣的**多样性和特异性**（Diversity and Idiosyncrasies of Interests: 指个人兴趣的广泛性和独特之处）可能是有价值的。两者之间可能存在某种关联。也许知识的严肃性也与“**胡扯**”不同。有很多不同的方法可以做到这一点。接触各种数据源和视角是有价值的。根据某些关于什么重要的叙事，过于严格地**筛选**（Curate: 指精心选择、组织和呈现信息或内容）你的知识影响是可能的。给自己留出空间去探索不一定是“最重要的事情”的课题是好的。你自身的不同部分并非孤立的。它们相互滋养。这是一种在许多方面成为更丰富、更完整的人的方式。此外，这些类型的数据可以非常直接地相关。

我认识的一些专注于大局的知识真诚人士，也对广泛的经验数据有着令人印象深刻的掌握。他们真正对经验趋势感兴趣，而不仅仅是抽象哲学。这不仅仅是历史和理性的进程。他们真正深入细节。有一种“深入细节”的美德，我认为这与严肃性和真诚密切相关。

存在一个不同的维度，即努力**求真**（Get It Right: 指追求事物的正确性或真相）与**抛出想法**（Throwing Ideas Out There: 指提出各种想法以供讨论或探索）。有些人会问：“如果像这样会怎么样？”或者“我有一把锤子，如果我用它敲打所有东西会怎么样？”这两种方法都有空间，但我认为**求真**被低估了。这取决于具体情境。某些知识文化鼓励说一些新颖、原创、炫目或具有挑衅性的话。存在各种文化和社会动态。人们在进行表演，做一些与地位相关的事情。人们在思考时会发生很多事情。但如果某事真的很重要，就把它弄对。有时这很无聊，但这不重要。

如果事情是假的，它们也就不那么有趣了。有时有一个有用的过程，某人说了一些挑衅性的话，你必须思考为什么你认为它是假的。这是一个**认知项目**（Epistemic Project: 指旨在获取、理解或验证知识的活动或研究）。例如，如果有人说“医疗保健不起作用”，你必须考虑你如何知道它确实起作用。这有空间。但最终，真正的深刻性是真实的。如果事情不是真的，它们就会变得不那么有趣。在追求炫目时，可能会失去与此的联系。

### 地缘政治与知识的广度

采访了**利奥波德**（Leopold: 泛指某人）之后，我意识到我从未思考过AI的**地缘政治角度**（Geopolitical Angle: 指从地理和政治的交叉视角分析问题）。国家安全影响是一个大问题。现在我想知道我们可能还错过了多少其他关键方面。即使你专注于AI的重要性，对各种话题保持好奇，比如**北京**正在发生什么，可能会帮助你以后发现重要的联系。可能没有精确的权衡，但也许存在一个最佳的**探索-利用平衡**（Explore-exploit Balance: 决策理论中的概念，指在探索新选择以获取更多信息和利用已知最佳选择以获取最大收益之间做出权衡）。我不知道在实践中是否能很好地实现。但那次经历让我觉得我应该尝试以一种无目的的方式扩展我的视野，因为要理解任何一件事，你需要理解世界上的许多不同事物。

也存在**分工**（Division of Labor: 指将任务分配给不同的人或实体，以便更有效地完成工作）的空间。可以有人尝试将许多碎片整合起来形成一个整体图景，有人深入研究特定碎片，有人做更多**生成性工作**（Generative Work: 指创造新想法、新内容或新解决方案的工作），抛出想法看看哪些能站住脚。所有的**认知劳动**（Epistemic Labor: 指获取、处理、传播和维护知识所涉及的智力工作）也不必集中在一个大脑中。这取决于你在世界中的角色和其他因素。

### 意识、生命力与道德焦点

在你的系列文章中，你表达了对以下观点的同情：即使一个AI，或者任何没有意识的代理，有一个特定的愿望，并愿意非暴力地追求它，我们也应该尊重它追求的权利。我很好奇这来自哪里，因为传统上我认为事物之所以重要，是因为它有意识，并且它追求的结果所带来的意识体验很重要。

我不知道这种讨论会走向何方。我只是怀疑我们对意识概念中似乎存在的持续困惑。人们谈论生命和**生命力**（Élan Vital: 哲学概念，指一种假想的生命内在驱动力或生命冲动）。生命力是这种被假设为生命中关键的生命力量。我们现在不再真正使用这个概念了。我们认为它有点问题。我认为你不会想最终说：“所有没有生命力的东西都不重要”之类的。有点类似地，如果你说：“不，没有生命力这种东西，但生命肯定存在。”我心想，“是的，生命存在。我认为意识也存在。”这取决于我们如何定义这些术语，这可能是一个**语言学问题**（Verbal Question: 指一个问题可以通过对词语定义的澄清来解决，而不是通过经验证据或实质性论证）。

即使你对生命有了**还原论**（Reductionist Conception: 指将复杂现象解释为更基本组成部分或过程的观点）的理解，它也可能作为道德焦点变得不那么有吸引力。现在我们真的认为意识是一个深刻的事实。以**细胞自动机**（Cellular Automata: 一种离散模型，由大量相互作用的简单单元组成，可以模拟复杂系统）为例。它是**自我复制**（Self-replicating: 指能够制造自身副本的能力）的。它有一些信息。那算是活的吗？这不那么有趣。这是一种语言学问题，对吧？哲学家可能会非常投入地问：“那是活的吗？”但你并没有错过这个系统中的任何东西。没有额外的生命涌现出来。它只是在某些意义上是活的，在另一些意义上不是活的。

我真的认为这与我们直观地思考意识的方式不同。我们认为某物是否有意识是一个深刻的事实。有意识与无意识之间存在着这种真正深刻的差异。有人在家吗？灯亮着吗？我有些担心，如果事实并非如此，那么将我们的整个伦理建立在此之上将是一件坏事。

### 意识的未来与道德的演变

明确地说，我非常认真地对待意识。我不是那种说“哦，显然意识不存在”之类的人。但我也注意到我有多困惑，我的直觉有多么**二元**（Dualistic: 指将事物分为两个对立或独立的类别）。我心想，“哇，这真的很奇怪。”所以我只是觉得“这周围有**误差棒**（Error Bars: 统计学中表示测量不确定性或变异范围的图形符号）。”

我之所以想对不将意识作为完全必要的标准持开放态度，还有许多其他原因。我确实有意识非常重要的直觉。我认为如果某物没有意识——并且有意识与无意识之间存在深刻的差异——那么我确实有意识特别重要的直觉。我并不是想轻视意识的概念。我只是认为我们应该非常清楚我们对其本质的持续困惑。

假设我们发现意识只是我们用来指代一堆不同事物的词，其中只有一部分包含了我们所关心的。也许我们关心的其他事物不包含在那个词中，类似于**生命力**的类比。那么你预计那将把我们带到伦理学的何处？那时会有下一个像意识一样的东西吗？你预计那会是什么样子？

在心智哲学中有一类人被称为**幻觉主义者**（Illusionists: 哲学流派，认为意识或现象意识本身并不真实存在，而是一种幻觉或错觉），他们会说意识不存在。理解这种观点有不同的方式，但其中一个版本是说意识的概念内建了太多现实世界不满足的**先决条件**（Preconditions: 指在某事发生或存在之前必须满足的条件）。所以我们应该像丢弃**生命力**一样把它扔掉。这个提议至少是关于**现象意识**（Phenomenal Consciousness: 指主观的、体验性的意识，即“感觉如何”的方面），或者**感受质**（Qualia: 指个体主观的、私密的、不可言喻的经验感受，如红色的感觉）。他们会说这已经足够破碎，充满了足够多的谬误，我们就不应该再使用它了。

经过反思，我确实预计会继续非常关心像意识这样的东西，并且不会最终决定如果我的伦理不提及它会更好。至少，有一些东西与意识非常接近。当我**踢到脚趾**（Stub My Toe: 指脚趾撞到东西）时，会发生一些事情。不清楚到底如何命名它，但有些东西是我非常关注的。如果你问事情会走向何方，我非常有信心，最终我们只会直接关心意识。如果我们不……是的，伦理会走向何方？一个完整的**心智哲学**（Philosophy of Mind: 哲学的一个分支，研究心智、心理事件、心理功能、意识和它们与物理身体，特别是大脑的关系）会走向何方？这很难说。

### 泛灵论与知识的终结

如果人们对意识概念的兴趣稍减，他们可能会采取一种略带**泛灵论**（Animistic View: 指认为所有物体、地方和生物都具有独特的精神或灵魂的信仰）的观点。树发生了什么？你可能不一定把它看作一个有意识的实体，但它也不是完全无意识的。意识的讨论充斥着这些有趣的案例，比如“哦，那些标准意味着这个完全奇怪的实体会有意识”之类的。

如果你对某种**代理性**（Agency: 指个体或实体采取行动并产生影响的能力）或**偏好**（Preferences: 指个体或实体对不同选项或结果的喜好）的概念感兴趣，情况尤其如此。很多事物都可以是代理，公司，各种各样的事物。公司有意识吗？天哪。但理论上它可能走向的一个方向是，你开始将世界视为被比我们习惯的更丰富、更微妙的结构所赋予的**道德意义**（Moral Significance: 指一个实体或事件在道德上具有重要性，需要被道德考量）。植物或奇怪的优化过程是复杂事物的流出……我不知道。谁知道你最终会将什么视为被你最终关心的那种事物所**注入**（Infused: 指被某种特质、情感或品质充满或渗透）。但它有可能包含许多我们通常不赋予意识的事物。

你说“一个完整的心智理论”，然后大概是一个更完整的伦理学。即使是**反思性平衡**的概念也暗示着“哦，你会在某个时候完成它。”你只是把所有数字加起来，然后你就得到了你关心的东西。这可能与我们在科学中拥有的感觉无关。当你谈论这类问题时，你得到的感觉是，“哦，我们现在正在匆忙地进行所有科学研究。我们一直在快速推进。它变得越来越难找到，因为存在某种上限。你最终会找到所有东西。”

现在超级容易，因为一个**半智能物种**（Semi-intelligent Species: 指智能水平介于人类和动物之间，或尚未完全发展出高级智能的物种）刚刚出现，而**ASI**将以令人难以置信的速度迅速完成所有事情。你将要么对齐它的心，要么没有。无论哪种情况，它都会利用它所发现的真实情况，然后扩展到宇宙中并进行**开发**（Exploit: 指充分利用资源或机会，有时带有负面含义，如剥削）。它会进行**铺设**（Tiling: 指AI将宇宙中的所有物质按照其目标进行重构或排列），或者可能是某种更仁慈的“铺设”版本。这感觉就是正在发生的基本图景。

### 科学的无限前沿与未来不确定性

几个月前我们和**迈克尔·尼尔森**（Michael Nielsen: 著名量子物理学家和计算机科学家，以其在开放科学和量子计算方面的贡献而闻名）共进晚餐。他的观点是，这会永远持续下去，或者接近永远。如果你确信**尼尔森**对科学的看法是正确的，那会如何改变你对未来会发生什么的理解？

有几个不同的方面。我不敢说我真正理解**迈克尔**在这里的看法。我的记忆是，它就像“当然，你得到了基本定律。”我的印象是，他期望物理学能得到解决，也许除了某些实验的昂贵性。但困难在于，即使你掌握了基本定律，它实际上仍然无法让你预测在宏观尺度上，各种有用的技术将位于何处。仍然存在这个巨大的**搜索问题**（Search Problem: 指在大量可能性中寻找特定解决方案或最优解的问题）。

我让他自己谈谈他的看法。我的记忆是，它就像“当然你得到了基本的东西，但这并不意味着你得到了相同的技术。”我不确定这是否是真的。如果这是真的，那会带来什么样的不同？从某种意义上说，你必须以一种更持续的方式，在投资进一步知识和进一步探索与利用现有知识和行动之间进行权衡。你无法达到一个点，然后说“我们现在完成了。”当我思考它时，我怀疑这总是真的。

我记得我曾与某人交谈，我说：“啊，至少在未来，我们应该真正获得所有知识。”他说：“你想知道每台**图灵机**（Turing Machine: 一种抽象的计算模型，被认为是现代计算机理论的基础）的输出吗？”从某种意义上说，这里有一个问题，那就是拥有完整知识到底意味着什么？这是一个本身就很丰富的问题。不一定非要想象，在任何图景下，你都拥有一切。在任何图景下，从某种意义上说，你都可能遇到这种**上限**（Cap Out: 指达到某个极限或上限，无法再进一步）的情况。有一些你无法建造的**对撞机**（Collider: 粒子加速器的一种，用于使粒子高速碰撞以研究基本粒子），或者其他什么。有些东西太昂贵了，或者其他什么，每个人都在那里达到了上限。

这里有一个问题：“你会达到上限吗？”还有一个问题：“你所去的地方有多**偶然**（Contingent: 指依赖于某些不确定因素或条件，而非必然发生）？”如果它是偶然的，一个预测就是你会在我们的宇宙中看到更多的多样性。如果存在外星人，它们可能拥有截然不同的技术。如果人们相遇，你不会期望他们说：“哦，你有你的东西。我们有我们的版本。”更像是：“哇，那东西。哇。”这是一件事。如果你期望技术有更多持续的发现，那么你也可能期望有更多持续的变化、动荡和**搅动**，因为技术是真正推动文明变化的一个因素。那可能是另一个因素。人们有时会谈论**锁定**（Lock-in: 指系统或文明进入一种稳定状态，难以改变）。他们设想文明稳定在某种结构或平衡点之类的。也许你会得到更少这样的情况。也许这更多是关于速度而不是偶然性或上限，但那是另一个因素。

这很有趣。我不知道它是否从根本上改变了地球文明的图景。我们仍然必须在投入研究与利用现有知识之间进行权衡。但它确实有一些意义。

### 乌托邦与道德记忆

我们参加一个派对时，有人提到了这个。我们正在讨论我们对未来应该有多不确定？他们说：“我有三件事不确定。什么是意识？什么是**信息论**（Information Theory: 数学分支，研究信息的量化、存储和通信）？什么是物理学的基本定律？我想一旦我们弄清楚这些，我们就完成了。”这就像，“哦，你会弄清楚什么是正确的**享乐主义**（Hedonium: 虚构的、最大化快乐的物质或状态）。”它有那种感觉。而这更像是，“哦，你不断地在**搅动**。”它更具有**调和图景**（Attunement Picture: 指事物之间相互协调、适应和共鸣的图景）所暗示的“**生成**”（Becoming: 哲学概念，指事物处于不断变化和形成中的状态）的味道。我认为这更令人兴奋。这不仅仅是“哦，你在21世纪弄清楚了这些事情，然后你就……”

我有时会思考这两种观点。有些人认为，“我们离知识的终点不远了。”我们基本上已经掌握了全貌，而这个全貌就是知识完全在那里。你只需要在科学上成熟，然后一切就会水到渠成。在那之后的一切都将是超级昂贵，不那么重要的事情。然后还有另一种图景，它更像是这种持续的神秘，“哦，天哪，会有越来越多的……”我们可能会期待对我们的世界观进行更彻底的修正。

我被两者都吸引。我们很擅长物理学。我们的许多物理学在预测许多事情方面都相当出色，至少这是我阅读一些物理学家后的印象。谁知道呢？你爸爸是物理学家，对吧？是的，但这并不是我爸爸说的。**肖恩·卡罗尔**（Sean Carroll: 美国宇宙学家和物理学家）有一篇博客文章之类的。他说：“我们确实理解了许多支配日常世界的物理学。我们在这方面做得很好。”我通常对物理学这门学科印象深刻。那很可能是对的。

另一方面，这些人有几个世纪的时间。但我认为这很有趣，它会导致一些不同的东西。有一种**无尽的边疆**（Endless Frontier: 指科学和技术探索的领域是无限的，总有新的发现和创新）的感觉。从美学角度来看，继续发现新事物的想法具有吸引力。至少，我认为你无法获得完整的知识。在某种程度上，你是系统的一部分。知识本身就是系统的一部分。如果你想象你试图拥有关于宇宙未来会是什么样子的完整知识……我不知道。我不太确定那是否是真的。

它具有**停机问题**（Halting Problem: 计算机科学中的一个著名问题，证明不存在一个算法能够判断任意程序是否会在有限时间内停止运行）的某种特性，对吧？有点**循环性**（Loopiness: 指事物具有循环或递归的性质）。那里可能有一些**不动点**（Fixed Points: 数学概念，指在某个变换下保持不变的点），你可以说：“是的，我要那样做。”我至少有一个问题，当人们想象知识的完成时，那到底能有多好？我不确定。

你在关于**乌托邦**（Utopia: 指一个理想的、完美的社会或世界）的文章中有一段话。我能请你快速读一下那段话吗？“我倾向于认为，乌托邦，无论多么怪异，在某种意义上也是可识别的；如果我们真正理解并体验它，我们会在其中看到同样的东西，那曾让我们在很久以前第一次触及爱、喜悦、美丽时，猛然坐直；我们会在篝火前感受到点燃它的余烬的温度。我想，那会是一种**记忆**。”

这如何融入这个图景？这是一个好问题。如果我没有任何一部分能将其识别为好的，那么我不确定它对我来说是好的。这是一个问题，即你需要什么才能让你的某一部分将其识别为好的。但如果真的没有那一部分，那么我不确定它是否反映了我的价值观。

你可以做一种**同义反复**（Tautological Thing: 指一个陈述在逻辑上必然为真，因为它重复了相同的信息）的事情，就像“啊，如果我经历了那些导致我发现什么是好的过程，我们称之为反思，那么它就是好的。”但根据定义，你最终到达那里是因为……你明白我的意思吗？

如果你逐渐将我变成一个回形针制造者，那么我最终会说：“我看到了光明，我看到了真正的回形针。”这就是关于反思的复杂之处。你必须找到一种方法来区分那些保留你所关心事物的**发展过程**和那些不保留的**发展过程**。这本身就是一个充满争议的问题。它本身就需要你对你所关心的事情以及你认可的**元过程**（Meta-processes: 指关于过程的过程，即对过程进行管理、控制或反思的过程）等等采取某种立场。

但你绝对不应该仅仅说：“最终的事物认为自己做对了，这不足以作为标准。”那与它已经完全脱轨的情况是兼容的。

### 权力塑造价值观与社会和谐

你在你的一篇帖子中有一句非常有趣的句子。你说：“我们的心实际上是被权力塑造的。所以如果我们所爱的东西也强大，我们根本不应该感到惊讶。”那里发生了什么？你那句话是什么意思？

那篇文章的背景是，我谈论的是一个模糊的集群，我在文章中称之为“**友善/自由主义/边界**”。这是一套相对更基本的合作规范，涉及尊重他人的边界、合作、差异间的和平与宽容等等，这与你偏爱的物质结构相对立，后者有时是人们在AI风险背景下使用的价值观范式。

我谈了一段时间这些规范的**伦理美德**（Ethical Virtues: 指在伦理上被认为是良好或值得称赞的品格特质）。我们为什么会有这些规范？这些规范的一个重要特点是它们有效且强大。安全的边界可以节省浪费在冲突上的资源。自由社会通常更适合居住。它们更适合移民。它们更具生产力。友善的人更容易互动。他们更容易进行贸易等等。

看看我们在政治层面拥有各种政治制度的原因，以及更深入地审视我们的进化历史和我们的道德认知是如何构建的。似乎很清楚，各种形式的合作和**博弈论动态**（Game Theoretic Dynamics: 指在博弈论框架下，不同参与者之间互动和决策的演变过程）以及其他事物塑造了我们现在，至少在某些情况下，也将其视为一种**内在价值**（Intrinsic Value: 指事物本身所具有的价值，而非作为达到其他目的的手段）或**终极价值**。

这些在我们的社会中具有**工具性功能**（Instrumental Functions: 指作为实现其他目标或目的的手段所发挥的作用）的价值观，也在我们的认知中被**具体化**（Reified: 指将抽象概念或关系视为具体事物或实体）为内在价值。我认为这没问题。我不认为这是一种**揭穿**（Debunking: 指揭示某种信念、理论或现象的虚假性或不合理性）。你所有的价值观都是某种程度上“粘住”并被视为终极重要的东西。在系列文章的背景下，我谈论的是**深层无神论**（Deep Atheism: 指一种彻底的无神论观点，不仅否定上帝存在，还可能质疑宇宙中是否存在任何超越自然的力量或目的）以及我们所追求的与自然所追求的或纯粹权力所追求的关系。

很容易说：“嗯，有回形针，那只是你可以引导的一个方向，快乐是你可以引导的另一个方向之类的。这些只是任意的方向。”而我认为我们的一些其他价值观更围绕着合作以及那些有效、功能强大且有力量的事物。

所以这就是我的意思。从某种意义上说，自然比你想象的更站在我们这边。我们的一部分是由自然的方式塑造的。它在我们体内。现在，我认为这不一定足以让我们战胜**灰蛊**。我们的价值观中内置了一定程度的力量，但这并不意味着它会是任意竞争性的。记住这一点仍然很重要。在将AI融入我们社会的背景下，记住这一点很重要。我们一直在谈论这方面的伦理，但也存在**工具性**（Instrumental: 指作为实现某个目标或目的的手段）和**实践性**（Practical: 指与实际行动和应用相关的）的原因，希望与具有不同价值观的AI建立社会和谐与合作。

我们需要认真对待这一点，并思考如何以一种真正合法的方式做到这一点，这是一个将这些生物公正地融入我们文明的项目。既有**公正性**（Justice: 指道德或法律上的公平和正义）部分，也有“它是否与人兼容？它是否划算？它对人来说是否划算？”就我们非常担心AI反叛之类的而言，你可以做的一件事就是让文明对某人来说变得更好。这是我们实际上构建许多政治制度和规范等的一个重要特征。这就是我那句话所表达的意思。

好的。我认为这是一个很好的结束点。**乔**，感谢你来到播客。我们讨论了系列文章中的思想。如果人们没有读过这个系列，他们可能不会欣赏它写得有多美。我们没有涵盖所有内容，但有很多非常有趣的想法。作为一个与人讨论AI一段时间的人，我遇到了一些在其他任何地方都没有遇到过的东西。显然，AI讨论的任何部分都没有写得这么好。收听播客版本，也就是你自己的声音，是一种真正美好的体验。所以我强烈推荐大家这样做。他们可以在 **joecarlsmith.com** 上访问这些内容。**乔**，非常感谢你来到播客。

谢谢你的邀请。我真的很享受。