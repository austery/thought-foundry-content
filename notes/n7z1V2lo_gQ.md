---
area: society-systems
category: business
companies_orgs:
- Google
- Meta
- Gartner
- Forrester
- Bilibili
date: 2025-10-31
draft: true
guest: ''
insight: ''
layout: post.njk
media_books: '[]'
people: '[]'
products_models:
- Map Reduce
- BigQuery
- Hadoop
- Apache log
project:
- ai-impact-analysis
- systems-thinking
- historical-insights
series: ''
source: https://www.youtube.com/watch?v=n7z1V2lo_gQ
speaker: Peter Pang
status: evergreen
summary: 本文回顾了大数据时代的兴起与衰落，将其与当前的AI热潮进行对比。文章分析了大数据未能实现其预言的原因，包括对数据规模的严重高估、硬件性能的快速提升、以及人类在数据驱动决策中的干预和不信任。通过深入探讨这些失败案例，作者强调了从历史错误中学习的重要性，以期对未来的技术热潮保持更冷静和客观的判断，避免重蹈覆辙。
tags:
- data
- data-driven-decision-making
- dynamic
- learning
- technology
title: 大数据时代的兴衰与技术预测的反思
---

### 大数据时代的兴衰：一场未兑现的预言

“**大数据**”（Big Data: 指的是处理和分析海量、多样化、高速增长的数据的技术和方法）这个词，我已经很久没有听人提起了。曾几何时，它也是风靡全球的技术热点，就像现在的AI一样，每个企业都在想方设法地把大数据技术加入到自己的系统中。2010年代是大数据热潮的顶峰，我恰好也是在那个时间踏入职场，恰好进入了大数据部门，也算是在一线亲身经历了那个时代的起落。

大数据时代死了，甚至可以说它没有活过，因为它的预言都没有成真。我们没有迎来指数级增长的数据规模，也没有进入完全由大数据驱动的商业时代。大多数企业靠巨资投入建设的大数据系统，沦为吃灰的数据仓库，只是偶尔吐出几份统计报表给领导看。它的没落是一个很好的教学案例，它体现出我们是如何错误地预测技术的未来，那些过于乐观的、过于客观的估算，是如何被现实所击破的。这些错误，或许能让我们对现在的或者以后的技术热潮有更冷静的判断。

### 数据规模的误判：硬件进步与“死亡交叉”

迈入21世纪，技术圈的第一件大事就是互联网泡沫的破裂。让计算机行业从废墟中重生的，是搜索引擎、电商平台、社交网络等服务的兴起。这些简单好用的服务，在互联网的帮助下迎来指数级的用户增长，由此带来的数据规模也超过了当时计算机硬件的处理能力。所以基于分布式计算的**大数据框架**（Big Data Framework: 一种用于高效存储、处理和分析大规模数据集的软件系统或工具集）应运而生。在科技巨头们的大力宣传下，这些技术席卷了全世界各地的大中小企业。某种程度上来说，我们是被“吓进”大数据时代的。

一张曲线图在当年就如同圣旨一般，出现在所有企业的讨论会上，它所对应的就是大数据的第一个预言：我们即将迎来指数级增长的数据，数据的规模将会是天文数字般的存在。在当时没有人敢反驳这张图，你想想，你反对它，是不是就间接地认为自己的公司没有未来、没有增长空间，做不到像Google和Facebook那样。而且这张图很巧妙地结合了胡萝卜和大棒，一方面它让企业为如何处理即将到来的大数据感到焦虑，另一方面，它也给企业带来了无穷的想象空间，这么多数据，能带来多少新的商机啊。

当时最大的瓶颈，就是计算机硬件的性能。即使硬件的性能发展严格遵循**摩尔定律**（Moore's Law: 指集成电路上可容纳的晶体管数量大约每两年翻一番，性能也随之提升的经验法则），做到指数级增长，比起预言中数据将会出现的指数级增长，还是完全不够用。**Scale up**（纵向扩展: 通过增强单台服务器的性能来提升处理能力）不行，我们就只能**scale out**（横向扩展: 通过增加服务器数量来提升处理能力），通过分布式技术、通过堆叠硬件的方式，跟上数据的增长速度。

现在作为**事后诸葛亮**（Hindsight Bias: 指人们在事情发生后，认为自己事先就能预料到结果的一种认知偏误；此处引申为马后炮式的分析）来说，当年的我们想象力是真的丰富。因为20年过去了，我们并没有看到数据有指数级的增长。**TB**（Terabyte: 数据存储单位，1TB = 1024GB）这个单位依然稀罕，**PB**（Petabyte: 数据存储单位，1PB = 1024TB）和**EB**（Exabyte: 数据存储单位，1EB = 1024PB）更是只得在一些科研项目里面会看到。反而是计算机硬件的性能，稳扎稳打，一路上涨。在不知不觉中，这两条曲线交叉了。这个**死亡交叉**（Death Cross: 在金融技术分析中，指短期移动平均线向下穿过长期移动平均线，预示市场可能下跌；此处引申为两条发展曲线的交汇点，象征着一个转折点），让很多所谓的大数据问题，现在都可以通过把所有的数据全部读进一台机器的内存来处理。

其中一个经典的例子就是：曾经国际象棋比赛的数据统计需要用7个节点的**Hadoop**（Hadoop: 一个开源的分布式计算框架，用于处理和存储大规模数据集）集群，花将近30分钟才能够算出结果。后来有人发现，这些原始数据的总量，已经塞不满一台普通笔记本电脑的内存了。所以现在，你完全可以把原始数据一次性读进内存，然后用一行bash命令，在12秒内完成一模一样的统计。

### 被高估的数据量与被低估的人性

那么当年的我们为什么会错得那么离谱呢？错误之一就是：我们严重高估了自身的数据规模。大数据时代的开端，可以说是2004年Google发表的**Map Reduce**（MapReduce: Google开发的一种编程模型，用于大规模数据集的并行处理）论文。Google的产品经验和技术分享是大数据预言的一个重要基石，因为大家讨论的是未来，那总得有参照物，所以这个参照物就落到了分享最多的Google身上。只有在一切都尘埃落定的时候，大家才发现这个参照物有点不现实。

Google的**BigQuery**（BigQuery: Google提供的一种全托管、无服务器的企业级数据仓库，用于大规模数据分析）系统是大数据时代的一个里程碑产品。而BigQuery的前总监曾经在一个技术论坛上分享过，他们的活跃用户里，数据储存量的中位数甚至都不足100GB。还有根据**Gartner**（Gartner: 全球知名的信息技术研究和顾问公司）、**Forrester**（Forrester: 全球知名的市场研究公司）这些国际顶级市场分析组织的分享，绝大多数企业的数据仓库体量达不到TB量级。所以在市场分析行业，百GB依然是标配的基准单位。

而错误之二就是：实际有用的数据比想象中更少。“先上车后补票”的心态，在任何一次技术热潮中都是大部分企业的第一想法。为了不让上级责怪自己为什么在新的时代浪潮面前无动于衷，决策层永远都是先把新技术装上，至于怎么用，以后再思考。现在的AI时代是这样，当年的大数据时代也是这样。所以当时几乎所有的大数据项目都是先立项，先把仓库建起来。至于要收集什么数据，因为大家都不知道之后要分析什么、怎么分析，所以为了避免因为收集不齐全，影响了以后的分析，那就不管三七二十一，把能**埋点**（Data Embedding/Tracking: 在应用程序或网站中预设代码，用于收集用户行为或系统运行数据）的地方都埋了，能收集的数据都收集了。

当年我所在的大数据部门，每天就看着成吨成吨的**Apache log**（Apache Log: Apache HTTP服务器生成的日志文件，记录了服务器的访问和错误信息）被运进来。网站上每一个组件的每一个动态，用户的每一个动作都被记录下来了。但这些log有多少被用上呢？很少很少，因为大部分都是噪音，没什么价值。大数据的“大”其实指的是两个维度，一是储存量，二是计算量。在大数据预言中，这两个量会肩并肩地一起上涨，但现实发展的结果是储存量随着日常运营线性增长了，但计算量基本维持在一个相对固定的区间。

造成这个结果，是因为在当代的大多数行业、大多数业务场景里，我们都更关心最近发生的事情。这是因为技术的发展给业务带来一个显著的变化：决策周期的缩短。以前一个季度，一个月才能响应，现在一个星期，甚至一天就能做出调整。正因为整体的决策效率的提高，旧数据变得没有那么大的吸引力了。就算我们勤勤恳恳地储存了十几年的数据，但是最受欢迎的永远都是最近24小时、最近7天、最近30天。我的这些个人观察和Google BigQuery的内部数据是一样的，BigQuery客户绝大多数的查询和分析都只针对24小时以内的数据，年纪大于一个月的数据，基本上就只在仓库里吃灰了。所以即使客户储存的数据已经达到TB量级，他们的计算量也还停留在GB级，甚至90%的查询所调用的数据都不超过100MB。

大数据最重要的预言其实不在技术层面，而在商业层面：“用科学的、客观的数据分析”，“给企业提供未来发展的引导”，“让拍脑袋做决定，变成用数据说话”。但是把如此重要的权力交给机器，没有多少领导会愿意接受。决策越是重要，对公司运作、预算分配、业绩考核的影响越大，人们越是要掌握在自己手上。在这场权力的游戏里，领导想要的不是数据的结论，而是让数据说出符合自己要求的结论。

我以前有一个做数据分析的同事就有很深刻的体会，他说：就算他的分析报告写得再好，分析再准确，如果结论不是上司所要的，对方就会揪着一些无关大雅的细节反复批评他。反之，如果结论就是上司想要的，对方甚至都不关心数据准不准。有几次他的上司甚至直接给他下达命题作文式的任务。做过数据分析的都知道，要编造一个结论是很容易的。就像那些做社会调查问卷的，故意挑选一些特定的人群，稍微调整一下话术，就能够让结论倾斜到自己想要的效果上。数据分析沦为管理层的传声筒，它的真正价值就会被埋没。

人类的另一个干扰因素，是对数据驱动的业务模式信心不足。于是我们会看到在很多企业里，数据驱动了，但又没有驱动。Bilibili的推荐系统就是一个很不错的例子。这种视频平台的推荐流，在你的想象中，可能是一个严丝合缝、精密运作的全自动流水线，是根据你的观看历史，根据博主的视频质量，根据准确捕捉的大数据，来决定下一个展示的视频。但实际上，这些被我们称为“推荐位”的曝光机会，很多都是人工安排的。这里会涉及到社区运营、产品部门等各种利益相关单位，所以整个流程繁琐且冗长，一个视频从被挑选出来，到出现在推荐位上，往往需要几天到一个星期。这种在数据驱动的自动化流程里插入人工干预的流程，是很多企业都会做的事情。有些还是归根于权力的分配，有些则是对于数据结论的不信任。不管是哪种原因，都会导致大数据的价值被进一步压缩。久而久之，大数据的主导权被剥夺，就会沦为一个普通的工具。

### 从大数据失败中汲取经验：拥抱反思，避免重蹈覆辙

我们最终没有迎来大数据时代，但这个旅途是值得的。它留下了丰厚的技术遗产和全新的商业模式。即使是我们犯下的错误：对未来发展曲线的估算错误，对技术落地的障碍的判断错误，也是很宝贵的经验。这也是为什么我喜欢研究失败，为什么这个系列有那么多期视频讨论的都是失败的案例，各种被淘汰的技术、错误的观念、代码漏洞等等。我们总是被教育正确的做法、正确的答案，但排除错误答案的方法更是不可或缺的。否则我们怎么样才能避免踩进下一个坑里呢？