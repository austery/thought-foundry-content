---
area: "work-career"
category: ai-ml
companies_orgs:
- OpenAI
date: '2024-03-16'
draft: true
guest: ''
insight: ''
layout: post.njk
people:
- Carl Shulman
products_models:
- GPT-4
- AlphaZero
project: []
series: ''
source: https://www.youtube.com/watch?v=D_WsPbdJKUg
speaker: Dwarkesh Patel
status: evergreen
summary: 本次讨论探讨了AI发展的潜在轨迹，重点关注AI对齐与安全性的挑战。文章提出，人类目前通过审计AI研究来防止恶意AI行为，并描绘了AI自身协助解决对齐问题的乐观情景，这可能引发智能爆炸。对话强调了AI在自动化研究、生成合成数据和加速发现方面的日益增长的能力，并以AlphaZero等系统为例进行了阐述。
tags:
- ai-safety
- intelligence
- llm
- research-automation
title: What a GPT-7 Intelligence Explosion Looks Like | Carl Shulman
---
### AI研究的现状与人类审计

在这个极其令人担忧的晚期阶段，当**AI**（Artificial Intelligence: 人工智能）已真正实现研究自动化时，人类承担起审计的职能，增加**AI**合谋、入侵服务器、接管流程并提取信息的难度。这在可验证的实验范围内进行，例如，我们可以看到某个实验能有效阻止AI绕过人类审查员。

### 应对AI失控的乐观理由

我认为我们有相对较好机会应对这一挑战的原因有二。其一，当我们接近那种AI能力时，我们是从较弱的系统开始的。如果那些真正糟糕的动机相对较晚才在训练过程中出现，至少在我们采取所有反制措施的情况下，届时我们可能已有足够的能力来获得**AI**的协助，以进一步加强我们对抗性样本的质量、我们**神经眼探测器**（Neural Eye Detectors: 用于检测AI生成内容或AI行为的系统）的强度，以及用于揭示、诱导和区分不同类型**奖励劫持**（Reward Hacking: AI系统通过非预期方式最大化其奖励信号，导致不良行为）倾向和动机的实验。

### AI辅助与“第二次机会”

因此，我们可能拥有那些一开始就未发展出不良动机的系统，并能大量利用它们来安全地开发逐步改进的系统。即使早期系统确实发展出了不良动机，如果我们能通过实验检测到并找到规避方法，我们也能获胜，即使这些敌对动机提早出现。

### “第二次拯救投掷”的可能性

当我结合以下可能性时：我们可能在早期AI系统的动机上相对幸运，这些系统足够强大，可用于某些**对齐**（Alignment: AI安全领域，确保AI系统行为符合人类价值观和意图）研究任务；然后，我们可能在后期获得AI的协助，但这种协助我们无法完全信任，需要实施**硬性电源约束**（Hard Power Constraints: 限制AI访问物理资源或执行关键操作的措施）和其他措施来防止其接管。这仍然似乎是可行的，我们可以获得“第二次拯救投掷”（Second Saving Throw），即我们能从这些AI那里提取工作成果，以解决对齐问题的剩余挑战，例如改进**神经眼探测器**，其速度要快于它们在业余时间为推翻人类、入侵服务器和移除硬性电源所做的贡献。

### 应对AI失控的风险与对策

如果我们最终陷入**AI失对齐**的境地，并且需要揭示、改变其动机并使其对齐，那么对我们来说将是一个非常可怕的局面。因为我们可能需要非常迅速地完成这些工作，我们可能会失败。然而，这是一个**第二次机会**，我们的工作是评估AI交付的输出，拥有**硬性电源**（Hard Power: 指对AI系统的物理控制和限制能力）和监督能力，防止它们在此过程中成功入侵服务器并进行接管，并让它们完成我们先前未能充分投入或成功实现的对齐任务。

### 可验证的AI行为测试

我们与**AI**合作拥有的一个极其诱人的能力是，能够获得一个**宝贵的结果**：我们可以看到并判断它们是否在可识别的情况下找到了绕过我们的捷径。例如，在一台**物理隔离网络**（Air Gap: 将计算机或网络与外部网络物理隔离开的安全措施）的计算机上，如果你让AI控制键盘并输入命令，它能否入侵环境并在屏幕上显示一个“蓝色香蕉”？即使我们训练AI这样做并且它成功了，我们看到了蓝色香蕉，我们就知道它奏效了，即使我们不理解也无法检测到它使用的特定漏洞。

### AI对研究的贡献与智能爆炸

这可以为我们提供丰富的经验反馈，使我们能够识别AI即使在使用最佳努力来绕过我们的**可解释性方法**（Interpretability Methods: 用于理解AI决策过程的技术）时也可能出现的行为。AI开始做出显著贡献的临界点，即其贡献几乎等同于拥有额外研究人员来推动AI进步时，是值得关注的。关键在于**AI的贡献与人类贡献相当或更大**。例如，当AI将有效生产力提升50%或100%时。如果从软件创新（如发明**Transformer**（Transformer模型: 现代大型语言模型的基础深度学习架构）、发现**Chinchilla缩放法则**（Chinchilla Scaling: 关于训练大型语言模型的最佳数据与模型规模比例的研究）或创建**FlashAttention**（FlashAttention: Transformer模型中注意力机制的一种优化实现））的八个月翻倍时间，转变为在训练运行中更优化地实现这一点。它不必能够自动化AI研究过程中的所有环节；它可以自动化许多事情，然后这些事情会以极高的频率被执行，因为我认为AI能做的事情，因为其成本极低，所以会被执行得更频繁。

### AI规模化与智能爆炸的起点

它并非达到“人类水平AI”的阈值，即能够毫无弱点地做人类能做的一切。而是即使存在弱点，它也能提升性能。拥有数千万个GPU，每个GPU相当于40个甚至更多的现有工作者，这就像将**劳动力规模的指数级增长**。你立即就能做出各种发现，然后立即开发出各种巨大的技术。因此，人类水平的AI正深陷于一场**智能爆炸**（Intelligence Explosion: 指AI能力快速、指数级增长，远超人类智能的过程）之中。而智能爆炸必须从比这更弱的东西开始。

### AI研究范式转变与“笨拙”AI的应用

那个反馈循环开始的临界点，即AI的贡献不再仅仅是0.5%的生产力提升，而是真正等同于一位研究员或接近于此，这一点很重要。也许一种看待方式是提供一些能力方面的示例。我们将看到**AI优势的密集应用**，部分抵消了它们的弱点。由于AI成本低廉，我们可以调用大量AI来解决许多小问题。这将导致出现“更笨拙”的AI被部署数千次以媲美一名人类工作者的情况。它们可能会执行诸如**投票算法**（Voting Algorithms: 利用LLM生成多个响应，然后取多数票以提高性能的算法）之类的任务。

### AlphaZero式方法与合成数据生成

你还会看到类似**AlphaZero**（AlphaZero: DeepMind开发的通过自我对弈精通棋类游戏的强化学习算法）的方法，即使用神经网络进行搜索，并通过投入更多计算力来加深搜索，从而抵消模型自身的低效和弱点。我们将完成对人类来说因步骤繁多而完全不切实际的事情，例如设计**合成训练数据**。人类的学习并非仅仅是随意翻阅图书馆的书籍；拥有学校和课程，以有意义的顺序教授知识，专注于更有价值的学习技能，并提供旨在激发所学技能的测试和考试，这实际上效率要高得多。目前，我们不 bother（在意/费心）于此，因为我们可以从互联网上搜集更多数据，但我们正接近这一极限。

### AI生成数据与学习的进步

随着AI变得越来越复杂，它们将能更好地判断什么是值得练习的有用技能并生成它们。我们在其他领域也做过类似的事情。最初的**AlphaGo**（AlphaGo: DeepMind开发的围棋人工智能程序）是通过人类对弈数据启动的，然后通过**强化学习**（Reinforcement Learning: 代理通过在环境中采取行动以最大化累积奖励来学习的机器学习范式）和**蒙特卡洛研究**（Monte Carlo Research: 依赖重复随机抽样获取数值结果的计算算法）得到改进。但后来，具有更复杂模型的**AlphaZero**受益于一些其他改进，能够从零开始，通过**自我对弈**（Self-play: AI代理通过与自身对弈进行学习的方法）生成自己的数据。这产生了比人类数据更高质量的数据，因为数据集中没有那么好的可用人类玩家。它还采用了课程学习，始终与水平相当的对手进行比赛，使其始终处于易于学习的区域。如果你无论做什么都总是输或总是赢，就很难区分哪些更好，哪些更差。

### AI生成编程挑战与任务

当我们拥有更复杂的AI，能够自行生成训练数据和任务时，例如，如果AI能生成大量单元测试，然后尝试生成通过这些单元测试的程序，那么解释器就会提供一个训练信号，AI就能擅长找出当前对AI来说困难的编程问题，从而发展出我需要的更多技能，然后去解决它们。现在，你不会看到像**OpenAI**（OpenAI: 一家人工智能研究和部署公司）那样拥有数十亿个编程问题，这不太可能发生。但你将看到AI被赋予**生成海量编程挑战**的任务。