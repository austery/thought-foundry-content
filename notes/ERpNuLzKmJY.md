---
author: Dwarkesh Patel
date: '2024-05-20'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=ERpNuLzKmJY
speaker: Dwarkesh Patel
tags:
  - llm
  - prompt-engineering
  - instruction-following
  - conversational-ai
  - rlhf
title: ChatGPT诞生内幕：OpenAI联合创始人John Schulman的讲述
summary: 本次访谈深入探讨了大型语言模型（LLM）的发展历程，从OpenAI的基础模型到指令遵循模型，再到ChatGPT的诞生。演讲者**John Schulman**回顾了早期模型面临的挑战，如提示复杂性，以及Google等公司在聊天AI领域的探索。他重点介绍了**GPT-3.5**和**GPT-4**的演进，**强化学习（RL）**在模型训练中的作用，以及如何通过混合指令和聊天数据，最终打造出更易用、行为更合理的**ChatGPT**。访谈还触及了微调API的局限性以及模型在可靠性和事实性方面的挑战。
insight: ''
draft: true
series: ''
category: ai-ml
area: tech-work
project:
  - ai-impact-analysis
people:
  - John Schulman
companies_orgs:
  - OpenAI
  - Google
products_models:
  - GPT-3
  - GPT-3.5
  - GPT-4
  - Lambda
  - Mina
  - ChatGPT
media_books: []
status: evergreen
---
### LLM的未来与聊天机器人

你是在什么时候，**OpenAI**联合创始人**John Schulman**，意识到**大型语言模型（LLM）**是未来的方向，并且开发一个像**ChatGPT**这样的聊天机器人，或者某种指令遵循的方式来指导它们，会是一件有用的事情？在**ChatGPT**出现之前，**OpenAI**就已经有了**指令遵循模型（Instruction Following Models）**。当时的想法是，我们有基础模型（base models），人们可以通过复杂的方式来提示它们。但这些模型很难提示，它们本质上是进行自动补全，所以你需要设置一个非常好的提示，并包含一些示例。

### 指令遵循模型的演进

因此，**OpenAI**的团队正在努力，他们尝试在基础模型的基础上，让模型更容易被提示。这样，如果你只是写一个问题，模型就能直接回答问题，而不是反问你更多问题。所以，我们有了这些指令遵循模型，它们有点像基础模型，但使用起来更简单一些。这些模型是部署在API中的原始模型，或者是在**GPT-3**之后推出的下一代模型。

### 早期聊天AI探索

与此同时，也有很多人在思考‘聊天’（chat）的可能性。比如**Google**发表了一些论文，提到了**Lambda**和更早的**Mina**，他们开发了聊天机器人。根据论文中的例子来看，这些模型更多用于有趣的场景，比如让模型扮演某个角色，进行角色扮演，而不是像‘帮我重构代码’这样具有实际功能的任务。

### Web GPT与会话式AI

所以，确实有很多人在思考聊天。我之前也参与过一个关于聊天的项目，叫做**Web GPT**，它更侧重于利用网络浏览和检索来回答问题。当你进行问答时，模型确实更适合处于聊天模式，因为你总是想追问，或者有时模型需要提出一个澄清性的问题，因为原始问题可能存在歧义。因此，在开发了**Web GPT**的第一个版本后，我们很清楚下一版本应该是会话式的。

### 基于GPT-3.5的开发

于是，我们开始着手开发一个会话式聊天助手。这个助手是基于**GPT-3.5**构建的，它在2022年初完成了训练。**GPT-3.5**在语言和代码方面表现出色，所以我们很快意识到它在编程辅助方面也非常强大，这是让我们感到兴奋的一点。我们为此投入了大部分时间，还加入了网络浏览功能。但后来我们逐渐淡化了这项功能，因为模型的内部知识已经足够丰富，网络浏览并不是它最吸引人的地方。

### GPT-4与RLHF的挑战

之后，我们考虑进行公开发布，此前它已经经过一段时间的内测，并向亲友开放。但就在那时，**GPT-4**在当年8月完成了训练。实际上，**OpenAI**当时最核心的**强化学习（RL）**工作是指令遵循模型，因为这些模型将被部署到生产环境中。所以，**GPT-4**的早期微调就使用了这整套技术栈。这些模型非常出色，大家在看到**GPT-4**的指令微调模型后都非常兴奋。它们偶尔能产生惊人的输出，但模型也明显不太可靠，经常出现幻觉，有时会给出非常离谱的输出，显然还没有完全准备好公开发布，尽管它本身非常优秀。

### 融合指令与聊天数据

我想，在那之后，大家似乎暂时忘记了聊天（chat）这个分支。但我们继续推进，并将指令遵循数据和聊天数据混合在一起，试图融合两者的优点。我认为，聊天模型显然更易于使用，行为也更合理，模型能更好地认识到自身的局限性。

### 任务定义的差异

聊天的另一个优势在于，我们之前的指令模型所面临的任务，比如‘以一种友好的或有帮助的方式完成这段文本’，这是一个定义模糊的任务。我认为这个任务既让模型感到困惑，也让负责数据标注的人类感到困惑。而对于聊天，人们似乎有一种直观的理解，知道一个‘有帮助的机器人’应该是什么样的。因此，更容易让人们理解模型应该做什么。

### 模型个性的塑造

因此，结果是模型拥有了更连贯的个性，并且更容易获得鲁棒的、合理的行为。那么，任何人是否都可以利用你们公开的微调API来制造出类似ChatGPT的产品？不完全是。我想，如果你当时能够使用**GPT-3.5**进行微调，你或许可以做出一个相当接近的东西。但我不太确定你是否能只进行一次微调，使用纯粹由人类编写的数据进行训练。我认为你需要进行多次迭代。

### API微调的局限性

如果你不采用**强化学习（RL）**（我们确实使用了），你就需要进行某种迭代式的监督微调，让**人类编辑模型生成的输出**。因为如果只用人类生成的数据训练，即使质量很高，模型也很难完美地拟合，因为它可能无法生成类似的数据。所以你需要进行迭代，使其更接近RL的模式。我认为，如果你那样做，或许能得到一个相当接近的结果，但这并非易事。

### 另一RL模型的优劣

另外，我们还有一个通过**RL**训练的指令遵循模型，它比**ChatGPT**早一点发布。如果你给它加上一个聊天的‘包装器’，也能得到一个相当不错的结果。但那个模型在某些方面有所不同，比如它擅长写作和诗歌，但它不太擅长认识到自身的局限性以及事实的准确性等方面。