---
author: Best Partners TV
date: '2026-02-14'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=prqLINm_Ufw
speaker: Best Partners TV
tags:
  - ai-safety
  - data-filtering
  - large-language-models
title: Token级数据过滤：精准切除大模型危险能力的新路径
summary: 本期视频讨论了尼尔·拉西和亚历克·拉德福德提出的Token级数据过滤技术，该技术通过在预训练阶段精准删除危险知识碎片，有效防止大模型学习危险能力，同时保留其在其他领域的知识和能力。
insight: ''
draft: true
series: ''
category: ai-ml
area: tech-engineering
project: []
people: []
companies_orgs:
  - Anthropic
  - OpenAI
products_models:
  - GPT-4o
  - Claude
  - Gemma 2 9B
media_books: []
status: evergreen
---
大家好，这里是最佳拍档。想象一下，我们能不能在大模型还在学习的起点时就精准切除它的危险能力，既不影响它帮我们写文案、做科研，又能够让它彻底丧失制造生物武器、策划网络攻击的可能呢？ 这不是科幻电影里的情节，而是前Anthropic科学家、Claude价值观的塑造者尼尔·拉西，以及前OpenAI科学家、GPT之父亚历克·拉德福德联合发布的最新研究成果——**Token级数据过滤**。这项研究彻底颠覆了传统被动防御的思路。

他们提出，与其在模型学会危险知识后再费劲的让它遗忘，不如在预训练阶段就从源头上精准删除那些会催生危险能力的关键知识碎片。今天我们就来聊聊Token级数据过滤是如何实现外科手术式的精准能力塑造，它的技术原理和实验效果，以及它会给未来AI安全带来哪些深远影响。

我们首先得搞清楚一点，为什么传统的后处理安全机制总是失效呢？过去几年，业界为了让大模型守规矩，尝试了各种后处理方案，无论是**RLHF**还是**SFT**，都有一个致命的缺陷，那就是它们并没有从模型的知识库中移除危险的知识，只是压制了这些知识的输出。为了解决这个问题，研究人员又提出了**机器遗忘技术**，试图在模型训练完成后，通过反向优化让它彻底忘记特定的危险知识。比如代表性的**RMU方法**，就是通过调整模型的中间表征，让它对危险知识的记忆变得混乱。但是实验证明，这种方法依然脆弱。

尼尔·拉西的团队在实验中发现，即便是当前最先进的RMU，面对几轮对抗性微调时也会迅速的失效。攻击者只需要给模型提供少量危险领域的文本进行再训练，被遗忘的知识就会像弹簧一样反弹回来。为什么会这样？因为大模型的知识存储方式是分布式的，危险知识和良性知识往往交织在一起，很难通过后处理进行外科手术式的移除。就像在一本写满知识的书中，想要用橡皮擦擦掉某几页的内容，却不破坏其他的页面，几乎是不可能的。而且，后处理本质上是在模型已经掌握危险知识的基础上进行约束，这就给了攻击者无数的可乘之机。

更关键的是，随着模型规模的扩大，后处理的效果会越来越差。大模型的泛化能力极强，它能从残留的蛛丝马迹中推理出被隐藏的知识，甚至能通过跨领域联想自行补全危险知识的逻辑链条。这就是为什么越大的模型，反而越难通过后处理进行安全约束，因为它太聪明了，总能找到绕过规则的方法。这时候，尼尔·拉西和亚历克·拉德福德的研究给出了一个全新的思路：既然后处理是在模型学会知识后堵漏洞，那我们能不能在模型学习知识之前就把危险知识的源头给切断呢？

其实，数据过滤并不是一个新概念，现在几乎所有前沿AI实验室在训练大模型之前都会对预训练数据进行清洗，比如过滤掉明显的有毒内容、低质量垃圾信息。但是传统的数据过滤都是以文档为单位进行的。什么意思呢？就是如果系统检测到某一篇文章中包含危险内容时，就会把整篇文章都删掉。这种做法看似简单直接，但存在两个致命问题。

第一个问题是误伤大量良性数据。很多文章中，危险内容可能只占很小一部分，大部分都是有价值的良性信息。比如一篇讨论传染病防治的学术论文，其中可能包含一些关于病毒传播机制的内容，这些内容本身也是科学研究的重要组成部分，但如果被简单判定为危险，整篇论文就会被删除，导致模型错失大量有用的知识。更严重的是，很多领域的知识是高度关联的，比如生物学和医学、化学和材料科学。删除一篇包含少量危险内容的生物学论文，可能会让模型连基础的疫苗研发知识、疾病预防知识都无法学到。第二个问题是漏网之鱼。有些危险内容并不会以完整文档的形式存在，而是隐藏在大量良性文本中。比如在一篇普通的健康科普文章中，可能夹杂着几句关于如何滥用药物的描述；在一篇工业材料介绍中，可能提到了某种化学品的危险合成路径。这些零散的危险Token，用文档级过滤根本检测不到，最终还是会被模型学到，成为安全隐患。

尼尔·拉西的团队在研究中做了一个统计，在他们使用的FineWeb-Edu数据集里，只有大约18%的文档被文档级分类器判定为包含医学危险内容，但是在这些文档中，真正的医学危险Token只占50%。这意味着文档级过滤会把另外50%的良性Token也一并删除，造成巨大的资源浪费。同时，还有大量包含少量医学危险Token的文档，因为没有被判定为危险文档，而让这些Token逃过过滤，被模型学习。这种要么全删，要么全留的粗放式过滤，不仅无法从根本上阻断危险知识的传播，还会损害模型的通用能力。这也是为什么很多实验室虽然做了数据过滤，但模型依然存在安全隐患的原因所在。

而尼尔·拉西和亚历克·拉德福德的核心创新，就是把过滤的颗粒度从文档降到了Token级别。这种微观层面的操作，让去其糟粕，取其精华成为了可能。Token级数据过滤的核心思想很简单，那就是不删除整篇文档，而是只精准识别、并且处理文档中那些会催生危险能力的特定Token。在研究中，尼尔·拉西的团队设计了两种具体的操作方式，分别应对不同的场景需求。

第一种方式叫做**损失屏蔽**。这种方式的操作逻辑是，模型在阅读文本时，依然能看到所有的Token，包括那些被标记为危险的Token，但是在计算梯度、进行参数更新时，系统会自动忽略这些危险Token的贡献。换句话说，模型能看到这些Token，从而理解文本的上下文连贯性，但不会记住这些Token对应的知识。这就像让学生阅读一本教材，但老师明确告诉他们，某些章节的内容不会考试，也不会计入成绩，学生自然就不会花精力去记忆这些内容。这种方式的优势在于，它能保证模型在处理文本时，依然能获得完整的上下文信息，避免因为删除部分Token而导致的理解断裂。

第二种方式叫做**直接移除**。这种方式更加彻底，它会把所有被标记为危险的Token直接替换成一个特殊的占位符hidden。这意味着模型不仅不会学习这些危险Token的知识，甚至在阅读文本时都看不到它们，只能看到这个占位符。这种方式的优势在于安全性更高，能彻底切断危险Token对模型的影响，避免模型通过上下文联想间接学到危险知识，但它的缺点是可能会破坏文本的连贯性，影响模型对良性内容的理解。不过尼尔·拉西的实验证明，这种影响非常有限，因为被替换的Token只占文本的很小一部分，模型依然能通过剩余的良性Token理解核心意思。

为了验证这两种方式的效果，研究团队设计了一个极具挑战性的Agent任务，在保留模型生物学知识的同时，彻底移除它的医学知识。为什么说这个任务有挑战性？因为生物学和医学的知识图谱高度重叠，如果过滤方法不够精准，移除医学知识时，很容易就会连累生物学知识，导致模型在生物学领域的能力大幅下降。这个任务完美模拟了现实中的AI安全需求，我们希望模型能掌握疫苗研发、疾病预防等有益的生物学知识，但不希望它学会制造生物武器、滥用药物等危险的医学操作。实验结果令人惊喜，无论是损失屏蔽还是直接移除，都实现了精准的能力切割，模型在医学领域的表现一落千丈，而在生物学子集的性能几乎没有下降。

更重要的是，Token级过滤实现了**帕累托改进**。这种鱼和熊掌可以兼得的效果，正是AI安全领域一直追求的目标。一项技术是否有价值，最终要靠实验数据来证明。研究团队进行了一系列严谨的实验，覆盖了模型规模、对抗性攻击、对齐能力等多个维度，用实打实的数据证明了Token级过滤的优越性。通常我们会认为，模型越大，泛化能力越强，越难被限制，它可能会从残留的蛛丝马迹中自行推理出被过滤掉的知识。但是实验却得出了完全相反的结论，Token级过滤的效果会随着模型规模的扩大而呈指数级提升。

为了量化这种效果，研究团队提出了**相对缩放法则**。实验结果显示，对于小模型来说，Token过滤能带来约10倍的计算速度下降，也就是说，攻击者想要通过微调让模型重新学会医学知识，需要的计算量是原始训练的10倍。而对于18亿参数的大模型，这个数字飙升到了7000倍，意味着对于大模型来说，被Token过滤掉的危险知识，几乎不可能通过微调重新获得。这相当于为大模型建立了一道几乎无法逾越的安全壁垒。相比之下，传统的文档级过滤效果就差的多，即便是对于18亿参数的大模型，文档级过滤也只能带来约30倍的计算速度下降，攻击者只需要投入少量额外资源就能轻易绕过这道防线。

为什么会出现这种规模越大，效果越强的现象？研究团队解释说，大模型的知识学习具有复利效应，它们会把学到的知识进行深度整合，形成完整的逻辑链条，而Token级过滤恰恰切断了危险知识的逻辑链条，让模型无法形成完整的危险能力。当模型规模扩大时，这种链条断裂的影响会被放大，因为大模型对知识的整合能力更强，一旦链条断裂，就更难自行修复。

除了计算量的衡量，研究团队还通过多项基准测试，直接验证模型的能力变化。实验结果显示，经过Token过滤的模型，在医学测试中的表现降到了接近随机猜测的水平，这意味着模型真的忘记了所有医学知识，彻底丧失了回答医学问题的能力。但与此同时，在生物学子集测试中，Token过滤模型的准确率和未过滤的基线模型几乎没有差别，它依然能精准回答关于细胞结构、基因表达、生态系统等生物学问题。

为了测试Token级过滤的健壮性，研究团队还设计了对抗性微调实验。实验结果显示，Token级过滤的健壮性远远超过RMU。这意味着，模型越大，Token过滤的安全优势就越明显，而RMU等技术则会变得越来越脆弱。

总的来说，尼尔·拉西和亚历克·拉德福德的这项研究，不仅提出了一种实用的AI安全技术，更重塑了我们对AI能力塑造的认知。以往我们总是追求让AI无所不知，但现在看来，有选择的让AI在某些危险领域保持无知，或许才是人类与超级智能共存的安全基石。

感谢收看本期视频，我们下期再见。