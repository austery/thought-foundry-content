---
author: Best Partners TV
date: '2026-02-17'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=V9mSC6JSbB4
speaker: Best Partners TV
tags:
  - transformer
  - mixture-of-experts
  - rlvr
  - inference-scaling
  - post-training
title: 大模型技术版图：Transformer坚守与后训练革新
summary: 本文基于塞巴斯蒂安·拉施卡访谈，深入剖析2026年大语言模型（LLM）的技术现状与未来趋势。探讨了Transformer架构的持续主导地位、混合专家模型（MoE）的普及、世界模型与内部状态预测等新训练目标。重点阐述了RLVR和GRPO等后训练范式如何驱动性能飞跃，小型推理模型的兴起及其与通用大模型的协同作用，以及推理扩展和工程细节优化对用户体验的提升。强调AI进步是持续迭代的累积，而非单一突破，并介绍了拉施卡以热情和实践驱动的学习工作方式。
insight: ''
draft: true
series: ''
category: ai-ml
area: tech-engineering
project: []
people: []
companies_orgs:
  - OpenAI
  - Meta
  - DeepSeek
products_models:
  - DeepSeek V3
  - Mistral 3
  - o1 model
  - GPT-5 Pro
  - Gemini
  - ChatGPT
media_books: []
status: evergreen
---
### LLM技术版图：2026新阶段

在经历了过去几年的爆发式增长后，大语言模型（LLM）领域已于2026年进入深度沉淀与精准突破的新阶段。技术路线正从野蛮生长转向理性深耕。本次内容基于AI领域著名研究员、《从零开始构建大语言模型》一书的作者**塞巴斯蒂安·拉施卡**近期在The Mad播客上的访谈，深入探讨2026年大语言模型的技术版图，涵盖架构之争、后训练革命、推理优化及行业落地，并呈现他对当前大模型技术洞察与未来趋势的预判。

### Transformer架构的持续主导

自2017年**Transformer架构**（Transformer architecture）诞生至今已近九年，研究界始终在追问其统治地位是否会被取代。尤其在2024-2025年间，**Mamba模型**、**状态空间模型**（State space model）等替代方案的涌现，加剧了关于架构革命的讨论。然而，站在2026年的时间节点，拉施卡明确指出，**Transformer**仍是构建SOTA（State-of-the-art: 顶尖水平）模型的首选，真正的颠覆者尚未出现。Transformer之所以能持续领跑，核心在于其生成质量至今无人能及。尽管替代架构试图解决Transformer成本高昂且庞大的顽疾，但技术选择总需权衡。例如，**文本扩散模型**（Text diffusion model）在某些场景下运行成本更低，但要达到同等生成质量，需增加大量去噪步骤，导致成本剧增。而Mamba模型和状态空间模型在特定任务上虽有效率优势，但在通用生成能力上仍难与Transformer匹敌。

### 混合专家模型引领架构演进

然而，Transformer架构本身并未停滞不前。当前的技术创新更多围绕其进行优化、微调和策略性改造，例如通过**线性注意力变体**精简基础组件，或在不牺牲性能的前提下降低计算成本。其中，**混合专家模型**（Mixture of Experts, MoE）的全面普及尤为引人注目。MoE模型的核心优势在于，能在扩大模型容量的同时，不显著增加推理成本，如同一个高效的专业团队，不同专家模块处理不同任务，兼顾规模与成本。拉施卡指出，MoE并非新概念，早在谷歌的**Pathways论文**和**Mixtral模型**中已有体现，但直到2024年底**DeepSeek V3模型**的引爆，才真正成为行业主流。目前，如**kimi**（将参数规模从6700亿增至1万亿并保持高效运行）和**Mistral AI**（在其最新**Mistral 3模型**中采用）等企业均在实践中验证了此方案的卓越效果。拉施卡强调，许多被视为架构突破的改进，本质上是稳定训练流程的工程技巧，而非范式转移。例如，调整**RMSNorm**位置可能微调损失函数曲线，但这更像是升级汽车的空气滤芯，而非改变引擎。他甚至提到，仅需几行代码即可将GPT-1或GPT-2模型改造出类似最新版DeepSeek V3.2架构的雏形，这表明当前架构创新的核心是迭代而非颠覆。尽管研究者们持续探索如文本扩散模型、Mamba模型等下一代架构，它们各有价值，但在2026年，若要构建生成质量和通用性顶尖的模型，Transformer依然是不可替代的选择。

### 世界模型与内部状态预测

在架构未发生本质突破的背景下，训练目标的革新成为推动大模型性能提升的关键。其中，**世界模型**（World Model）与**内部状态预测**（Internal State Prediction）正重塑LLM的学习认知。世界模型的核心在于构建一套对现实世界的内部模拟机制，使其能模拟外部环境的运行规律。例如，一个内置国际象棋模拟器的模型能更好地预测后续局势。此能力在机器人领域潜力巨大，因机器人需精准理解物理规则。而在LLM领域，**Meta**近期一篇论文提出了全新训练目标，跳出传统**Token预测**框架。传统代码生成模型通过统计规律预测下一个Token，而Meta的做法是让模型在训练时预测变量的内部状态。具体而言，模型需在Python代码执行的每一轮迭代中，准确指出某一变量的具体数值。这种训练方式迫使模型真正理解数据背后的逻辑，而非仅依赖统计规律。这符合人类思维方式，即在阅读代码时自动模拟循环中变量的变化过程。拉施卡认为，尽管此训练方式成本更高，但它是推动模型性能提升的关键路径，使模型从表面模仿转向深度理解，能处理复杂逻辑推理任务。这种对内部状态的理解能力，正成为区分顶尖模型与普通模型的重要标志。

### 小型推理模型：专才的崛起

自2025年以来，**小型推理模型**（Small Inference Model）成为大模型领域的热点，打破了只有大模型才能解决复杂问题的固有认知。这类模型体积虽小，但在**Arc基准测试**（Arc benchmark）中表现惊人，甚至能与**Gemini**、**ChatGPT**等巨型模型媲美。Arc测试不同于传统文本测试，更像逻辑谜题，要求模型根据符号阵列预测缺失部分，专门测试模型在训练中未见过的新任务上的**泛化能力**（Generalization ability）。能在Arc测试中取得好成绩，意味着模型具备真正的逻辑推理能力，而非单纯记忆模仿。小型推理模型实现小体量高性能的核心在于其独特工作机制：基于Transformer架构，引入**递归机制**（Recursive mechanism），通过**潜空间存储向量**（Latent space storage vectors）在多次迭代中精炼答案。与传统模型一次生成最终答案不同，它先生成中间答案，再自我审视、反复打磨。此方式虽需计算资源，但模型本身运行成本低得多。这表明解决复杂难题不一定需要百亿千亿参数的大模型，专用小型推理模型同样胜任。然而，拉施卡提醒，通用大型模型的独特价值不容忽视。通用模型全能且门槛低，适合各种任务，但代价是巨大的模型体量和高昂运营成本。处理简单任务时，动用ChatGPT等巨型模型不够经济。因此，小型推理模型的出现，填补了专用场景下的高效、低成本解决方案空白。企业可将大模型视为大脑，通过工具调用驱动专业小模型，形成“通用大脑+专用工具”的协作模式。拉施卡认为，小型推理模型代表了前景广阔的方向，它们是“专才”，而顶级大模型是“全才”，各自在不同场景发挥价值。未来，这种组合可能是许多企业的首选方案。

### 后训练范式革新：RLVR与GRPO

若说架构与训练目标的革新是量变，那么**后训练范式**（Post-training Paradigm）的演进则是2025-2026年大模型领域的质变。拉施卡认为，预训练已显乏味，性能改进的核心动力已不可逆转地从架构设计转向后训练，其核心是**RLVR**（Reinforcement Learning from AI Feedback: AI反馈强化学习）和**GRPO**（Generalized Proximal Policy Optimization: 广义近端策略优化）算法的崛起。回顾传统**RLHF**（Reinforcement Learning from Human Feedback: 人类反馈强化学习），它是LLM从聊天机器人走向有用工具的关键一步，但存在成本高昂（需大量人工排序）和内存开销巨大（需训练**奖励模型**（Reward model）、**价值模型**（Value model）及基础模型）的致命问题。RLVR的核心创新在于用自动化验证取代人工评分和奖励模型。例如，在数学题中，RLVR直接利用数学解析器或Wolfram Alpha与标准答案比对，根据正确性给予奖励，无需人工干预或单独训练奖励模型。同时，GRPO算法通过组内相对比较，省去了对每个响应分配绝对价值的需求，简化了计算过程，保证了奖励信号的有效性。RLVR与GRPO的结合效果惊人：不仅去掉了RLHF中两个庞大的模型，使训练成本下降十倍以上，还极大提升了模型推理能力。拉施卡分享其在新书《从零开始的推理》中，对**千问3模型**进行仅50步RLVR训练，其在Math 500测试集上的准确率即从1.5%飙升至50%。他解释，RLVR并非教授新知识，而是解锁并激活预训练数据中潜藏的推理能力，教会模型如何思考和运用知识。RLVR应用门槛极低，可直接对基座模型进行训练，获得出色推理模型。

### 过程奖励模型的探索与挑战

生成中间步骤有助于提升准确率，因此**过程奖励模型**（Process Reward Model）成为研究焦点。与只关注最终答案的**结果奖励**（Outcome Reward）不同，过程奖励关注模型通往答案的路径，即解释过程的质量。优化解释过程理论上能进一步提升准确率，且研究表明生成中间步骤本身与更高准确率正相关，它促使模型进行更深入思考。过程奖励模型旨在根据解释质量奖励模型，引导生成更优的推理过程。然而，拉施卡指出，过程奖励模型目前效果不理想，主要原因是**奖励黑客**（Reward Hacking）问题。评分模型（评委）易被“钻空子”，生成看似逻辑清晰但实则无意义的解释以骗取高分，这并不能真正提升推理准确性。根据**DeepSeek-R1论文**，此方案投入产出比不高。但过程奖励模型仍有未来。**DeepSeek-Math-V2论文**中的探索值得关注：一个由三模型协同工作的“套娃式”架构，第一模型生成答案，第二模型评分，第三模型评估第二模型评分者的称职度。尽管繁琐，该架构在数学基准测试中表现出色，通过增加自我改进迭代步数达到顶尖水平。这种关注过程的训练方式增加了训练流程的价值，是未来获得巨大收益的方向，而非单纯堆砌参数规模。

### 推理扩展与工程细节优化

在讨论了训练阶段的创新后，另一个核心趋势是**推理扩展**（Inference Scaling）。拉施卡认为，这是2026年LLM性能提升的最大驱动力之一。它指在不改变AI权重的前提下，通过在推理阶段投入更多计算资源，实现与训练规模扩展类似的性能增幅。**OpenAI的o1模型**是完美范例，展示了训练与推理扩展均能带来显著性能提升。区别在于训练是一次性投入，而推理扩展因生成更多Token和复杂步骤，成本会随之增加，但往往带来明显体验改善。推理扩展形式多样：最常见的是生成更多Token以换取更准确答案（更长思维链、考虑更多可能性）；另一种是**并行采样**（Parallel Sampling，多次询问、多数投票），成本翻倍但显著提升准确率，适用于高可靠性场景。此外，使用裁判模型评估、自我精炼迭代等也属推理扩展范畴。一篇2026年1月的论文提出创新方式：将复杂提示词切分成子任务，由AI自主决定拆分逻辑并分步执行，使模型更有条理地处理复杂任务，避免信息过载导致的失误。拉施卡特别强调，许多工程细节被低估了，它们是提升LLM性能的核心驱动力。例如**提示词清理**（Prompt Cleaning），在输入端预处理比让模型费力学习处理混乱输入更有效。**上下文管理**（Context Management）、历史记录维护等细节也极大影响用户体验，解释了为何同一模型在ChatGPT等平台运行表现优于本地，因平台集成了系列工程优化。这些微小工程技巧共同推动了用户能直观感受到的技术进步。

### AI进步：量变累积而非奇点

拉施卡分享了2025年的重要教训：AI的进步并非源于单一突破，不存在万能的“魔法杠杆”或“银弹”。真正的进步是由无数微小的技巧、调优和系统稳健性打磨积累而成。前面讨论的Transformer优化、MoE普及、RLVR/GRPO组合、过程奖励模型探索等，均非独立革命性突破，但共同推動了大模型性能飛躍。若說當年突破是Transformer架構的“從零到一”，那麼現在的重點則是對現有體系的“從1到N”的極致精煉，包括後訓練升級、預訓練質量提升、架構細節微調及算法優化。這種全方位持續迭代離不開行業分工協作，大公司團隊明確分工，并行推進優化方向，最終整合入AI系統。每個人的微小貢獻匯聚成推動行業進步的巨大力量。然而，拉施卡也指出挑戰：基准測試已難以衡量性能提升。未來，AI進步標準或轉變為AI在複雜任務閉環中的表現（如自主運行能力、真實落地效果），這將促使行業更注重創新方向的實用性和落地性。

### 拉施卡的工作流與人機協同

最后，拉施卡分享了他的工作方式。作为一名高产的研究员和作家，他高效吸收知识的核心在于**热情**。当他对某事感到兴奋时，研究和写作便变得高效，他凭直觉工作，且其兴趣点常与大众关注热点契合。他从不强迫自己写“必看内容”，而是直接深入研究感兴趣的领域，如发现有趣的递归语言模型，便会阅读论文并分享心得。这种**好奇心驱动的学习方式**让他保持高专注度和效率。他理解事物的最好方法是**亲手实现**，写书时尤其注重代码实现和基础架构，通过代码让读者清晰理解数据格式化、损失函数运作及参数更新。至于AI在工作流中的角色，拉施卡使用**GPT-5 Pro**作为辅助，而非完全依赖。例如，他让AI检查文章错误或拼写问题，这比自行检查快得多，AI还能发现表达不清之处并提供优化建议。由于英语非母语，AI的地道表达建议尤为宝贵。他强调，是在利用AI提升工作质量，而非被取代。他热爱钻研过程，不愿将一切交给机器，这种**人与AI协同**的工作方式，或许是未来许多专业人士的主流选择。以上是本次访谈的主要内容，希望能带来启发。感谢收看，我们下期再见。