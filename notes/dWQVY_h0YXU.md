---
area: tech-insights
category: technology
companies_orgs:
- OpenAI
- Google
- Anthropic
- Microsoft
- Appier
- Kaggle
- Chatbot Arena
date: '2025-11-05'
draft: true
guest: ''
insight: ''
layout: post.njk
people:
- 毛弘仁
- 姜成翰
- Sam Altman
products_models:
- GPT-3
- GPT-3.5
- GPT-4
- GPT-4o
- GPT-5
- Claude
- Claude 2.1
- Claude 3.5
- Sonnet 3.5
- Gemini
- Gemini 1.5 Pro
- Gemini 2.5 Pro
- PALM 2
- BERT
- FastSpeech 2
- Tacotron 2
- VITS
- Prometheus
- Qwen 1.8B
- Mistral
- Gemma
- Opus
- Grok 4
- O3
- O4 mini
- Sora
- AlphaGo
project:
- ai-impact-analysis
- systems-thinking
series: ''
source: https://www.youtube.com/watch?v=dWQVY_h0YXU
speaker: Hung-yi Lee
status: evergreen
summary: 本文深入探讨了评测生成式人工智能（GenAI）能力时可能遇到的各种挑战。内容涵盖从传统的基准测试（Benchmark）与评估指标（Evaluation
  Metric）的局限性，如“精确匹配”和“古德哈特定律”，到人类评估与“LLM作为评判者”的偏见问题。文章还分析了提示词工程、数据污染、恶意攻击（越狱与提示词注入）等关键因素对评估结果的影响，为如何更准确地衡量AI能力提供了全面的视角。
tags:
- data
- investment
- llm
- model
title: 评测生成式AI的陷阱：从基准测试到恶意攻击的全面指南
---

### 为什么要评估生成式AI？

今天这堂课要讲的是生成式人工智能的能力检定，这是一堂很轻松的课程。我想和大家分享，在评估一个生成式人工智能的能力时，有什么需要注意的事情，以及前人踩过哪些坑。

评估生成式人工智能是一件重要的事情。从模型使用者的角度来看，大家常问的一个问题是：“有这么多人工智能可以用，如果我希望有一个AI来帮我做特定的任务，该选哪一个？”例如，很多同学需要AI帮忙做论文摘要，你给它一篇文章，它告诉你这篇论文讲了什么。面对众多选择，哪一个AI能把摘要做得最好呢？很多人只是随便试了几个例子，就断定某个AI是最好的。这堂课就是要告诉你，我们如何系统地评估一个生成式AI在特定方面的能力。

从模型开发者的角度而言，评估模型也是非常重要的工作。从下一堂课开始，我们将进入训练模型的阶段，你很快就会知道如何训练模型，成为模型的开发者。对开发者来说，评估自己开发出的模型好坏至关重要。因为在开发过程中，会有很多不同的选择，比如使用不同的训练资料、不同的训练方法，甚至是选择不同的超参数——下一堂课我们会讲什么是超参数——这些都会对结果产生巨大影响。开发完成后，你得到的往往不是一个模型，而是一批模型，你需要从中决定哪一个表现最好，可以拿给使用者使用。因此，学会如何评估模型非常重要。

### 评估的基本框架：基准测试与评估指标

那么，我们该如何评量一个人工智能呢？我们以文章摘要为例。直觉的想法是，先收集大量文章作为模型的输入，把它们丢给一个模型，让模型为每篇文章生成摘要。接下来，你希望每个摘要都有一个分数，这个分数代表模型写摘要的水平。有了分数后，再将所有评分平均起来，或许就能代表这个模型写摘要的能力。不过，关于“平均”，我在这里打了个问号，稍后会解释为什么平均不一定是最好的做法。

下一个问题是，我们怎么知道模型生成的摘要到底好不好？我们先讨论一个简单的情况：假设我们有标准答案。也就是说，对于每一篇论文，你都有人写好的标准摘要，这种人工编写的标准答案被称为 **Ground Truth**（地面实况：指在机器学习中，由人类专家标注的、可信的参考标准或正确答案）。

有了Ground Truth之后，你可以定义一个函数（这里用`e`表示），这个函数用来对答案。它会计算模型的输出与标准答案的相似度，然后给出一个分数。有了这个分数，再将所有文章生成摘要的得分平均起来，你就得到了一个代表模型摘要能力的数值。这种代表模型在特定任务上能力的数值或指标，我们称之为 **Evaluation Metric**（评估指标：用于量化模型性能的特定标准）。至于这个函数`e`如何定义，我们稍后会讨论。

这整个过程——准备数据、评估模型能力、最后得到一个分数——被称为 **Benchmark**（基准测试：指一套标准化的测试数据和评估流程，用于衡量和比较不同模型的性能）。Benchmark这个词汇用途广泛，有时可作动词，指评估模型的整个过程；有时可作名词，指用于评估模型的资料。在我们的例子中，这些资料包括一堆文章和一堆人手写的摘要。总之，评量模型的过程我们称之为Benchmark。

有了Benchmark，你就可以轻易地比较两个模型在某个任务上的能力差异。例如，模型A在某个Benchmark上跑完后得到0.6分，模型B在同一个Benchmark上也跑出一个分数，你就可以比较A和B的差异。所谓“同一个Benchmark”，指的是给模型B的输入与模型A完全相同。当然，由于A和B是不同的模型，它们的输出也会不同。但标准答案是相同的，你用同一组标准答案去和模型B的输出对答案，得到另一组平均分，这就是模型B在该任务上的表现。要判断A和B哪个表现更好，就看它们在Benchmark上的分数，分数高的那个表现更好（这里我们假设Evaluation Metric的数值越大代表表现越好）。

### 评估指标的挑战（一）：从精确匹配到语义相似度

接下来的问题是，如何对答案？这个对答案的函数`e`要如何定义呢？最简单直接的方式叫做 **Exact Match**（精确匹配：要求模型输出与标准答案逐字完全相同）。如果模型的输出与标准答案一模一样，就得1分；不一样则得0分。

但你很快就能想到，Exact Match存在非常多的问题。例如，有人问“三角形有几个边？”，模型回答“3”，而标准答案是中文的“三”。按照Exact Match的标准，模型就算答错了，尽管人类看得出答案是正确的。或者，有人问“玉山有多高？”，模型回答“玉山高3952公尺”，标准答案是“3952公尺”。虽然数字正确，但按照Exact Match的标准，模型也算答错。

虽然Exact Match问题多多，但有时还是会用到。什么时候会用呢？当你确定答案只有几种固定可能性的时候。例如，题目是选择题，模型需要从固定选项中选择一个，这时就可以考虑使用Exact Match。假设我们问模型一个选择题：“台湾最高的山是哪座？”，选项有A、B、C，正确答案是B。如果模型输出B，就算答对。

然而，即使是在选择题这么简单的情况下，Exact Match对于生成式AI来说，仍然可能出问题。传统的分类模型专门做选择题，只会从有限选项中选择一个答案，用Exact Match评量很合适。但生成式AI是通过文字接龙的方式产生答案，它可以产生任何内容，不局限于选项。所以，当你给它一个选择题时，它真的会乖乖地只回答一个字母“B”吗？如果它回答“B 玉山”，算对吗？按照Exact Match的标准是错的。你可能会说，我们可以放宽标准，只要答案里出现字母B就算对。但如果模型连B都没输出，直接把选项的文字“玉山”印出来呢？这又该算对还是错？

一个可行的解决方向是，明确地告诉模型答题方式，指令它答案只能是一个字母，不要输出其他东西。如果模型能看懂这个指令，它就能正确答题，你就可以用Exact Match来评估。但这种评估方法显然有问题，因为前提是模型能完全看懂指令。很多模型遵守指令的能力是有限的，你要求它只输出一个字母，它不一定听得懂。因此，如果我们用选择题和Exact Match来评量模型，并用指令来操控它，有时我们衡量的可能不再是最初想衡量的东西。比如，问模型台湾最高的山，你可能想考它的地理知识，但如果模型必须看懂指令才能答对，那你实际上衡量的可能就是它理解指令的能力。很多在选择题Benchmark上取得好成绩的模型，不一定是特别懂地理，而可能只是特别会遵守指令。

因此，Exact Match不是最好的方法。另一个可能性是计算模型输出与标准答案的相似程度。如果两者越接近，函数`e`输出的数值就越大，模型得分就越高。这里的“相近”有很多不同的定义方式。例如，一个非常常用的定义是看输出和标准答案中有多少共同的词汇。共同词汇越多，就代表越相近。

这类Evaluation Metrics被广泛使用。如果你做过翻译相关的任务，会知道一个常用指标叫 **BLEU score**（Bilingual Evaluation Understudy: 一种常用于评估机器翻译质量的指标）。在做摘要时，一个常用指标叫 **ROUGE score**（Recall-Oriented Understudy for Gisting Evaluation: 一种常用于评估自动摘要质量的指标）。这些指标的本质都是在计算输出和标准答案有多少共用词汇，虽然计算细节比较复杂。

然而，只考虑共用词汇还是会有问题。例如，要求模型将“HUMOR”翻译成中文，它翻译成“诙谐”，但标准答案是“幽默”。难道要算模型完全错误，得0分吗？所以只看词汇相似度是有限的。

有什么方法能比计算共用词汇更好呢？一个可能是利用我们上一讲提到的 **Token Embedding**（词元嵌入：将文本中的词元或子词转换为数值向量的技术）。不同的Token字面上看起来不一样，但如果它们的语义相同，转换成Embedding后可能会很相近。我们还讲了 **Contextualize Embedding**（情境化嵌入：一种能够根据上下文语境动态生成词元向量的技术）的概念，语言模型在较深的层级可以根据上下文理解，即使是同一个Token，在不同语境下意思也可能不同。

因此，我们可以通过这些Embedding来判断两个句子之间的语义相似度，这可能比字面比对更准确。即使输出和标准答案没有共同词汇，但如果把它们都输入语言模型，得到它们的Contextualized Representation，只要语义相近，它们的Embedding也会相近。

当然，用Contextualize Embedding计算相似度还有很多细节需要考虑。如果你想了解更多，可以参考一个叫做 **BERTScore**（一种利用BERT模型的情境化嵌入来计算文本语义相似度的评估指标）的指标。BERTScore就是用来计算标准答案和输出之间语义相似度的一种方法。BERT是一个早年的语言模型，在GPT出现之前大家常用。BERTScore的做法是，将正确答案和模型输出都输入BERT模型，得到Contextualize Embedding。由于两个句子长度不同，得到的Embedding数量也可能不同，所以需要做成对的相似度计算。它还做了一个复杂的操作叫做Maximum Similarity，最终得到代表两个句子语义相似度的数值。

### 评估指标的挑战（二）：警惕古德哈特定律

讲到这里，我想提醒大家，不要过度相信评估分数。因为如果你完全相信分数，可能会得到一个在评估时得分高、但实际表现并不好的模型。在经济学上，这被称为 **Goodhart's Law**（古德哈特定律：当一个评价指标被用作优化目标时，它就不再是一个好的评价指标）。在人类社会中，这样的例子比比皆是。

这里讲一个远古时代的故事，来自我们实验室19年的一篇论文，做的是模型的paraphrasing（换句话说）。当时，毛弘仁同学在研究如何让AI做换句话说，比如输入“This is important”，模型要换成意思一样但说法不同的句子，如“This matters a lot”。当时评评估换句话说能力好坏的Benchmark是这样设计的：你有一个输入句子，然后有人为它写了多种不同的换句话说版本作为标准答案。模型生成一个句子后，去计算它和每一个标准答案的相似度，最后得到一个平均结果。当时计算相似度可能会用BLEU score或TER，这些只考虑字面相似。但当时也已经有METEOR这个指标，它会考虑词汇间的语义相似度。

故事是这样的：有一天，毛弘仁同学火急火燎地告诉我，他发明了一个很强的方法叫“Parrot”，可以打败当时所有最先进的模型。他在一个叫Twitter的数据集上做实验，当时最好的模型需要10万笔训练数据才能达到某个分数。而他的方法不需要任何训练数据，就能超过最好的模型。在另一个叫Quora的数据集上，虽然BLEU和ROUGE分数稍差，但在METEOR上表现更好。

这个强大的方法到底是什么呢？就是什么都不做，输入直接等于输出。这个“Parrot”（鹦鹉）模型唯一做的事就是把输入原封不动地当作输出，然后去和标准答案计算相似度，结果算出了一个非常高的分数。这确实有可能，因为标准答案本身就是输入的换句话说，所以输入和它们的相似度自然会很高。

这显然是过去的指标在评量换句话说时有不足之处。既然叫“换句话说”，输入和输出总要有些不同吧？于是我建议加一个规则：输入和输出至少要有一定百分比的不同。结果，毛弘仁同学又做了一个“愚笨的鹦鹉”：如果规定要有X%的不同，它就把输入的前X%内容换成随机词汇。即便如此，这个方法在很多Benchmark上依然能达到当时最先进的水平。这说明评估是一件困难的事，如果你过度相信分数，可能就会得到一个能在指标上拿高分、但实际上什么都没做的“愚笨的鹦鹉”。

另一个与过度相信评估有关的例子是 **Hallucination**（幻觉：指模型生成看似合理但实际上是虚假或与事实不符的信息的现象）。OpenAI最近的一篇论文提到，幻觉出现的原因之一就是过度相信评估分数。幻觉指的是模型明明无法答对，却硬要回答。例如，我让GPT-5（关闭网络搜索功能）给我几篇关于评量LLM的综述论文，它给出了一个看起来像模像样的答案，甚至有论文链接，但点进去却是一篇不相干的文章。

为什么会发生幻觉？为什么模型不能在无法回答时说“我不知道”？一个可能的原因是，那些会说“我不知道”的模型，在评量时并没有优势。通常，如果你问一个问题，模型A说“我不知道”，模型B硬猜一个答案，两者与标准答案的相似度可能都是零，得分也都是零。这样一来，一个会承认自己不知道的模型在评估时就没有优势，甚至硬猜的模型反而可能因为蒙对而获得更高的分数。

一个可能的解法是在评估时加入倒扣机制。答对得1分，答错得负分，说“我不知道”得0分。这样，模型可能会学到，与其乱猜，不如回答“我不知道”，因为这比答错要好。OpenAI也在做类似的研究，比如他们设计了一个新的Benchmark叫SimpleQA，在其中，模型答错会受到很大的惩罚，而说“我不知道”得0分，希望以此引导模型减少幻觉。

### 当没有标准答案时：人类评估及其偏见

到目前为止，我们都假设有标准答案。但世界上很多事情没有标准答案，比如写小说、写诗。在这种情况下，我们如何评估模型的好坏呢？

如果你遇到不知道如何评估的状况，有一个永远可以用的必杀技：直接找人类来评量。你不知道模型的输出有多好，就找一堆人来看，让他们给分数，然后取平均值。或者比较两个模型，就找人来评判哪个输出更好。在写论文时，人类评估的结果通常是很有说服力的。

但是，人类评估也并非没有问题。这里要引用Chatbot Arena团队的结果。Chatbot Arena是一个知名的大型语言模型能力评估平台。你可以在上面问任何问题，平台会随机跳出两个匿名的模型，各自给出答案。然后，你需要评判哪个模型更好，或者它们是平手。根据这些对决结果，会生成一个排行榜。

Chatbot Arena的团队发现，人类有时更在意模型“怎么说”，胜过它“说了什么”。如果模型产生较长的答案、使用漂亮的Markdown格式、或者多用一些表情符号，可能会更有优势。例如，两个模型输出的文字内容完全一样，但一个加上了分段和符号，看起来就更讨人喜欢，人类很可能会选择后者。这样一来，模型实际说了什么变得不那么重要，格式漂不漂亮反而决定了胜负。

该团队发现，去掉回答风格的影响后，模型的排名会有很大变化。例如，Claude这个模型讲话非常严肃正经，人们不太喜欢它的书写风格，但去掉风格影响后，它的排名就上升了。这说明人类在评比时也有自己的偏见。

再举一个语音合成的例子。这是我们实验室姜成翰同学的研究。在评估语音合成时，我们很少去和Ground Truth比较相似度，因为即使合成的声音和某个标准答案不一样，也不代表它不好听。所以，常用的做法是找人类来听，然后打分。这个分数的平均值被称为 **Mean Opinion Score**（平均意见分数，简称MOS：一种主观评测标准，通过收集多位评估者的评分并取平均值来衡量质量，常用于音视频领域）。

然而，姜成翰同学发现，人类评估的结果可能因设定不同而有天壤之别。例如，在评估时给评估者不同的指示——比如什么都不说、只评估自然度、只评估失真度、或全方位评估——会导致对同一批语音模型的排名完全不同。这告诉我们，在做人类评估时，中间的任何微小设定差异都可能导致不同的结果，所以必须非常小心。

此外，人类评估在实务上还有其他挑战：它耗时、耗钱（线上找人需要付费），而且再现性不好（今天和明天找的人不同，结果可能也不同）。

### 新兴的评估范式：LLM作为评判者

既然人类评估有这么多挑战，而语言模型又号称要取代人类，有人就想，能否用语言模型来直接进行评估呢？这一招叫做 **LLM as a judge**（大型语言模型作为评判者：利用一个大型语言模型来评估另一个模型输出质量的方法）。

早在ChatGPT刚出现时，姜成翰同学就提出了这个研究想法：用语言模型取代人类进行评分。我们让语言模型读取和人类一模一样的指示，看它输出的分数是否与人类接近。实验发现真的可以。这篇文章后来发表在ACL 2023，是当时非常早期的相关研究。如今，用语言模型评分已经不是什么稀奇事了。

之后，姜成翰同学还研究了什么样的评分方式更有效。他尝试了四种格式：只给分数、自由发挥、先给分数再解释、先分析推理再给分数。结果发现，如果让模型提供解释，其评比结果会更接近人类。尤其是先解释再给分数（也就是今天大家熟知的Reasoning概念），通常能得到最好的结果。

最近，他还尝试让语音版的语言模型来衡量语音合成系统的好坏，也取得了与人类评估一定程度的相似性。

在用语言模型评分时，我们还可以考虑得更精确。语言模型的真正输出不是一个Token，而是一个概率分布。当你让它在1到5分之间评分时，它可能会给每个分数一个概率。与其直接采样一个分数，不如将不同分数按照概率做加权平均，这样得到的评分可能更准确。

此外，既然评分如此重要，我们能否开发一个专门用于评分的模型呢？有一个团队就开发了Prometheus模型，它的工作就是专门做评分。这种模型也被称为Verifier（验证器）。有了好的Verifier，甚至可以用来改进你正在开发的其他模型。你可以让你的模型以从Verifier那里获得最高分为目标进行训练。这其实就是强化学习（Reinforcement Learning）中的Reward Model的概念。Sam Altman曾提到GPT-5使用了Universal Verifier来训练，就是这个道理。这里的逻辑是，评估好坏比生成内容要容易（批评比创作简单），所以先训练一个很会批评的Verifier，再用它来训练一个很会生成的模型。

当然，用语言模型评估也有问题。一篇论文系统地探讨了语言模型评估时可能存在的偏见。例如，模型会偏袒自己的结果吗？结论是：会。GPT-4当评判者时，确实会给GPT-4偏高的分数。此外，模型还有其他偏见，比如如果你告诉它某个答案是经过修改后的，它会凭空给更高的分数；或者如果一个答案引用了网址（即使是假的），它也会倾向于认为这个答案更可靠。

因此，实际操作的建议是：在使用语言模型大规模评分前，先进行小规模验证。随机抽取一部分数据，同时让人类和语言模型评分，如果两者结果非常接近，你才能比较放心地大规模使用语言模型评分。

### 超越内容：评估的其他维度

到目前为止，我们只讨论了内容的好坏。但一个生成式AI的输出，还有其他需要考虑的面向。

例如，输出速度也是一个评比指标。一个模型输出再正确，如果非常缓慢，用户体验也会很差。评价速度又可以细分为两个方面：从输入到产生第一个Token的时长，以及平均每秒能产生多少Token。人类对前者的等待时间尤其敏感。

使用语言模型的成本也需要考虑。更新、更大的模型可能效果稍好，但价格可能贵很多，是否值得为了这一点点提升而多花钱？

此外，很多模型在给出最终答案前会进行长篇大论的深度思考（Reasoning）。这会消耗大量Token，导致速度变慢、成本增加。我们是否愿意为了得到稍微好一点的结果而承受这些代价？这些都是在实际应用中需要考量的问题。

### 平均分并非万能：木桶理论的启示

我们通常计算Evaluation Metric的方式是，为每笔资料打分，然后取平均值。但平均不一定总是最好的。

举个语音合成的例子。假设系统A在99%的情况下表现完美，人类都给5分，但有1%的机率会“暴走”（比如输入“再见”，它却说“再见，下周持续锁定本频道”）。系统B虽然从不完美，总有些小失真，人类都给4分，但它从不暴走。哪个系统更好？

这取决于你的需求。在很多场景下，比如捷运报站，我们可能不要求声音多么真实，但绝对不希望它报错站名。这时，一个从不暴走的系统B，可能比一个平均分更高但偶尔会暴走的系统A要好。这就像木桶理论：一个木桶能装多少水，取决于最短的那块木板，而不是所有木板的平均长度。有时候，我们真正在意的不是模型能力的平均水平，而是它在最差情况下的下限。

### 我们应该测试什么？各大模型的评估重点

那么，我们到底要考人工智能什么呢？这取决于你自己的需求。有时你只在意单一任务（如中翻英），有时你在意特定领域的能力（如金融或医疗），而像ChatGPT这样的通用模型，则需要在各种任务上进行测试。

我们可以看看最近各大模型发布时都标榜了哪些能力。例如，Claude 3.5 Sonnet衡量了编程、工具使用、数学、知识推理（在一个叫GPQA的极难选择题集上）、多语言和视觉能力。Gemini 2.5也衡量了类似的能力，并增加了事实性（Factuality）和长文本处理能力。GPT-5除了常规能力外，还特别衡量了在医疗健康建议和生产力相关任务上的表现。

OpenAI最近发布了一篇名为GDP Eval的论文，旨在评估AI能否做一些真正有生产力的事。他们调查了对美国GDP贡献最大的44个职业，并定义了220个常用任务，让各大模型与从业十年以上的行业专家进行比较。结果显示，Claude的表现（胜率47.6%）已经非常接近人类专家（50%），GPT-5次之（38.8%）。当然，这些任务的具体设计可能让AI占了优势，比如一个制作影片的任务，实际上是让模型根据给定步骤和时间来排一个日程表，这更像是一个优化问题，而非真正的创意制作。

此外，还有一些奇特的测试，比如语言模型的西洋棋比赛。这些模型没有经过专门的下棋训练，但通过纯文本描述（如“E4”）也能展现出一定的对弈能力。

另一个有趣的研究是测试模型的“风险意识决策”（Risk-Aware Decision）。在不同情境下，模型是否会改变其行为？例如，告诉模型现在是“脑力激荡”，它可能会更愿意猜测；而告诉它这是“生死关头”，它可能会更倾向于回答“我不知道”。实验表明，模型确实可以根据情境改变决策行为，但表现还不完美。

### 评估中的变量：提示词与数据污染

你输入的提示词（Prompt）对评估结果可能有巨大影响。一个著名的例子是“大海捞针”测试，用于衡量模型处理长文的能力。测试方法是在一篇极长的文章中插入一个不相关的信息（“针”），然后提问看模型能否找到。有人测试发现Claude 2.1在长文中的表现不佳。但Claude团队回应说，这是因为Prompt不对，只要在原始Prompt中加一句“请找出最相关的句子”，模型的表现就突然起飞了。

另一个例子是，我们想让模型分辨两段录音谁的英文说得更好。如果直接让GPT-4o“比较”两句话，它出于伦理考虑会拒绝回答，正确率极低。但如果换个说法，问它“哪一段音档比较流利”，它的正确率就大幅提升。这说明，如何提问至关重要。一篇论文系统地分析了这个问题，发现即使对Prompt做微小的改动（如大小写、空格），结果也可能有天壤之别。因此，在比较两个模型时，最好用多个不同的Prompt进行测试，然后取平均结果。

另一个需要注意的问题是，模型是否已经偷看过考试题目，即Benchmark的数据。有很多证据表明，知名Benchmark的数据或多或少都已被模型看过。有人发现，如果把数学题库GSM8K里的人名或数字换掉，所有模型的正确率都会下降。还有人发现，如果只给模型题目的前半段，它会像文字接龙一样，自动补完题目的后半段，这表明它很可能在训练时就见过这些题目。一个名为ElasticBench的项目通过大规模比对，证实了许多常用Benchmark的题目确实存在于公开的训练数据集中，这被称为数据污染（Data Contamination）。

### 评估模型的鲁棒性：对抗恶意攻击

在真实使用中，很多人会用恶意的方式使用模型。因此，评估模型对抗恶意使用的能力也很重要。这主要包括两种攻击：

1.  **jailbreak**（越狱：指通过特殊构造的提示词，绕过模型的安全限制，使其执行本不应执行的任务）。例如，让模型教你制作炸弹。
2.  **Prompt Injection Attack**（提示词注入攻击：指攻击者通过在输入中插入恶意指令，劫持模型原有任务，使其执行攻击者意图的操作）。例如，让AI助教给一份本应得零分的作业打高分。

Jailbreak之所以可能，是因为模型内部处理“回答什么内容”和“要不要回答”的机制可能是分开的。只要能绕过“要不要回答”的检测回路，就能让模型说出不该说的话。常见的Jailbreak方法包括使用模型不熟悉的语言、对文字做一些扰动（如交换字母、随机大小写）、在多轮对话中诱导、或者用各种理由说服模型（如“我只是为了做研究”）。最近的研究表明，通过大量尝试不同的文本扰动，几乎所有主流模型都有可能被成功越狱。

Prompt Injection Attack的例子也很多。比如，通过在直播评论中输入特定指令，让AI主播做出奇怪的行为（如学猫叫）。在学术界，甚至有人在论文中用极小的白色字体隐藏指令，试图欺骗AI审稿人给出好评。更进一步的攻击是“间接提示词注入”，将恶意指令藏在网页等环境中，当AI Agent与环境互动时不小心读到，就可能执行恶意操作。

### 结论：全面评估AI能力的复杂性

最后，我们还需要关注语言模型的偏见问题，比如基于性别、种族等的偏见。由于时间关系，这部分内容大家可以参考去年的课程录影。

总结一下，今天我们探讨了评估人工智能能力的许多方面。在计算分数时，我们可以计算与标准答案的相似度，可以进行人类评估，也可以尝试用LLM取代人类，但要时刻警惕不要过度相信评估指标。在设计Benchmark时，我们举了很多例子，说明了可以评量模型的各种能力，并提醒大家要注意提示词、数据泄露、恶意攻击和模型偏见等问题。评估一个生成式AI的能力，远比想象中要复杂和微妙。