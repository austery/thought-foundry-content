---
area: tech-insights
category: technology
companies_orgs:
- 智谱AI
date: '2025-12-10'
draft: true
guest: ''
insight: ''
layout: post.njk
products_models:
- GRM 4.5 V纤文3VL系列
- Kimi VL系列
- SteerPE P3 321B系列
project:
- ai-impact-analysis
series: ''
source: https://www.youtube.com/watch?v=XJ1O5BmdLuw
speaker: AI超元域
status: evergreen
summary: 本文深度评测了智谱AI新发布的GLM-4.6V系列模型。重点测试了轻量级的9B Flash模型在本地部署的效果，以及106B模型在图像理解、手写公式OCR、UI复刻和原生Function
  Calling方面的卓越性能。测试结果显示，其多模态推理和代理能力在多个基准测试中超越了现有主流模型，为多模态Agent提供了统一的技术底座。
tags:
- model
- multimodal-ai
- ocr-capability
- society
- technology
title: 智谱GLM-4.6V深度实测：本地部署9B模型，原生Function Calling赋能，多模态能力全面超越SOTA
---

### GLM-4.6V系列模型发布与性能概览

智谱AI在昨天下午发布了GLM 4. 6V系列模型。这个系列的模型包含9B参数的GLM 4. 6V flash模型。它是一款轻量级模型，专为本地部署，还有低延迟应用而优化。还有另一款106 B参数的GLM. 4. 6V，它是专门为云端还有高性能集群场景而设计的模型。

GLM 4. 6V系列模型在视觉理解精度上达到了同等参数规模的**SOTA**（State-of-the-Art: 指当前技术水平的最高标准）。并且首次在模型架构中将**Function Calling**（原生Function Calling: 模型调用外部工具或API的能力，实现从感知到执行的动作链）能力原生融入了视觉模型中。这打通了视觉感知到可执行行动的链路，为真实业务场景中的多模态agent提供了统一的技术底座。

根据官方发布的基准测试，可以看到GLM 4. 6V和GLM 4. 6 flash在通用视觉问答、多模态推理、多模态代理、多模态长上下文OCR与图表理解，还有空间感知与定位这些基准测试中，他们的得分超越了GRM 4.5 V纤文3VL系列，还有kimi VL系列，还有ste、 pe、 p3、 321、 B系列。

而且这两款模型的上下文窗口都达到了128 K tokens。这意味着它可以同时理解文档中的文本布局、图表、表格，还有图形等内容。从而无需转换为纯文本，就可以精准的理解，复杂的包含大量图像的文档内容。而且它还支持前端复现与可视化编辑。我们只需要提供一张UI截图，它就能够使用前端技术，为我们复刻我们所提供的UI。而且经过我一上午的测试，最大的感受就是这款模型的多模态能力，相比其他开源的视觉大模型确实有了非常大的提升。

本期视频我们将从多方面来测试GLM 4. 6V系列模型的综合能力。视频的开始，我们会先演示本地部署GLM 4. 6V flash这款9B参数的模型。并且测试在本地调用这款模型效果如何？。然后我会重点测评Glm 4. 6V这款106 B参数的模型。我们将从图像理解、图像标注、OCR等方面重点测试这款模型的多模态能力。并且我们还会测试这款模型的function calling能力。

### 本地部署GLM-4.6V Flash模型测试

想本地使用GLM 4. 6V flash这款模型非常简单。Windows用户和Mac用户可以直接使用LM studio。我们直接点击LM studio的这个搜索，然后直接输入GLM 4. 6V。在这里我们就看到了GLM 4. 6V的这些模型。然后我们只需选中，并且点击下载模型就可以。而且我们还可以使用官方推荐的**VLRM**（智谱AI推荐的视觉语言模型部署框架）进行部署。这样的话我们就可以部署到生产环境下进行使用。

下面我们可以测试一下，部署到生产环境下进行使用。我这里测试用的系统是乌班图系统。这里用到的显卡是RTX A6000显卡。下面我们先运行这一条命令，创建一个虚拟环境。然后在执行这条命令来激活这个虚拟环境。好，下面我们可以先执行这条命令来安装VLRM，然后再执行这条命令安装transforms。在执行这条命令来安装图像处理的依赖。下面我们就可以用VLRM来加载这款模型，并且进行推理。我们直接执行这一条命令。这里的模型ID就是GLM 4. 6 flash。然后我们直接运行这条命令。这里正在下载模型的权重文件。好，这里运行成功端口是8000。下面我们就可以通过VLRM提供的这个API接口来调用这款模型。我们只需要在Open WebUI的设置这里设置好它的Base URL。然后在对话框中，我们就可以使用GLM. 4. 6V flash这块模型。我们可以上传一个图像测试。然后我这里就上传一个比较搞笑的这个图像，然后输入提示词，为左上角带帽子的男性，加一段此刻内心的想法。然后我们看一下它输出的内容。它输出的是这派对，气氛倒是挺热闹。不过我好像太习惯这种喧哗，还是先看看大家玩的开心吧。或许等下也能加入其中。可以看到它还是能准确理解这个图像的这种场景和氛围的。

### 106B模型UI复刻能力测试

下面我们开始重点测试GLM 4. 6V这款106比参数模型的综合能力。想使用这款模型非常简单。海外用户只需要使用Z点AI这个平台就可以直接在线使用这款模型。国内用户可以通过BMO D平台来选择GLM 4. 6V，从而在线使用这款模型。

好，下面我们开始测试这款模型的综合能力。我这里准备了一张比较复杂的UI截图。可以看到这张UI截图的布局比较复杂，而且包含的元素非常多。下面我们就用这张截图测试一下，让GLM 4. 6V百分百复刻这个截图。我们直接上传这个图像，然后输入提示词，用你认为最合适的前端技术，百分百来复刻这个UI。然后我们直接发送。这里他提到，他将使用React加TypeScript加Tailwind CSS来复刻这个仪表盘。下面我们看一下它复刻的这个UI效果。和原图相比的话，它复刻的这个效果还是非常不错的。除了在原图中这个火箭图像，它这里预留出来了位置，让我们单独放入这些火箭的图像。原图中这个图表的卡片宽度比较宽，它这里复刻之后，这个宽度稍微窄立一些。然后我们可以让它继续去调整。

### 复杂文档与手写公式OCR识别测试

下面我们继续测试。我这里准备了一张笔记的图像，然后我们就测试一下这款模型的**OCR**（Optical Character Recognition: 光学字符识别技术）能力。我们先上传图像，在输入提示词，提取图上完整的内容，并保持原有格式输出。然后我们对比一下它提取的是否正确。这里是标题和原图笔完全正确。然后这一段内容提取了也完全正确。甚至保持了原图上这些序号，还有列表的这些格式。这里的内容就对应图中的这一部分内容，它提取的也是完全正确的。下面的这些内容，它提取的也都是正确的。这一部分内容，它提取的也都是正确的。

下面我们再准备一张更加模糊的PDF扫描件进行测试。可以发现这个PDF扫描的非常的不清晰，而且这上面的小字几乎无法识别。然后输入提示词，对图像进行OCR，直接发送。然后我们对比下提取的是否正确。在标题文档ID与页码，这里和原文档相比是完全一致的。然后这一部分内容包含这个图表中的这些内容，它唯一提取错的地方，就是这个单元格中的内容，因为在这个图像上，这个单元格被这个噪点给遮挡了。下面这段内容对应的就是这一段内容，提取的也是没问题的。最后一段内容对应图中的这一部分，这里也是完全没问题的。最下面的内容就是对图中这个图表的描述，它描述的还是非常准确的。而且在图中这一部分的小文字，它也成功提取了出来。

下面我们加大难度。这里我准备了一个拍摄的照片。上面包含手写的公式，而且可以看到它的排版是非常复杂的，这些公式也是比较潦草的。然后我们就输入提示词，让它对图像进行OCR。然后我们对比一下它提取的是否正确。可以看到提取的这一部分，对应图中的这一部分，这是完全正确的。这里就对应图中的这部分也是没问题的。下面这里就对应图中的这一部分，也是没问题的。然后这一部分对应图中的这一部分也是没问题的。下面这里就对应图中的这一部分，也是正确的。然后这里还识别出了这个竖排的这些内容，这里也是正确的。这样看来，它能准确的对这些手写的公式进行OCR识别。

### 图像理解、标注与图文混排能力展示

刚才我们测试的是OCR场景。下面我们测试一下它对图像的理解能力。这里我准备了一张图像。可以发现这是一个模拟清明上河图的图像，但它并不是清明上河图。在图像上有两个现代元素，包括这个自行车，还有一个摩托艇。然后我们直接输入提示词，图中有哪些不合理之处？。它这里成功识别出来了。图中有现代交通工具，包括摩托艇，还有自行车，它能成功理解图中的时代错乱，还有交通工具不匹配。然后我们在当前对话中继续输入提示词，让它标注出图中的摩托艇和自行车。看一下它的图像标注能力。这里它成功输出了标注后的图像。我们看一下，它用红色的方框标注出了图中的摩托艇，还有图中的自行车。

下面我们再加大难度。在这一张图像上，我们放大之后，在这里可以看到一条鳄鱼，还有一条恐龙。然后输入提示词，让它标注出图中的鳄鱼和恐龙。这里输出了标注后的图像。这里还用文字描述了鳄鱼和恐龙的位置。然后我们放大图像。这里用红色方框将鳄鱼标注了出来。然后这里将这个恐龙也标注了出来。然后我们继续测试。这里我准备了一张图像。图中并排站着好几种品种的猫。然后我输入提示词，从左到右标记每只猫的品种名称。然后我们看一下它标注的是否准确。第一只是暹罗猫，然后苏格兰之耳猫、孟加拉猫、布偶猫、俄罗斯蓝猫、斯芬克斯猫，还有异国短毛猫。可以看到它输出的这几只猫的品种名称是完全没问题的。

下面我们再加大难度。这里我提供了一张图像。这张图像中有10种不同品种的狗并排站着，然后输入提示词，从左到右标记出每只狗的品种和名称。从左到右，它输出的是纽芬兰犬、柴犬，还有边牧、腊肠犬、金毛巡回犬，还有法斗、还有贵宾犬、还有大丹犬、还有巴哥犬、以及萨摩耶。它对这10种狗的品种识别的是非常准确的。

下面我们还可以测试一下这一款模型的图文混排输出能力。然后我们输入提示词，搜索2025年新建第11次试飞的资讯，并生成一张图文并茂的新闻介绍。可以看到这里，它调用了网络搜索工具，然后这里调用了图像搜索工具。这里它就输出了图文并茂的新闻内容。这里还给出了标题“历史性时刻”。这里还给出了具体的描述，还有技术上的意义，还有未来展望。像这样我们就可以使用GLM 4. 6V，让它生成图文混排的内容。

下面我们还可以上传一份arXiv上的论文。这篇论文是关于微调大模型的论文。在输入提示词翻译这篇论文，并输出图文并茂的公众号文章。好，下面我们看一下它输出的这个图文混排的效果。这里是给出的标题，下面这一段就是摘要。这里提到本文深入剖析大语言模型微调的全流程。然后这里是关于第一章的内容，在这里它输出了这些图像。然后在这一部分内容，这里也包含了这些图像内容。下面这里就是微调的过程。这里也包含图像内容。可以看到它根据这篇论文输出的图文混排的公众号内容效果还是非常不错的。像这样我们就可以使用GLM 4. 6V上传论文内容，让它用图文混排输出来解析论文的内容。

### API调用与Function Calling实战

刚才我们是在网页版中测试的GLM 4. 6V的综合能力。下面我们还可以通过API的方式来调用这款模型测试这款模型的Calling能力。想通过API调用这款模型非常简单。国内用户可以直接使用Big Model。在Big Model中，我们点击AI，在右侧这里我们就可以创建我们自己的API。创建好之后，我们直接点击复制。海外用户可以通过Z点API这个平台，在右上角这里点击API，在点击API，然后我们就可以点击创建按钮来创建我们的API。创建好之后，我们就可以复制我们的API，以便接下来使用。

好，下面我们就可以参考官方提供的Function Calling示例代码，然后根据我们自己的需求构建自己的Function Calling代码。为了方便演示，我将在Google Colab上我构建的这些代码。在左侧侧边栏这里，我们点击这个钥匙，在这里我添加好了智谱AI的API Key。这里就填写API Key。我创建的这个代码，就是通过工具调用，来为我们制定旅行规划。下面我们可以详细看一下这个代码。这里就是安装所需的依赖。我们直接执行这段代码就可以。然后这段代码就是导入所需的这些依赖。在下面这一部分就是初始化客户端。包括从环境变量中获取到API Key。然后下面这里就是工具函数。从本地路径读取图像，并且将图像转为Data URI。然后下面这段代码就是制定旅游规划的代码，也就是业务函数。这里包含基本校验，然后日期格式必须是ISO格式。然后下面的代码就是Function Calling标准的JSON Schema。这里包含名称、描述，还有这些参数。然后下面这里就是工具执行的路由。

然后这段代码就是使用GLM 4. 6V多模态能力加Function Calling。这里我们设置了提示词，你是多模态旅行规划Agent，能看懂旅行规划海报，形成截图，并通过这个工具生成结构化的行程。在这里就是我们上传的图像。下面这里的内容就是输出一些执行日志，方便我们去查看它的工具调用。然后这里我们就提供了一张简单的旅行规划的截图。我们可以看一下，这里就是我们提供了一张旅行规划的截图。可以看到手机上写的就是新加坡三日游行程。包括第一天、第二天还有第三天。然后在左侧这里，我们点击这个图标。这里我们将刚才查看的图像上传到了这个位置。在这里就是我们输入的任务，根据这张新加坡三日游的行程图，规划一个2025年12月20日，实际可执行的三天行程。

好，下面我们就可以运行一下这个代码，看一下最终的效果。好，这里开始输出执行的步骤。首先就是读取图像，然后以**Base64**（一种将二进制数据编码为ASCII字符串的编码方式，常用于网络传输）编码的方式发送给模型。然后下面这里就是发送给模型的工具。然后这里就是输出的第一轮消息，在这里就开始调用工具，也就是调用了我们这个函数。下面这里就是本地函数的执行结果。然后这里就是追加到这个相应的工具消息。这里就是携带工具的结果后，准备发送给模型完整的Message。下面就是准备发送给模型的完整的这些信息。到这里就是执行的第二轮的输出。包括最终回答，还有内部结构。这里就是显示的最终返回给用户的这些内容。这里输出的是根据提供的图片信息和日期，你规划出新加坡三日游的行程。然后这里还给出了预算。然后下面这里就是详细的每一天的行程。包括第一天滨海湾探索。然后我们和图像上对比。第一天就是滨海湾探索。第二天就是文化与乐园。这里跟图像上是完全对应的。第三天就是自然与离岛，这里就是第三天的内容。这里还给出了详细的行程说明，包括交通建议、门票预定、餐饮推荐，并且说，此行程紧密贴合图片中的景点和主题，兼顾探索文化自然与购物，适合首次到访的游客。

通过测试，我们成功实现了通过API来调用GLM 4. 6V，并且成功实现了Function Calling，而且还实现了正确读取我们提供的截图上的这些旅行规划。最终为我们制定了详细的新加坡3日游的这个旅行方案。

### 总结

通过多方面的测试，可以感受到GLM 4. 6V这个系列的模型，在多模态方面的能力确实有了非常大的提升。无论是图像理解、图像标注OCR、图文混排输出，还是Function Calling效果都非常不错。本期视频所用到的代码和指令，我都会放在视频下方的描述栏或者评论区。如果你在视频下方无法找到的话，也可以通过我的博客去查找本期视频所对应的笔记。好，本期视频就做到这里。欢迎大家点赞、关注和转发，谢谢大家观看。