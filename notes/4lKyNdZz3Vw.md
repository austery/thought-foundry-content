---
author: Internet of Bugs
date: '2025-12-10'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=4lKyNdZz3Vw
speaker: Internet of Bugs
tags:
  - ai-misinformation
  - industry-propaganda
  - existential-risk-hype
  - science-communication-critique
  - ai-narratives
title: 科学YouTuber如何误导你：揭穿AI宣传中的谎言
summary: 本文批判性地分析了YouTube上流行的AI内容，指出许多科学频道传播关于AI的‘必然性’、‘特殊性’和‘致命性’等不实叙事。作者‘Internet of Bugs’的Carl认为，这些宣传往往由AI行业资助，旨在转移公众对AI当前带来的实际危害（如虚假信息、操纵和失业）的关注，并提出了识别这些‘谎言’（L.I.E.S.）的方法，强调关注即时风险和问责制的重要性。
insight: ''
draft: true
series: ''
category: general
area: society-systems
project:
  - ai-impact-analysis
people:
  - Carl
  - Hank Green
companies_orgs:
  - SciShow
  - Kyle Hill
  - Kurzgesagt
  - 3Blue1Brown
  - OpenAI
  - Anthropic
  - Google DeepMind
  - Control AI
products_models:
  - ChatGPT
media_books:
  - giant brains or machines that think
  - If Anyone Build It, Everyone Dies
  - The Hitchhiker's Guide to the Galaxy
status: evergreen
---
### 引言：科学YouTuber与AI的“末日预言”

今天，我想和大家谈谈那些将人工智能（AI）视为“迫在眉睫的末日”的YouTuber。毫无疑问，YouTube上有大量的AI相关评论。毕竟，那些花大量时间撰写脚本和制作视频的人，很可能对生成脚本和视频的软件有很多看法。当然，YouTuber们会制作视频来表达他们对AI的看法，因为YouTuber们会制作视频来表达他们对一切事物的看法。

其中有一些关于大型语言模型（LLMs: 一种人工智能，经过大量文本数据训练，能够理解和生成人类语言）如何工作的优秀YouTube内容，比如3Blue1Brown的系列节目，非常精彩，我曾多次推荐并附上链接。但仔细想想，YouTube上许多关于AI的视频似乎有所不同。

来自不同创作者的无数YouTube视频，呈现出一种惊人一致的叙事。但这种叙事是否有些古怪？这就像你从那些坚持认为世界即将终结的疯狂传教士那里听到的故事一样。在任何其他情况下，如果不是因为AI行业背后的资金和营销，以及AI行业劫持了过去一个世纪左右的科幻桥段，我们都会意识到这是多么荒谬。

当这些言论来自那些为了吸引点击而夸大其词地报道当前趋势的视频制作者时，这已经够糟糕了。更糟糕的是，这些视频竟然来自那些声称自己是、并且已经建立了良好声誉的——研究深入、负责任、事实准确且具有教育意义的频道。而这些视频有时甚至会歪曲过去和现在，以证明一个耸人听闻的AI未来是合理的。

现在，有很多频道和视频我本可以挑选。今天，我只从SciShow、Kyle Hill和Kurzgesagt这几个频道中选取一些例子，它们各自的AI相关视频已经积累了数百万的观看量。我将向大家展示一些片段，指出其中的错误信息和未经证实的论断，然后展示这些内容是如何支撑AI行业试图做的事情，它们对真实的人们造成了多大的伤害，以及我们应该做些什么。我会尽量平静地进行，这对我很困难，因为我……

欢迎来到“虫网”（Internet of Bugs）。我叫Carl。我自20世纪80年代起就是一名软件专业人士，我正尽我所能，让互联网成为一个更安全、更可靠、更少出错的地方。在我看来，AI是当前互联网的最大威胁，因为AI生成的代码仍然充斥着错误和安全问题，而且围绕AI的叙事正在激励公司解雇那些本可以修复错误、改善我们所有人在线体验的合格软件开发人员。

但同样，尽管这超出了我特定的软件相关YouTube领域，因为这些AI公司及其产品正在造成比bug更糟糕的问题，它们正将互联网变成虚假信息（disinformation）的传播媒介，并对无辜人类造成更严重的伤害，例如AI精神病（AI Psychosis: 因过度接触或依赖AI而产生的心理健康问题或幻觉）、煽动麻烦制造者进行危险的自残——据称如此——并且可能造成我们尚未发现的问题。

好的，在开始之前，快速说明一下。我最近发布了一个视频，是关于我对SciShow一期关于AI的节目进行了抨击，我认为那期节目尤其令人无法容忍。但由于那期SciShow视频只是YouTube上AI虚假信息海洋中的一滴水珠，而且太多人认为那期视频更多的是关于“SciShow不好”，而不是“AI宣传不好”，所以我将用这个视频替换它，并将其中一些脚本内容整合到这里。所以，如果其中一些内容对本频道的常客听起来很熟悉，那就是原因。

好的，许多这些YouTube视频中推崇的第一个一致观点是，我称之为AI的“必然性”。即超级智能（Superintelligence: 理论上远超人类最聪明大脑的智能形式）无论我们做什么都会发生，所以没有必要与之对抗。让我给你们看一些例子。

毫无疑问，ChatGPT是这些生成式AI中最显眼的一个，但它只是开始。还会有更多，它们会更好，然后它们会变得更快，接着会发生什么？“这些模型就像它们将要变得一样糟糕，我完全接受这一点。”“就像那样是真的。”过去，AI是狭隘的，只能擅长一项技能，但在其他方面都很糟糕。通过构建更快的计算机并投入更多资金进行AI训练，我们将获得更新、更强大的AI。这些技术将继续在全球范围内发展和传播。如果美国停止开发AI，中国不会停止。

所以这里有几种变体。一种是AI会变得越来越好，越来越好的叙事。另一种是，“如果我们不开发它，中国就会。”我发现很有趣的是，这些观点被反复提出，就好像它们是显而易见的，没有任何更好的理由，就像St. Greene所说的那样，“就像那样是真的。”我不相信它只会变得更好，或者这种叙事。事实上，我制作了一个完整的视频，讲述了我认为这是AI行业给我们最大的谎言。我会在下面附上链接。但不仅仅是我。AI批评者多年来一直在说这句话，行业内的人也终于承认，仅仅增加更多的计算机是不够的。但对于这次讨论重要的是，这种必然性的思维方式的逻辑结论是，试图阻止AI发展是徒劳的。现在，谁可能想让你相信这一点，又为什么呢？

让我们继续讨论这些AI炒作视频似乎共有的下一个特征，我称之为“特殊性”。这是AI是如此特殊、如此新颖、与人类以往任何经历都如此不同，以至于过去的经验和角色都不再适用。

这是Kurzgesagt一个视频的片段，标题很可笑：“AI，人类的最终发明。”“1997年，一个AI击败了国际象棋世界冠军，震惊了世界，证明了我们可以制造超越我们的机器。”哦，好吧。让我弄清楚。1997年，一个国际象棋程序终于证明了我们可以制造超越我们的机器。真的吗？你这么认为？

让我向你介绍埃德温·伯克利（Edwin Berkeley）1949年出版的《巨脑或思考的机器》（Giant Brains or Machines That Think）第七页，引用：“机械大脑可以做的思考类型。有许多种思考是机械大脑可以做的。除其他外，它们可以学习你告诉它们的东西，在需要时应用指令，查找和记住数字，加、减、乘、除和四舍五入，查找表格中的数字，查看结果并做出选择，一个接一个地进行长串这些操作，写出答案，确保答案是正确的，知道一个问题已经完成并转向另一个问题，确定大部分自己的指令，并独立工作。它们比你或我做得更好，未受干扰。”

现在这听起来像是对ChatGPT的现代反应。好吧，除了“确保答案是正确的”这一项，因为ChatGPT做不到这一点。事实证明，关于AI将如何改变一切，以我们从未见过的方式，绝大多数说法都与20世纪40年代和50年代关于巨脑（即计算机）即将改变一切，以我们从未见过的方式的说法惊人地相似。

如果你阅读关于技术如何影响社会的历史，你会发现，老年人倾向于对新发明的技术感到恐慌，就像他们的父母或祖父母对他们认为正常的科技感到恐慌一样。再引用一段话：“许多年轻人能够自由地接触浪漫小说、小说和戏剧，这毒害了许多有前途的青年的思想，败坏了他们的道德。”这句话出自 Reverend Ennis Hitchcock 1790年出版的《布鲁姆斯格罗夫家族回忆录》（Memoirs of the Bloomsgrove Family）。几乎每一代人都坚持认为，现在发生的事情是不同的，如此不同，以至于它正在改变一切。然而，生活仍在继续。

但事实是，几十年来，计算机在许多方面已经超越了我们。我们现在已经习以为常，自我们父母上小学以来，计算机在算术和文本搜索方面就比我们做得更好。但“AI证明了X可以发生”这样的论点，而实际上X发生在20世纪40年代，这只是这些视频声称AI是特殊的一种方式。有时它们还会决定凭空捏造。

这是Kurzgesagt再次：“人类还没有准备好迎接即将发生的事情，无论是在社会上、经济上还是道德上。”好吧，你怎么知道的？基于什么？猜猜一定是汉克·格林（Hank Green）的“就像那样是真的”学校。

那么，接下来的这些片段展示了另一种声称特殊性的方式，在这种情况下，只是对过去的事件不诚实。

“我们真的处在一个悬崖边上。以前从未发生过这样的事情，而且速度如此之快。技术发展迅速。我们从莱特兄弟到第一架商用飞机只用了11年。但即使与飞机、抗生素和核能相比，我们开发人工智能的速度也超过了它们。这一切都发生得非常快，比手机快，比电脑快。”

这完全是错误的。让我们从最后举一个例子。我们开发AI的速度并不比开发核能的速度快。甚至不接近。曼哈顿计划于1942年8月启动。四个月后的12月，他们就运行了一个200瓦的核反应堆，三年后的1945年8月，广岛和长崎的核爆炸造成了超过10万人死亡。在其成立10年后的1952年，美国和苏联都拥有了具备核能力的战略轰炸机，相互保证毁灭的时代开始出现。

OpenAI成立于2015年。在其成立三年后，OpenAI是否取得了像广岛那样具有影响力的成就？没有。OpenAI或任何与AI相关的事物，是否曾取得过像广岛那样具有影响力的成就？所以，谢天谢地，没有。在OpenAI成立10年后，是否存在相当于战略核轰炸机舰队的AI？就像曼哈顿计划成立10年后那样？同样没有。那么，AI到底是如何 supposed to be 比那发展得更快呢？是的，它不是。而SciShow应该知道得更多，做得更好。

但这种核能的比较引出了下一个关于这些YouTuber的垃圾AI论调的特征，这通常，但并非总是，以使人类灭绝为讨论的重点。所以我称之为“致命性”。这是AI将杀死我们所有人，或者至少几乎杀死我们所有人的想法。你知道，像《终结者》系列里的天网（Skynet）那样。

这种特别口味的虚构内容最近变得极其流行，自从一本名为《如果有人建造它，所有人都会死》（If Anyone Build It, Everyone Dies）的书出版以来，这本书充斥着未经证实的胡言乱语。我写了一篇关于那本书的视频，如果你想了解更多信息，我会在下面附上链接。

AI所谓的致命性是基于一些非常不稳定的假设，其中一个就是超级智能是可能的。另一个是，超级智能将拥有我们无法理解的、超越科学的、神一般的知识、欺骗和预测能力。还有另一个假设是，超级智能可以通过一个称为递归自我改进（Recursive Self-Improvement: AI通过创建更智能版本的自身来加速自身智能增长的过程）的过程，可能在非常短的时间尺度内实现，即AI制造一个更智能的版本，然后那个版本制造一个更智能的版本，以此类推。这是一个直接来自《银河系漫游指南》（The Hitchhiker's Guide to the Galaxy）的假设：“我所说的是即将到来的计算机，一台我没有资格计算其最近运行参数的计算机，但我将为你设计它。”

我不相信末日论者所说的超级智能，无论是人造的还是非人造的，都可能存在。这不是我的个人观点，而是因为超级智能，正如末日论者通常描述的那样，已经被成熟的数学和哲学原理所排除。不过今天我不会深入探讨，那将是另一个视频的主题，我正在制作中，所以如果你想让YouTube在你完成后推荐给你，请订阅。

所以，这里快速看一下一些YouTuber是如何推销AI致命性的观点的。

“人类的最终发明，人工智能。”“未来AI可能难以控制的许多原因。”“通用人工智能（AGI: 具备与人类相当的广泛认知能力的AI，能够理解、学习和应用知识于任何任务）可能是人类的最后发明。”“它有可能成为地球上最智能、因此最强大的存在，一个盒子里的神，它可以颠覆文明并带来我们的终结，而人类却无法找到阻止它的方法。”“如果或当一个超级智能AI决定摧毁我们。”

现在我们已经看到了一些关于必然性、特殊性和致命性的例子。让我们来谈谈这个叙事似乎来自哪里。

所以，跳转到SciShow视频的结尾，这里有一个披露信息，它比你乍一看认为的要重要得多。“感谢Control AI赞助了本视频。”Control AI联系过我，所以我对他们进行了一些研究，一旦我深入了解，我就开始到处看到他们的说辞。我真的很不喜欢，因为在我看来，他们就像AI行业的宣传工具。请注意，我不是说他们是OpenAI及其同类公司的宣传工具。在我看来，他们就像是，让我告诉你为什么。

所以，这是Hank引用的一份声明，我将把它放在屏幕的左上角。这份声明已经由许多AI高管、科学家和名人签署。但Hank没有告诉你的是，实际上存在两个不同的声明。

“在一份由诺贝尔奖得主、计算机科学家甚至AI公司首席执行官签署的声明中，这些人警告说，应对AI的风险，‘应该与大流行病和核战争等社会规模的风险并列，成为全球优先事项。’”

现在，在左下角，我放上了Control AI的网站。你会注意到，Control AI在Hank本视频中引用的‘灭绝声明’中提到了，然后立即将话题转向关于AI促进增长和创新的说法，特别是提到了医学。我的意思是，注意到那里巨大的话题转变了吗？这与他们的情况非常相似，你很快就会看到。

现在，在右边，我放上了red-lines.ai网站，这是另一个声明，它起源于第80届联合国大会。现在让我们看看Hank朗读的声明中省略了什么。Redline声明谈到了“广泛的虚假信息”，而AI造成的广泛虚假信息已经是一个真实存在的问题，但SciShow的声明完全忽略了它。我会在下面附上相关链接。

Redline声明还谈到了“对个人的大规模操纵，包括儿童”，这同样是一个真实存在的问题，而且同样，下面有链接，但SciShow的声明再次完全忽略了它。同样，一项源自联合国的声明警告“大规模失业”和“系统性人权侵犯”。这些也已经是真实存在的问题，并且实际上正在发生，但同样，左边的声明忽略了它，而且，我再次在下面附上了这些问题实际发生的参考资料。

声明中最接近谈论灭绝的是提到了大流行病，虽然大流行病很糟糕，但它们不是灭绝级别的威胁，因为，正如我们在2020年、1918年以及中世纪的黑死病以及许多其他时候发现的那样，人类，或者至少足够多的人，会采取行动来限制这些疫情的传播和影响，从而使我们中的许多人得以生存，无论是隔离还是其他方式。

但关于这份声明，最能说明问题的是，“确保所有先进AI提供商都负责任。”我想知道为什么Control AI的人可能不想让人们谈论或思考这一点。

Control AI的网页和SciShow在这个视频中，真的、真的想让你思考和担心潜在的、假设的、某天可能发生的、人类可能灭绝的风险，可能因为AI。而我，我为此制作了一个完整的视频，我会在下面附上链接，我认为我们的注意力需要集中在右边声明所强调的“当下”问题上，比如虚假信息、操纵和就业、人权侵犯，以及让OpenAI和其他提供商负责。

而且我不是唯一一个这样做的人，所以让我们来看看这些声明的签署者。仅签署右边声明的人中，至少有五位诺贝尔和平奖获得者，三位诺贝尔经济学奖获得者，一位诺贝尔化学奖获得者，至少三位前国家元首，以及一位前联合国主席，还有约翰·霍普菲尔德（John Hopfield），他与杰弗里·辛顿（Geoffrey Hinton）共同获得了2024年诺贝尔物理学奖，而辛顿签署了这两份声明。

现在，让我们看看谁只签署了灭绝声明。Sam Altman，OpenAI的CEO，以及Anthropic和Google DeepMind的CEO。嗯，想象一下。三家最大AI公司的CEO认为我们应该担心AI带来的灭绝，但认为我们不应该担心AI造成的虚假信息、操纵或人权侵犯，而且他们绝对不希望我们担心确保所有先进AI提供商都负责任。右边有五位诺贝尔和平奖获得者，左边有三位大型AI公司的CEO。你站在哪一边？你期望一个名为Control AI的组织站在哪一边？你希望SciShow站在哪一边？

诚然，Control AI花了大约七个月的时间，直到2024年中期，都在反对深度伪造（DeepFakes: 使用AI技术创建的逼真但虚假的图像或视频）。这是一件好事，尽管他们不再进行这项活动，而深度伪造的问题比以往任何时候都严重。哦，好吧。

而且，Control AI确实列在Red Lines声明的“合作伙伴”部分。从这一点来看，你会期望，或者至少我期望，如果Control AI真正扮演着合作伙伴的角色，那么Control AI就会支持，或者至少提及，联合国声明中的倡议。唉，并非如此。

Control AI只有一个网页谈论Red Lines倡议。这是那个网页。它实际上不在Control AI的网站上，它只是在他们的“AI新闻”子站上。这个页面确实包含了Red Line声明，或者说差不多。它实际上只是Red Line声明的一个截图，所以这个声明在Control AI的网站上并不以文本形式存在，而且Red Line声明中的任何风险都无法在其网站上被搜索引擎索引。

那么Control AI关于Red Lines声明包含了什么呢？嗯，所以这段引文，“我们敦促我们的政府建立明确的国际界限，以防止AI的普遍不可接受的风险。”好吧，好吧。考虑到他们实际上没有提及任何风险，却包含了关于风险的部分，这很奇怪。

而Control AI对Red Lines AI说：“声明没有具体说明红线应该是什么。”虽然技术上是正确的，但我认为这是不诚实的，因为声明确实列出了Control AI未能提及的几项风险，以及Control AI同样没有提及的问责呼吁。然后Control AI将话题转向谈论灭绝威胁，你知道，这是联合国Red Line声明明确省略的事情。我告诉过你，Control AI有突然改变话题的模式。

我在这里屏幕上放了几张Google搜索截图，显示了Red Line声明中的风险，这些风险在Control AI的网站上根本没有提及，至少在我录制这个视频的时候是如此。

我反复听到人们说，那些警告存在性威胁的末日论者也担心AI造成的即时问题。就像我刚才向你展示的那样，Control AI的网站情况并非如此。我所看到的大部分来自“AI将使我们灭绝”阵营的内容，都淡化或忽略了近期的问。

所以，让我们谈谈为什么这很重要。灭绝论证的问题在于，没有清晰、可行的解决方案来解决灭绝问题，因为它在目前只是假设性的。将焦点放在一个完全没有具体特征的问题上，会导致无休止且 fruitless 的辩论，并且对任何实际受害者都没有帮助或缓解，比如那些因聊天机器人鼓励的致命自残——据称——而受害的青少年的父母，或者那些可能被聊天机器人鼓励自残的青少年，他们的父母毫不知情且无法干预，我们以后可能会发现这些情况。

相比之下，许多即时问题都有明确的解决方案。例如，对开发或托管被证明生成了操纵或鼓励任何人伤害自己或他人的内容的服务的，处以严厉的、明确的刑事处罚。这样的法规将立即帮助青少年和父母，因为公司将被迫认真对待这些问题，否则将面临风险。而且，作为一项奖励，让公司对其软件的行为负责，将迫使它们放慢速度并了解如何控制其软件，这反过来也将极大地增进我们对如何控制其输出的理解，这必然会改善我们对对齐问题（alignment problem）的理解，而这必然会降低任何最终假设性失控的超级智能可能导致大量人类伤亡的风险。

因此，关注即时问题既有助于短期，也有助于长期的两种情况，而关注假设性问题则完全忽略了即时情况，即使假设性情况最终发生，也可能无济于事，因为我们不知道如何处理假设性情况。

但尽管“当前风险问责制”监管显然符合公众利益，根据我和联合国发布的Red Line声明，这不符合那些试图将尽可能多的钱装进自己口袋的AI行业人士的利益。因此，最大的AI公司的CEO们签署了以灭绝为导向的声明，而没有签署呼吁更多即时风险或要求他们负责的声明。

随着所有这些AI公司、非营利组织和大型教育YouTube频道都在推动这种末日叙事，我们这些试图保持现实的人很难跟上这一切，更不用说制作关于这一切的视频了。而且，制作一个包含事实和引用的视频比照搬AI行业的说辞要困难得多，也耗时得多。

但尽管我只有一个人，我将继续尽我所能。比如，给你一些需要注意的东西，来判断什么时候是行业宣传，就像我们在这段视频中讨论的那样， namely 必然性、特殊性和致命性。我想知道是否有更简单的方法来记住它。啊，对了。

在消费AI内容时，寻找谎言。嗯，现在我想起来了，我应该把超级智能（Superintelligence）也加进去，作为AI行业宣传中另一个纯粹是假设性的东西，但它完全没有实际证据支持。这应该让它更容易记住。致命性（Lethality）、必然性（Inevitability）、特殊性（Exceptionalism）和超级智能（Superintelligence）。当你看到AI内容时，寻找L.I.E.S.。如果你发现了它们，那么它很可能更多的是宣传，而不是实际应用。

所以祝我好运，如果你愿意，可以订阅。我想请大家在评论区保持友善，但这毫无用处，所以我不会 bother。永远要记住，互联网充满了bug和虚假信息，任何说不同话的人可能只是在重复行业说辞，试图分散你对这个行业——据称——正在伤害真实人们的注意力的所有方式，而你正在观看这个视频。让我们小心点。感谢观看。