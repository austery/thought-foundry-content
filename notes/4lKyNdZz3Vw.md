---
author: Internet of Bugs
date: '2025-12-10'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=4lKyNdZz3Vw
speaker: Internet of Bugs
tags:
  - ai-narrative-control
  - existential-risk-hype
  - immediate-harms
  - industry-propaganda
title: 为什么你喜欢的科普UP主在散布AI“谎言”？揭露行业宣传的“L.I.E.S.”陷阱
summary: 本文深入剖析了部分知名科普YouTube频道（如SciShow, Kurzgesagt）如何传播由AI行业资助或引导的、关于人工智能的“必然性、例外性、致命性”等夸大叙事（L.I.E.S.）。作者认为，这种叙事转移了公众对当前AI带来的真实危害，如虚假信息传播和工人失业的关注，并呼吁关注联合国红线声明所强调的即时问责制。
insight: ''
draft: true
series: ''
category: geopolitics
area: society-systems
project:
  - us-analysis
  - geopolitics-watch
people:
  - Sam Altman
  - Geoffrey Hinton
  - Hank Green
companies_orgs:
  - Control AI
  - OpenAI
  - Anthropic
  - Google DeepMind
  - SciShow
  - Kurzgesagt
products_models:
  - ChatGPT
media_books:
  - If Anyone Build It, Everyone Dies
status: evergreen
---
### 警惕将AI视为“世界末日”的YouTuber言论

今天，我想和大家谈谈那些将人工智能（AI）视为“世界末日即将来临”的YouTuber们。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Today, I want to talk to you about YouTubers who treat AI as if it stands for "apocalypse imminent."</p>
</details>

毫无疑问，在YouTube上有大量的AI相关评论。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">There is, I'm sure I don't have to tell you, a ton of AI-related commentary here on YouTube.</p>
</details>

毕竟，那些花费大量时间撰写脚本和制作视频的人，很可能对那些可以生成脚本和视频的软件有很多看法。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">After all, people who spend a lot of time writing scripts and making videos are likely to have a lot of opinions about software that generates scripts and videos.</p>
</details>

当然，YouTuber们会制作关于他们对AI看法的视频，因为YouTuber们会制作关于他们对**一切**看法的视频。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And of course, YouTubers make videos about their opinions on AI because YouTubers make videos about their opinions on absolutely everything.</p>
</details>

确实也有一些关于**LLMs (Large Language Models: 基于海量文本数据训练的深度学习模型)** 工作原理的优秀YouTube内容，比如我多次推荐并附在下方的3Blue1Brown系列视频，那真是太棒了。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And there is some great YouTube content on how large language models work, like the series on 3Blue1Brown that is amazing that I've recommended many times and I've linked below.</p>
</details>

但是，如果你仔细想想，YouTube上关于AI的许多视频似乎有所不同。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But a lot of the YouTube videos about AI, if you think about it, seem different.</p>
</details>

来自许多不同创作者的YouTube视频数不胜数，它们呈现出一种惊人一致的叙事。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">There are countless videos on YouTube, from a bunch of different creators, that present a surprisingly consistent narrative.</p>
</details>

但这种叙事很奇怪，对吧？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But an odd one?</p>
</details>

这是一种你会期望从那些坚持认为世界末日即将来临的疯狂传教士那里听到的故事。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">It's the kind of story you'd expect to hear from those crazy preachers that keep insisting that the world is about to end.</p>
</details>

在任何其他情况下，如果没有背后所有的金钱、营销，以及AI行业劫持了过去一个世纪以来的科幻小说套路的方式，我们都会意识到这有多么荒谬。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">In any other context, without the money and the marketing behind all this and the way that the AI industry has co-opted so many of the sci-fi tropes from over the last century or so, we would all realize how preposterous this is.</p>
</details>

当这些言论来自那些为了博取点击量而以最夸张的方式评论当前趋势的人时，就已经够糟糕了。更糟糕的是，这些视频来自那些声称并且已经建立了良好研究、负责任、事实性和教育性声誉的频道。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And it's bad enough when it comes from people who were making videos about the current trends in the most exaggerated way possible just to try to get clicks. What's worse is when the videos are from channels that claim to be, and have built a reputation for being, well-researched, responsible, factual and educational.</p>
</details>

这些视频有时甚至歪曲过去和现在的情况，以证明一个耸人听闻的AI未来是合理的。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And those videos sometimes even misstate the past and present in order to justify a sensationalist AI future.</p>
</details>

现在，有很多频道和很多视频我本可以挑选。今天，我只挑选了来自SciShow、Kyle Hill和Kurzgesagt的几个例子，他们各自的AI相关视频累计获得了数百万次观看。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Now, there are a lot of channels that a lot of videos I could have picked. Today, I've just picked a few examples from SciShow, Kyle Hill and Kurzgesagt, each of which has accumulated millions of views across their AI-related videos.</p>
</details>

我将向你们展示一些片段，指出其中的错误信息和缺乏根据的断言，然后我将向你们展示这如何助长了AI行业试图做的事情，这对真实的人们造成了多大的伤害，以及我们应该做些什么来替代。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I'm going to show you some clips, I'm going to point out the misinformation and the unsupported assertions, and then I'm going to show you how that props up what the AI industry is trying to do, how harmful that is being to real people, and what we should be doing instead.</p>
</details>

我会尽量保持冷静，但这对我来说会很困难，因为我……

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And I'm going to try to do it as calmly as I can, which is going to be hard for me because I'm pi-------------</p>
</details>

欢迎来到“Bug的互联网”（Internet of Bugs）。我叫卡尔。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Welcome to the Internet of Bugs. My name is Carl.</p>
</details>

我自20世纪80年代以来一直是一名软件专业人士，我正在尽我所能使互联网成为一个更安全、更可靠、更少出现Bug的地方。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I've been a software professional since the 1980s, and I'm trying to do what I can to make the Internet a safer, more reliable and less buggy place.</p>
</details>

在我看来，AI是当前互联网面临的最大威胁，因为AI生成的代码仍然充满了Bug和安全问题，而且围绕AI的叙事正在促使公司解雇本可以修复Bug并改善我们所有人在线体验的称职软件开发人员。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And in my professional opinion, AI is the biggest current threat to the Internet, because the code AIs generate is still riddled with bugs and security problems, and because the narrative around AI is motivating companies to lay off competent software developers that could otherwise be fixing bugs and improving the online experience for all of us.</p>
</details>

而且，尽管这超出了我特定的软件相关YouTube领域，但AI公司及其创造物正在造成比Bug更糟糕的问题：它们正在将互联网变成虚假信息传播的媒介，甚至更糟的是，它们以AI精神错乱的形式对无辜人类造成伤害，鼓励麻烦制造者进行自我伤害——据称——并且可能以我们尚未发现的方式造成问题。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But also, even though it's outside my particular software-related YouTube niche, because these AI companies and their creations are causing problems even worse than bugs, they're turning the Internet into a transportation medium for disinformation and worse, inflicting harm on innocent humans in the form of AI psychosis, encouraging trouble to keep dangerous to commit self-harm - allegedly - and likely causing problems in ways that we haven't even found out about yet.</p>
</details>

好的，在开始之前快速说明一下。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Okay, quick housekeeping note before we get started.</p>
</details>

我最近发布了一个关于SciShow关于AI的特定剧集的吐槽视频，我认为那个视频特别过分。但由于那个SciShow视频只是YouTube上AI虚假信息海洋中的一滴水，而且太多人认为那个视频更多是关于“SciShow很糟糕”，而不是“AI宣传很糟糕”，所以我将用这个视频来替换它，并且我已经将该视频脚本中的一些内容整合到了这个视频中。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I recently made a video that was a rant about a specific SciShow episode about AI that I thought was particularly egregious, but because that one SciShow video is really just a drop in the ocean that is AI disinformation on YouTube, and because too many people thought that that video was more about "SciShow is bad" rather than "AI propaganda is bad," I'm replacing that video with this one, and I've incorporated some of that video script into this one.</p>
</details>

所以，如果一些老观众觉得有些内容听起来很熟悉，那就是这个原因。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So if some of this sounds familiar to regular watchers of this channel, that's why.</p>
</details>

### 叙事一：AI的“必然性”

好的，许多这些YouTube视频中推送的第一个一致观点，我称之为AI的必然性。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Okay, the first consistent idea that's pushed in many of these YouTube videos is what I'll call the inevitability of AI.</p>
</details>

这种观点认为，**ASI (Artificial Superintelligence: 人工超级智能)** 无论我们做什么都将发生，因此反抗它是没有意义的。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The idea that super AI is going to happen no matter what we do, and so there's no point in fighting it.</p>
</details>

让我给你们看一些例子。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Let me show you some examples.</p>
</details>

不要误会，ChatGPT是这些生成式AI中最引人注目的，但它只是第一个。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And make no mistake, chat GPT is the most visible of these generative AIs, but it's just the first.</p>
</details>

还会有更多，而且它们会更好，然后那些会变得更好、更快，然后会发生什么？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">There will be more, and they will be better, and then those will get better, faster, and then what happens?</p>
</details>

这些模型已经像它们能达到的最好水平一样糟糕了，我完全接受这一点。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">These models are as bad as they are ever going to be, which I totally accept.</p>
</details>

就像，这是真的。过去，AI是狭隘的，只能擅长一项技能，但在所有其他方面都很差。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Like that is true. In the past, AI was narrow and able to become good at one skill, but was rather bad in all the others.</p>
</details>

仅仅通过构建更快的计算机并将更多的资金投入到AI训练中，我们就会得到新一代更强大的AI。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Simply by building faster computers and pouring more money into AI training, we'll get as new, more powerful generations of AI.</p>
</details>

这些技术将继续在全球范围内发展和传播。如果美国停止开发AI，中国不会停止。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">These technologies are going to continue to develop and spread globally. If the United States stops developing AI, China is not going to.</p>
</details>

所以这里有一个主题的几种变体。一种是AI会变得越来越好的叙事。另一种是，如果我们不制造它，中国就会制造它。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So there are a couple of variants on a theme here. One is the AI is going to get better and better and better narrative. And the other is, well, if we don't build it, China's going to.</p>
</details>

我发现很有趣的是，这些观点一次又一次地被提出，就好像它们是显而易见的，除了像圣格林（St. Greene）所说的“就像，这是真的”之外，没有任何更好的理由。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I find it very interesting that those ideas are presented over and over as if they're just obvious, with no better justification than, as St. Greene says, like, that is true.</p>
</details>

我不相信它只会变得更好，或者说这种叙事。事实上，我做了一个完整的视频，说明为什么我认为这是AI行业给我们最大的谎言之一。我会在下面链接。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I don't buy that it'll only ever get better or narrative. In fact, I made a whole video about how I think that's the biggest lie we get from the AI industry. And I'll link that below.</p>
</details>

但这不仅仅是我个人的看法。AI批评者多年来一直在说这一点，行业内的人也终于承认，仅仅增加更多的计算能力是不够的。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But it's not just me. AI critics have been saying that for years and people from the industry are finally admitting that just adding more computers isn't enough.</p>
</details>

但对于这次讨论重要的是，这种“必然性思维”的逻辑结论是：试图阻止AI发展是徒劳的。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But what's important for this discussion is that the logical conclusion of the inevitable way of thinking is that trying to stop AI development is pointless.</p>
</details>

现在，谁可能希望你相信这一点？为什么？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Now, who might want you to believe that and why?</p>
</details>

### 叙事二：AI的“例外性”

让我们继续讨论这些AI炒作视频似乎共有的下一个特征，我称之为例外性。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Let's move on to the next one of the attributes that these AI hype videos seem to share, which is what I'll call exceptionalism.</p>
</details>

这就是AI是如此特殊、如此新颖、与人类以前经历过的任何事物都如此不同，以至于以前的角色和经验都不适用的观点。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">This is the idea that AI is something so exceptional, so new, so different than anything humanity has experienced before that previous roles and experiences just don't apply.</p>
</details>

这是来自Kurzgesagt一个荒谬地命名为“AI，人类的最终发明”的视频中的片段。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So here's a clip from a Kurzgesagt video called, ridiculously: "AI, humanity's final invention."</p>
</details>

“1997年，一个AI通过击败国际象棋世界冠军震惊了世界，证明了我们可以制造出超越我们的机器。”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">In 1997, an AI shocked the world by beating the world champion in chess, proving that we could build machines that could surpass us.</p>
</details>

哦，好的。让我弄清楚。1997年，一个国际象棋程序终于证明了我们可以制造出超越我们的机器。真的吗？你这么认为？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Oh, okay. Let me get this straight. In 1997, a chess program finally proved that we could build machines that could surpass us. Really? You think so?</p>
</details>

让我向你介绍埃德温·伯克利（Edwin Berkeley）1949年出版的《巨型大脑或会思考的机器》的第7页，其中写道：“机械大脑可以做的思考类型。它们可以做很多种类的思考。除其他外，它们可以学习你告诉它们的东西，在需要时应用指令，读取和记住数字，加、减、乘、除和四舍五入，查找表格中的数字，查看结果并做出选择，连续执行这些操作，写出答案，确保答案正确，知道一个问题已经完成并转向另一个问题，确定它们自己的大部分指令，并独立工作。它们比你我做得好得多。”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Let me introduce you to page seven of "giant brains or machines that think," by Edwin Berkeley, published in 1949, quote, "The kinds of thinking a mechanical brain can do. There are many kinds of thinking that mechanical brains can do. Among other things, they can learn what you tell them, apply the instructions when needed, read and remember numbers, add, subtract, multiply, divide and round off, look up numbers and tables, look at a result and make a choice, do long chains of these operations one after another, write out an answer, make sure the answer is right, know that one problem is finished and turn to another, determine most of their own instructions, and work unattended. They do these things much better than you or I, unquote."</p>
</details>

这听起来像是对**ChatGPT (Chat Generative Pre-trained Transformer: OpenAI开发的一款基于Transformer架构的对话式大型语言模型)** 的现代反应。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Now that sounds like a modern reaction to ChatGPT ChatGPT.</p>
</details>

嗯，除了“确保答案正确”这一点，因为ChatGPT可做不到这一点。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Well, with the exception of "make sure the answer is right" because ChatGPT ain't doing that one.</p>
</details>

事实证明，关于AI将以我们从未见过的方式改变一切的绝大多数说法，与20世纪40年代和50年代关于巨型大脑（即计算机）将如何以我们从未见过的方式改变一切的说法极为相似。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">It turns out that the vast majority of the claims about how AI is going to change everything in ways we've never seen before are incredibly similar to the claims in the 1940s and 1950s about how giant brains, aka computers, were about to change everything in ways we've never seen before.</p>
</details>

如果你阅读技术如何影响社会历史，你会发现老一辈人倾向于对新发明技术感到恐慌，其方式与他们的父母或祖父母对他们认为正常的技术感到恐慌的方式完全相同。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">If you read about the history of how technology impacts society, you'll find that older humans tend to freak out about newly invented technologies in exactly the same way that their parents or grandparents freaked out about tech that they consider normal.</p>
</details>

后面还有一句引文：“许多年轻人可以自由接触浪漫小说、小说和戏剧，毒害了许多有前途的青年的思想并腐蚀了他们的道德。”这出自1790年希区柯克牧师（Reverend Ennis Hitchcock）的《布鲁姆斯格罗夫家族回忆录》。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Later this quote, "The free access which many young people have to romances, novels, and plays has poisoned the mind and corrupted the morals of many a promising youth." That's from "memoirs of the Bloomsgrove family" by Reverend Ennis Hitchcock published 1790.</p>
</details>

几乎每一代人都坚持认为现在发生的事情是不同的，如此不同，以至于正在改变一切。然而，生活仍在继续。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Almost every generation insists that what's happening now is different, so different that it's changing everything. And yet, life goes on.</p>
</details>

但事实是，计算机在几十年中超越我们的方式有很多很多。我们只是把计算机自我们父母上小学时起就比我们擅长算术和文本搜索这件事视为理所当然。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But the fact remains that there are many, many, many ways that computers have surpassed us over the decades. We just take it for granted now that computers have been better than us at arithmetic and searching text since my parents were in elementary school.</p>
</details>

但是“AI证明了X会发生”这样的论点，而事实上X早在20世纪40年代就发生了，这只是这些视频声称AI具有例外性的方式之一。有时它们还决定凭空捏造事实。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But arguments like "AI proves that X can happen" when in fact X happened in the 1940s is only one of the ways that these videos claim that AI is exceptional. Sometimes they also decide to just make things up.</p>
</details>

Kurzgesagt又来了：“人类还没有为接下来发生的事情做好准备，无论是在社会上、经济上还是道德上。”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Here's Kurzgesagt again: "Humanity is not ready for what will happen next, not socially, not economically, not morally."</p>
</details>

好吧，你确切地是怎么知道的？这又基于什么呢？猜猜这一定是汉克·格林（Hank Green）的“就像，这是真的”学派那一套。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Okay, and you know that how, exactly, and that's based on what exactly? Guess it must be the Hank Green School of, "like, that is true."</p>
</details>

所以接下来的片段展示了另一种声称例外性的方式，在这种情况下，是通过对过去事件不诚实。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So these next clips have another way of claiming exceptionalism, In this case, by just being disingenuous about past events.</p>
</details>

“我们确实正处在一个悬崖边上。以前从未发生过这样的事情，而且速度从未如此之快。”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">We really are at a precipice here. Nothing has happened before like this, and not so fast.</p>
</details>

技术发展很快。我们从莱特兄弟到第一家商业航空公司只用了11年时间。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Technology moves fast. We went from the Wright brothers to the first commercial airline in just 11 years.</p>
</details>

但即使与飞机、抗生素和核能相比，我们开发人工智能的速度也超过了所有这些。这一切发生得非常快，比手机、比计算机都快。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But even compared to aircraft, antibiotics, and nuclear power, the speed at which we are developing artificial intelligence beats them all. This is all happening really fast, faster than cell phones, faster than computers.</p>
</details>

所以这完全是公然的谎言。让我们从最后面挑一个例子。我们开发AI的速度并没有超过开发核能的速度。而且差距还很大。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So this is just blatantly false. Let's pick one example from the end there. The speed at which we're developing AI does not beat the speed at which we develop nuclear power. And it's not even close.</p>
</details>

**曼哈顿计划 (Manhattan Project: 二战期间美国主导的核武器研发项目)** 成立于1942年8月。四个月后的12月，他们就运行了一个200瓦的核反应堆；再过三年，即1945年8月，广岛和长崎发生了核爆炸，造成超过10万人死亡。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The Manhattan Project was founded in August of 1942. By December, four months later, they had a nuclear reactor running at 200 watts and three years after that, in August 1945, more than 100,000 people were killed in nuclear explosions at Hiroshima and Nagasaki,</p>
</details>

在其成立十年后的1952年，美苏两国都拥有了具备核打击能力的战略轰炸机，**MAD (Mutual Assured Destruction: 相互确保摧毁)** 时代也开始形成。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">10 years after its founding in 1952. Both the US and the Soviet Union had nuclear-capable strategic bombers and the area of mutual assured destruction was beginning.</p>
</details>

OpenAI成立于2015年。OpenAI在其成立三年后取得了像广岛事件那样有影响力的成就了吗？没有。OpenAI或任何与AI相关的事物，在其成立后十年内，是否取得了像广岛事件那样有影响力的成就？谢天谢地，没有。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">OpenAI was founded in 2015. Had OpenAI accomplished anything as impactful as Hiroshima by three years after its founding? No. Has OpenAI or anyone or anything related to AI accomplished anything as impactful as Hiroshima ever? So no, thank God.</p>
</details>

现在，在OpenAI成立十年后，是否存在一个相当于战略核轰炸机队的AI对应物，就像曼哈顿计划成立十年后存在的那样？同样没有。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Is there now, 10 years after OpenAI's founding, an AI equivalent of a strategic nuclear bombing fleet, as there was 10 years after the Manhattan Project? Again, also no.</p>
</details>

那么，AI到底是如何被声称比那发展得更快呢？是的，它没有。SciShow应该知道得更清楚，也应该做得更好。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So how exactly is it that AI is supposed to be developing faster than that? Yeah, it's not. And SciShow should know better, and do better.</p>
</details>

### 叙事三：AI的“致命性”

但这种核比较引出了YouTube上这些垃圾AI论断所共有的下一个特征，这些论断通常（但并非总是）以消灭人类为讨论点。所以我称之为致命性。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But this nuclear comparison leads to the next attribute of this garbage AI claims by YouTubers, which often, but not always, is discussed in terms of making humanity extinct. So I'm going to call it lethality.</p>
</details>

这就是AI将杀死我们所有人，或者至少是几乎所有人。你知道，就像《终结者》系列中的天网（Skynet）那样。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">This is the idea that AI is going to kill all of us, or at least almost all of us. You know, Skynet from the Terminator series, that sort of thing.</p>
</details>

这种特定形式的虚构在最近变得非常流行，自从一本名为《如果有人建造它，所有人都会死》（If Anyone Build It, Everyone Dies）的书问世以来，这本书纯粹是毫无根据的胡言乱语。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">This particular flavor of fabrication has gotten incredibly popular recently, since the release of a book called If Anyone Build It, Everyone Dies, which is just more an unsubstantiated nonsense.</p>
</details>

我做了一个关于那本书的视频，如果你想了解更多信息，我会在下面链接。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I made a video about that book, which I'll link below if you want even more information about that.</p>
</details>

AI所谓的致命性建立在一些非常站不住脚的假设之上，其中之一是超级智能是可能的。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The supposed lethality of AI is based on some really shaky assumptions, one of which is that artificial superintelligence is possible.</p>
</details>

另一个假设是，超级智能将拥有超越科学允许的、神话般的、上帝般的知识、欺骗和预测能力。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Another is that artificial superintelligence will have incomprehensibly advanced mythical God-like powers of knowledge, deception, and prediction beyond what science allows.</p>
</details>

另一个假设是，超级智能可以通过一种称为**递归自我改进 (Recursive Self-Improvement: AI不断创建比自己更智能的版本，从而实现指数级智能增长的假设过程)** 的过程达到，即AI制造一个更聪明的版本，那个版本又制造一个更聪明的版本，以此类推。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And another of which is that artificial superintelligence can be reached, probably in very short time scales, by a process called recursive self-improvement, which is where AI makes a smarter version of itself, and that version of a smarter version of itself, and that version makes a smarter version of itself, and so on and so on.</p>
</details>

这是一个直接取自《银河系漫游指南》的假设。“我说的不是别人，而是将要在我之后的计算机，一台我没有资格计算其最近操作参数的计算机，但我会为你设计它。”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">It's a hypothesis that's taken straight out of the Hitchhiker's Guide to the Galaxy. "I speak of none, but the computer that is to come after me, a computer whose nearest operational parameters, I am not worthy to calculate, yet I will design it for you."</p>
</details>

我不相信末日论者所描述的超级智能，无论是人工的还是非人工的，可能存在。这不是我的个人观点，而是因为末日论者通常描述的超级智能被公认的数学和哲学原理所排斥。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I do not believe that the doomer idea of superintelligence, artificial or not, can possibly exist. And that's not my personal opinion, that's because superintelligence, as doomers usually describe it, is precluded by well-established mathematical and philosophical principles.</p>
</details>

我今天不会深入探讨这一点，那将是另一篇独立的视频，我正在制作中，所以如果你想在我完成后让YouTube推荐给你，请点击订阅按钮。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I'm not going into that today though, that's going to have to be its own video, I'm working on it, so hit the subscribe button if you want YouTube to suggest it to you when I get it done.</p>
</details>

所以，这里快速看一下一些YouTuber是如何推动AI致命性观点的。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So here's a quick look at how some YouTubers push the idea of AI lethality.</p>
</details>

“人类的最终发明，人工智能。”“未来AI可能无法控制的许多原因。”“**AGI (Artificial General Intelligence: 人工通用智能，指具备人类同等认知能力的AI)** 可能是人类的最后一次发明。”“它有可能成为地球上最智能、因此最强大的存在，一个可以颠覆文明并带来我们终结的盒子里的神，而人类却无法想出阻止它的方法。”“如果一个超级智能AI决定摧毁我们，或者当它决定这样做的时候。”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">"Humanity's final invention, artificial intelligence." "Many reasons why future AI might be impossible to control." "AGI might be the last invention of humanity." "It's possible that it could become the most intelligent and therefore most powerful being on Earth, a god in a box that could subvert civilization and bring about our end with humanity unable to come up with a way to stop it." "If or when a super-intelligent AI decides to destroy us."</p>
</details>

### 叙事来源：谁在推动末日叙事？

现在我们已经看到了一些关于必然性、例外性和致命性的例子。让我们谈谈这种叙事似乎来自哪里。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So now we've seen some examples of inevitability and exceptionalism and lethality. Let's talk a bit about where that narrative seems to be coming from.</p>
</details>

所以，跳到SciShow视频的结尾，这里有一个披露声明，它的重要性远超你最初的想象。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So jumping to the end of the SciShow video, here's a disclosure that's a lot more important than you'd think at first.</p>
</details>

“感谢Control AI赞助本视频。”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">"Thanks to Control AI for sponsoring this video."</p>
</details>

Control AI联系过我，所以我对他们做了一些研究，当我深入了解后，我开始在各个地方看到他们的论点。我真的很不喜欢这样，因为它们给我的感觉就像它们正在充当AI行业的宣传部门。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So I've had Control AI reach out to me, and so I've done some research on them, and once I looked into them I started seeing their talking points everywhere. I really don't like it because they feel to me as if they are acting as a propaganda arm of the AI industry.</p>
</details>

请注意，我不是说他们是OpenAI及其同伙的宣传部门。在我看来，他们的行为就像是，让我告诉你为什么。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Note that I'm not saying they are a propaganda arm of openAI and friends. It just feels to me like they are acting like it, let me tell you why.</p>
</details>

这是汉克（Hank）引用的一份声明，我将把它放在你屏幕的左上角。这份声明由许多AI高管、科学家和名人签署。但汉克不会告诉你的是，实际上存在两个不同的声明。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So here is Hank quoting from a statement that I'm going to put in the upper left of your screen. That's been signed by a bunch of AI executive scientists and famous people, but what Hank isn't going to tell you is that there are actually two different such statements.</p>
</details>

“在一份由诺贝尔奖获得者、计算机科学家甚至AI公司CEO签署的声明中，这些人警告说，解决AI风险应该‘与大流行病和核战争等社会规模风险一样，成为全球优先事项’。”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">"In a statement signed by Nobel Prize winners, computer scientists, and even AI company CEOs, these people warned that addressing the risk of AI, quote, "should be a global priority alongside other societal-scale risks such as pandemics and nuclear war."</p>
</details>

现在我在左下角放上Control AI的网站。你会注意到，Control AI在他们视频中引用的这份声明中提到了汉克引用的“灭绝声明”，然后立即将话题转移到关于AI促进增长和创新的主张上，特别是医疗任务。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Now in the lower left, I'm putting Control AI's website. You'll note that Control AI mentions the extinction statement that Hank quoted from in this video, and then immediately changes the subject to claims about AI boosting growth and innovation, specifically missioning medicine.</p>
</details>

我的意思是，注意那里突然的巨大主题转变？正如你稍后会看到的，这是一种相当常见的模式。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I mean, notice the big abrupt subject change there? It's quite the pattern with them, as you'll see in a minute.</p>
</details>

现在在右侧，我放上red-lines.ai网站，那是源自第80届联合国大会的另一份声明。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Now on the right side, I'm putting up the site red-lines.ai with the other statement, which originated during the 80th UN General Assembly.</p>
</details>

让我们看看汉克宣读的声明中遗漏了什么。红线声明谈到了“广泛的虚假信息”，而AI造成的广泛虚假信息已经是真实存在的问题，但SciShow的声明完全忽略了这一点。我会在下面放上相关链接。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Now let's look at what was omitted from the statement that Hank read. The redline statement talks about "widespread disinformation," and widespread disinformation from AI is already a real problem, but the SciShow statement completely ignores it. I'll put links about that below.</p>
</details>

红线声明还谈到了“对个人（包括儿童）的大规模操纵”，这也是一个真实存在的问题，同样，下面有链接，但SciShow的声明再次完全忽略了这一点。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The redline statement also talks about "large-scale manipulation of individuals, including children," and again, already a real problem, and again, links below, but the SciShow statement again just completely ignores it.</p>
</details>

同样，一份源自联合国的声明警告说“大规模失业”和“系统性人权侵犯”。这些也是现在正在发生的真实问题，但左边的声明再次忽略了它们，同样，我把这些问题实际发生的参考资料放在下面了。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Likewise, a statement that originated from the UN warns about "mass unemployment" and "systemic human rights violations." These are also already real problems that are actually occurring now, but again, the statement on the left ignores it, and again, I've put references to those problems actually happening now below.</p>
</details>

关于灭绝，这份声明最接近的提及是“大流行病”，虽然大流行病很糟糕，但它们不是灭绝级别的威胁，因为正如我们在2020年、1918年以及中世纪黑死病期间以及其他许多时候发现的那样，人类，或者至少足够多的人类，会采取行动限制这些疫情的传播和影响，从而使我们中的许多人得以幸存，无论是隔离还是其他措施。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The closest the statement gets to talking about extinction at all is mentioning pandemics, which, while bad, are not extinction-level threats, because, as we found out in 2020, and 1918, and during the Black Plague in the middle ages and a bunch of other times, the human race, or at least enough of us, will take actions to limit the spread and impact of such outbreaks, such that many of us will survive, be that quarantine or whatever.</p>
</details>

但关于这份声明最能说明问题的一点是：“确保所有先进的AI提供商都承担责任。”我怀疑Control AI的人为什么可能不希望人们谈论或思考这一点。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But the most telling thing about this statement is, quote, "ensuring that all advanced AI providers are accountable." I wonder why the Control AI people might not want people talking about that or thinking about that.</p>
</details>

Control AI的网页和SciShow在这个视频中，非常非常希望你思考和担心未来某一天可能因为AI而导致人类灭绝的潜在假设性风险。而我——我做了一个完整的视频来讨论这个，我会在下面链接——认为我们的注意力需要集中在右边声明强调的“当下”问题上，比如虚假信息、操纵和就业、人权侵犯，以及让OpenAI和其他提供商承担责任。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The Control AI webpage and SciShow in this video really, really want to make you think about and worry about the potential hypothetical someday maybe sometime in the future risk of humanity may be going extinct possibly because of AI. Whereas I, and I made a whole video about this that I'll link below, think that our attention needs to be focused on the "now" problems emphasized by the statement on the right, like disinformation, manipulation and employment, human rights violations, and holding open AI, and the other providers accountable.</p>
</details>

我不是唯一一个这么想的人，所以让我们看看这些声明的签署者。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And I'm not the only one, so let's take a look at the signatories of these statements.</p>
</details>

仅签署右边声明的人中，至少有五位诺贝尔和平奖获得者，三位诺贝尔经济学奖获得者，一位诺贝尔化学奖获得者，至少三位前国家元首，以及一名前联合国主席，还有约翰·霍普菲尔德（John Hopfield），他与杰弗里·辛顿（Geoffrey Hinton）共同获得了2024年诺贝尔物理学奖，而辛顿签署了这两份声明。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Of the people that only signed the statement on the right are at least five Nobel Peace Prize recipients, three recipients of the Nobel Prize for Economics, one for Chemistry, at least three former heads of state, and a former president of the United Nations, as well as John Hopfield, who shared the 2024 Nobel Prize in Physics with Geoffrey Hinton, and Hinton signed both statements.</p>
</details>

现在，让我们看看谁只签署了灭绝声明。OpenAI的首席执行官**萨姆·奥特曼 (Sam Altman)**，以及Anthropic和Google DeepMind的首席执行官。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Now, let's look at who only signed the extinction statement. Sam Altman, the CEO of OpenAI, as well as CEOs of Anthropic and Google DeepMind.</p>
</details>

嗯，真想象不到。三家最大AI公司的CEO认为我们应该担心AI带来的灭绝风险，但却不认为我们应该担心AI造成虚假信息、操纵或人权侵犯，而且他们绝对不希望我们担心确保所有先进AI提供商承担责任。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Hmm, imagine that. The CEOs of three of the largest AI companies think that we should worry about extinction from AI, but don't think we should worry about AI causing disinformation, manipulation, or human rights violations, and they definitely don't want us to worry about ensuring that all advanced AI providers are accountable.</p>
</details>

右边是五位诺贝尔和平奖获得者，左边是三位大型AI公司的CEO。你站在哪一边？你会期望一个名为Control AI的组织站在哪一边？你希望SciShow站在哪一边？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Five Nobel Peace Prize recipients on the right, three CEOs of huge AI companies on the left. Which side are you on? Which side would you expect a group called Control AI to be on? Which side would you hope that SciShow would be on?</p>
</details>

诚然，Control AI确实花了大约七个月的时间（直到2024年中期）来反对深度伪造（DeepFakes）。这是一件好事，尽管他们不再进行这项运动了，而深度伪造的问题比以往任何时候都更严重。哦，算了。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Now, it's true that Control AI did spend seven months or so, until mid-2024, campaigning against DeepFakes. And that's a good thing, although they're no longer running that campaign, and DeepFakes are a bigger problem than ever. Oh well.</p>
</details>

而且，Control AI确实在红线声明的“合作伙伴”部分有列出。从这一点来看，你会期望——或者至少我会期望——如果Control AI真正充当合作伙伴，那么Control AI就会支持或至少提及联合国声明中的倡议。唉，事实并非如此。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And it's also true that Control AI is listed in the "partner" section of the red lines statement. From that, you would expect, or at least I would expect, that if Control AI was truly acting as a partner, that Control AI would be supporting, or at least mentioning, the initiatives in the UN statement. Alas, not so much.</p>
</details>

Control AI只有一个网页谈论红线倡议。这是那个页面。它实际上不在Control AI的网站上，它只是在他们的“AI新闻”子站上。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">There's only one web page from Control AI that talks about the red lines initiative. Here's that page. Which isn't actually on the Control AI site, it's just on their "AI news" substack.</p>
</details>

这个页面确实包含了红线声明，或者说它看起来是这样呈现给你的。它实际上只是红线声明的截图，所以这个声明并不以文本形式存在于Control AI的网站上，而且他们网站上的任何搜索引擎都无法索引其中任何一个风险。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And this page does include the red line statement, or kind of it appears to you. What it actually has is just a screenshot of the red line statement, so this statement doesn't exist as text on the Control AI site, and none of the risks from red line statement are mentioned in the text, and none of them can be indexed by a search engine on their site.</p>
</details>

那么Control AI包含了关于红线声明的哪些内容呢？好吧，他们写道：“我们敦促我们的政府建立明确的国际边界，以防止AI带来的普遍不可接受的风险。”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So what does Control AI include about the red lines statement? Well, so this quote, "We urge our governments to establish clear international boundaries to prevent universally unacceptable risks for AI."</p>
</details>

好吧，考虑到他们实际上没有提到任何风险，他们却包含关于风险的部分，这很奇怪。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Well, okay. It's odd for them to include the part about risks considering they don't actually mention any of the risks.</p>
</details>

Control AI对红线AI说：“该声明没有具体说明红线应该是什么，”虽然技术上是正确的，但我认为这是不真诚的，因为该声明确实列出了几项Control AI未能提及的风险，同时也呼吁了Control AI同样没有提及的问责制。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And Control AI says about red lines AI, quote, "The statement doesn't specify what the red line should be," which, while technically true, I think is disingenuous because the statement does list several risks that Control AI fails to mention, as well as call for accountability that Control AI also doesn't mention.</p>
</details>

然后Control AI将话题转移到谈论灭绝威胁上，你知道，这是联合国红线声明明确遗漏的东西。我告诉过你，Control AI有突然改变话题的模式。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And then Control AI changes the subject to talking about an extinction threat, you know, the thing that the UN red line statement explicitly omitted. I told you there was a pattern of Control AI abruptly changing the subject.</p>
</details>

我在这里展示了几次谷歌搜索，显示了红线声明中存在的、但在Control AI网站上（至少在我录制此视频时）完全没有提及的风险。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And I'm putting up here on the screen several Google searches showing risks from the red line statement that are not mentioned at all on the Control AI site, at least at the time I was recording this video.</p>
</details>

我一再从人们那里听说，那些警告存在性威胁的末日论者也担心AI正在造成的即时问题。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I've heard over and over from people that the doomers who warn about the existential threats also worry about the immediate issues that AI is causing.</p>
</details>

就像我刚才向你展示的，Control AI的网站情况并非如此。我所看到的“AI将导致人类灭绝”阵营的大部分内容，都在淡化或忽略近期的（即时）问题。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Like I just showed you, though, with the Control AI website, that has not been my experience at all. Most of what I've seen out of the "AI will make a extinct camp" downplays or ignores the near-term problems.</p>
</details>

### 关注当下：为什么即时问责更重要

所以让我们谈谈为什么这很重要。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So let's talk about why that matters.</p>
</details>

灭绝论证的问题在于，由于它在当前阶段仅仅是假设性的，因此没有清晰、可操作的路径来解决这个灭绝问题。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The problem with the extinction argument is that there's no clear, actionable path to solving the extinction problem, because it's only hypothetical at this point in time.</p>
</details>

对一个完全没有具体特征的问题的关注，会导致无休止且徒劳的争论，并且对任何实际的受害者都无济于事或无法提供帮助，比如那些因聊天机器人鼓励而自杀的青少年的父母，或者那些可能正在被聊天机器人鼓励伤害自己但父母不知情且无法干预的青少年，我们以后可能会发现这些情况。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The focus on a problem with no concrete characteristics at all leads to endless and fruitless debates, and it provides no help or relief for any of the actual victims, like the parents of the teenagers that have committed chatbot encouraged fatal self-harm, allegedly, or the teenagers that are currently probably allegedly being encouraged to harm themselves by chat bots, whose parents are unaware and unable to intervene, that we'll probably find out about later.</p>
</details>

相比之下，许多即时问题都有明确的解决方案。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">By contrast, many of the immediate problems have clear-cut solutions.</p>
</details>

例如，对开发或托管被证明生成了操纵或鼓励任何人伤害自己或他人的内容的服务的行为，处以严厉的、明确的刑事处罚。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">For example, severe, explicit criminal penalties for developing or hosting a service that can be shown to have generated content that manipulated or encouraged anyone to harm themselves or others.</p>
</details>

这样的法规将立即帮助青少年和父母，因为公司将被迫正视这些问题，否则将面临巨额罚款。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Regulations like that would immediately help the teenagers and the parents, because the companies would be forced to reckon with these problems or risk-roll penalties.</p>
</details>

作为一个额外的红利，让公司对其软件行为负责，将迫使他们放慢速度并理解如何控制其软件，这反过来会极大地增进我们对如何控制其输出的理解，从而必然改善我们对**对齐问题 (Alignment Problem: 确保AI目标与人类价值观一致的技术和哲学挑战)** 的理解，并必然减少最终可能导致大量人类伤亡的、假设性的、未对齐的超级智能的风险。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And as a bonus, making companies liable for what their software does will force them to slow down and to understand how to control their software, which in turn will also greatly increase our understanding of how to control their output, which cannot help but improve our understanding of the alignment problem, and that cannot help but reduce the risk of any eventual hypothetically misaligned superintelligence that might lead to huge numbers of human casualties.</p>
</details>

因此，关注即时问题有助于两种情况：短期和假设的长期。而关注假设性问题则完全忽略了即时情况，即使最终的假设情况真的发生，可能也无济于事，因为我们不知道如何处理那个假设情况。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So focusing on the immediate problem helps both cases, the near-term and the hypothetical long-term, while focusing on the hypothetical problems ignores the immediate case completely and still may not be useful in the future, even if the eventual hypothetical case were to come to pass, because we don't know what to do about the hypothetical case.</p>
</details>

尽管“当前风险问责制”监管显然符合公共利益——根据我和联合国红线声明的观点——但这不符合那些试图把尽可能多的钱放进自己口袋里的AI行业人士的利益。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But although "current-risk accountability" regulation is clearly in the public interest, according to me and according to the red line statement that came out of the UN, it's not in the interest of the people in the AI industry who are trying to put as much money in their pockets as they can.</p>
</details>

因此，最大的AI公司的CEO们签署了关注灭绝的声明，而没有签署那些呼吁关注更直接风险或要求问责的声明。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And so the CEOs of the biggest AI companies sign the extinction-oriented statement and don't sign the one calling out more immediate risks or calling for them to be held accountable.</p>
</details>

有了所有这些AI公司、非营利组织和大型教育YouTube频道都在推销这种世界末日叙事，我们这些试图保持现实的人很难跟上所有这些信息，更不用说制作关于所有这些内容的视频了。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And with all these AI companies, nonprofits and large educational YouTube channels pushing this apocalyptic narrative, it's hard for those of us trying to be realistic to even keep up with all of it, much less make videos about it all.</p>
</details>

将一个包含事实和引用的视频组合起来，比照读AI行业的论点要耗费更多的工作和时间。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And putting together a video with facts and references is so much more work and so much more time-consuming than reading off AI industry talking points.</p>
</details>

但尽管我只是一个人，我仍将尽我所能。比如告诉你一些需要注意的事项，以便分辨出什么是行业宣传，就像我们在本视频中讨论的：必然性、例外性和致命性。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But although I'm only one person, I'm going to keep doing what I can. Like giving you things to look out for, to tell when something is an industry propaganda, like the ones we've discussed in this video, namely inevitability, exceptionalism, and lethality.</p>
</details>

### 结论：寻找L.I.E.S.

我想知道是否有一种更简单的方法来记住它们。啊，就这样。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I wonder if there might be an easier way to remember that. Ah, there you go.</p>
</details>

在消费AI内容时，寻找谎言。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">When consuming AI content, look for the lie.</p>
</details>

嗯，现在我想起来了，我应该把超级智能也加进去，作为AI行业宣传不停唠叨的另一个纯粹是假设性的东西，但它完全没有得到任何实际证据的支持。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Hmm. You know, now that I think about it, I should add superintelligence as another purely hypothetical thing that the AI industry propaganda won't shut up about, but that it's completely unsupported by any actual evidence.</p>
</details>

这样更容易记住了。致命性（Lethality）、必然性（Inevitability）、例外性（Exceptionalism）和超级智能（Superintelligence）。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">That should make it even more easy to remember. Lethality, Inevitability, Exceptionalism, and Superintelligence.</p>
</details>

当你看到AI内容时，寻找这些谎言（L.I.E.S.）。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">When you see AI content, look for the L.I.E.S.</p>
</details>

如果你发现了它们，那么它很可能更多的是宣传，而不是实用信息。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And if you find them, there's a good chance it's more propaganda than it is practical.</p>
</details>

祝我好运，如果你愿意的话，欢迎订阅。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">So wish me luck, and feel free to subscribe if you're so inclined.</p>
</details>

我本来想请求大家在评论区保持友善，但那毫无用处，所以我不会费心了。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I would ask for people to be kind in the comments, but it wouldn't do any good so I'm not going to bother.</p>
</details>

请永远记住，互联网充满了Bug和虚假信息，任何声称相反的人可能只是在重复行业论点，并试图让你从行业正在（据称）伤害真实人民的各种方式中分心，就在你观看此视频的时候。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Just always remember that the internet is full of bugs and misinformation, and anyone who says different might just be parroting industry talking points and trying to distract you from all the ways the industry is, allegedly, harming real people as you are watching this.</p>
</details>

大家要小心。感谢收看。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Let's be careful out there. Thanks for watching.</p>
</details>