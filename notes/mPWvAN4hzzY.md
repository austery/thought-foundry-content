---
author: Hung-yi Lee
date: '2025-11-14'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=mPWvAN4hzzY
speaker: Hung-yi Lee
tags:
  - neural-network-training
  - optimization-vs-generalization
  - deep-learning-techniques
  - loss-functions
title: 神经网络训练指南：优化、泛化与关键技巧深度解析
summary: 本讲座系统性地介绍了训练神经网络的各种技巧，并围绕机器学习的三个核心步骤——定义损失函数、选择函数范围、优化寻找最佳函数——进行分类。讲座强调，在评估一个新方法时，必须明确其目标是改善“优化”（降低训练损失）还是“泛化”（避免过拟合）。内容涵盖了从各类优化器（如 Adagrad、Adam）到网络架构设计（如 CNN、残差连接），再到损失函数策略（如交叉熵、数据增强）等一系列关键技术。
insight: ''
draft: true
series: ''
category: technology
area: tech-insights
project:
  - ai-impact-analysis
people:
  - Hung-yi Lee
  - 何愷明
companies_orgs:
  - PyTorch
products_models:
  - Adagrad
  - RMSprop
  - Adam
  - AdamW
  - Convolutional Neural Network (CNN)
  - Multi-layer Perceptron (MLP)
  - Support Vector Machine (SVM)
  - BERT
media_books: []
status: evergreen
---
### 引言：如何评估一个训练技巧？

今天这堂课要向大家介绍在训练类神经网络时会用到的各种技巧。在上一堂课中，我们已经讲过机器学习的三个步骤：第一是定义损失函数（Loss Function）；第二是决定函数的选择范围；第三步则是选出一个最好的函数。完成这三步、训练出一个模型后，不能马上进行测试，还必须进行**验证**（Validation: 在最终测试前，使用一部分未参与训练的数据来评估模型性能并调整超参数的过程），如果验证结果足够好，才会进入测试环节。否则，就需要回到学习的三个步骤，检查是哪个环节出了问题。

上周我们还讲解了深度学习的基本观念，所谓的深度学习或类神经网络，就是划定一个更大的函数选择范围，从而有机会包含更多不同的函数。

今天，我们将介绍一些训练类神经网络时常用的技巧。由于内容非常多，时间有限，许多技术可能只会以流水账的方式带过，并会告知大家在过去课程的哪些地方可以找到更详细的资料。如果你没有训练类神经网络的经验，今天这堂课可能会比较吃力。但即便无法完全跟上，也没有关系。

这堂课的重点是，每当你听到一个关于训练类神经网络的方法或技巧时，要问自己一个问题并填写下面的表格：这个方法改变了机器学习三个步骤中的哪一个？它是一个新的损失函数吗？是不同的网络架构（即划定了不同的选择范围）吗？还是一个更有效的寻找最低损失函数的方式？

接下来，你要问自己，这个方法到底带来了什么好处？很多人在介绍新方法时只会说它“好”，但“好”有很多不同的面向。今天课程中提到的方法，其好处大致可以分为两个方向：

1.  **更好的优化 (Optimization)**：这意味着我们可以找到更低的训练损失 (Training Loss)。
2.  **更好的泛化 (Generalization)**：这能让训练损失和验证损失更接近，避免**过拟合**（Overfitting: 模型在训练数据上表现很好，但在未见过的新数据上表现差的现象）的发生。

每个方法针对的目标和解决的问题是不同的，因此你需要根据具体情况选择合适的方法。

举例来说，假设你先训练了一个线性模型（Linear Model），发现其损失可以降得很低。接着，你训练一个类神经网络，却发现它的损失降得没有线性模型那么低。有些人会不假思索地说这是“过拟e合”，但这是错误的。如果深度学习模型的训练损失都降不下去，这不叫过拟合，而是在优化过程中出了问题。深度网络的参数比线性模型多，理论上它的损失至少应该和线性模型一样低，甚至更低。如果做不到，就说明你的优化策略有问题，此时应选择能强化优化的技巧。

反之，如果你发现训练损失已经足够低，但验证损失没有随之下降，这才是过拟合。这时，你才应该选择能改善泛化的方法。因此，理解一个方法是为了强化优化还是泛化而设计的至关重要。

接下来，我们将以流水账的方式介绍大量与训练类神经网络相关的方法。我们会按照它们所改变的步骤来分类，并说明该方法是为了优化还是泛化而设计。我们将从第三步开始，倒序讲解。

### 优化策略（第三步）：选择最佳函数

我们先来复习一下优化是如何进行的。通常我们使用最简单的**梯度下降**（Gradient Descent: 一种迭代优化算法，通过沿着函数梯度最陡峭的方向逐步移动来寻找函数的局部最小值）。这里的“Vanilla”一词原意是香草，用在这里表示最原版、最简单的梯度下降。

优化的目标是，在给定的损失函数和参数选择范围（Theta）内，找到一个能使总损失（大 L）最小的 Theta。梯度下降的做法是：

1.  随机选择一组初始参数 Theta0。
2.  计算在 Theta0 位置，参数 Theta 对总损失 L 的梯度（Gradient），记作 G0。
3.  用 G0 乘以学习率（Learning Rate）来更新 Theta0，得到 Theta1。
4.  在 Theta1 位置计算新梯度 G1，用 G1 更新 Theta1 得到 Theta2，如此反复，直到梯度变得非常小，无法再更新参数为止。

接下来要介绍一系列的**优化器**（Optimizer: 用于调整神经网络权重和学习率以减少损失的算法）。最原始的梯度下降只根据当前计算出的梯度来决定更新方向。而优化器则会综合考虑从 G0 到 Gt 所有历史梯度，共同决定下一步的方向。在 PyTorch 等深度学习工具包中，更换优化器通常只需要修改一行代码中的一个英文单词。虽然实现简单，但理解其背后的原理非常重要。

#### 学习率的挑战与自适应调整

为什么我们需要优化器？一个主要原因是**学习率**（Learning Rate: 控制模型权重更新幅度的超参数）非常难以调整。

上周的例子中，学习率设为 0.001 看起来已经很小，但第一步就飞到了山谷对岸，再一步就消失了。而设为 0.0001 又太小，更新 100 次参数后才刚刚走到谷底。学习率太大或太小都不行，难以找到一个合适的数值。

在这个例子中，问题在于不同方向（横轴 X 和纵轴 Y）的地势变化差异巨大。X 轴方向地势变化快，梯度大，需要小步慢走（小学习率）；Y 轴方向地势平缓，梯度小，需要大步快走（大学习率）。

最简单的梯度下降对所有参数使用相同的学习率，这显然不是好方法。我们应该“因材施教”，为每个参数设置不同的学习率。但如何在没有上帝视角的情况下判断哪个方向的梯度大、哪个小呢？我们可以根据历史梯度的大小来动态决定学习率。

例如，通过观察历史梯度 G0 到 G3，我们发现第一个维度的数值通常是三位数，梯度较大，因此应给予较小的学习率。而第二个维度的数值通常只有零点几，梯度较小，应给予较大的学习率。

#### Adagrad：为每个参数定制学习率

这个方法被称为 **Adagrad**（Adaptive Gradient Algorithm: 一种自适应学习率的优化器，对频繁更新的参数使用较小的学习率，对不频繁更新的参数使用较大的学习率）。其数学公式如下：

在第一次迭代时，计算出梯度 G0。对于参数的第 i 个维度，我们将至今所有梯度的第 i 个维度的值取平方，累加后开根号，得到一个值 Sigma。然后，用初始学习率 eta 除以这个 Sigma。

-   **Sigma 越大**，代表该方向历史梯度越大，整体学习率就越小。
-   **Sigma 越小**，代表该方向历史梯度越小，整体学习率就越大。

这个过程会持续进行。在第 t 步，Sigma 会累积从 G0 到 Gt 所有梯度的平方和。这样，每个维度都有了自己动态调整的学习率。

有人可能会问，累加平方时是否需要取平均？可以取也可以不取。不取平均的好处是，随着训练的进行，Sigma 会越来越大，这自然地实现了学习率随时间衰减的效果，有助于模型最终收敛。

在代码演示中，我们发现使用普通梯度下降时，学习率难以调整。而换用 Adagrad 后，我们可以将初始学习率设置得更大（例如 1.0）。虽然初期更新可能不稳定，但模型很快就能走上正轨，在同样步数内比普通梯度下降走得更远。

#### RMSprop：关注近期的梯度变化

Adagrad 有一个很强的假设：过去梯度的大小可以很好地预测未来。但现实中，梯度的变化可能非常剧烈。例如在一个香蕉形状的损失曲面上，同一个方向在不同位置的梯度大小可能截然相反。

Adagrad 对学习率的更新非常缓慢，因为它会累加所有历史梯度，很久以前的梯度和最近的梯度影响力相当。当梯度突然变化时，Adagrad 无法及时调整。

为了解决这个问题，**RMSprop**（Root Mean Square Propagation: Adagrad 的一种改进，通过使用梯度的移动平均来防止学习率过快下降）应运而生。它给予最近的梯度更大的影响力。其核心思想是：在计算 Sigma 时，不再是简单地累加所有历史梯度的平方，而是引入一个衰减因子 alpha。

`新的 Sigma^2 = alpha * (旧的 Sigma^2) + (1 - alpha) * (当前梯度的平方)`

alpha 值越小，代表历史信息越不重要，最近的梯度信息越重要。这样，RMSprop 就能更灵活地根据梯度的近期变化来调整学习率。

在对一个名为 Rosenbrock 函数（香蕉函数）的复杂损失曲面进行优化的实验中，Adagrad 在 1000 步后仍离最优点有显著差距。而 RMSprop 在同样步数下可以走得更远，因为它能更快地适应梯度变化，调整学习率。不过，由于其学习率更新较快，RMSprop 有时训练不够稳定，初始学习率不宜设置得过大。

#### Momentum：引入动量克服局部最优

梯度下降除了学习率问题，还面临其他挑战。当梯度很小时，优化就会停止。最著名的问题就是陷入**局部最小值**（local minima），梯度为 0，训练停止。但实际上，多数训练停止并非因为陷入局部最小值，而是其他“坑”，比如**鞍点**（saddle point），即梯度为 0 但并非局部最小值的点，或者仅仅是梯度变得非常小，导致我们失去耐心。

在物理世界中，一个滚下山坡的球不会因为地势平坦就立刻停下，因为它有**动量**（Momentum: 在优化算法中，模拟物理动量的概念，帮助加速梯度下降并越过局部最小值）。我们可以将这个概念引入优化算法。这样，即使走到梯度很小的地方，由于之前积累的动量，参数更新仍能继续。

Momentum 方法的实现方式是，记录并累加过去所有的梯度。参数的更新方向不再是当前的梯度方向，而是累积的动量方向。

`新的动量 m = (历史动量) + (当前梯度 g)`
`更新参数 = -学习率 * m`

与 RMSprop 类似，我们也可以引入一个衰减因子 beta，让近期的梯度有更大的影响力。

`新的动量 m = beta * (旧的动量) + (1 - beta) * (当前梯度 g)`

在对一个充满大量局部最小值的 Rastrigin 函数进行优化的实验中，RMSprop 很快就陷入一个坑里停滞不前。而 Momentum 方法凭借其动量，成功地“翻越”了几个山丘，虽然在最低点附近会因动量而来回震荡，但最终能够停在更好的位置。

#### Adam：动量与自适应学习率的结合

我们已经介绍了 RMSprop（累加梯度的平方，控制学习率）和 Momentum（累加梯度本身，决定方向）。这两个方法可以结合使用，其结合体就是 **Adam**（Adaptive Moment Estimation: 结合了 Momentum 和 RMSprop 思想的优化器），也是目前默认最常用的优化器之一。

Adam 同时记录梯度的累积（动量）和梯度平方的累积（用于自适应学习率），并用这两者共同来更新参数。在香蕉函数的实验中，RMSprop 未能走到最低点，而 Adam 在同样步数下成功找到了全局最优解。

#### 学习率调度 (Learning Rate Scheduling)

除了优化器本身，我们还可以动态改变初始学习率 eta。这种技术称为**学习率调度**（Learning Rate Scheduling）。一种常见的策略是“预热后衰减”（Warm up then decay）：

1.  **Warm up (预热)**：在训练初期，逐渐增大学习率。这相当于给优化器一个探索地形的机会，让它在地图上多跑跑，从而更好地估算动量和梯度方差。
2.  **Decay (衰减)**：在预热期过后，逐渐减小学习率。这有助于参数更新的步伐慢慢变小，最终稳定地“着陆”在最优解附近，避免因学习率过大而在最优解两侧来回震荡。

#### 优化器总结

-   **核心思想**：通过综合历史梯度信息来更智能地更新参数。
-   **分类**：
    -   **自适应学习率** (Adagrad, RMSprop): 根据梯度大小调整每个参数的学习率。
    -   **动量** (Momentum): 累积历史梯度以冲过平坦区域和局部最小值。
    -   **混合方法** (Adam, AdamW): 结合以上两者优点。
-   **作用**：优化器改变的是机器学习的第三步（搜索最佳函数）。它带来的好处是**更好的优化**，即有能力在训练数据上找到更低的损失。但它通常对**泛化没有直接帮助**，甚至可能因为在训练集上过度优化而导致验证集效果变差。

### 超越优化器：Dropout 与参数初始化

#### Dropout：为训练增加难度以提升泛化

有些方法虽然改变的是优化步骤，但其目标却是为了获得更好的泛化能力。**Dropout**（随机失活: 一种正则化技术，在训练过程中随机将一部分神经元的输出设置为零，以防止过拟合）就是这样一个著名的方法。

-   **训练时**：随机丢掉一些神经元，让网络在“残缺”的状态下学习。这好比在腿上绑着重物练习轻功。
-   **测试时**：所有神经元全部启用，火力全开。这好比解开重物，自然能“飞”起来。

Dropout 改变的是第三步，但它带来的好处是**更好的泛化**。它能让训练损失和验证损失更接近。然而，它对优化本身没有好处，甚至会因为随机丢弃神经元而让训练变得更差，导致训练损失看起来更高。因此，Dropout 应该在你发现模型已经过拟合（训练损失低，验证损失高）时使用，而不是在优化不畅（训练损失降不下去）时使用。

#### 参数初始化 (Initialization)

梯度下降的起点，即参数的初始位置，也会极大地影响最终的训练结果。从不同的位置下山，可能会到达不同的山谷（局部最小值）。

有人可能认为强大的优化器可以克服初始化带来的差距，但即使有好的优化器，好的初始化仍然能带来显著差异。在一个实际案例中，我们对比了随机初始化和**何恺明初始化**（Kaiming Initialization: 一种专门为使用 ReLU 激活函数的深度网络设计的权重初始化方法）。在使用 Adam 优化器的情况下，随机初始化的损失只能降到 7-9，而使用何恺明初始化后，损失可以轻松降到 1 以下。

更有趣的是，即使一个优化器强大到总能找到全局最小值，初始化依然重要。因为一个复杂的损失曲面可能存在多个全局最小值（例如，多个参数组合都能使 MSE 损失为 0）。这些全局最小值有好有坏：

-   **好的全局最小值**：位于一个平坦的盆地中。即使验证数据与训练数据有微小差异（loss surface 有一点平移），损失值的变化也不大。
-   **坏的全局最小值**：位于一个陡峭的峡谷中。微小的数据差异就可能导致验证损失急剧上升，造成严重的过拟合。

好的初始化，就有可能引导我们走向更平坦、更“好”的全局最小值。

#### Pre-train：通用的大绝招

在 2025 年，有一个通用的初始化大绝招，就是**预训练**（Pre-train: 先在一个大规模通用数据集上训练模型完成一个“借口任务”，然后将训练好的参数作为下游特定任务的初始权重）。

假设我们的目标任务是图像分类。在教模型分类之前，我们可以先让它完成一个**借口任务**（Pretext Task），比如预测一张图片被旋转了多少度。这个任务的标签可以轻易地自动生成，因此可以利用海量的无标签数据。当模型学会了理解图像旋转后，它所学到的参数就可以作为分类任务的良好起点。

预训练的神奇之处在于，它对**优化和泛化同时都有帮助**。它提供了一个更好的起点（优化），同时因为模型见过了大量数据，学到了更通用的特征，也提升了泛化能力。

### 模型架构（第二步）：选择函数范围

现在我们来看第二步：选择函数范围，这通常指的是改变网络架构。我们希望找到一个“刚刚好”的函数范围——既不能太小，以免错过好的函数；也不能太大，以免导致过拟合。这需要借助我们的**领域知识**（Domain Knowledge）来设计一个既紧凑又包含最优解的网络架构。

#### 卷积神经网络 (CNN)

**卷积神经网络**（Convolutional Neural Network, CNN: 一种常用于处理图像等网格状数据的深度学习模型）是利用领域知识设计网络架构的经典范例。

对于一张 1000x1000 像素的彩色（RGB 三通道）图片，如果使用传统的**全连接网络**（Multi-layer Perceptron, MLP），输入特征维度高达 300 万。网络第一层的每一个神经元都需要连接到这 300 万个输入像素，参数量将达到数十亿级别，这非常巨大。

但我们真的需要每个神经元都看整张图片吗？基于对图像处理的理解，我们有两个观察：

1.  **局部性 (Locality)**：检测一个基本模式（如鸟嘴、眼睛）通常只需要观察图像的一个小区域，而不需要看整张图。
2.  **平移不变性 (Translation Invariance)**：同一个模式（如鸟嘴）可能出现在图像的任何位置，但检测它的方法应该是相同的。

基于这两个观察，CNN 做了两个关键简化：

1.  **感受野 (Receptive Field)**：每个神经元只负责观察输入图像的一小块区域（即感受野），而不是整张图。这极大地减少了连接数和参数量。这相当于强制将神经元连接到感受野之外的像素的权重设为 0，从而缩小了函数的搜索范围。
2.  **参数共享 (Parameter Sharing)**：对于不同位置的感受野，使用同一组参数（称为滤波器或卷积核）去进行检测。这意味着检测左上角鸟嘴的神经元和检测中间鸟嘴的神经元共享完全相同的权重。这进一步缩小了函数范围，因为你强制不同区域的神经元必须有相同的参数。

CNN 并非比全连接网络更复杂，恰恰相反，它是全连接网络的一种**简化和特例**。它通过引入领域知识，划定了一个更小但更有效的函数搜索空间，这个空间虽然小，但正好包含了我们需要的、适合处理图像的函数，从而有效避免了过拟合。因此，CNN 的主要贡献在于**提升泛化能力**。

#### Skip Connection：为深层网络搭建高速公路

网络结构的设计不仅影响泛化，也会影响优化。在很深的网络中，靠近输入层的参数更新（delta w）对最终输出（delta L）的影响可能会经过层层传递而变得非常微弱，这个现象称为**梯度消失**（Gradient Vanishing）。反之，影响也可能被层层放大，导致**梯度爆炸**（Gradient Exploding）。这两种情况同时存在，使得深层网络的学习率极难调整，优化非常困难。

**跳接/残差连接**（Skip Connection / Residual Connection: 允许网络中的信息跳过一层或多层直接向后传递的结构）的出现就是为了缓解这个问题。它在层与层之间建立了一条“高速公路”，让输入信号 a 可以直接加到该层的输出上。

`输出 = F(a) + a`

这样一来，即使是底层参数的变化，也可以通过这条高速公路直接影响到最终输出，保证了梯度的有效传递，使得损失曲面变得更加平滑，优化过程也因此变得更加容易。因此，Skip Connection 改变的是网络架构，但它带来的主要好处是**更好的优化**。

#### Normalization：规范每一层的输出

在训练过程中，另一系列与优化相关的方法是**归一化**（Normalization）。其基本思想是在数据进入下一层之前，对当前层的输出进行一次操作，将其限制在某个固定的范围内（例如，均值为 0，方差为 1）。

这包括对输入数据做的预处理，也包括在网络中间层加入的操作，如**批量归一化**（Batch Normalization）和**层归一化**（Layer Normalization）。由于每一层输出的数值范围都变得稳定和一致，不同维度的梯度大小不会相差悬殊，这使得学习率更容易调整，优化过程也更加稳定。因此，Normalization 主要带来的好处是**更好的优化**，但因为它给函数范围增加了额外的限制，通常也对**泛化**有一定帮助。

### 损失函数（第一步）：定义优化目标

最后我们来看第一步：定义损失函数。

#### 分类问题的损失函数：为何不用准确率？

对于回归问题，我们常用均方误差（MSE）作为损失函数。但对于分类问题，比如图像分类，我们最终关心的是**准确率**（Accuracy）。那为什么不直接用准确率作为损失函数来优化呢？

问题在于，准确率是不可微的。当你微调一个参数时，模型的输出分数（比如 y2 从 1 变为 1.01）可能没有改变最终的分类结果（y1 仍然大于 y2），因此准确率毫无变化，计算出的梯度为 0。只有当参数的改变恰好跨过一个临界点，导致分类结果翻转时，准确率才会发生阶跃式的变化。这样的损失函数充满了平坦的区域和断崖，无法使用梯度下降进行优化。

因此，我们需要一个可微的、平滑的函数来替代准确率。在分类问题中，最常用的就是**交叉熵**（Cross-Entropy: 一种用于衡量两个概率分布之间差异的损失函数）。

它的计算过程是：
1.  将网络输出的原始分数通过 **Softmax** 函数转化为一个概率分布。
2.  将真实的标签（Ground Truth）也表示为一个概率分布（例如，正确类别为 1，其他为 0）。
3.  计算这两个概率分布之间的交叉熵。两个分布越接近，交叉熵损失就越小。

交叉熵是平滑且可微的，非常适合梯度下降。因此，我们用交叉熵作为损失函数来**训练**模型，但在**验证和测试**时，我们仍然使用我们真正关心的准确率来评估模型。这在训练和测试之间引入了一个额外的差异，可能对泛化有些许负面影响，但这使得优化成为可能。

#### 数据与正则化：从根源改善泛化能力

**1. 收集更多数据**
克服过拟合最直接、最粗暴的方式就是收集更多的训练数据。数据越多，就越能代表真实世界的分布，模型就越难过拟合。增加数据改变的是损失函数的定义（因为求和项变多了），它带来的好处是**更好的泛化**，但同时也会让优化变得更困难，因为模型需要满足更多样本的需求。

**2. 数据增强 (Data Augmentation)**
在无法收集更多数据时，我们可以通过**数据增强**（Data Augmentation: 通过对现有数据进行变换来创造新数据的技术）来凭空制造数据。例如，将一张鸟的图片左右翻转，它仍然是一只鸟；或者将其变模糊，它也还是一只鸟。通过这种方式，可以扩充数据集，提升模型的泛化能力。

一个更有趣的方法是 **Mixup**，它将两张图片（如猫和狗）按一定比例混合在一起，它们的标签也按同样比例混合（变成 50% 猫，50% 狗）。这迫使模型学习类别之间的线性关系，是一种非常有效的正则化手段。

数据增强带来的好处是**更好的泛化**，但同样会使优化更具挑战性。

**3. 半监督学习 (Semi-supervised Learning)**
我们也可以利用大量的无标签数据来改善模型。这种结合有标签和无标签数据的方法称为**半监督学习**（Semi-supervised learning）。这通常基于一些假设，例如：
-   **非黑即白假设**：模型对无标签数据的预测应该非常确定（即输出的概率分布熵很小），而不应模棱两可。
-   **物以类聚假设**：如果两个无标签数据点在特征空间中很相似，那么它们的预测结果也应该相似。

通过在损失函数中加入反映这些假设的项，无标签数据可以帮助模型学习到更好的决策边界，从而提升**泛化**能力。

**4. 正则化 (Regularization)**
我们还可以定义一些与数据无关，仅反映我们对参数偏好的损失项。我们通常更喜欢平滑、简单的函数，而不是剧烈震荡的函数。在实践中，“简单”通常被定义为“参数值接近于 0”。

**L2 正则化**（L2 Regularization）就是在原始损失函数的基础上，增加一项所有参数平方和的惩罚项。这会鼓励模型的权重尽可能小。

这个操作在优化过程中的体现，等价于在每次参数更新前，都将参数乘以一个略小于 1 的系数（如 0.999），这个过程也称为**权重衰减**（Weight Decay）。

L2 正则化或权重衰减，其主要目的是**提升泛化能力**，防止模型权重过大导致过拟合。它对优化本身没有帮助，甚至会使优化变得更困难。将权重衰减与 Adam 结合，就得到了 **AdamW** 优化器，它在保持 Adam 强大优化能力的同时，也具备了更好的泛化性能。

### 总结

今天我们介绍了大量训练神经网络的技巧。如果你没有记住所有细节，请务必记住这个核心框架：当你听到一个新方法时，首先思考它改变了机器学习三步骤中的哪一步，然后判断它的主要目的是为了**优化**（让训练损失更低）还是为了**泛化**（让模型在未见数据上表现更好）。理解了这一点，你就能在面对不同问题时，做出更明智的技术选择了。