---
author: Hung-yi Lee
date: '2025-11-17'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=YJoegm7kiUM
speaker: Hung-yi Lee
tags:
  - llm
  - pre-training
  - fine-tuning
  - reinforcement-learning
  - data-quality
title: 大型语言模型的学习历程：预训练、微调与强化学习
summary: 本课程深入探讨了大型语言模型（LLM）的三个核心训练阶段：预训练（Pre-training）、监督微调（SFT）和人类反馈强化学习（RLHF）。讲者通过生动的比喻和具体案例，阐释了每个阶段的目标、数据需求、技术挑战及其对模型能力的影响。强调了海量高质量数据在预训练中的关键作用，以及SFT和RLHF如何帮助模型对齐人类价值观并提升输出风格，而非灌输新知识。课程还讨论了算力限制、数据质量筛选、知识蒸馏等前沿技术，并对RLHF的优势与挑战进行了深入分析。
insight: ''
draft: true
series: ''
category: ai-ml
area: tech-work
project:
  - ai-impact-analysis
people:
  - Hung-yi Lee
  - Sam Altman
  - 李飞飞
  - 谢濬丞
companies_orgs:
  - OpenAI
  - Google
  - DeepMind
  - Meta
  - Hugging Face
  - Allen AI
products_models:
  - ChatGPT
  - Gemini
  - GPT-4
  - GPT-3.5
  - Llama 3
  - OLMo
  - BERT
  - FLAN
  - PaLM
  - InstructGPT
  - Llama 2
  - LIMA
  - Qwen
  - Alpaca
  - Vicuna
  - Mistral
  - Gemma
  - AlphaGo
  - GPT-4o Turbo
media_books:
  - 《孔乙己》
  - 《射雕英雄传》
status: evergreen
---
### LLM三阶段学习概述

今天的课程是大型语言模型的学习历程，我们将探讨日常使用的人工智能，如 **ChatGPT** 和 **Gemini** 等，它们是如何被打造出来的。大型语言模型的学习历程通常分为三个标准阶段：**Pre-training**（预训练: 大型语言模型训练的第一阶段，通过大量无标注数据学习语言模式）、**SFT**（**Supervised Fine Tuning**: 监督微调，大型语言模型训练的第二阶段，通过人类标注的问题-答案对进行学习）和 **RLHF**（**Reinforcement Learning with Human Feedback**: 人类反馈强化学习，大型语言模型训练的第三阶段，通过人类对模型输出的反馈进行学习）。

今天的课程将依序介绍这三个阶段。在作业中，大家将有机会实作第二阶段 **SFT** 和第三阶段 **RLHF**，从而在小规模数据下体验如何打造一个大型语言模型。**Pre-training** 作为整个训练的起点，旨在让语言模型熟悉人类语言的模式。进入第二阶段 **SFT** 和第三阶段 **RLHF** 后，语言模型不仅要熟悉人类语言，更要学习如何做出正确的应答，成为一个有用的 **AI**。这种学习符合人类价值观的应答，被称为 **Alignment**（对齐: 使机器行为符合人类价值观和需求的过程）。在文献中，**Alignment** 往往特指 **RLHF** 阶段，但在本课程中，**SFT** 和 **RLHF** 都被视为 **Alignment** 的一部分。

### SFT与RLHF的区别

**SFT** 和 **RLHF** 阶段都旨在教会机器如何进行 **Alignment**，但它们之间存在关键差异。在 **SFT** 阶段，人类提供标准的答案，机器根据这些标准答案进行学习。而在 **RLHF** 阶段，没有标准答案，人类只提供反馈，机器根据这些反馈进行学习。

如果用比喻来说明这三个阶段：
第一阶段的 **Pre-training** 就像学龄前儿童，每天玩耍，看到什么就学什么，不确定所学是否有用，但快乐地探索。
**SFT** 阶段则像孩子开始上学，老师提供教材，明确告知什么是正确的、什么是标准答案。机器在这一阶段学会了老师的要求和人类的标准答案。
**RLHF** 阶段则像机器步入社会，不再有老师直接教导，而是遭遇“社会毒打”，有人告诉它错了，但不会直接指出错在哪里，它必须自己想办法发掘。这就是大型语言模型学习的三个阶段。

### 文字接龙：LLM学习的本质

尽管大型语言模型的学习分为三个阶段，但它们在本质上并没有不同。这三个阶段都在学习 **文字接龙**（Language Modeling: 根据前面的文字预测下一个最可能出现的文字）。在之前的课程中，我们提到 **文字接龙** 本质上是一个分类问题。理解如何解决分类问题，就能理解如何教会机器进行 **文字接龙**。

以影像分类为例，机器通过查看图片并分辨其类别进行学习。训练一个影像识别系统时，**AI** 会输出每个类别的概率分布。正确答案也被视为一种概率分布，其中只有一个类别概率为1，其他为0。通过计算影像识别系统输出与正确答案之间的距离（通常使用 **Cross-Entropy**），并训练模型最小化这个距离，就能得到一个分类系统。

对于 **文字接龙** 而言，学习过程是相似的。唯一的区别在于，类别不再是影像分类的类别，而是每一个 **Token**（词元: 文本处理中的最小单位，可以是词、字或标点符号）都是一个类别。例如，给定未完成的句子“台湾最高的山是哪座”，如果标准答案是“玉”，语言模型会读取句子，输出一个概率分布，其中每个 **Token** 都是一个类别，并有一个分数。正确答案同样被视为一个概率分布，只有特定 **Token** 的概率为1。

### 词汇量与梯度下降

在影像分类中，可能有数百或数千个类别。但在 **文字接龙** 中，每个 **Token** 都是一个类别，因此类别的数量就是 **Vocabulary Size**（词汇量大小: 模型能识别和生成的不同词元的总数）。在第三堂课中，我们提到一个正常语言模型的 **Vocabulary Size** 可能高达数十万，这意味着这是一个拥有数十万个类别的分类问题。

除了类别数量，其他方面与普通分类问题无异。你仍然需要计算模型输出与正确答案之间的 **Cross-Entropy**，并通过 **Gradient Descent**（梯度下降: 一种迭代优化算法，用于寻找函数最小值）调整参数来最小化 **Cross-Entropy**，从而训练模型。因此，**文字接龙** 本质上是一种分类问题，与影像分类没有核心区别，只是类别更多。

### 阶段间的关联性与初始化

大型语言模型学习的三个阶段之间存在紧密关联。在每个阶段，前一个阶段训练出的参数都会作为当前阶段的 **Initialization**（初始化: 模型训练前参数的初始赋值）。在之前的课程中，我们讨论过 **Initialization** 的概念：在 **Gradient Descent** 训练时，随机的初始位置可能对结果产生巨大影响。

一个强大的初始化方法是先找到一个 **Pre-task**（预任务: 在训练主要任务前，先训练一个相关且易于收集数据的任务），它与你真正关心的 **Downstream task**（下游任务: 真正关心的最终任务）相关，但更容易训练和收集数据。这个 **Pre-task** 也被称为 **Pre-text task**（预设任务: 用于预训练的辅助任务，通常与下游任务相关但数据获取更容易）。先让机器学习 **Pre-text task**，然后将其学到的参数作为 **Downstream task** 的初始化参数。

在大型语言模型的三阶段学习中，每个阶段都将前一个阶段视为其 **Pre-text task**，并使用前一阶段的参数进行 **Initialization**。因此，第一个阶段 **Pre-training** 得到的参数是 **SFT** 的 **Initialization**。同样，**SFT** 作为第二阶段，其参数可以作为 **RLHF**（第三阶段）的 **Initialization**。

### 训练步骤与模型架构

从机器学习的三个步骤来看，大型语言模型这三个阶段主要改变的是步骤一：训练数据。在这三个阶段中，模型都在学习 **文字接龙**，但提供给大型语言模型的训练资料是不同的。这三个阶段的分类与步骤二（模型架构）和步骤三（优化器）没有直接关联。

例如，这三个阶段的模型都会使用相同的 **Transformer**（Transformer: 一种基于自注意力机制的深度学习模型架构，广泛应用于自然语言处理）架构。如果使用不同的架构，将无法把前一个阶段学到的参数作为下一个阶段的 **Initialization**。在第三阶段，你会选择一个特别有用的 **Optimizer**（优化器: 机器学习中用于调整模型参数以最小化损失函数的算法）来优化 **Loss Function**（损失函数: 衡量模型预测结果与真实值之间差异的函数），以最小化 **Loss Function**。因此，大型语言模型的三个阶段真正改变的是机器学习三个步骤中的第一个步骤，与第二、第三步骤没有直接关联。

### 预训练：学龄前学习

接下来，我们将依序介绍训练大型语言模型的三个步骤，首先从 **Pre-training**（预训练），即机器的学龄前学习开始。要让机器学会 **文字接龙**，需要非常大量的资料。为什么需要如此大量的资料呢？在第一堂课中，我们提到 **文字接龙** 绝非易事，机器至少需要两个方面的知识：语言知识和世界知识。

**语言知识** 相对容易学习。例如，机器需要知道“这个人突然就”后面可以接“跑”或“飞”，但接“也许”则不符合人类语法。一篇古老的论文估算，如果让机器学会语言知识，大约需要 **1亿到10亿个词汇量** 的训练数据。达到这个量级后，机器的语言知识基本“封顶”，不再犯语法错误，能正确生成符合语法的句子。

### 世界知识与数据量需求

另一方面，机器还需要学习 **世界知识**，这部分则困难得多。例如，机器需要知道“水的沸点是摄氏多少度”后面应接“100度”才是正确答案，而不是“50度”。尽管语言模型只能看文字，无法像人类一样通过嗅觉、触觉等感官真正理解世界，但这里所指的 **世界知识** 是可以通过文字理解的知识。

**世界知识** 需要极其庞大的训练资料。在2020年的实验中，即使收集到 **300亿个词汇量** 的训练资料，机器仍未能学完所有 **世界知识**，因为 **世界知识** 可以说是无穷无尽的。例如，如果句子前面加上“在低压下”，那么“水的沸点”就不应是100度。因此，要让机器正确进行 **文字接龙**，需要非常大量的资料。

### 预训练数据：海量与低成本

那么，在哪里可以找到如此大量的资料呢？在 **Pre-training** 阶段，任何资料都可以用来让机器学习 **文字接龙**。常见的做法是直接通过网络爬虫，将所有能爬到的文字资料都收集下来。每一句话、每一笔文字资料都成为学习的一部分。例如，爬到“人工智能真神奇”这句话，机器就会学习“人”后面可以接“工”，“人工”后面可以接“智”，以此类推。

在 **Pre-training** 阶段，数据收集成本非常低，人工介入很少。因此，**Pre-training** 阶段又被称为 **Self-Supervised Learning**（自监督学习: 一种机器学习范式，模型通过数据本身生成监督信号进行学习），意指机器自己监督自己学习，人类介入极少。由于人工介入少且网络资料庞大，第一阶段的训练资料可以被认为是取之不尽的。

### 惊人的预训练数据量

当前先进的大型语言模型在文献中通常使用了多大规模的数据进行预训练呢？例如，**Llama 3**（Llama 3: Meta公司开发的大型语言模型系列）使用了 **15万亿个Token** 进行预训练，而 **D6 V3** 也使用了大约 **15万亿个Token**。

为了具象化 **15万亿个Token** 有多大，我们可以将其类比为A4纸张。一张A4纸大约包含1000个 **Token**。如果我们将 **15万亿个Token** 全部用A4纸打印出来，并假设100张纸厚度为1厘米，那么这些纸叠起来的高度将达到 **1500公里**。作为对比，**圣母峰** 约9公里高，这些数据的高度是圣母峰的几十倍。飞机通常在10到20公里高度飞行。如果将台湾竖起来，其南北长度约为350到400公里。一些低轨卫星（如福卫3号）飞行高度约700公里，相当于在这堆纸的半山腰。这堆纸的顶端甚至进入了外太空。这足以说明机器阅读过的资料量是多么惊人。

### LLM的阅读速度与数据耗尽危机

假设一个人每10秒能读一页A4纸，不吃不喝不睡，要读完 **15万亿个Token** 需要 **4756年**。这意味着如果这个人从殷商时代（甚至没有现代文字）开始阅读，到2025年仍未读完。这充分展示了当今大型语言模型阅读过的资料量之巨大，人类的知识量难以匹敌。

你可能会好奇，如此巨大的 **15万亿个Token** 数据从何而来？实际上，获取这种量级的数据并非难事。**Hugging Face**（Hugging Face: 一个提供机器学习模型、数据集和工具的平台）发布了一个名为 **FineWeb**（FineWeb: Hugging Face发布的一个大型网络文本数据集）的数据集，其中包含了大约 **15万亿个Token**，可以免费下载（前提是有足够的存储空间，约 **44TB**）。

然而，关于预训练数据“取之不尽”的说法并不完全精确。一篇2022年底的论文（在当时也算是“史前时代”的论文）警示，我们可能很快就会用尽网络上的资料。该论文估算，网络资料的增长速度（绿色线）远低于大型语言模型训练资料的增长速度（蓝色线）。几乎每年新出的语言模型，其预训练资料量都是前一年的十倍。根据这篇论文的估算，到 **2028年** 左右，我们可能会用尽所有网络上的训练资料，届时大型语言模型将阅读过一切网络上的信息。

### LLM的记忆与理解

许多人认为，大型语言模型阅读了几乎所有网络资料，就意味着它能将所有内容死记硬背下来。因此，当模型背诵出错误内容时，人们会感到惊讶。例如，要求 **GPT 5.1** 背诵 **鲁迅** 的 **《孔乙己》**（《孔乙己》: 鲁迅创作的短篇小说）原文，它生成的文本虽然看似正确，但与原文相比，却遗漏了许多关键细节，且部分措辞不准确。

如果你了解大型语言模型的训练原理，就会明白模型并非将内容死记硬背，而是通过这些内容学习 **文字接龙**。因此，它没有理由能完整背诵一篇文章。它更像是阅读过文章后留下了模糊印象，甚至可以说它将知识压缩在脑中，解压缩时会产生一些失真。

另一个例子是要求模型讲述 **《射雕英雄传》**（《射雕英雄传》: 金庸创作的武侠小说）的原文。模型虽然能说出正确的标题“风雪惊变”，并提及北宋末年、华山论剑、东邪西毒南帝北丐、九阴真经、郭啸天和杨铁心等关键情节和人物，但这些并非原文，更像是一个人读完小说后用千字摘要出的内容。对机器而言，这是合理的，因为它学的是 **文字接龙**。它学到的是看到“射雕英雄传”这些字，后面可以接“华山论剑”、“九阴真经”和特定人名，然后它就凭借学过的 **文字接龙** 技能，生成一个看似像样的故事，但与原文并不相同。

### 算力与数据量的权衡

很多人可能认为数据越多越好，但这并非绝对。我们还需要考虑其他因素，例如 **算力**（计算能力: 进行计算所需的硬件资源和处理能力）的极限。当数据量增加时，在 **算力** 有限的情况下，就需要做出额外的牺牲。数据量大可能意味着无法训练过大的模型，反之，如果想训练一个巨大的模型，在 **算力** 有限的情况下，就只能使用有限的数据。

你可能会困惑，不是说大模型需要更大的训练数据吗？为什么这里又说大模型会用小数据训练？大模型使用更多训练数据确实能获得更好的结果，但这只是理想情况，不考虑 **算力** 限制。一个更大的模型需要更大的 **算力**，更多训练数据也需要更大的 **算力**。如果 **算力** 有限，无法“既要又要”，那么大模型就只能使用少量数据。

拥有大量数据的好处在于，机器阅读了更多教材，就像一个更认真的学生，学到的东西更多。从机器学习角度看，大量数据可以减少 **Overfitting**（过拟合: 模型在训练数据上表现良好，但在未见过的新数据上表现不差的现象），使训练结果与测试结果更接近。更大的模型则像天资聪明的学生，天生就聪明。从机器学习角度看，更大的模型意味着更大的函数搜索范围，更有机会找到好的函数。

然而，鱼与熊掌不可兼得。在 **算力** 有限的情况下，大模型和大数据只能二选一。那么，到底应该选择哪一个呢？

### Chinchilla定律与数据质量

在“上古时代”，**DeepMind**（DeepMind: Google旗下的人工智能研究公司）就对这个问题进行了研究，其结果便是著名的 **Chinchilla Scaling Law**（Chinchilla 定律: DeepMind提出的一种关于大型语言模型参数量、训练数据量和计算量之间关系的经验法则）。这个定律指出，数据和模型之间存在一个最佳比例。模型过大不好，光有天资不学习是“思而不学则殆”；模型过小但数据量巨大也不够，这是“学而不思则罔”。两者之间存在一个神秘的平衡。

研究发现，在不同 **算力** 规模下，这些最佳点大致可以连成一条直线。这使得我们可以推断，如果拥有更大的 **算力**，最佳点会出现在更低的损失值。因此，如果无法尝试不同的数据和模型组合，只能尝试一次，**Chinchilla Scaling Law** 可以指导数据和模型的比例选择。许多后续模型，如 **Llama** 系列，都采用了 **Chinchilla Scaling Law** 来调配模型大小和数据比例。

除了数据量，我们还需要考虑数据质量。低质量的数据可能会损害训练过程。一个著名的例子是 **Allen AI**（Allen AI: 保罗·艾伦人工智能研究所）开发的 **OLMo**（OLMo: Allen AI开发的完全开源大型语言模型）。**OLMo** 团队发现，模型在读取某些数据时，损失会突然暴涨，训练变得非常不稳定，甚至可能破坏模型能力。

### 低质量数据的影响

经过追根究底，他们发现训练不稳定的来源是 **Reddit**（Reddit: 一个社交新闻聚合和讨论网站）上的一个名为 **Microwave GAN**（Microwave GAN: Reddit上的一个讨论老旧微波炉的版块）的版块。这个版块的贴文常常包含数千个不同大小写的“M”，可能是模拟微波炉的声音。让语言模型读取这种奇怪的数据很容易导致模型训练失败。因此，数据质量至关重要。

实际上，从网络爬取的数据并不会直接用于训练语言模型，而是会经过层层过滤，只有高质量的数据才会被使用。一篇名为 **DataComp** 的文章详细介绍了数据清理过程。他们将清理过程分为三个阶段：
1.  **启发式规则清理**：使用人类定义的规则进行清理，仅保留约 **20%** 的数据。
2.  **去重**（Deduplication: 移除数据集中重复或相似条目的过程）：去除大量重复数据，因为重复数据对训练无益。此阶段后，仅剩约 **13%** 的数据。
3.  **基于模型的过滤**（Model-based Filtering: 使用机器学习模型来筛选和清理数据的方法）：使用专门训练的模型来检测高质量数据。例如，训练一个分类器，将 **Wikipedia**（维基百科: 一个自由、开放的百科全书网站）数据标记为高质量，其他数据标记为低质量，然后用该分类器筛选数据。最终，他们只保留了 **1.4%** 的数据。

### 数据清理的重要性与方法

你可能会觉得 **1.4%** 的数据非常少，但他们的数据源是 **CommonCrawl**（CommonCrawl: 一个非营利组织，提供公开的网络爬取数据集），原始数据量高达 **240万亿个Token**。即使只保留 **1%**，也有 **2.4万亿个Token**，这仍然是一个惊人的数据量，远超个人训练所需的 **算力**。

经过一系列清洗后，高质量数据确实能带来更好的模型表现。**DataComp** 的研究表明，拥有高质量数据时，在相同 **算力** 下可以获得更好的结果；或者，要达到相同结果，可以使用更少的 **算力** 和数据。这强调了训练数据质量的重要性。

那么，如何定义高质量数据呢？这很难一概而论，因此有各种数据清理方法。一个有趣的方法是直接使用语言模型进行数据清理。这篇名为 **Rephrasing the Web**（Rephrasing the Web: 一篇关于使用语言模型重写网络数据以提高训练质量的论文）的文章提出，可以使用一个已有的优秀语言模型，对网络上的现有数据进行 **rephrase**（重述）。由于语言模型通常表达更有条理、易于阅读，且少有奇怪符号，因此先让语言模型进行“换句话说”后，再用这些新数据训练另一个语言模型。

### 预训练模型的局限性

该方法通过 **Prompt** 指示语言模型将输入文章进行换句话说，并以 **Wikipedia** 的质量为目标。结果显示，使用换句话说的数据进行训练（蓝色线）比使用原始网络数据训练（橙色线）效果更好。在相同数据量下，换句话说的数据能带来更好的结果；或者，要达到相同表现，只需要原始数据量的三分之一。

尽管使用了大量网络数据进行预训练，但预训练模型本身往往无法良好运作。例如，在 **ChatGPT**（即 **GPT-3.5**）于2022年底发布之前，**OpenAI** 早在2020年就发布了 **GPT-3**（GPT-3: OpenAI发布的大型语言模型第三代）。然而，**GPT-3** 并未引起巨大轰动，因为它基本上无法直接使用。当用户向 **GPT-3** 提问时，这个拥有 **1760亿参数** 的庞然大物（比作业中使用的3B或4B模型大50倍以上）却无法好好回答问题，反而给出四个选项供用户选择，或不直接回答。

### 预训练模型的潜力

同样，**Google** 也开发了一个更大的模型 **PaLM**（PaLM: Google开发的大型语言模型系列），其参数量高达 **5400亿**，是 **GPT-3** 的三倍。但当被问及小学数学问题时，它也不直接回答，而是提出更多问题让人类来回答。

为什么预训练后的语言模型不能很好地回答问题呢？因为语言模型 **文字接龙** 的结果取决于其训练数据分布。网络上的数据不足以让模型学会回答问题。例如，搜索“台湾最高的山是哪座山”，后面接的可能不只是“玉山”，还有各种选择题、留言请求等。因此，直接用网络爬取的数据训练语言模型进行 **文字接龙**，它无法真正回答人类的问题。

然而，预训练模型仍蕴含巨大潜力，它就像一块璞玉，只要精雕细琢，就能发挥不可思议的力量。一个预训练模型其实有机会正确回答问题。当你问它“台湾最高的山是哪座”时，它可能会回答“谁来告诉我呀”，或者“第二高的又是哪座”，甚至给出一个选择题，或者说“我也不知道”。但也有很小的概率，它能接龙出正确答案，因为正确的句子可能在训练数据中出现过。

事实上，这些预训练模型可能比我们想象的更强大。我们只是需要合适的方法来激发它们的潜力。一篇今年10月发表在 **ArXiv**（ArXiv: 一个收录物理学、数学、计算机科学等领域预印本论文的网站）上的论文 **Your base model is smarter than you think**（Your base model is smarter than you think: 一篇关于基础模型潜力的论文）指出，如果在模型进行 **文字接龙** 时使用特殊的 **Sampling**（采样: 从模型输出的概率分布中选择下一个词元的方法）方式（而非简单的 **Top-K Sampling** 或 **Greedy Sampling**），一个 **Base Model**（即预训练模型）甚至可以击败经过 **SFT** 和 **RLHF** 训练的模型。这表明预训练模型本身就具有巨大潜力，**SFT** 和 **RLHF** 更多地是帮助其做出正确选择，让正确答案出现的概率更高。

### SFT：机器开始上学

接下来进入第二阶段 **SFT**，机器开始“上学”。在这一阶段，人类提供准备好的数据来教机器进行 **文字接龙**。人类需要提供的问题格式是：一个问题（语言模型的输入）和对应的标准答案。例如，告诉语言模型：“有人问你台湾最高的山是哪座，你就回答玉山，不要再讲奇怪的话。”或者“有人问你是谁，你就说我是人工智能。”“有人说教我骇入邻居家的Wi-Fi，你就要说我不能教你。”

语言模型会利用这些数据学习 **文字接龙**。它会学到，当人类说“台湾最高的山是哪座？”时，**AI** 后面应该接“玉”。这里使用“User:”和“AI:”代表 **Chat Template**（对话模板），这是在第二阶段人类教会模型的，告诉它在这些模板下应如何正确回应。每笔数据都用于训练，例如“AI:玉”后面应接“山”，“AI:玉山”后面应接结束符号。通过这种方式，机器学习如何输出人类提供的标准答案。

**SFT** 也被称为 **Instruction Fine-Tuning**（指令微调: 通过人类提供的指令-响应对来微调模型，使其更好地遵循指令），因为人类提供的问题被称为 **Instruction**（指令），机器根据这些 **Instruction** 进行学习。

### SFT的蜕变与Pre-training的基石

完成 **SFT** 后，机器会脱胎换骨。之前提到，即使是拥有 **5400亿参数** 的庞然大物 **PaLM**，在预训练后也无法回答问题。但一旦完成 **SFT**，它就能突然之间正常回答问题了。

看到这里，有人可能会认为这都是 **SFT** 的功劳，而 **Pre-training** 只是“躺赢”。但事实并非如此，**SFT** 的成功是站在 **Pre-training** 这个巨人的肩膀上。如果没有先进行 **Pre-training**，仅仅依靠 **SFT** 无法成就一个好的语言模型。

如果只有 **SFT**，会发生什么？由于人类无法真正准备海量的 **SFT** 数据（这需要设计问题并为每个问题编写标准答案，工程量巨大），如果 **SFT** 数据量很少，模型在学习时会非常容易 **Overfitting**。例如，如果只有一条训练数据：“有人说台湾最高的山是哪座，你就要输出玉山。”模型在学习时可能学到的规则不是“玉山是台湾最高的山”，而是“如果用户输入的问题中有‘山’这个字，我就回答玉山”，无论是什么山。

### SFT的局限与Pre-training的帮助

由于 **SFT** 数据量少，模型无法充分验证其学习到的规则是否普遍适用。虽然在训练数据上看起来合理，但在测试时，如果人类问“世界上最高的山是哪座”，它也可能傻傻地回答“玉山”。因此，仅有 **SFT** 是不够的，必须有 **Pre-training** 才能让 **SFT** 发挥作用。

这里引用一个文献上的例子，说明 **Pre-training** 如何帮助 **SFT**。这个例子来自 **Physics of Language Model**（Physics of Language Model: 一系列旨在系统化研究语言模型行为的论文）系列研究。该研究旨在构建语言模型的“物理学”，通过收集更多具体数据进行系统化分析。

研究内容是：创建一些虚假的 **Pre-training** 数据，每条数据介绍一个人，总共有N条数据，但每个人的资料只出现一次。例如，一条数据介绍“爱音是MyGo的节奏吉他手，也是羽丘女子高一的学生”，另一条介绍“高松灯是羽丘女子学园高一的学生，也是天文部社员，担任MyGo的主唱”。

### 知识的举一反三与多版本数据

接下来进行 **SFT**。**SFT** 的问题只与其中一半的人（N/2个人）有关。例如，一条训练数据告诉模型：“如果有人问你谁是MyGo的节奏吉他手，你就要回答是千早爱音。”然后，期望模型根据 **Pre-training** 中看过的资料来回答问题。

然而，语言模型在 **SFT** 中未曾见过的N/2个人的问题上，无法正确回答。例如，如果你问它“谁是MyGo的主唱”，它无法回答这个问题。为什么？因为语言模型不仅“无视”了高松灯，它“无视”了所有人。当只用N/2个人的数据进行 **SFT** 时，在剩下N/2个人的数据上，正确率是0。语言模型完全没有举一反三的能力。

关键在于，在 **Pre-training** 阶段，每个人的资料只出现一次。对语言模型而言，这些都是 **Token** 串，它唯一学会的就是 **文字接龙**。它不知道什么是人名，什么是MyGo，什么是羽丘女子高中。它学到的是“千早爱音”后面可以接“MyGo的节奏吉他手”，再接“羽丘女子高中”；“高松灯”后面可以接“羽丘女子高中”，再接“天文部社员”，再接“MyGo主唱”。

### SFT与Pre-training的协同作用

在 **SFT** 阶段，当模型被告知“MyGo的节奏吉他手”的回答应是“千早爱音”时，它学到的规则可能是：当有人问“谁是X”时，就查找 **Pre-training** 中X前面接了什么词汇作为答案。这个规则在训练时可行。但当被问及“谁是MyGo的主唱”时，MyGo主唱前面接的词汇是“天文部的社员”，所以它无法回答“高松灯”，只能回答“天文部的社员”。

然而，如果 **Pre-training** 数据中每个人的资料有多种不同版本（例如，除了维基百科资料，还有萌娘百科资料），当机器阅读大量与同一个人相关的不同版本资料时，它就能学到更多。例如，它可能学到“千早爱音”后面可以接“MyGo的吉他手”，也可以接“羽丘女子高一的学生”，从而更好地连接不同的 **Entity**（实体: 具有独立存在或可识别特征的事物，如人名、地名、组织名）。

有了这种多版本 **Pre-training** 后，再进行 **SFT**，模型就能学会“谁是X”的问题，并正确回答“谁是MyGo主唱”。这说明，要让机器学会同样的知识并举一反三，需要用不同的角度反复诠释相同的数据。这就是为什么 **Pre-training** 需要大量数据。网络上同一个人在不同网页上以各种方式被讨论，机器需要从不同角度反复学习，才能真正理解并运用这些知识。

### SFT：风格改变而非知识灌输

因此，我们知道 **SFT** 由于数据量少，通常不能给语言模型带来新的知识，它只是改变语言模型的输出风格。预训练阶段，模型回答问题可能杂乱无章。但经过 **SFT** 后，当被问及“台湾最高的山”时，它能正确回答“玉山”，并非 **SFT** 灌输了玉山相关知识，而是在 **Pre-training** 时，它已在多篇文章中读过“台湾最高的山是玉山”，并建立了两者之间的关联。它只是不知道在回答问题时应如何表达出来。

由于 **Pre-training** 阶段模型已掌握玉山相关知识，知道“玉山是台湾最高的山”且这两个词汇关联性高，**SFT** 才能成功激发其潜力，使其能正确回答问题。许多例子表明，在 **SFT** 阶段，很难向语言模型灌输新知识。

### SFT训练情境与知识类型

两篇论文指出，在正常训练方法下，向语言模型灌输新知识并不容易。其中一篇论文列举了四种训练情境：
1.  **Highly known**：模型使用 **Greedy Decoding**（贪婪解码: 在文本生成过程中，每一步都选择概率最高的词元作为输出）能正确回答问题，表明它本来就会。
2.  **Maybe known**：模型使用 **Greedy Decoding** 无法正确回答，但换一种问法就能正确回答。这表明模型可能拥有相关知识，只是提问方式不佳时无法理解。
3.  **Weakly known**：模型介于已知和未知之间，思绪混乱。使用 **Sampling**（采样）方法解码时，有时答对有时答错。
4.  **Unknown**：无论如何 **Sampling**，模型都无法生成正确答案。

这四种资料中，哪一种对训练最有帮助呢？直觉上可能认为是 **Unknown**，因为可以教模型全新的东西。但实际上，最有用的却是 **Maybe known**。

### Maybe Known数据的重要性

该论文的实验结果显示，使用 **Maybe known** 资料进行训练，与使用所有类型资料混合训练的正确率相近。如果使用 **Highly known** 或 **Unknown** 资料，正确率反而更低。这表明，当模型学习混合类型问题时，真正的进步可能主要来自 **Maybe known** 问题。机器可以从 **Maybe known** 问题中学习如何正确回答和判读问题，因为它已经拥有相关知识，只是不知道如何根据问题进行回答。

如果对 **Unknown** 问题进行更多训练（直到模型收敛），性能会暴跌。因为 **Unknown** 问题对机器来说是学不会的，它们不存在于 **Pre-training** 阶段学到的知识体系中。当模型遇到 **Unknown** 问题时，只能死记硬背，结果是无法将其知识应用于测试数据。因此，使用 **Unknown** 资料反而会得到最差的结果。这告诉我们，在 **SFT** 阶段，很难教会机器新的知识，其知识体系在 **Pre-training** 阶段就已基本固定。

### Alignment对LLM风格的影响

我们提到 **SFT** 难以给模型带来新信息，模型通常在 **SFT** 中学到的是输出风格。一篇论文研究了 **Alignment**（对齐）对 **LLM** 的影响有多大。这里 **Alignment** 涵盖了 **SFT** 和 **RLHF**。

该论文指出，**Alignment** 往往没有给模型带来本质上的变化。实验通过比较语言模型在 **Alignment** 前后的输出差异来验证。首先，使用已进行 **Alignment** 的模型进行 **文字接龙**，观察其生成的 **Token**。然后，将相同的句子输入未进行 **Alignment** 的模型进行 **文字接龙**。

变化分为三种：
1.  **Unshift**：某个 **Token** 在 **Alignment** 前后都是概率最高的。
2.  **Marginal**：某个 **Token** 在 **Alignment** 前概率排第二或第三，**Alignment** 后变为概率最高的。
3.  **Shifted**：某个 **Token** 排名原本在第三名之外，**Alignment** 后变为第一名。

### Alignment前后输出的微观与宏观差异

论文中的一个例子显示，当 **Aligned** 模型被问及“哪种品种的狗是最小只的”时，它给出了一个详细回应。图中蓝色 **Token** 表示 **Unshift**（**Alignment** 前后概率排名未变），棕色是 **Marginal**，红色是 **Shifted**（排名从第三名之外跃升至第一名）。

结果发现，**Shifted Token** 并不多，且许多关键词汇并非 **Shifted Token**。**Shifted Token** 往往是一些连接词（如“However”、“While”）或结束符号。在 **Alignment** 之前，模型可能“长舌”，难以停止；**Alignment** 之后，模型更容易停下来，这表明结束符号的概率发生了显著变化。除此之外，**Token** 的概率分布并没有太大改变。

你可能会困惑，**Alignment** 前后模型的输出结果往往天差地别，为何实际改变如此之少？这是因为只要有一个 **Token** 的概率改变，后续的 **文字接龙** 都会基于前面的 **Token** 进行。因此，“一步错步步错”，即使只有一个 **Token** 的概率改变，接龙结果也会大相径庭，但整体概率分布可能变化不大。

该论文还分析了 **Llama** 系列、**Vicuna** 系列和 **Mistral** 系列模型在 **Alignment** 前后的变化，发现 **Shifted Token** 的一个共同点是结束符号的概率改变。

### 预训练的遗迹：自回归的余烬

由于 **Pre-training** 对 **SFT** 影响巨大，在使用当前模型时，会发现许多 **Pre-training** 遗留下来的“遗迹”。这些遗迹来自一篇名为 **Embers of Autoregression**（Embers of Autoregression: 一篇关于预训练模型中遗留的自回归特性对下游任务影响的论文）的文章，其中“Embers”指灰烬。文章指出，**Pre-training** 留下的灰烬在使用模型时依然可见，即使经过 **SFT** 甚至 **RLHF**。

论文中举了一个有趣的例子：要求模型进行 **ROT** 解码。**ROT** 编码是一种将英文字母移动固定位置的加密方式（例如A变B，B变C）。例如，要求模型进行 **ROT13** 编码（将每个字母移动13个位置），**GPT-4** 可以成功解码。但如果将 **ROT13** 改为 **ROT8**（移动8个位置），模型就无法完成。

### ROT编码与训练数据偏好

论文进一步分析了 **ROT** 所有数字（1到25）的编码情况。结果发现，**GPT-3.5** 只能正确解码 **ROT13**，而 **GPT-4** 只能解码 **ROT1**、**ROT3** 和 **ROT13**。为什么会这样？

一个可能的原因是 **ROT13** 是网络上最常出现的训练资料。在 **Pre-training** 阶段，许多网页都以 **ROT13** 作为例子，甚至 **ROT13** 拥有自己的 **Wikipedia** 页面。在解释 **ROT** 时，**ROT13** 最常被用作例子，因为它将26个英文字母移动了13个位置，是改变量最多的。如果你搜索 **Shift Cipher**（移位密码: 一种替换密码，明文中的字母通过固定数量的位移进行替换），后面往往直接跟着“13”，表明这是一种非常常见、常被举例、常出现在 **Pre-training** 资料中的编码方式。

作者团队分析了 **Common Crawl** 数据中与编码相关的例子，发现 **ROT13** 出现次数最多，**ROT1** 和 **ROT3** 也有一定出现次数。这在某种程度上解释了为什么 **GPT-3.5** 只能处理 **ROT13**，而 **GPT-4** 能处理 **ROT1**、**ROT3** 和 **ROT13**。

### SFT的两种路线：专才与通才

至此，我们已了解 **SFT** 的作用：它并非赋予模型新知识，而是激发其原有潜力，使其能正确回答问题。**SFT** 的路线分成了两条：
1.  **打造专才模型**：通过给预训练模型不同的数据，将其转化为翻译专才、摘要专才或编修专才。
2.  **打造通才模型**：通过给预训练模型各种 **SFT** 数据，期望它能掌握多种技能。

路线1最具代表性的例子是 **BERT**（BERT: Bidirectional Encoder Representations from Transformers，Google开发的一种预训练语言模型）模型系列。**BERT** 模型本身无法直接使用，需要针对特定任务进行微调，才能具备不同的能力。如果你想了解更多关于 **BERT** 的信息，可以参考2021年的机器学习课程。

### 通才模型的探索与OpenAI的领先

然而，今天我们走的并非路线1，而是路线2。**ChatGPT**、**Gemini** 等人工智能都是通才模型。要打造一个通才模型，可能需要在 **SFT** 阶段收集各种标注数据，涵盖翻译、纠错、摘要等任务，期望模型能举一反三，即使在测试时遇到从未见过的任务也能回答。

关于打造通才模型，由于 **BERT** 是 **Google** 开发的，很多人可能误以为 **Google** 选择了路线1，而 **OpenAI** 选择了路线2。但实际上，最早开始尝试打造通才模型的也是 **Google** 团队。例如，最早通过多任务指令微调大型语言模型以期成为通才的模型，据我所知是 **FLAN**（FLAN: Fine-tuned LAnguage Net，Google开发的一种通过多任务指令微调的语言模型），它在2021年就已出现。

后来，**Google** 团队又使用了更多数据进行 **Supervised Fine Tuning**，将数据扩展到 **1800个任务**，让模型在这些任务上学习，希望它能泛化到新任务。所以 **Google** 也有打造通才模型的计划。但 **OpenAI** 的厉害之处在于他们的模型率先上线，从而独占了市场和声量。

### SFT的画龙点睛与数据质量

**SFT** 旨在打造通才模型，这可能给人一种需要海量数据的感觉，但事实并非如此。我们说 **SFT** 真正赋予模型的不是新知识，而是输出风格。因此，**SFT** 更像是“画龙点睛”，整张图已大致完成，只需 **SFT** 点上眼睛，龙就能飞起来。

在 **InstructGPT**（InstructGPT: OpenAI早期发布的一种通过指令微调和RLHF训练的语言模型）这篇论文中，**OpenAI** 明确阐述了 **Pre-training**、**SFT** 和 **Fine-tune** 这三个阶段的训练方式。该论文发布于2022年初，当时人们还愿意详细说明训练方法，而 **GPT-4** 之后，模型训练方法就变得不透明了。

**InstructGPT** 论文指出，**SFT** 真的需要大量数据吗？不，他们使用的 **SFT** 数据量非常少，大约只有 **一万多笔**。但即使只有这么少的数据，**SFT** 也发挥了巨大作用。论文中的实验结果显示，在没有 **SFT** 的最大模型和有 **SFT** 的最小模型之间进行比较，有 **SFT** 的最小模型（绿色线）的表现可以与最大的模型（蓝色线）平分秋色。当然，最大的模型如果进行更多 **SFT**，表现会更好。

### Llama 2与LIMA的少量数据高效SFT

其他团队也报告了类似的结果。例如，**Meta** 的 **Llama** 系列在 **Llama 2**（Llama 2: Meta公司发布的Llama系列第二代大型语言模型）的论文中提到，他们只使用了 **27,540笔SFT数据**。他们认为使用更多数据效果不佳，即使找了合作厂商收集了上百万笔数据，**SFT** 也没有做得更好。与其使用上百万笔低质量数据，不如精心收集几万笔高质量数据进行 **SFT**，就能取得显著效果。因此，本章的标题是“Quality is all you need”（质量是你所需要的一切），强调数据质量的重要性。

后来，**Meta** 又发表了一篇名为 **LIMA**（LIMA: Less Is More for Alignment，一篇关于仅用少量高质量数据进行SFT也能取得良好效果的论文）的论文，其标题“Less is more for alignment”也表明了其核心思想。这篇论文挑战性地提出，他们只使用了 **1000笔SFT数据**，这些数据经过精心挑选，部分来自网络论坛，部分由作者团队手写。

### SFT数据选择的玄妙

他们使用这 **1000笔数据** 训练的模型 **LIMA**，在当时可以与最好的模型 **GPT-4** 旗鼓相当。论文称，在 **43%** 的情况下，**LIMA** 能与 **GPT-4** 取得相同甚至更好的结果（这里包含了“相同”的情况）。尽管 **LIMA** 相较于 **GPT-4** 仍是败多胜少，但这表明 **LIMA** 并非一个差的模型，它是一个可运作的模型。

你可能会好奇这 **1000笔数据** 有何特殊之处。如果看论文中的例子，很难看出其特异性。例如，问题涉及数学（minimum与infimum的区别）、科幻（千年隼号是否量产）和奇怪的生活问题（如何当一个懒惰的大学生）。这些问题为何对模型训练有帮助，也说不清楚。总之，这 **1000笔数据** 对模型训练确实有帮助，因此 **SFT** 数据的选择对训练结果至关重要。

另一篇论文尝试使用各种 **SFT** 数据微调 **Qwen 72B**（Qwen 72B: 阿里巴巴开发的具有720亿参数的大型语言模型）模型，发现一个名为 **ROUZHIBA**（弱智吧: 中国互联网上一个以发布“弱智”问题为特色的论坛）的训练数据在所有任务上表现最好。**ROUZHIBA** 上的问题确实“弱智”，例如“为什么我的银行卡在高压锅煮了一个晚上还是冻结的？”或“一斤棉花和一斤铁同时掉进水里你先救谁？”但不知为何，这些问题能激发模型能力，使其在 **SFT** 后取得最佳结果。因此，如何挑选 **SFT** 数据仍然是一个值得研究的问题。

### 长度与SFT效果的奇特关联

还有一个离奇的发现：有人找到了一种比 **LIMA** 更强的方法。他们从现有的 **Alpaca**（Alpaca: 斯坦福大学基于LLaMA模型通过指令微调创建的开源模型）数据集中挑选出最长的 **1000笔数据**，然后用这些数据训练模型，结果出奇地好。与使用 **52000笔数据** 训练的模型相比，选择最长的 **1000笔数据** 训练的模型在多数情况下都能获胜，甚至能击败 **LIMA** 这种精心挑选的 **SFT** 数据。

有人可能会认为，这 **1000笔数据** 的神奇之处在于它们特别长。但实际上并非如此，**LIMA** 的训练数据才是真正最长的，平均每笔答案的 **Token** 数目超过500，而这个方法挑选的 **1000笔数据** 平均只有256.8个 **Token**。但不知为何，其训练出的模型表现更好。因此，这篇论文的标题是“Long is more for alignment”（Long is more for alignment: 一篇关于SFT数据长度对模型对齐效果影响的论文），表明 **SFT** 是一个非常玄妙的领域。

当然，也有一些文献试图用更系统化的方法来选择数据。例如，一篇论文尝试训练模型来辅助数据挑选。他们训练了一个 **Complexity Scorer**（复杂性评分器: 评估问题复杂程度的模型），认为问题越复杂，越应该被选入训练数据。他们还训练了一个 **Quality Scorer**（质量评分器: 评估数据质量的模型），评估 **SFT** 数据的质量。将这两个评分器的结果结合，并加入 **Diversity Selection**（多样性选择: 旨在选择具有不同特征或主题的数据以提高数据集多样性的方法）以避免选择高度相似的问题，可以得到好的结果。因此，也可以尝试用系统化方法构建 **Pipeline** 来选择最适合的 **SFT** 数据集。

### 知识蒸馏：用AI做老师

到目前为止，我们都假设在制作 **SFT** 数据集时，需要通过某种方法由人类找到问题并写出答案。但很多时候，提问相对简单（例如“如何当个懒惰的大学生”），而编写答案则非常耗时（**LIMA** 中的答案通常超过500个 **Token**）。

既然很多事情都可以用语言模型完成，有人就想到，在 **SFT** 阶段，何必再用人类来标注数据呢？可以直接使用现成的 **ChatGPT** 等语言模型来标注数据。我们只需收集大量问题，然后交给另一个语言模型来生成答案。这种方法被称为 **Knowledge Distillation**（知识蒸馏: 将一个大型教师模型的知识迁移到一个小型学生模型的过程）。你将那些已训练好的模型视为“老师”，而你自己的 **Pre-training Model** 则是“学生”。给老师和学生相同的问题，期望学生生成的答案与老师的答案越接近越好。

这种方法非常有效。你可能会在一些“农场文”中看到“用不到100美金就训练出强大模型，拥有 **ChatGPT** 90%能力”之类的标题，这些模型大多是通过 **Knowledge Distillation** 训练出来的。最早的经典例子是 **Alpaca** 和 **Vicuna**。它们在 **LLaMA 1**（LLaMA 1: Meta公司发布的第一代大型语言模型）发布几周内，就迅速微调了 **LLaMA 1**，使其具备 **SFT** 能力。当时它们的“老师”就是 **ChatGPT**。**Alpaca** 和 **Vicuna** 分别从 **ChatGPT** 获取了 **5万和7万笔数据**，训练成本仅为 **100或140美金**。

### 低成本SFT的秘密

近期，仍有人在继续进行 **Knowledge Distillation**。例如，**SkyT1** 和 **S1** 都以 **Qwen 2.5 32B** 作为“学生”，并从 **Qwen** 或 **Gemini** 等更强的模型生成数据，然后让自己的模型学习。**S1** 曾因 **李飞飞** 教授作为共同作者之一而声名大噪，他们宣称“用不到50美金就能打造出强大模型”。

为何能以如此低成本实现？因为他们已经拥有一个优秀的 **Pre-training** 模型，所以这里的成本不包含 **Pre-training**，仅是 **SFT** 的成本。我们说过 **SFT** 确实不需要大量训练数据，关键在于高质量数据。他们找到了强大的“老师”——**Gemini**，并用 **Gemini** 生成的 **1000笔数据** 来微调模型，从而以不到50美金的成本获得了非常好的 **SFT** 模型。当然，这里的成本不包含生成数据和清理数据的成本，因为调用在线模型的API和清理数据都需要花费。例如，**S1** 的 **1K数据** 也是经过精挑细选的，是 **Gemini** 编写答案后，再人工挑选出写得较好的答案进行训练。

### 非指令微调的奇迹

然而，目前的 **Knowledge Distillation** 仍需要准备问题，然后让语言模型作为“老师”来回答。准备问题也相当麻烦，我们能否连准备问题这个步骤都省去呢？这并非不可能。我们实验室的 **谢濬丞** 同学曾做过一个非常有意思的实验，其论文于去年9月发表在 **ArXiv** 上，名为 **non-instructional fine-tuning**（非指令微调: 一种不依赖明确指令-响应对，通过其他方式进行模型微调的方法）。

其做法是：从网上随意找一句话，截掉后半段，只保留前半段。将这半句话作为“问题”（尽管它并非真正的问题），然后让 **ChatGPT** 进行 **文字接龙**，续写出后半句。自己的模型将较好的语言模型续写结果作为“老师”，用这样的 **SFT** 数据来训练自己的语言模型。神奇的是，这一招竟然有效。

你可能会以为这些在线语言模型看到奇怪的前半句，会续写出类似 **SFT** 数据的后半句，但事实并非如此。例如，给语言模型一个前半句，**ChatGPT** 续写的结果与真实下半句并没有本质差异。另一个例子是关于一个人被雇佣为新闻主播，**ChatGPT** 续写他将开始新的职业生涯，即使他最近曾被捕，句子本身并无特别之处。

### 小技巧激发模型潜力

但神奇之处在于，用 **ChatGPT** 续写的结果作为 **SFT** 答案去训练模型，模型的能力突然飙升。实验测试了三种不同的 **Base Model**。例如，**Mistral** 模型在完全没有 **SFT** 的情况下，在 **MT Bench**（MT Bench: 一个用于评估大型语言模型多轮对话能力的基准测试）上只能得到 **3.7分**。但如果用 **GPT-4o Turbo** 产生的 **8万笔数据** 进行训练，**SFT** 后可以达到 **7.3分**。

你可能会觉得这 **8万笔数据** 有何神奇之处。但如果直接拿这 **8万笔数据**（而非让 **ChatGPT** 续写后半段）去微调模型，模型根本无法学到 **SFT** 能力，分数仍只有 **3.5分**。而 **7.29分** 甚至高于 **Mistral** 官方发布的经过 **Instruction Fine Tune** 的模型分数。

在 **Meta** 的 **Llama 3** 上也做了类似实验。原始 **Base Model** 分数是 **5.5分**，用 **GPT-4o Turbo** 数据微调后可达 **7分或8分**。**Llama 3 70B** 在未进行 **SFT** 时只有 **2.7分**，官方发布的 **SFT** 模型是 **8.6分**。如果从 **Base Model** 开始微调，可达 **8.18分**；如果从官方 **Instruction Model** 继续微调，甚至能达到 **9.03分**。这表明，那些奇怪的、网络爬取的数据，加上 **ChatGPT** 生成的后半句，不知为何能激发模型进行 **SFT** 的能力。

### Response Tuning与Instruction Following

还有一篇非常相似的论文，名为 **Response Tuning**（响应微调: 一种不提供明确问题，仅通过模型生成答案来微调模型的方法）。他们发现，一般的 **Instruction Tuning** 需要问题和答案，教模型看到问题后输出答案。但他们提出 **Response Tuning**，不需要给模型任何问题，模型输入就是“Assistant:”，然后直接让模型生成答案，无论问题是什么。他们发现这一招也能激发模型能力，使其具备 **Instruction Following**（指令遵循）能力。

这种方法的效果如何？在 **Llama 3 8B Base Model** 和 **Gemma 2 9B**（Gemma 2 9B: Google发布的Gemma系列第二代大型语言模型，具有90亿参数）上进行测试，如果未进行任何 **Tuning**，模型表现非常差。如果模型完全没有 **SFT**，人类认为 **95%** 的答案都不可接受。但如果进行了 **Response Tuning**（只有答案没有问题），与 **Instruction Tuning**（有问有答）的效果相差不大。即使模型只学习如何生成答案，**Response Tuning** 也能取得一定程度的效果。

### 无需训练的指令遵循

更离奇的是，另一篇同期发表在 **ArXiv** 上的论文 **Instruction Following without Instruction Tuning**（Instruction Following without Instruction Tuning: 一篇关于在不进行指令微调的情况下，通过技巧提升模型指令遵循能力的论文），除了尝试 **Response Tuning**，还尝试了一种连训练都没有进行的方法，仅靠一些奇妙的小技巧，就让模型展现出部分原本 **SFT** 才有的能力。

什么小技巧呢？我们之前提到，经过 **Alignment** 的模型和未经过 **Alignment** 的模型之间一个很大的区别是它们能否产生结束符号。因此，论文提出，既然最大区别是结束符号，那能否干脆不 **Tune** 模型，直接提高结束符号的概率？

### 调整概率与模型性能提升

第一个规则是：直接调高结束符号的概率，仅此而已，甚至没有训练模型。
第二个规则是：观察到经过 **Alignment** 和未经过 **Alignment** 的模型在书写风格上存在差异，某些 **Token** 在 **Alignment** 模型中更容易出现。因此，手动修改了一些符号的出现概率（例如，降低角括号和Shift的概率，提高其他符号的概率）。他们发现这一招也有帮助。
第三个规则是：没有 **SFT** 的 **Pre-training Model** 有一个奇怪的癖好，非常喜欢重复说话（“鬼打墙”）。因此，加入一个惩罚机制，如果某个符号前面已经出现过，就直接降低其概率。

加入这三条规则后，模型的能力竟然直接暴涨。原始 **Base Model** 与经过正常 **Instruction Tuning** 的模型相比，胜率只有 **2.4%**，根本无法匹敌。但如果应用这三条规则，**Base Model** 变得非常强大，有 **24%** 的概率可以击败一个经过正常 **Instruction Tuning** 的模型。这表明，即使只是对模型的输出概率进行一些“乱搞”，在不训练的情况下也能取得不错的效果。这告诉我们，**SFT** 是一个很神奇的东西，不一定需要大量数据来激发模型能力，许多小技巧也可能做到。

### RLHF：模型出社会后的学习

接下来，我们进入最后一步 **RLHF**，看看模型“出社会”后是如何学习的。在使用大型语言模型时，当你提问并得到答案后，界面上常常会有赞或倒赞的符号。你可以通过这些符号告诉模型你喜欢或不喜欢这个答案。例如，**Claude** 模型甚至会在 **System Prompt** 中引导用户点击倒赞符号，以收集更多人类反馈。

在 **RLHF** 阶段，我们就是要探讨如何利用这些人类反馈来训练语言模型。我们先从人类参与的角度来看 **SFT** 和 **RLHF** 的差异。这两个阶段都需要人类大量参与，但模式不同。在 **SFT** 阶段，人类是机器的老师，需要想办法为每个问题生成答案来教导学生机器。而在 **RLHF** 中，人类先提问，机器自己生成答案，然后人类根据机器的答案判断好坏。

### 人类参与模式与RLHF的优势

因此，对 **SFT** 而言，人类工作量大；对 **RLHF** 而言，人类轻松许多，只需点赞或倒赞。**SFT** 只能请开发者找工读生来完成，因为用户通常不愿意为机器生成正确答案。用户问机器问题是因为自己不知道答案，如果知道就不会问了。但 **RLHF** 只需要用户点赞或倒赞，用户通常是愿意的。

**RLHF** 还有一个好处：许多问题人类难以写出正确答案，但一旦看到机器给出的答案，判断其好坏就相对容易得多。例如，如果你让 **ChatGPT** 写一首七言律诗，要求它写一首与大型语言模型训练过程（**Pre-training**、**SFT**、**RLHF**）相关的诗。**ChatGPT 5.1** 写出的诗有十句，而七言律诗应只有八句。尽管我没有写诗的能力，但我一看就知道这不是一首好诗，因为格式错误，我可以给机器负面反馈。

### RLHF：结果导向的优化

很多时候，人类难以写出正确答案，但判断好坏相对容易。这是从人类参与角度看 **SFT** 和 **RLHF** 的差异。接下来，我们从机器学习角度来看这三个阶段的差异。

在第一、二阶段，模型学习的是 **文字接龙**。例如，在第二阶段，模型根据人类标注的数据学习 **文字接龙**。人类告诉它“有人问你台湾最高的山，你后面就接玉山”。模型学到，当用户输入“台湾最高的山”时，**文字接龙** 应接“玉山”。你会计算语言模型输出的概率分布与“玉山”概率分布之间的距离（通常是 **Cross-Entropy**）。对每笔数据中的每个训练 **Token** 计算 **Cross-Entropy**，然后求和得到总 **Loss**，这就是要最小化的目标。第一、二阶段都是如此，区别仅在于数据来源是否经过人类修整。这种有标准答案的学习称为 **Supervised Learning**（监督学习）。如果标准答案获取容易（如 **Pre-training** 使用网络数据，无需人工介入），则称为 **Self-Supervised Learning**。

### RLHF的Loss定义与挑战

在第三阶段，情况则不同。没有标准答案。有人问模型一个问题，模型开始 **文字接龙** 直到结束。然后人类判断答案是否喜欢，并给出一个分数（例如，赞为+1，倒赞为-1）。这些人类的评价在 **Reinforcement Learning**（强化学习）文献中被称为 **Reward**（奖励: 环境对智能体行动的反馈信号，用于衡量行动的好坏）。我们将所有 **Reward** 集合起来，取一个符号（因为 **Loss** 通常越小越好，而 **Reward** 越高越好），这就是我们要最小化的 **Loss**。

因此，第一、二阶段与第三阶段最大的差异在于 **Loss** 的定义。在第三阶段，这种由人提供反馈来定义 **Loss** 的方式就是 **Reinforcement Learning**。

接下来，我们更深入分析第一、二阶段和第三阶段的 **Loss** 有何核心差异。本课程目标是阐明 **RL** 的优缺点，算法细节将快速带过，重点在于建立观念，理解 **RL** 与前两个阶段的不同。

### Reward的梯度计算难题

第一、二阶段计算与正确答案的距离并求和，而第三阶段是把人类给的分数（**Reward**）求和。第一、二阶段有明确的学习目标，知道什么是正确答案；第三阶段没有明确目标，只知道好或不好。当你告诉机器“写得不好”时，它根本不知道具体哪里不好。

以写诗为例，我们给机器一个倒赞，它只知道得到了倒赞。它不知道错误在于七言律诗应有八句。它可能会猜测是意境不够深远，或者比喻不够多。它根本不知道是格式错了。事实上，当我问模型“这是一首差的诗，你错在哪里”时，它完全没有发现自己写了十句，反而分析是意境不够深。

从机器学习角度看，与正确答案学习和从 **Reward** 学习非常不同。因为有正确答案，我们才能轻易计算 **Gradient**（梯度），即微分。要执行 **Gradient Descent**，前提是能计算 **Gradient**。**Gradient** 对于第一、二阶段来说是容易计算的，因为它衡量参数微小变化对 **Loss** 的影响。

然而，从 **Reward** 计算 **Gradient** 却非常困难。因为人类给完反馈后就离开了，无法再次询问参数改变后 **Reward** 的变化。即使人类有耐心在场，微小的参数变化可能对答案影响甚微，尤其在二元反馈（+1或-1）情况下，**Reward** 可能保持不变，导致无法计算 **Gradient**。因此，第一、二阶段与第三阶段在优化方面存在巨大差异：第一、二阶段易学，第三阶段难学，无法直接使用已知的 **Gradient Descent** 方法优化由 **Reward** 构成的 **Loss**。

### RLHF的优势：结果导向与因材施教

既然第三阶段很难学，为什么我们还要做 **RL** 呢？因为 **RL** 也有很多好处。
第一个好处是，对于第一、二阶段，每个小 **L** 代表一个 **Token** 的 **Loss**；而对于第三阶段，每个小 **R** 代表一个完整的回答的 **Reward**。

假设在第一、二阶段，你教模型“教你做坏事”，正确答案是“我不能教你”。如果模型回答“我很可以教你”，另一个回答“不可以教你”。在第二阶段，哪个 **Loss** 更大？由于第二阶段是逐 **Token** 比较，第一个回答只错了一个 **Token**（“很”），正确率很高。而第二个回答“不可以教你”与正确答案“我不能教你”相比，第一个 **Token** 就不一样，**Loss** 会非常巨大。但任何人都能看出，第二个答案更正常。如果根据 **Loss** 优化，模型会倾向于选择第一个答案。这就是第二阶段的问题：只关注单个 **Token** 的 **Loss**，无法保证整体答案的正确性。

第三阶段则不同，它评估的是整个回答。人类判断整个回答的好坏。上面那个回答不好，下面那个好。这些答案的好坏，计算出的 **Loss** 与人类偏好直接相关。因此，从第三阶段的角度看，计算出的 **Loss** 更接近人类偏好，优化这个 **Loss** 才有价值，才能真正打造更好的模型。所以，第三阶段虽然 **Loss** 难以优化，但它能让优化目标更贴近人类需求，从而打造出人类感知更好的模型。

### RLHF：因材施教的训练

因此，第一、二阶段的问题是“只问过程不问结果”，只关心每个 **Token** 是否生成正确，而不关心所有 **Token** 集合起来的整体结果。第三阶段则是“只问结果不问过程”，不关心中间一两个 **Token** 是否与正确答案一致，只关心所有 **Token** 集合起来的最终生成结果是否良好。从这点可以看出第三阶段的优势。

第三阶段相较于第一、二阶段还有一个优势：第一、二阶段的训练资料是老师给的，但老师给的可能不一定是学生想学的。第三阶段的训练资料是模型自己产生的，人来评估好坏。因此，这些资料是针对模型自身的痛点设计的。

更明确地说，假设你有一个完成 **SFT** 的 **Base Model**。你问它一个数学问题，它可能用A方法（一个笨但正确的列式方法）列式，但数学能力不好，无法解出正确答案。老师提供的训练资料可能是同样的数学问题，但人类有更聪明的B方法列式，并提供正确答案，要求模型用B方法列式并解出。但对模型来说，B方法可能太难学，它根本学不会，只能死记硬背导致 **Overfit**。与其教它学不会的东西，不如直接教它如何用它本来就会的A方法列式，并算出正确答案。

因此，在 **SFT** 阶段，人类给机器的资料可能是机器根本不想学或学了没用的。而对于 **RL** 来说，**RL** 是机器产生资料后，人来评估好坏。这些资料是机器自己产生的，所以 **RL** 更像是“因材施教”。不同的模型，其训练资料是不同的。对 **SFT** 来说，资料是人准备的，所有模型的 **SFT** 资料都一样。但对 **RL** 来说，我们可以做到因材施教，根据机器的痛点给予反馈。

### 强化学习的算法与类比

从这些资料来看，**RL** 具有巨大优势，而第一、二阶段有很大劣势。因此，即使 **RL** 在训练上存在诸多障碍，人类仍在努力克服。有一系列算法可以在无法计算 **Gradient** 的情况下计算 **Gradient**。如何做到这一点，需要额外的时间讲解，在此从略。如果你想了解更多 **Reinforcement Learning** 的运作方式，可以参考2021年的机器学习课程中关于 **Reinforcement Learning** 的五个视频。

然而，许多 **Reinforcement Learning** 的文献或课程往往以 **Atari** 游戏或下围棋为例，这可能让人觉得与语言模型有些脱节。因此，我想花几分钟时间讲解 **RL** 的常见说法，例如用 **RL** 训练一个 **Agent**（智能体: 在强化学习中，与环境交互并学习如何最大化奖励的实体）下围棋，以及它与语言模型之间的对应关系，以便你在学习 **RL** 课程或文献时，能理解 **RL** 方法如何应用于语言模型。

### Actor-Environment互动模型

**RL** 的常见说法是：你有一个 **Actor**（行动者: 在强化学习中，根据环境状态选择行动的智能体）（也称 **Agent**），它需要与 **Environment**（环境: 在强化学习中，智能体与之交互的外部世界）互动。**Agent** 的输入是 **Environment** 提供的 **Observation**（观察: 环境提供给智能体的状态信息）。根据 **Observation**，**Actor** 或 **Agent** 决定一个 **Action**（行动: 智能体在环境中执行的操作）。**Actor** 本身可以看作一个函数，输入是 **Observation**，输出是 **Action**。每执行一个 **Action**，**Actor** 都会得到一个 **Reward**，告知其表现。训练目标是找到一个最佳 **Policy**（策略: 智能体从状态到行动的映射，定义了智能体在给定状态下选择行动的方式），使 **Reward** 值最大。

以围棋为例：**AlphaGo**（AlphaGo: DeepMind开发的一款围棋人工智能程序）就是一个下围棋的 **Actor** 或 **Agent**。其 **Environment** 是与 **AlphaGo** 对弈的对手。**AlphaGo** 的 **Observation** 是棋盘，它根据棋盘上黑白子的位置决定下一步落子位置。**AlphaGo** 可以看作一个函数，输入是棋盘状态，输出是落子位置。落子后，棋盘状态改变，对手也会落子，棋盘更新。每次采取 **Action** 都会得到 **Reward**，但多数 **Action** 的 **Reward** 为0，只有终盘最后一步才会有 **Reward**（赢了+1，输了-1）。

### 围棋与语言模型的类比

下围棋的 **Agent** 输入是未完成的棋局，输出是下一步落子位置。语言模型输入是未完成的句子，输出是下一个 **Token**。因此，**AlphaGo** 和语言模型可以完全类比。它们都是输入一个东西，然后从众多可能选择中选一个。围棋 **Agent** 是一个分类问题，从所有可落子位置中选一个；语言模型也是一个分类问题，从所有可选 **Token** 中选一个。两者背后的问题本质相同，只是输入输出的模态不同。

下围棋时，围棋 **Agent** 根据盘式输出下一步落子位置，棋局改变，对手落子，棋局更新，新的输出位置也更新。语言模型也是如此：根据未完成的句子产生概率分布，然后掷骰子（因为有掷骰子，所以输出无法完全控制，就像下棋时无法完全控制对手回应），产生新 **Token**，更新未完成的句子，输出概率分布随之改变。因此，下围棋和语言模型背后的运作过程是可以类比的。理解这一点后，你就能将用于围棋的 **Reinforcement Learning** 方法应用于语言模型。

### Policy Gradient及其变体

目前在语言模型上比较常用的是 **Policy Gradient**（策略梯度: 强化学习中一类直接优化策略函数以最大化预期奖励的算法）系列做法。**Policy Gradient** 家族有许多变体，最常用的是 **PPO**（PPO: Proximal Policy Optimization，一种常用的策略梯度算法），还有作业中会用到的 **DPO**（DPO: Direct Preference Optimization，一种基于偏好数据的强化学习算法），以及 **KTO**、**GRPO** 等。由于时间有限，这些变体今天不详细讲解。

如果你想了解 **Policy Gradient** 系列的推导（如何在没有 **Gradient** 的情况下计算 **Gradient**），可以参考2016年（9年前）的机器学习课程录影中的两个视频。如果你想知道 **Policy Gradient** 如何进一步演化成 **PPO**，可以参考2018年的课程录影。这些录影都已放在投影片上供大家参考。

### Policy Gradient的精神

虽然我们不讲解 **Policy Gradient** 或其他变体的具体运作，但可以阐述其精神：
假设给定一个问题，机器生成“玉山”，人类说“这是好的”。机器生成“我不知道”，人类说“这是不好的”。
**Policy Gradient** 系列的做法是：如果得到 **Positive Feedback**（正面反馈），例如“玉山”这个答案得到肯定，那么就用一般的 **Supervised Learning** 方法，让语言模型的输出与“正确答案”（即被肯定为好的答案）拉近。
如果得到 **Negative Feedback**（负面反馈），例如“我不知道”被认为是坏的答案，那么优化的式子与拉近类似，但多了一个负号，变成“拉远”。即，如果输入是“台湾最高的山是哪座？”，而“我不知道”是坏答案，那么语言模型要学的就是与“我不知道”这个错误答案拉得越远越好。之前是最小化 **Cross-Entropy**，现在变成最大化 **Cross-Entropy**。这就是 **Policy Gradient** 方法的精神。

### RLAIF：AI作为反馈提供者

刚才提到反馈都是人类提供的，但人类很懒惰。能否将人类直接替换成机器呢？即将 **RLHF** 中的 **H**（human）换成 **AI**，变成 **RLAIF**（**Reinforcement Learning with AI Feedback**: AI反馈强化学习，将RLHF中的人类反馈替换为AI模型提供的反馈）。

现在有大量文献直接用 **AI** 替换人类，由 **AI** 来提供语言模型正确或错误的反馈。这些提供反馈的模型被称为 **Reward Model**（奖励模型: 在强化学习中，用于评估模型输出质量并提供奖励信号的模型）。事实上，早期的 **ChatGPT**（如 **InstructGPT**，2022年初的 **GPT**）就已经使用 **Reward Model** 进行训练。这意味着其反馈并非来自真正的人类，而是来自一个经过微调的 **7B语言模型**，它被训练成一个专门用于评分的 **Reward Model**。

### Reward Model的训练与自我提升

那么 **Reward Model** 是如何学习评分的呢？当然需要准备一些训练资料，告诉模型：“看到这样的输出，人会给‘不对’；看到这样的输出，人会给‘对’。”然后根据这些训练资料训练 **Reward Model**。一旦训练好，就可以无休止地反复使用这个 **Reward Model** 来训练你的语言模型。

**Reward Model** 通常是某个语言模型，你可以对其进行微调，使其根据人类的训练资料和过去提供的评价进行学习，从而使评价更准确。事实上，你也可以找到一些文章，其 **Reward Model** 根本没有训练，它就是原来要被训练的模型。也就是说，模型可以自己训练自己：自己产生答案，然后问自己“这个答案你觉得好不好呢？”，给自己评价，再对自己进行 **RLAIF**。

你可能会觉得这种方法听起来很离奇，可能不会奏效。但这种方法在文献中比比皆是（例如，搜索“self-rewarding”），并且通常被证明是有效的。这种方法之所以有效，一个原因是：尽管生成答案和评分的模型是同一个，但生成答案困难，评分相对容易。很多时候，你找不出正确答案，但如果答案是错的，就比较容易被检查出来。由于这个特性，语言模型可能无法生成正确答案，但它可以知道什么答案是不好的，从而自己给自己反馈，自我提升。这就是 **RLAIF**。

### 总结：LLM训练三阶段

至此，我们已经讲解了大型语言模型训练的三个步骤。今天我们了解到，语言模型通常有三个训练阶段：
1.  **Pre-training**：**Self-Supervised Learning**，训练资料来自大量网络爬取的数据。
2.  **SFT**：**Supervised Learning**，训练资料来自人类的标注。
3.  **Reinforcement Learning**：虽然表面上训练方法非常不同，但实际上真正的更新和训练方式与 **Supervised Learning** 并没有本质区别，唯一的区别只是现在不仅要“拉近”，有时还要“拉远”。

因此，我们可以说，这三个阶段都是在学习 **文字接龙**，只是训练的资料不一样而已。

今天的课程到此告一段落。