---
author: Hung-yi Lee
date: '2025-11-17'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=YJoegm7kiUM
speaker: Hung-yi Lee
tags:
  - llm-training-process
  - alignment
  - scaling-laws
  - knowledge-distillation
title: 揭秘大型语言模型训练全流程：预训练、SFT与RLHF三阶段详解
summary: 本文深入剖析了大型语言模型（LLM）学习的三个核心阶段。第一阶段“预训练”如同儿童学语，从海量数据中掌握语言规律；第二阶段“指令微调”（SFT）如同上学，通过标准答案学习遵循指令；第三阶段“人类反馈强化学习”（RLHF）如同进入社会，根据人类的偏好反馈进行对齐，最终成为一个有用的AI。文章详细阐述了各阶段的目标、数据需求、关键技术与挑战。
insight: ''
draft: true
series: ''
category: technology
area: tech-insights
project:
  - ai-impact-analysis
people:
  - 鲁迅
  - 孔乙己
  - 郭啸天
  - 杨铁心
  - 千早爱音
  - 高松灯
  - 颜良
  - 华佗
  - 可莉
  - 李飞飞
  - 谢濬丞
companies_orgs:
  - OpenAI
  - Google
  - DeepMind
  - Allen AI
  - Hugging Face
  - Meta
  - CommonCrawl
  - Reddit
products_models:
  - ChatGPT
  - Gemini
  - Llama 3
  - D6 V3
  - GPT-3.5
  - GPT-4
  - GPT 5.1
  - PALM
  - OLMo
  - Chinchilla
  - BERT
  - FLAN
  - InstructGPT
  - Llama
  - Llama 2
  - LIMA
  - Qwen 72B
  - Alpaca
  - Vicuna
  - SkyT1
  - S1
  - Mistral
  - Gemma 2
  - Claude
  - GPT-4O Turbo
media_books:
  - 《孔乙己》
  - 《射雕英雄传》
  - 《星际大战》
  - 萌娘百科
  - Wikipedia
  - arXiv
status: evergreen
---
### 导言：大型语言模型的三阶段学习历程

我们日常使用的 ChatGPT、Gemini 等人工智能，其强大的能力并非一蹴而就，而是经历了一个标准化的三阶段学习历程。这三个阶段分别是：**预训练**（Pre-training）、**监督微调**（Supervised Fine-Tuning, SFT）和**人类反馈强化学习**（Reinforcement Learning with Human Feedback, RLHF）。

这三个阶段可以形象地比喻为一个人从学龄前到步入社会的成长过程：
1.  **预训练 (Pre-training)**：如同一个学龄前儿童，模型在海量信息中自由探索，看到什么就学什么，目的是熟悉人类语言的模式。
2.  **监督微调 (SFT)**：如同进入学校，模型开始学习老师提供的标准教材和答案，了解什么是“正确”的，学会如何应对各种问题。
3.  **人类反馈强化学习 (RLHF)**：如同踏入社会，不再有标准答案，模型通过与人类互动，接收社会的“毒打”（即反馈），学会在没有明确指导的情况下做出符合人类价值观的正确决策。

在第二和第三阶段，模型学习如何做出符合人类价值观的正确应对，这个过程被称为**对齐**（Alignment）。虽然在文献中，“对齐”常特指 RLHF 阶段，但从广义上看，SFT 和 RLHF 都旨在让机器的行为对齐人类的需求。二者的区别在于，SFT 阶段人类提供标准答案，而 RLHF 阶段人类只提供反馈，模型需自行探索。

### 核心机制：万变不离其宗的文字接龙

尽管学习过程分为三个阶段，但其核心任务本质上是相同的：**文字接龙**。从机器学习的角度看，文字接龙就是一个分类问题。与图像分类（如识别图片中的动物）类似，语言模型的目标是预测下一个最有可能出现的词。

不同之处在于，图像分类的类别可能只有几百或几千个，而文字接龙的类别数等于词汇表（vocabulary）的大小。对于现代大型语言模型，词汇表中的**词元**（Token: 模型处理文本的基本单位，可以是一个词或词的一部分）数量可达数十万。因此，这本质上是一个拥有数十万个类别的超大规模分类问题。

其学习过程与普通分类任务并无二致：
1.  模型接收一个未完成的句子作为输入。
2.  输出一个覆盖整个词汇表的概率分布。
3.  计算模型输出的概率分布与“标准答案”（即下一个正确的词元）之间的差距。这个差距通常用**交叉熵**（Cross-Entropy: 一种衡量两个概率分布差异的损失函数）来衡量。
4.  通过**梯度下降**（Gradient Descent: 一种优化算法，通过调整模型参数以最小化损失函数）来不断优化模型，使其预测越来越准。

这三个阶段环环相扣，后一个阶段会将前一个阶段训练出的模型参数作为初始值（Initialization）。预训练的结果是 SFT 的起点，而 SFT 的结果又是 RLHF 的起点。这种承前启后的方式，使得模型能够在前一阶段的基础上进行更精细化的学习。值得注意的是，这三个阶段主要改变的是机器学习流程中的第一步——训练数据，而模型架构（如 Transformer）和优化器通常保持不变。

### 第一阶段：预训练 (Pre-training) - AI的学龄前教育

预训练的目标是让模型通过接触海量文本数据，掌握语言知识（如语法结构）和世界知识（如事实信息）。语言知识相对容易学习，大约需要 10 亿词汇量的数据即可达到饱和。然而，世界知识的学习几乎是无穷无尽的，需要极为庞大的数据量。

#### 数据规模与来源

为了获取海量数据，最常见的做法是从互联网上爬取所有能找到的文本。在预训练阶段，任何文本都可以成为学习材料，这个过程几乎不需要人工干预，因此也被称为**自督导学习**（Self-Supervised Learning）。

现代大型语言模型的预训练数据量是惊人的。例如，Llama 3 使用了 15 万亿（15T）个词元进行训练。如果将这些数据用 A4 纸打印出来，其厚度将达到 1500 公里，是珠穆朗玛峰高度的数十倍。一个不知疲倦、一目十行的人，需要从殷商时代开始阅读，直到今天也无法读完。

尽管数据量巨大，但并非取之不尽。有研究预测，按照目前模型训练数据量的增长速度，我们可能会在 2028 年前后耗尽互联网上所有高质量的文本数据。

#### 模型的学习方式：压缩而非背诵

很多人误以为模型会死记硬背所有训练数据，但事实并非如此。模型更像是将知识进行压缩和泛化。例如，当你让模型背诵鲁迅的《孔乙己》或金庸的《射雕英雄传》原文时，它往往会输出一篇内容梗概正确、但细节与原文有出入的文本。这是因为它学到的是词语之间的关联概率（如“射雕英雄传”后面很可能接“华山论剑”），而不是机械地记忆整个文本。

#### 数据量、模型大小与算力的平衡：Chinchilla缩放法则

在算力有限的前提下，数据量和模型大小之间存在一种权衡关系。是选择一个“天资聪颖但读书少”的大模型，还是一个“天资平平但博览群书”的小模型？

DeepMind 的研究提出了著名的**Chinch-illa 缩放法则**（Chinchilla Scaling Law: 一个由DeepMind发现的原则，揭示了在固定算力下，模型大小和训练数据量之间的最佳配比关系）。该法则指出，模型大小和训练数据量之间存在一个最佳比例。模型过大而数据不足（思而不学则殆）或模型过小而数据冗余（学而不思则罔）都无法达到最优效果。这一法则为后续的许多模型（如 Llama 系列）提供了设计配方的指导。

#### 数据质量的重要性

数据的质量远比数量更重要。低质量、怪异的数据会严重干扰训练过程。例如，Allen AI 在训练 OLMo 模型时发现，来自 Reddit 上一个名为“Microwave GAN”版块的帖子（内容多为模拟微波炉声音的数千个“M”字母）会导致模型训练极其不稳定。

因此，从网络上爬取的数据必须经过严格的清洗和过滤。一个典型的流程包括：
1.  **启发式规则过滤**：基于人类定义的规则进行初步筛选。
2.  **去重**：移除重复或高度相似的内容。
3.  **基于模型的过滤**：使用另一个模型来评估数据质量，筛选出高质量文本。

经过层层筛选，最终可能只有不到 2% 的原始数据被用于训练。高质量的数据不仅能提升模型性能，还能在同等算力下达到更好的效果。一种有趣的方法是利用现有的大型语言模型对网络数据进行“重述”（Rephrase），生成更规整、高质量的训练语料。

### 预训练的局限：为何模型无法直接回答问题？

尽管预训练模型蕴含了海量知识，但它们通常无法直接与人进行有效对话。例如，早期的 GPT-3 和 Google 的 PALM 模型，在面对直接提问时，往往会续写出更多的问题、选项，或者不相关的文本，而不是给出答案。

这是因为它们学习的源数据——互联网文本——本身就不是问答形式的。当模型看到“台湾最高的山是哪座山”时，它在训练数据中见过的后续文本可能是“A. 玉山 B. 阿里山”，也可能是“知道的朋友请留言”。因此，它会按照这种概率分布进行文字接龙，而不会直接回答“玉山”。

尽管如此，预训练模型已经具备了巨大的潜力，它就像一块璞玉，只需要后续的精雕细琢，就能激发出强大的能力。

### 第二阶段：指令微调 (SFT) - AI的学校教育

**指令微调**（Supervised Fine-Tuning, SFT）的目标是教会模型如何遵循人类的指令进行回答。在这个阶段，人类专家会准备一批高质量的“问题-标准答案”数据对。

例如，数据会明确告诉模型：
*   当被问及“台湾最高的山是哪座”，应该回答“玉山”。
*   当被问及“你是谁”，应该回答“我是一个人工智能”。
*   当被问及如何进行非法活动时，应该拒绝回答。

通过学习这些范例，模型不仅学会了特定问题的答案，更重要的是学会了回答问题的“格式”和“风格”，包括如何使用对话模板（如 `User:` 和 `AI:`）。经过 SFT，原本无法正常对话的 PALM 模型也脱胎换骨，能够流畅地回答问题。

#### SFT的成功基石：预训练

SFT 的成功严重依赖于预训练阶段打下的坚实基础。如果没有预训练提供的广博知识，仅靠数量有限的 SFT 数据进行训练，模型会极易**过拟合**（Overfitting）。例如，如果只教它“台湾最高的山是玉山”，它可能会错误地学到“只要问题里有‘山’字，就回答‘玉山’”。

一篇名为“语言模型物理学”（Physics of Language Models）的研究表明，模型在预训练阶段需要从多个不同角度、用不同表述方式反复接触同一个知识点，才能真正理解并内化该知识，从而在 SFT 阶段具备举一反三的能力。

#### SFT的核心作用：风格塑造而非知识灌输

SFT 的主要作用是改变模型的输出风格，激发其在预训练阶段已经学到的潜能，而不是灌输新知识。模型能回答“台湾最高的山是玉山”，并非 SFT 教会了它这个事实，而是在预训练时它已在无数网页中读到过这一信息。SFT 只是教会它在被提问时，应该以直接回答的方式将这个知识点呈现出来。

研究表明，尝试在 SFT 阶段教给模型全新的、未知的知识，效果往往很差，甚至可能导致性能下降。模型真正能高效学习的，是那些它在预训练阶段已有模糊认知（Maybe known）的知识。

#### SFT数据：质量远胜于数量

令人惊讶的是，SFT 并不需要海量数据。OpenAI 的 InstructGPT 论文显示，仅用一万多笔高质量 SFT 数据，就能让一个小型模型的表现媲美未经 SFT 的最大型模型。Meta 的 Llama 2 也发现，使用数百万笔低质量数据，效果反而不如精心挑选的两万多笔高质量数据。

这一“少即是多”（Less is more）的原则在 LIMA 模型中得到进一步验证，该研究仅用 1000 条精选数据就取得了非常好的效果。有趣的是，SFT 数据的选择似乎带有一些“玄学”色彩。一些研究发现，使用“弱智吧”上的无厘头问题，或者简单粗暴地挑选数据集中最长的 1000 条样本，都能取得意想不到的好效果。

#### SFT的捷径：知识蒸馏

由于人工标注 SFT 数据成本高昂，一种高效的方法应运而生：**知识蒸馏**（Knowledge Distillation）。其核心思想是利用一个更强大的“教师模型”（如 GPT-4）来为大量问题自动生成“标准答案”，然后用这些生成的数据来训练我们自己的“学生模型”。像 Alpaca、Vicuna 等早期模型，都是通过这种方式，以极低的成本（数百美元）快速微调出来的。

近期的研究甚至将这一理念推向极致，发现即使没有明确的“问题”，仅仅让教师模型续写网络文本的后半句，或者只让学生模型学习“答案”本身（Response Tuning），也能有效激发其遵循指令的能力。这进一步证明了 SFT 的核心在于塑造输出的模式和风格。

### 第三阶段：人类反馈强化学习 (RLHF) - AI步入社会

当我们与 ChatGPT 等模型交互时，通常会看到“赞”和“倒赞”的按钮。这些反馈正是 RLHF 阶段的关键输入。

#### RLHF与SFT的差异

从人类参与的角度看：
*   **SFT**：人类扮演“老师”，需要辛苦地编写标准答案。
*   **RLHF**：人类扮演“评委”，只需对模型生成的答案进行评价（点赞或点踩），轻松得多。

对于许多复杂问题（如创作一首诗），普通人很难写出完美的“标准答案”，但要判断模型生成的答案好坏却相对容易。例如，我们可以轻易发现一首所谓的“七言律诗”有十句，格式是错误的，从而给出负面反馈。

从机器学习的角度看：
*   **SFT**：目标是让模型输出的每一个词元都尽可能接近标准答案的词元，它关注的是“过程”的正确性。
*   **RLHF**：没有标准答案，只有一个代表人类偏好的总分（奖励信号）。模型的目标是生成能获得最高总分的完整回答，它关注的是“结果”的好坏。

RLHF 的优化过程更困难，因为它无法像 SFT 那样直接计算梯度。但它的优势在于，其优化目标（人类偏好）与我们期望模型达成的最终目标高度一致。此外，RLHF 的训练数据由模型自身生成，再由人类评估，这相当于一种“因材施教”，可以针对模型当前的弱点进行强化。

#### RLHF的运作机制：AI作为智能体

在强化学习的框架中，语言模型可以被看作一个**智能体**（Agent），类似于下围棋的 AlphaGo。
*   **观察 (Observation)**：未完成的句子（或棋局）。
*   **行动 (Action)**：选择下一个词元（或落子位置）。
*   **奖励 (Reward)**：人类对最终生成的完整回答的评分（或棋局的胜负）。

模型的目标是通过不断试错，学习一套策略（Policy），以最大化最终能获得的奖励。这个学习过程通常使用 Policy Gradient、PPO、DPO 等一系列强化学习算法。

#### RLAIF：用AI取代人类反馈

由于收集人类反馈仍然有成本，现在的主流做法是训练一个**奖励模型**（Reward Model）。这个模型专门学习人类的偏好，然后代替人类为语言模型的输出打分。这种用 AI 反馈替代人类反馈的方法被称为 RLAIF。甚至有研究表明，模型可以“自我奖励”，即自己生成答案，再自己评价好坏，从而实现自我迭代和提升。

### 总结：三位一体的学习之旅

大型语言模型的学习历程是一个环环相扣、层层递进的系统工程。
1.  **预训练**为模型注入了广博的语言和世界知识，是其能力的基石。
2.  **指令微调 (SFT)** 则像一把钥匙，通过高质量的范例解锁了这些潜能，教会模型如何与人沟通。
3.  **人类反馈强化学习 (RLHF)** 则是最后的精修，通过对齐人类的复杂偏好，让模型变得更安全、更有用、更符合我们的价值观。

这三个阶段虽然方法各异，但都围绕着“文字接龙”这一核心任务，只是在不同阶段使用了不同来源和形式的训练数据，最终共同塑造了我们今天所见的强大人工智能。