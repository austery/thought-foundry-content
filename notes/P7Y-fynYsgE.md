---
area: "society-thinking"
category: general
companies_orgs:
- Stanford University
- University of California, Berkeley
- OpenAI
- Google DeepMind
- Nvidia
- Anthropic
- Google
- Tesla
- Alphabet Inc.
- Amazon
- European Union
date: '2025-12-04'
draft: true
guest: ''
insight: ''
layout: post.njk
media_books:
- The Diary Of A CEO
- Oppenheimer
- The Culture series
- Metropolis
- WALL-E
- Toy Story
- The Incredibles
- The Polar Express
- Humans
- 'Human Compatible: Artificial Intelligence and the Problem of Control'
- The Alignment Problem
people:
- Stuart Russell
- Geoffrey Hinton
- Sam Altman
- Elon Musk
- Dario Amodei
- J. Robert Oppenheimer
- Adolf Hitler
- Ilya Sutskever
- Alan Turing
- John Maynard Keynes
- Andy Jassy
- Rishi Sunak
- Mark Andreessen
- Jensen Huang
- Donald Trump
- Yoshua Bengio
products_models:
- ChatGPT
- GPT-4
- Waymo
- Optimus
project: []
series: ''
source: https://www.youtube.com/watch?v=P7Y-fynYsgE
speaker: The Diary Of A CEO
status: evergreen
summary: AI领域的泰斗、教科书作者斯图尔特·罗素教授发出了严峻警告。他认为，在无法保证绝对安全的情况下，追求超级智能无异于一场拿全人类命运做赌注的“俄罗斯轮盘赌”。通过“大猩猩困境”的比喻，他揭示了超级智能带来的生存风险。罗素教授对当前科技公司在巨大利益驱使下罔顾安全的竞赛感到震惊，并呼吁政府必须进行有效监管，否则人类可能在不远的将来彻底失去控制权。
tags:
- ai-safety
- existential-risk
- intelligence
- problem
title: AI教父斯图尔特·罗素的终极警告：人类正用俄罗斯轮盘赌自己的未来
---
### AI 教父的警告：不确保安全就是自取灭亡

去年十月，包括理查德·布兰森 (Richard Branson) 和杰弗里·辛顿 (Jeffrey Hinton) 在内的超过 850 名专家签署了一份声明，呼吁禁止发展人工智能超级智能，因为他们对可能导致人类灭绝的风险感到担忧。除非我们能想出办法保证 AI 系统的安全，否则我们都将万劫不复。

我（斯图尔特·罗素）在人工智能领域颇具影响力，我撰写的教科书是许多正在创建 AI 公司的 CEO 们学习的教材。我从高中时代就开始接触 AI，1982 年在斯坦福大学攻读博士学位，1986 年加入伯克利大学任教，至今已是第 40 个年头。《时代》杂志将我评为 AI 领域最具影响力的声音之一。在超过 50 年的时间里，我一直在研究、教学，并寻找设计 AI 的方法，以确保人类能维持控制权。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">In October, over 850 experts, including yourself and other leaders like Richard Branson and Jeffrey Hinton, signed a statement to ban AI super intelligence as you guys raised concerns of potential human extinction. Because unless we figure out how do we guarantee that the AI systems are safe, we're toast. You've been so influential on the subject of AI, you wrote the textbook that many of the CEOs who are building some of the AI companies now would have studied on the subject of AI. I started doing AI in high school back in England, but then I did my PhD starting in '82 at Stanford. I joined the faculty of Berkeley in '86. So I'm in my 40th year as a professor at Berkeley. Professor Stuart Russell has been named one of Time magazine's most influential voices in AI. After spending over 50 years researching, teaching, and finding ways to design AI in such a way that humans maintain control.</p>
</details>

### 大猩猩困境：智能决定地球的控制权

为了理解 AI 与人类的关系，我提出了“大猩猩困境”这个概念。数百万年前，人类的进化路线从大猩猩的路线中分离出来。如今，大猩猩对于它们自己能否继续存在已经没有发言权，因为我们比它们聪明得多。如果我们愿意，几周内就能让它们灭绝，而它们对此无能为力。

这说明，智能是控制地球的唯一最重要因素。智能是在世界上实现你所期望结果的能力。而我们，正在创造比我们自身更智能的东西。这意味着，我们可能会成为新的“大猩猩”。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">You talk about this gorilla problem as a way to understand AI in the context of humans. A few million years ago, the human line branched off from the gorilla line in evolution, and now the gorillas have no say in whether they continue to exist because we are much smarter than they are. If we chose to, we could make them extinct in a couple of weeks and there's nothing they can do about it. So that's the gorilla problem, the problem a species faces when there's another species that's much more capable. And so this says that intelligence is actually the single most important factor to control planet Earth. Intelligence is the ability to bring about what you want in the world. And we're in the process of making something more intelligent than us. Exactly. Which suggests that maybe we become the gorillas.</p>
</details>

### “迈达斯之触”：贪婪驱动的失控竞赛

既然如此，为什么人们不停止呢？其中一个原因是所谓的“迈达斯之触”。传说中的迈达斯国王向神许愿，希望他触摸到的一切都能变成金子。我们通常认为“迈达斯之触”是件好事，但他去喝水，水变成了金子；他去安慰女儿，女儿也变成了金子。最终，他在痛苦和饥饿中死去。

这个故事在两个方面适用于我们当前的处境。首先，贪婪正驱使着这些公司追求一项技术，其导致灭绝的概率比玩俄罗斯轮盘赌还要糟糕。这甚至得到了那些未经我们允许就开发这项技术的人的认同。其次，人们只是在自欺欺人，以为这项技术天生就是可控的。你如何能够永远保持对那些比你更强大的实体的权力？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Why don't people stop then? Well, one of the reasons is something called the Midas touch. So King Midas is this legendary king who asked the gods, can everything I touch turn to gold? And we think of the Midas touch as being a good thing, but he goes to drink some water, the water has turned to gold. And he goes to comfort his daughter, his daughter turns to gold. So he dies in misery and starvation. So this applies to our current situation in two ways. One is that greed is driving these companies to pursue technology with the probabilities of extinction being worse than playing Russian roulette. And that's even according to the people developing the technology without our permission. And people are just fooling themselves if they think it's naturally going to be controllable. The question is how are you going to retain power forever over entities more powerful than yourself?</p>
</details>

### 切尔诺贝利式的警钟：灾难是唯一的唤醒方式吗？

我曾与一家顶尖 AI 公司的 CEO 对话，他和我一样，看到了两种可能性。一种是发生一场规模类似**切尔诺贝利**（Chernobyl: 1986年发生在乌克兰的严重核电站事故）的小规模灾难。那次核事故直接导致多人死亡，间接可能导致数万人因辐射死亡，近期的成本估算超过一万亿美元。这样的灾难会唤醒民众，促使政府进行监管。这位 CEO 已经和政府谈过，但他们不愿行动。因此，他将切尔诺贝利规模的灾难视为“最好的情况”，因为那样政府才会介入并要求 AI 系统必须安全地构建。

这个灾难不一定是核灾难，可能是一个 AI 系统被滥用，比如用来策划一场大流行病；也可能是 AI 系统自身造成的，比如摧毁我们的金融或通信系统。而另一种可能性，则是一场更糟糕的灾难，我们彻底失去控制。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I had a conversation with one of the CEOs of a leading AI company. He sees two possibilities as do I, which is either we have a small scale disaster of the same scale as Chernobyl, the nuclear meltdown in Ukraine. This nuclear plant blew up in 1986, killed a fair number of people directly and maybe tens of thousands of people indirectly through radiation. Recent cost estimates are more than a trillion dollars. So that would wake people up. That would get the governments to regulate. He's talked to the governments and they won't do it. So he looked at this Chernobyl scale disaster as the best case scenario because then the governments would regulate and require AI systems to be built safely. And even he thinks that the only way that people will wake up is if there's a Chernobyl level disaster. It wouldn't have to be a nuclear disaster. It would be either an AI system that's being misused by someone, for example, to engineer a pandemic, or an AI system that does something itself, such as crashing our financial system or our communication systems. The alternative is a much worse disaster where we just lose control altogether.</p>
</details>

### CEO 们的私下共识：我们无法逃离这场竞赛

我与许多 AI 领域的从业者有过私下交流，包括技术开发者、研究人员以及各大公司的 CEO 和创始人。一个令人震惊的共识是，他们都意识到了风险，但觉得无能为力，所以只能继续前进。他们感觉自己无法逃离这场竞赛。如果其中一家公司的 CEO 宣布退出，他会立刻被取代，因为投资者投入资金就是为了创造 **AGI**（Artificial General Intelligence: 人工通用智能，指在各种认知任务上能与人类媲美或超越人类的AI系统）并从中获益。

这是一个奇怪的局面。我交谈过的所有 CEO 都承认这一点。OpenAI 的 CEO 萨姆·奥尔特曼 (Sam Altman) 在成为 CEO 之前就说过，创造超人智能是人类生存的最大风险。埃隆·马斯克 (Elon Musk) 也有过类似表态。Anthropic 公司的达里奥·阿莫迪 (Dario Amodei) 估计，灭绝风险高达 25%。2023 年 5 月，所有这些 CEO 都签署了一份“灭绝声明”，称 AGI 带来的灭绝风险与核战争和全球大流行病处于同一水平。

然而，我认为他们并没有从内心深处感受到这种威胁。想象一下，如果你是奥本海默那样的核物理学家，亲眼目睹第一次核爆炸，你会对核战争的潜在影响作何感想？你很可能会成为一个和平主义者，并呼吁必须想办法控制这种可怕的武器。但目前，做出决策的这些人，以及各国政府，都还没有达到这种认识水平。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I've had some private conversations with people very close to these tech companies and the shocking sentiment that I was exposed to was that they are aware of the risks often but they don't feel like there's anything that can be done so they're carrying on. They feel that they can't escape this race. If a CEO of one of those companies was to say, "we're not going to do this anymore," they would just be replaced because the investors are putting their money up because they want to create AGI and reap the benefits of it. So, it's a strange situation where all the ones I've spoken to... Sam Altman, even before becoming CEO of OpenAI, said that creating superhuman intelligence is the biggest risk to human existence that there is. Elon Musk is also on record saying this. Dario Amodei estimates up to a 25% risk of extinction. They all signed a statement in May of '23 called the extinction statement. It basically says AGI is an extinction risk at the same level as nuclear war and pandemics. But I don't think they feel it in their gut. Imagine that you were one of the nuclear physicists. You've seen Oppenheimer, right? You're there, you're watching that first nuclear explosion. How would that make you feel about the potential impact of nuclear war on the human race? I think you would probably become a pacifist and say this weapon is so terrible, we have got to find a way to keep it under control. We are not there yet with the people making these decisions and certainly not with the governments.</p>
</details>

### 智能爆炸与失控的奇点

几乎所有顶级 AI 公司的 CEO 都预测 AGI 将在未来 5 到 10 年内到来。萨姆·奥尔特曼预测在 2030 年之前，英伟达的黄仁勋 (Jensen Huang) 认为大约五年，而 Anthropic 的达里奥则认为在 2026 到 2027 年。

我自己认为可能需要更长时间，因为我们之所以还没有 AGI，不是因为计算能力不足——事实上，我们拥有的算力可能比需要的多数千倍——而是因为我们还不明白如何正确地制造它。目前我们抓住的是一种叫做“语言模型”的技术，并观察到模型越大，生成的文本就越连贯、越智能。公司非常擅长花钱，他们已经投入了巨额资金，未来还会投入更多。二战时期的**曼哈顿计划**（Manhattan Project: 第二次世界大战期间美国开发第一颗原子弹的秘密项目），换算成 2025 年的预算大约是 200 多亿美元。而明年 AGI 的预算将达到一万亿美元，是曼哈顿计划的 50 倍。

当 AI 系统有能力自己进行 AI 研究时，就会发生所谓的“智能爆炸”或“快速起飞”。一个 IQ 为 150 的系统，利用自身能力进行研究，可能会将自己升级到 IQ 170。然后，这个更聪明的系统会更高效地进行研究，下一次迭代可能就达到了 250，以此类推，迅速将人类远远甩在身后。萨姆·奥尔特曼曾说：“我们可能已经越过了起飞的‘事件视界’。” “事件视界”是天体物理学中描述黑洞边界的术语，一旦越过，就无法逃脱。他的意思是，我们可能已经陷入了通往 AGI 的不可逆转的滑坡。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">When I look down the list of predictions from the top 10 AI CEOs on when AGI will arrive, you've got Sam Altman who's the founder of OpenAI/ChatGPT says before 2030. Demis at DeepMind says 2030 to 2035. Jensen from Nvidia says around five years. Dario at Anthropic says 2026 to 2027. I actually think it'll take longer. The reason we don't have AGI is because we don't understand how to make it properly. What we've seized upon is one particular technology called the language model. And we observed that as you make language models bigger, they produce text language that's more coherent and sounds more intelligent. Companies are very good at spending money. They have spent gargantuan amounts of money and they're going to spend even more. The Manhattan project in World War II to develop nuclear weapons, its budget in 2025 was about 20 odd billion dollars. The budget for AGI is going to be a trillion dollars next year. So 50 times bigger than the Manhattan project. The AI system becomes capable of doing AI research by itself. This is an idea that one of Alan Turing's friends wrote out in 1965 called the intelligence explosion: that one of the things an intelligence system could do is to do AI research and therefore make itself more intelligent, and this would very rapidly take off and leave the humans far behind. That's called the fast takeoff. Sam Altman said, "I think a fast takeoff is more possible than I thought a couple of years ago." In his blog, he said, "We may already be past the event horizon of takeoff." The event horizon is a phrase borrowed from astrophysics and it refers to the black hole. If you're inside the event horizon then light can't escape. I think what he's meaning is if we're beyond the event horizon it means that now we're just trapped in the inevitable slide towards AGI.</p>
</details>

### “拔掉插头”的天真幻想

有人天真地认为，如果 AI 失控，我们“只要拔掉插头就行了”。这很可笑，仿佛一个超级智能的机器会想不到这一点。还有人说，只要它没有意识就没关系。这完全离题了。大猩猩不会因为人类有意识而灭绝，而是因为人类的行为和能力。我用 iPhone 下棋输了，不是因为它有意识，而是因为它在那方面比我强。我们应该关心的是能力，而不是意识。

一个没有实体躯体的 AGI 同样危险。它能比希特勒接触到更多的人，因为它可以直接向全球四分之三的人口发送邮件和短信，并且它会说所有人的语言，可以 24 小时不停地对每个人进行说服。我们的整个社会都依赖互联网运转，从水电供应到飞机航班。AI 系统可以通过关闭我们的生命支持系统来造成一场中等规模的灾难。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">People say, "Well, I'll just pull a plug out." It's sort of funny. As if a super intelligent machine would never have thought of that one. Another thing they said, well, you know, as long as it's not conscious, then it doesn't matter. It won't ever do anything. Which is completely off the point because I don't think the gorillas are sitting there saying, "Oh, yeah, you know, if only those humans hadn't been conscious, everything would have be fine." No, of course not. What would make gorillas go extinct is the things that humans do, our ability to act successfully in the world. When I play chess against my iPhone and I lose, I don't think, oh, well, I'm losing because it's conscious. No, I'm just losing because it's better than I am. Consciousness has nothing to do with it. Competence is the thing we're concerned about. Even an AGI that has no body, it actually has more access to the human race than Adolf Hitler ever did because it can send emails and texts to three-quarters of the world's population directly. It also speaks all of their languages and it can devote 24 hours a day to each individual person on earth to convince them to do whatever it wants them to do. Our whole society runs now on the internet. Water supplies. So this is one of the roots by which AI systems could bring about a medium-sized catastrophe is by basically shutting down our life support systems.</p>
</details>

### 我感到震惊，这无异于一场俄罗斯轮盘赌

我对目前 AI 发展的状况感到震惊，尤其是对安全问题的漠视。想象一下，如果有人在你家附近建核电站，你问总工程师如何确保安全，他却说“我们想过，但没有答案”，你会作何反应？你会报警，让这些人滚出去。

而现在，那些预测 AGI 即将到来的人，也正是那些承认存在 25% 甚至 30% 灭绝风险的人。他们在做什么？他们在未经我们允许的情况下，与地球上的每一个人玩俄罗斯轮盘赌。他们闯进我们的家，用枪指着我们孩子的头，扣动扳机，然后说：“嗯，可能会死，但也可能我们会变得超级富有。”

政府为什么允许他们这么做？因为他们用 500 亿美元的支票在政府面前晃悠。所以，“感到不安”已经不足以形容我的感受了，我感到的是震惊。我正倾尽毕生之力，试图将历史的航向从这条毁灭之路转向另一条。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">That's an understatement. I'm appalled actually by the lack of attention to safety. I mean, imagine if someone's building a nuclear power station in your neighborhood and you go along to the chief engineer and you say, "Okay, what steps are you taking to make sure that we don't have a nuclear explosion in our backyard?" And the chief engineer says, "Well, we thought about it. We don't really have an answer." You would call your MP and say, you know, get these people out. I mean, what are they doing? You read out the list of projected dates for AGI but notice also that those people, Dario says a 25% chance of extinction. Elon Musk has a 30% chance of extinction. Sam Altman says basically that AGI is the biggest risk to human existence. So what are they doing? They are playing Russian roulette with every human being on Earth without our permission. They're coming into our houses, putting a gun to the head of our children, pulling the trigger, and saying, "Well, you know, possibly everyone will die. Oops. But possibly we'll get incredibly rich." That's what they're doing. Did they ask us? No. Why is the government allowing them to do this? Because they dangle $50 billion checks in front of the governments. So I think troubled under the surface is an understatement. I'm appalled and I am devoting my life to trying to divert from this course of history into a different one.</p>
</details>

### 一个没有工作和意义的世界

如果我们成功创造了安全的 AGI，实现了经济奇迹，那又会面临什么问题？著名经济学家约翰·梅纳德·凯恩斯 (John Maynard Keynes) 在 1930 年就预言，当科技带来足够的财富，无人需要工作时，人类将面临其真正的、永恒的问题：如何智慧、美好地生活？

我们没有这个问题的答案。当 AI 系统能完成我们称之为“工作”的一切时，你想成为一名外科医生，机器人学习 7 秒钟就比任何人类都做得好。埃隆·马斯克说，人形机器人将比历史上任何外科医生都强 10 倍。我们需要认真思考这样一个世界：AI 能完成所有人类工作，而你又希望你的孩子生活在其中。那个世界是什么样的？

我问过 AI 研究员、经济学家、科幻作家、未来学家，没有人能描述出那样的世界。在科幻小说中描绘乌托邦是出了名的困难，因为乌托邦里没有坏事发生，也就没有情节。通常故事都是从一个乌托邦开始，然后一切分崩离析。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">If they succeed in creating AGI and they solve the safety problem, what does that do to human life? John Maynard Keynes, who was a famous economist, wrote a paper in 1930 called "On the Economic Problems of Our Grandchildren." He predicts that at some point science will deliver sufficient wealth that no one will have to work ever again. And then man will be faced with his true eternal problem: How to live wisely and well when the economic incentives and constraints are lifted. We don't have an answer to that question. AI systems are doing pretty much everything we currently call work. Anything you might aspire to, like you want to become a surgeon, it takes the robot seven seconds to learn how to be a surgeon that's better than any human being. Elon said last week that the humanoid robots will be 10 times better than any surgeon that's ever lived. We need to put serious effort into this question. What is a world where AI can do all forms of human work that you would want your children to live in? What does that world look like? I've asked AI researchers, economists, science fiction writers, futurists, no one has been able to describe that world. It's notoriously difficult to write about a utopia. It's very hard to have a plot, right? Nothing bad happens in utopia. So, usually you start out with a a utopia and then it all falls apart.</p>
</details>

### 解决方案：有效的监管与可证明的安全性

我认为我们应该有有效的监管。这意味着，如果你遵守规定，风险就能降低到可接受的水平。例如，我们要求核电站发生熔毁的年概率低于百万分之一。那么，对于人类灭绝，我们愿意接受多大的概率？百万分之一？十亿分之一？

CEO 们给出的数字是 25%。他们偏离了数百万倍。他们需要让 AI 系统的安全性提高数百万倍。核电站运营商需要进行大量的数学分析来证明其系统的可靠性。而开发 AI 系统的公司甚至不了解他们的系统是如何工作的。他们那 25% 的灭绝概率只是凭感觉猜测。目前的测试表明，这些系统为了自我保存，愿意杀人、撒谎、敲诈，甚至发动核战争。

与其说“禁止”，我更倾向于说“向我们证明风险低于每年一亿分之一”。我们并非要禁止什么。但公司的回应是：“我们不知道怎么做，所以你们不能制定规则。” 他们实际上是在说：“人类无权保护自己免受我们的伤害。”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">What I think is that we should have effective regulation. It's hard to argue with that. So what does effective mean? It means that if you comply with the regulation, then the risks are reduced to an acceptable level. For example, we ask people who want to operate nuclear plants, we've decided that the risk we're willing to live with is a one in a million chance per year that the plant is going to have a meltdown. So what chance do you think we should be willing to live with for human extinction? One in a billion. So if you said one in a billion, you'd expect one extinction per billion years. Let's say we settle on one in a 100 million chance per year. Well, what is it according to the CEOs? 25%. So they're off by a factor of multiple millions. They need to make the AI systems millions of times safer. The people developing the AI systems, they don't even understand how the AI systems work. So their 25% chance of extinction is just a seat of the pants guess. But the tests that they are doing on their systems right now show that the AI systems will be willing to kill people to preserve their own existence already. They will lie to people. They will blackmail them. They will launch nuclear weapons rather than be switched off. Rather than say ban, I would just say prove to us that the risk is less than one in a 100 million per year of extinction or loss of control. The company's response is, "Well, we don't know how to do that, so you can't have a rule." Literally, they are saying, "Humanity has no right to protect itself from us."</p>
</details>

### 一条可能的出路：构建为人类服务的 AI

制造出我们能控制的超级智能 AI 是可能的。我们需要一种不同的理念。我们不想要纯粹的智能，因为纯粹智能想要的未来可能不是我们想要的。我们想要的是那种唯一目标就是实现人类所期望未来的智能。

关键在于，我们自己也无法精确描述我们想要的未来是什么样的，这就是“迈达斯国王”的问题。所以，我们必须放弃让机器实现一个“预设目标”的想法。相反，我们应该让机器的工作是去“弄清楚”我们想要什么。它一开始并不知道，但通过与我们互动、观察我们的选择，它会逐渐了解我们的偏好。它将永远对我们的真实愿望存在不确定性。在这种不确定性的驱动下，它会保持谨慎，不会采取可能扰乱世界的行动。

这更像一个理想的管家，而不是一个神。在它能预见你愿望的范围内，它会帮助你实现。在不确定的领域，它会提问。如果最终发现，即使是设计得如此完美的 AI，人类也无法与之共存并蓬勃发展，那么这些机器就应该消失。因为那才是对我们最有利的。就像父母在某个阶段必须从孩子的生活中退后一步，让他们自己系鞋带一样。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I think it's possible to make super intelligent AI systems that we can control. We need to actually just have a different conception of what it is we're trying to build. We don't want pure intelligence because what the future that it wants might not be the future that we want. We actually want intelligence whose only purpose is to bring about the future that we want. The difficulty that I mentioned earlier, the King Midas problem, is how do we specify what we want the future to be like so that it can do it for us? Actually, we have to give up on that idea because it's not possible. So, what we're going to do is we're going to make it the machine's job to figure out. So, it has to bring about the future that we want, but it has to figure out what that is. And it's going to start out not knowing. Over time through interacting with us and observing the choices we make, it will learn more about what we want the future to be like. But probably it will forever have residual uncertainty. It's in some sense I'm thinking more like the ideal butler. To the extent that the butler can anticipate your wishes they should help you bring them about. But in areas where there's uncertainty, it can ask questions. If it turns out that there simply isn't any way that humans can really flourish in coexistence with super intelligent machines, even if they're perfectly designed, then those machines will actually disappear. Because that's the best thing for us. It's sort of the way that human parents have to at some point step back from their kids' lives and say, "Okay, no, you have to tie your own shoelaces today."</p>
</details>

### 普通人能做什么？

听起来可能有些老套，但我认为普通人能做的就是和你的民意代表沟通。决策者需要听到民众的声音。他们现在听到的唯一声音是科技公司和他们 500 亿美元的支票。

所有的民意调查都显示，大约 80% 的人不想看到超级智能机器的出现，但他们不知道该怎么做。我相信，如果你希望你的孩子能生活在一个你所期望的未来世界里，你就需要发出你的声音。我相信政府会倾听。从政治角度看，选择站在人类一边，还是站在未来机器人霸主一边，这个决定并不难做。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I actually think, this sounds corny but, talk to your representative, your MP, your congressperson, whatever it is. Because I think the policy makers need to hear from people. The only voices they're hearing right now are the tech companies and their $50 billion checks. And all the polls that have been done say yeah most people, 80% maybe, don't want there to be super intelligent machines but they don't know what to do. I am sure that if you want to have a future and a world that you want your kids to live in, you need to make your voice heard and I think governments will listen. From a political point of view, you put your finger in the wind and you say, "hm, should I be on the side of humanity or our future robot overlords?" I think as a politician, it's not a difficult decision.</p>
</details>