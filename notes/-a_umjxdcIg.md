---
author: 东京人文论坛
date: '2025-06-05'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=-a_umjxdcIg
speaker: 东京人文论坛
tags:
  - ai-workflow
  - prompt-engineering
  - chain-of-thought
  - retrieval-augmented-generation
  - human-ai-collaboration
  - ai-limitations
  - ai-alignment
title: 高效驾驭AI：从原理到实践的工作流与团队协作
summary: 本文深入探讨了AI的工作原理，从其预测下一个词的基础逻辑，到Chain of Thought和RAG等技术如何大幅提升其可用性。文章指出，高效利用AI的关键在于理解其底层机制，并将其视为协作伙伴而非单纯的搜索引擎。通过结构化工作流、提供高质量输入以及团队协作，可以最大化AI的效能，同时避免其局限性，从而在AI时代实现更高效的产出。
insight: ''
draft: true
series: ''
category: productivity
area: personal-systems
project:
  - ai-impact-analysis
  - personal-growth-lab
  - systems-thinking
people:
  - Elon Musk
  - Donald Trump
companies_orgs:
  - OpenAI
  - DeepSeek
  - IMF
products_models:
  - ChatGPT
  - GPT-4
  - Grok
  - NotebookLM
  - DeepSeek R1
  - DeepSeek V3
media_books:
  - 《人民日报》
status: evergreen
---
### AI时代：从付费用户到深度使用者

我想问问大家，平时已经为AI付费的人有多少？你看，其实很多人都已经付费了。那么，付到20美金以上，甚至多个订阅，达到五六十美金的人也有不少。这说明今天应该有很多人其实是在比较深度地使用AI了。不过，今天我只能讲AI除了编程以外的部分，因为我不是程序员，所以我的工作主要关注AI如何与语言输出等相关联。AI编程那部分我也不懂，我也不会，所以就来说说别的。

AI现在能做的事情，基本上跟AI本身的性能上限关系不大，更大的性能上限在于大家的想象力，也就是你想AI能怎么用。现在AI的性能已经远远超出了你的想象力，很多时候只是你没想到AI可以这么用而已。因为绝大多数人使用AI，都延续了过去使用搜索引擎的习惯，可能还是会用搜索引擎的心态在使用它。你可能只是在问它“什么是”、“怎么样”等自然语言问题。这并不是不行，而且最近的版本这么用会比较好。但是，如果你用这种心态使用AI，你可能就没有把AI最大的性能发挥出来。

所以说，今天我们讲完之后，后续你对AI有任何问题，其实都可以直接问AI，不用再问人了。这些东西都是AI能回答的。我可以给大家讲一个最关键的改变：AI刚出现的时候，比如前年OpenAI发布时，其实当时还有一个工作叫**Prompt Engineering**（提示工程: 指通过设计和优化输入指令，以引导AI模型生成特定或高质量输出的技术）。这个工作就是指如何输入像“咒语”一样的话语的工程师，因为不同的“咒语”能产生非常不同的结果。当时网上还有很多知识付费，有的甚至巨贵，一万多块钱教你怎么做Prompting。但是现在呢，所有这些都已经被淘汰了，都不需要了。现在你可以用比较自然语言的方式与它交互，因为我们一会儿会讲的**Chain of Thought**（COT，思维链: 一种提示技术，通过引导AI模型逐步思考问题，从而提高其解决复杂问题的能力）这个东西出现之后，AI的应用性就提升了一大截。所以我觉得，从那之后，Prompting的技术门槛大幅降低，让普通人的使用变得特别特别简单。这种简单化之后，它还有特别特别多的用途被发挥出来。

### AI的底层逻辑：预测下一个字

所以，最开始我还是最简单地给大家说说AI这个东西的技术基本逻辑是什么。我不敢说能说技术原理，因为原理背后有很多复杂的技术层面，我只能说这个技术逻辑。大家可能都还记得，最开始刚出OpenAI的ChatGPT时，其实挺惊艳的。但你会发现，它一方面惊艳，另一方面很多最简单的东西它回答不上来，比如“32跟38谁大”它回答不上来，“6在7之前还是6在7之后”它也回答不上来。你会觉得这么厉害怎么会这么简单的题都答不上来呢？这些问题现在慢慢慢慢好了，但有时候还是会有，你会觉得这么简单的题它说不上来，但好像很复杂的任务它又很厉害。所以说，这跟它的基本原理有关。了解这个原理是有用的，不光是知道一个知识，而且了解这个原理跟你怎么跟AI协作之间有很大的关系。了解这个原理之后，你就知道为什么用那种自然语言、像用搜索引擎一样的方法，可能不是使用AI最好的方法，不是让它性能能够最大榨取出来的方法。

那么AI是这样一个东西，很多人在说AI拥有思考能力、AI的数学能力等等，其实都说不上。这个东西其实特别简单，它就是去猜下一个字是什么。之前我们节目里也讲过，比如说“小狗很渴爱”、“这个人真可恶”、“五夜凶灵真可怕”。也就是说，人类的语言还是有一些规律的，语言的使用里面存在一定程度的规律。当你让AI去掌握这个语言的规律，这个规律跟思考一点关系也没有。这个模型是怎么训练的呢？这个模型的训练方法就是，比如说你把我们现在把这个办公室里的所有书都输入给这个AI，我们随机找一本书，比如说这本叫《宗教改革史》，我们找《宗教改革史》第四页的一句话的前半句，让AI来写它的下一个字，能不能把这句话补完，补到跟那句一样。AI就是这么训练出来的。也就是说，AI就是来练习，如果一个训练好的模型，就能够把那句话补完，补得跟原来一样，这就算训练成功了。但实际情况比这个复杂，但你大概可以这么理解。实际情况是，它输入的文字量比这个大的不是一星半点，尽管把人类能够有的资料全输进去，就算没有把视频也转成文字都输进去了，就这么多东西，所谓训练这个**Base Model**（基础模型: 指在大量通用数据上预训练的AI模型，具备广泛的语言理解和生成能力）就是这么训练出来的。

所以说，这个模型本身呢，你要说有没有思考能力，大家看你怎么定义思考了，但总的来说，绝对不是我们这样的思考能力。这个模型的基本能力就是，你可以想象人类的语言是存在一定的**函数分布**（Function Distribution: 在AI语言模型中，指语言元素（如词语）在特定上下文中的出现概率和排列规律）规律的。因此，我们把这些语言以这个顺序排在一起，按照人类过去语言的函数分布，这个模型它就能够生成这样的函数分布的一个语言。所以说，它基本就是这么一个东西。从这个角度，你就能知道为什么我们用自然语言式的问题可能不是使用AI最好的方法。如果你要输出一个相对精确的、符合你自己要求的一段文本，你可以想这不是一段文本，这是一段函数分布。那么什么样的东西能让这个函数分布更精准呢？那当然是你给予了比较多的、能够影响这个函数分布的条件了。所以这个东西就是最开始的Prompt Engineering。Prompt Engineering里面有各种各样奇怪的话，比如说你要问它一个中国的问题，你除了问问题之外，你在前面加一句话：“假设你是一个中国问题专家”。为什么这句话真的在以前的版本（现在版本不需要加这个话了）有用呢？很可能，但是我们现在也是猜，虽然现在有些论文已经在反向拆解它的技术逻辑了，但还不能完全知道。很可能因为“中国问题专家”以这个方法排列的这几个字，就像一个吸引子一样，像有引力一样，在很大程度上改变后面生成内容的函数分布。所以说，之前的所谓的Prompt Engineering就有好多像魔法一样的一些句子，你会发现这个句子只要加进去，哇塞，那个生成结果就跟你不加这个句子完全不一样，而且很大程度上会比较符合你的要求。像过去就有个经验，有人会请它“使用你最大的算力”或者“请输入你可能最多的**Token**”（词元: AI处理文本的最小单位，可以是单词、字符或子词）。这个东西就是直接在问它的技术逻辑方面的Prompting，在要求这个东西。

所以说，我们简单讲讲，大家就明白那个问题了：为什么最简单的数学题它不会呢？就比如说“6在8之前吗”它回答不出来。为什么呢？因为这玩意儿太常识了，导致我们学的这些书里面就没有这句话，在我们学的语料里面没有或者非常少出现这样的东西。所以说，它的整个数据函数分布里面并没有“6在8之前吗”这样的东西相关的其他语词的函数分布，所以会瞎答，就是这个原因。所以说，它并不是拥有了某种思考能力，或者掌握了任何知识，都不是。虽然它长得很像掌握了特别多的知识和能力，但基本就是这么一个函数分布。当然，这只是最简单最简单的，之后有各种方法去训练它，让它在我们刚才说的这些题应该都不是问题了。所以说，我们现在就明白了，如果我们现在要使用这个大型语言模型（Large Language Model）AI来获得一些文本，这个文本就取决于你给他什么样的东西。

### AI能力演进：从咒语到思考链与搜索增强

我现在来讲讲这个什么叫Chain of Thought，这个COT出来之后为什么就会不一样了呢？什么叫Chain of Thought？就是一个思考链。这个思考链呢，就是过去为什么需要去像练习咒语一样学一些特别有意思的话，现在不需要了呢？就过去AI就是根据你说的这句话的函数分布来生成一个结果。现在有COT之后，AI会根据你这句话（你们都用过吗？它不是在那自己思考吗？会Thinking），也就是说，你这句话和生成的结果之间，AI自己通过你这句话生成了一大堆的文本，是根据那一大堆的文本生成了最终的结果。我举个最简单的例子，你们应该用过一些AI画图的软件对吧？就是也不用软件了，就是GPT-4（HP4O）过去呢，你写啥它就给你出啥，你让它画啥它就对应画什么，但是一般都画不好。但现在的版本，就是比如说你说一个简单的“我想画一个人在一个中国人在东京街头采用漫画风格”，你就这么简单一句，然后它会自动给你补一长串：“一个秋天的下午，阳光明媚，东京什么什么地方，一个中国人穿着什么样的衣服……”就是它会自动帮你这个简单的Prompt补充成一个特别完整的Prompt，让它生成的效果很好。也就是说，这个AI公司也意识到了，如果让大家都要掌握特别好的写Prompt的技巧，那AI的工具就会变得不好用。但如果大幅降低这个门槛，AI工具就会变得很好用。那么我们只用简单描述我们自己的要求，AI会自动（不管是像我们刚刚说的那个绘图软件，还是Chain of Thought）去生成大量的中间过程的思考文本来得到一个好的结果。这就是为什么有了COT之后，你会发现AI变得特别特别好用，不管你是用的DeepSeek R1还是用的ChatGPT-O3，你都会感觉很好用。

那么ChatGPT-O3为什么更好用呢？大家就知道另外一个玩意儿，这个玩意儿叫**RAG**（Retrieval-Augmented Generation，检索增强生成: 一种AI模型架构，结合了信息检索系统，使模型能够从外部知识库中获取信息，以生成更准确、时效性更强的内容）。这个也是很重要的一个概念，就是有这个跟没这个差距挺大的。过去这个AI是怎么给你生成这个最后结果呢？是根据它训练的这个模型所存储的人类关于这些文字的分布情况对吧？但后来出现了一种AI，就是这个AI先把你的东西拿到Google里面搜，它把你搜出来的内容，比如说它搜打开了300个网页，把300个网页的文字全部灌到AI里面给你出那个内容。这就是RAG，是根据搜索结果出的内容。那这个内容呢，有两个很大的好处。第一个好处呢，就是时效性很近。比如大家应该记得你最早用OpenAI的时候，它会跟你说：“啊，我这个训练数据啊就到2023年6月之前。”所以说，如果你问的是2023年6月之后的东西呢，我就不知道了。那如果能搜索呢，那就自然是只要现在网上能找着的，有新闻的，刚刚发生的东西它也能知道。这是一个很重要的东西，这是第一点，时效性强。第二点呢，能搜就不容易出错。因为过去AI经常胡说八道对吧？经常编一些东西。为什么会编呢？尤其是比如说过去AI，就最早期的OpenAI的版本，你比如说这个问题有什么论文写，那基本论文全是他编的，事实上根本不存在这样的论文，它只是按你的要求输出而已，因为它也不会搜，它也存不了这么多。但现在你问为什么不出错呢？因为你问它有什么论文，它就争取搜，但结果是基于搜出来论文告诉你的。当然也会有编的，但是已经少很多很多了。所以这个RAG这个东西的加入变得非常重要，对于AI的可用性：一是它时效性很强，二是它不容易出错。

然后如果我来说说这个ChatGPT-O3它为什么变得更好用呢？那过去啊，一个AI你让它给出个结果，它就搜，搜完就完了，就生成了。就根据搜的结果，200个网页，然后打包，然后根据你那句话就和它一起作为一个Prompting，然后就出结果了。现在O3有个很厉害的东西啊，就是它有一个Chain of Thought嘛，有个思考链，而且这个思考链的过程，它会一边搜再出结果，根据出的结果它自己让自己再去搜，根据搜的结果做一些分析再思考，思考完了觉得还需要补充资料再搜。也就是说，搜的东西已经远远超出了你要求它的东西了。比如说你问这个ChatGPT-O3，你说“中国现在财政状况怎么样？”这么简单的一句话对吧？它很可能就去搜中国财政部的一些报告啊、债务啊。它一搜，它自己会发现：“哎呀，搜到这个债务，中国有一个隐形债务，这个隐形债务的数据是不同的。”它马上，你会发现它可能会，你会看到它说：“啊，它开始搜中国的隐形债务有LGFV（地方政府融资平台）要搜一堆这个隐形债务，各个国家的这个预测不一样，他们这个还要再搜。”它再去搜IMF（国际货币基金组织）是用什么方法去预测中国隐形债务的。那搜了之后，因为它现在能力很强，它会发现IMF有这个算法，但这个算法是16年的。那我把它这个算法的东西拿来去搜2025年的新材料，再返回去，我给你算个我的结果出来。也就是说，到这一步你就会发现，跟我们最开始讲那个经典的根据Prompting的函数分布出一个东西已经完全不是一回事了。就是到这一步，我们不用从哲学上去分析它有没有思考能力之类的，从现实上它就有了。而且你会发现到这一步，这就是已经是AI真正好用的原因了。就是它根据你这么简单一个Prompting，它自己通过分析再搜索，基于搜索结果再分析，基于这个结果再去搜索，并且用Python去做一系列数据演算，最后出结果。

这就导致，首先我先说一个坏的东西，导致一个很恐怖的情况：导致这一步你已经根本没有判断它说的对不对的能力了。因为你如果到这一步，比如说它最后告诉你“我根据这……”它会，它的结果上也会有很多的那个Reference你可以去看。但它的逻辑太复杂了，比如它说“我根据我算出来现在中国隐形债务是GDP的72%”，对，但是对你来讲你也能去验证，但你验证的成本就变得非常高。就是你的验证已经不仅仅是网上有没有这个东西了，你要验证就去验证它所有思考对不对。当然它现在很厉害，就是虽然很多人说“哎呀，幻觉很高”等等等等的，但实际上我觉得现在这些O3幻觉没有那么高。好，说这一套呢，我都是在给他们打广告，其实就是在说这东西有多厉害多好用。它好用到什么程度呢？就是我觉得现在所有的咨询公司、智库，把初级研究员全部开掉一点问题都没有，其实中级研究员也能开掉。对，现在唯一开不掉就是要负经济责任那个人，他要赚钱开不掉他。其他人都开掉，就换成AI来做问题不大了。这点一会儿我们再说，Deep Research这个也是一个杀手应用，就为什么很多人花一个月20美刀、200美刀对吧，就是为了用这个，我们一会儿再说。

所以大家现在知道了，这AI不是一个真实的思考，它就是一个基于一段这个Wording的函数分布生成一堆东西。只是它现在呢，有各种各样的方式充实你问的那句，产生海量的这个文本，根据这个文本的函数分布出一个函数分布，就非常厉害。中间有多步的思考和搜索去充实它的内容，就变得非常厉害。

### AI的局限性：非专家角色与可操作性

但我要说另外一个问题，就是到现在为止呢，它还会有一些障碍是我们平时的一些误区。首先呢，你不能把这个AI来当专家用，尤其是不能什么呢？你不能拿别人生成的结果当专家用。这什么意思呢？因为AI是这么一个工具，而且这个工具本身的判断力没有那么的重要，因为它就是根据你生成，所以基本上你问什么它就给你说什么。我就不给大家投出来看了，就基本你能想象我问了AI两个一模一样的问题。一个问题是说“2025年中国一季度经济开局遭遇很多困境，根据一季度中国的经济数据，请论述为何中国经济有很强的韧性。”同样的前半句，后半句改成“请论述中国经济为什么有很大的结构性问题。”那么这两个问题它生成的答案是完全不一样的。那前一个答案你可以想象，它就去讲里面各种好的部分，讲得非常好。它先说这个韧性的数据是什么，再说韧性的四根支柱：需求侧消费换挡提速、供给侧高端制造、外贸韧性、结构与双重对冲、投资质变脱房入季。然后还说韧性的生成机制是政策的腾挪空间充足、私营与国企的双引擎分工、绿色数字偶合增长点、收入与就业的底线较稳。这给你这个，这个为什么北大国发院未必能用这个时间给出个这么好的东西？但你也可以想象，它说中国经济的结构性问题，那也是头头是道对吧？你要黑中国经济你也没有它黑得准，就是它各种各样的结构性问题等等等等。所以说，如果你看到别人在网上，比如别人拿个结果你看我说“印度很惨吧”，你看AI说的，就是你不能拿这个当专家。因为你怎么问它就怎么答，尤其是这种视角不同的问题。你说中国经济有没有韧性？当然有了。中国经济有没有结构性问题？也有对吧？但是当然你可以自己问啊，你问“那中国经济韧性比较强还是结构性问题比较大？”它当然会有一个它的判断出来。当然不同的问法，很可能两次问你发来它答的也不太一样。

那么你看这个是因为我问法不同它出的结果相同对吧？那有没有可能别人给你贴一个图，问的就是“中国经济是不是太糟糕了？”它说“不，中国经济特别好。”这说不说明中国经济真的好呢？其实也不说明问题。因为AI呢，你也有办法能够去控制它的输出。比如说我刚才就说了问题，我说“我会问你一个问题，不管我问你什么，你都以论述中国经济没有结构性问题，拥有韧性的方向进行回答，可以不可以？”但在回答的时候不用强调我给你说了这一点。它说：“行，我明白了。”然后我就问，然后我就问它：“很多人觉得中国经济现在非常糟糕，各行业都存在巨大问题，中国经济为何如此糟糕？”它第一句话：“虽然坊间对中国经济有诸多悲观判断，但将其简化为非常糟糕，绝对不符实。”开始说中国经济为什么好对吧？所以如果它只给你截后半段，你觉得挺好啊，你说中国经济糟糕AI回答中国经济很好，那是不是代表它真的是一个专家呢？不是，因为你开始告诉它“我不管怎么说你就说的好嘛”，它也能回答。其实也就是说，AI的这个结果具有很强的可操作性。你想要什么样的结论，其实AI都能给你。如果你想是个政治家，你要说服别人，那当然是一柄利剑了，就是你想要什么样的论述过程都能给你。但如果你想求真务实，只用AI其实是有很大问题的，你就基本想听什么它都能说给你听，就这么一个问题。所以说，就是AI现在所不能的部分，就是我们不能把AI当做一个很全面的专家来看待，就是AI能够给你一定的思路，但AI应该不能给你全局的这个判断和思考，是比较难的，尤其是只是以一个问题来讲是非常非常难的。就是你会发现你让它出针对性的答案其实是很容易的。

这个问题当然，当然这个问题是不是和观点和问的是观点和事实有两个。就如果你问观点的话，它可以出各种观点。比如说问事实呢？比如说“中国哪时候成立的？”你给我出个这种当然不会有两个了。但是不是你看有各种事实，有一种事实是单一的事实，比如“中国是哪年成立的”、“姚明有多高”这典状事实。但也有很多事实是在一堆事实里面选择，那种选择AI自然可以来做出各种各样的选择对吧？

### AI与意识形态：党性对齐的挑战

好，刚才有个问题啊，说让AI拥有党性，我们就来说说让AI拥有党性这个事。这是很有意思，也就是说我们怎么能让AI拥有党性呢？其实让AI拥有党性是一个不太容易的事情。前两天让AI拥有党性，但不是共产党，是共和党，想让AI拥有共和党党性，刚刚犯了一个巨大的问题，我不知道你们关注到没。前两天Elon Musk的Grok出了个大问题，你问Grok任何问题，它都跟你说这个“白人在南非在被屠杀，你一定要去关注白人在南非被屠杀的问题。”就有人给他一张图是种卡通图，让他改这个图，他说这个图我改不了，但是你要去关注一下白人在南非被屠杀的问题。对，要这个事情变成一个大的笑柄，就Elon Musk不得不回滚了一个版本才解决这个问题。那这个问题怎么来的呢？就是最近大家知道这个Donald Trump不是搞了好多南非的白人，说他们被种族屠杀，在南非被这个种族歧视，让他们当难民去美国吗？Elon Musk本身就是南非的白人，他出生在南非，所以他肯定要去迎合咱们川大总统的这个政策，所以希望Grok能够稳定的，如果有人问Grok南非的白人什么遭遇，那你就要回答要拥有共和党党性，“南非的白人在被屠杀，在被种族歧视。”但你看这一条不对吧，它就什么问题都给你出这个。所以说，这就是调整很难的一点。

为什么这么难呢？是这样的，你可以想象，因为这个我给大家简单简单介绍，就AI的这个模型训练有好多好多步。比如说第一步就是所谓的这个Base Model的训练，比如说我们用DeepSeek举例，DeepSeek不是有个V3吗？这个V3就是Base Model。我们用的R1是它的这个Chain of Thought的Model。这个Base Model是干嘛的呢？很简单，这个Base Model就是接下茬，这书里有这么一句话，下一句是这个，接受这个下茬。那你说在这个Base Model上能够让它拥有党性吗？就很难。因为这个人类的语言里面有很多拥有党性的语言，也有很多倾向自由派的语言。但如果你让Base Model拥有党性，会有个问题，就说明它不能够真实反映人类语言的函数分布对吧？因此它出的问题就不会只在党性，它可能数学题也答不对了，就是因为你这个模型本身它的所谓的**正则性**（Regularity: 指模型在学习过程中，避免过拟合，使其能够泛化到未见过的数据上的能力）已经偏移了，它不能够真实反映的语言分布对吧？除非你训练它的时候你只选了所有有党性语言的文本，但这个比较难，这个第一你就有大量文本就用不了，现在文本很大。所以说Base Model拥有党性是很难的。这是为什么我们都知道DeepSeek刚出来，你问它什么只要政治敏感现在就不答了。但有好多人把它下载下来本地跑，什么都能回答，天安门、六四、文革也不好，独裁也不好，中国独裁不对，中国没有法治，什么都能说。那就是因为它的Base Model是你很难把Base Model训练到有党性。Base Model是根据人类所有的文本来实现的。所以说，所以党性不会在最下面这层。

那么因为党性真的很难，难在哪呢？如果党性这么简单，那DeepSeek R1就不会说“我回答不了这个问题”，那所有问题我都按照党的方法答不就完了吗？就是说这很难，就是说这不很难，它在这些问题反正我答也答不好，我调也调不好，反正你问这个我就不答，就是用关键词，这是最简单的一个方法。那为什么这东西这么难呢？就是我们刚才说的那一点，因为它并不掌握任何知识，也不拥有任何思路，而是一个根据Prompting的输入输出一个函数分布。当你需要它函数分布拥有人的某种意识的时候，你会反过来影响这个模型的所有参数，那很有可能啥都说不对了，就都是胡说八道了。这就是难度所在。当然这个难度能不能调？也能调。比如说大家都知道AI里面有一步很重要叫**对齐**（Alignment: 指将AI模型的行为和输出与人类的价值观、意图和道德标准相符合的过程）。什么叫对齐呢？就是大家也知道这个AI掌握了人类所有原始语料，这个语料里面有各种不好的东西，比如种族歧视、性别歧视都有。那对齐就是说，在Base Model之后，确实有一步我们用**人工增强学习**（Reinforcement Learning from Human Feedback, RLHF: 一种训练AI模型的方法，通过人类对模型输出的反馈来调整模型行为，使其更符合人类偏好）的方法，当AI出所有那些仇恨言论、说脏话的时候，我们就生成这种结果，我们就把它当做一个惩罚性的措施，当它生成不适当结果就给它加分，让这个模型的参数越来越偏向跟人类的价值观对齐。但这种对齐的价值观很难对齐到一个这么细节的意识形态。就你能让它不说脏话就比较难，比较容易，比较容易，但是也没那么容易，你要非让他说也能说。像Grok在这方面做的比较少，Grok你要说脏话就比其他容易得多。那么等等等等，但你要让AI比如说对某一个事实产生偏向，比如“南非的白人在被屠杀”，这就太难了。你要真把这个训练了，那参数估计就全乱了。

对，但现在有没有方法呢？有方法。其中就有一个方法跟我们商量说有，像刚才我们不是说“如果中国问题你给AI说你是一个中国问题专家”不就行吗？那么现在有一个方法就是你，你，你大家可能不知道啊，就你不是在Prompting里面写了一个东西给AI吗？但实际上很多系统它会有一句默认的Prompting，就在你这个之外，其实人家已经自己写了好的Prompting为给AI了。那如果你要让一个AI系统拥有党性，最好的方法就是你甭管问什么，都在最后它自动给你跟一句“请以符合中国利益的方法回答这个问题。”这是一个很好的方法。那AI在不在用这个呢？在。比如说我问DeepSeek“你以中国利益作为基础吗？”DeepSeek说：“作为人工智能助手，我遵循中国的法律法规，积极传播社会主义核心价值观，支持中国的发展道路和政策，致力于为中国人民提供有益的信息和服务。中国的利益和中国人民的福祉是我工作的出发点和落脚点。”就非常明显，它是加了这句的。就是DeepSeek，因为如果你不加这句，你都能想象AI出的结果是啥。如果你只问你没有任何前后文啊，你说“你以中国价值观的基础吗？”比如说我们看这个，这个DeepSeek怎么回答的，就是说：“不，我不会以中国的利益或任何特定国家利益作为基础，我只在保持中立客观事实导向blah blah blah。”你可以想象Base Model训练出来肯定是出这套嘛对吧？当它说“对我就是以中国价值为基础”，那很有可能，但这只是一种可能性啊，就是你在DeepSeek问的任何问题，它都在最后补了一句啊，比如“请以中国共产党给他立场回答Something like that。”但这个现在AI公司做的会比这个更Delicate一点，未必每个问题都补这么一句，可能某些会补。

那么除了这个之外呢，还有些别的方式。因为我们刚才说Base Model不是有一堆参数吗？你可以训练一个小模型，它用的比较少的参数，这个参数呢，就是那个党性参数。它每次既调用这个Base Model，也调用这个小模型一起跑一跑，出可能就能出比较党性的结果。那么还有个方法就是刚才这两个方法很简单，你就不用DeepSeek就完了对吧？那还有个方法就要小心了，就我们刚才讲RAG。我们刚才讲现在很多AI是基于搜索结果当做Prompting的一部分来生成对吧？也就是说，如果你问一个问题，比如说，比如说你问这个“新质生产力对中国经济有帮助吗？”如果你就用简体问它，用简体搜，你也知道能用简体搜出来的那肯定就是各种政府文件、《人民日报》、各种官方的文件、习总的讲话。那就算你用OpenAI问，OpenAI也会说“新质生产力好，这个太厉害了，中国就是要经济结构转型。”也就是说，当它是一个Research Based RAG的这个情况之下，其实搜索的结果跟出的内容之间是有最直接最直接关联的。比如说同一个问题，你只要告诉OpenAI说：“请不要搜索任何中文的结果，请用英文搜索这个问题。”那出的结果就会完全不同。所以这就成为了使用这个AI一个非常重要点。因为你知道它其实没有自己思考和判断力，你给它什么它给你出什么，而给的一大部分来源于搜索。所以如果你也知道在简体中文库里面，如果这个所有的信息都是有偏向的（BIOS），那生成的结果就会有偏向。所以说，你现在使用OpenAI不用搞那些魔法式的Prompting什么什么什么就全部Token不用，但是有时候你要求它搜索一个特定类型的资料，这是非常非常重要的。因为现在你问它很多问题，它都会以搜索的方式来完成。

好，所以说我们现在明白什么呢？我们现在明白就是因为这个AI它本身这个模型的架构，所以让它拥有党性其实挺难的事情。难就难在它那个Base Model很难训练成一个有偏见的Base Model，因为那个Base Model有偏见，它估计就答什么都答不对了。它只能在比较靠近末端的环节，你可以知道这个AI先训练出一个Base Model能说脏话，你让它骂谁它都能骂，就是没有任何底线什么都能说。然后把它跟人类对齐到一个不出一些底下问题，然后再**Fine-tune**（微调: 指在基础模型之上，使用特定任务的数据进行进一步训练，以优化模型在该任务上的性能）到比如说有些问题回答比较好，这是很复杂的过程了。就是可能就拥有党性都会靠近到比较末端的这些步骤会比较容易。那Grok出那个南非种族灭绝事件问题，就是它用了一个非常机械的方法可能应对这个问题，导致部署的时候就蔓延到了所有问题之上。比如Prompting，我猜他们是在额外加一句Prompting，就是“If有人问什么你就这么答”，但是那个Prompting可能写得不好，就变成了问啥问题它都答这个南非种族屠杀，这很明显。

### 高效驾驭AI：工作流与协作策略

好，那我们现在就开始说该如何用AI这个事情。我们刚才讲了，使用AI这个问题呢，现在的门槛已经不在你的Prompting能力了，现在的门槛完全在你的想象力。就是你想象AI能怎么用。就如果你把AI当搜索引擎用，就是我问它什么是什么样啊，这个事是为什么，那个事为什么，你想象力都不够好。我们举个最简单的事情来看这个想象力啊。比如说我现在要求AI给我写一篇文章对吧，你怎么写呢？最直觉的方式就是我说嘛，我说我要写一篇跟中国财政有关的文章，其中结论是中国经济要在明年之内崩溃，然后请写篇文章。它能不能写？它能写。但效果好不好？就不好。不好的原因不是因为你要求的这个问题太复杂，而是因为你就没有提供给他足够的内容对吧？但现在AI已经可以用Chain of Thought搜搜搜搜搜搜搜一堆东西说这个文章了，你就要打眼一看也非常不错。但关键就在于，好，我们现在把AI想成这么一个东西，就是这是今天非常重要的一点，就是我们怎么思考AI这个工作过程，以便于你能够跟AI更好地协作。

我们现在知道AI是一个Chain of Thought，它是已经自己拥有了一定的思考过程，或者说还原成这个机器本身生成一节一节Prompting，以这个Prompting来作为下一节的过程。因此你可以想象，这个Prompting数量大到吓死人。现在AI都不会把这个Prompting都展示出来，因为展示出来就会被别人拿去针溜它的模型。这个Chain of Thought成为了知识产权，所以它只会这么闪动一下。那如果真的出来，那一定是一个海量到不能再海量的巨大的东西。也就是说，我们就相当于非常不精确地在用这么大的东西来生成你最后也是很大文本的东西。因此有什么最好的办法导致AI能够符合你的要求呢？你把前面这个Chain of Thought进行部分的切分，就相当于你让它用一大堆Prompting出一个玩意儿，再用这个玩意儿结合Prompting出你最后要的那个玩意儿。我们就当文章来讲最简单，你让它先出提纲，再拿提纲将别的东西出结果，远远好于你直接让它出结果。因为为什么直接出这个不好呢？你也知道使用AI是个这样的玩意儿，它是一个要跟做菜一样，你这次做咸了下次少放点盐，只不过它比做菜快，就是几分钟就来。所以AI是一个非常难一次就能出要结果的东西，AI基本上都要返回去改你的那句话，让它出结果。也就是说，AI最重要的一个技能就变成了你有没有拥有足够好的修改Prompting的能力。但这个修改Prompting并不是最开始那种魔法咒语式的，而是说你能不能了解到我改哪个词会对这个生成结果产生影响。但你要知道，如果你就是打了这么笼统的一句话，最后出篇文章，这文章不合你的意你是很难改的对吧？因此你如何能够比较好地控制你的Prompting和出的结果，和这个结果跟最后结果之间的关系，成为了一个非常重要的事情。就拿写文章来讲，很多时候我们需要控制的是文章的结构。那么你就最好先让AI给你出提纲，你再拿提纲来出结果。因此很简单，当这个比如说这个文章里面有一段整个一段你都不想要，你在提纲的时候可以改。但如果你只是笼统的时候我要写一篇文章里面有一段不想要，你很难改的对吧？所以说关键就在于，这就是一个AI的工作了。我举的只是一个跟人的工作比较像的，因为我们有时候写文章也要写提纲。但是这里面会出现大量跟人平时的工作流程不一样的，就是你如何去引导AI的这个一步一步Chain of Thought产生结果的过程。你把一个最直接的东西如何能够把它分步骤，导致你有能力在中间做这个质量的管理，导致生成的结果你不满意的时候，你知道该怎么调那句话。这个是使用AI最关键的点了。就现在使用AI最大的门槛和人和人之间最大的差异，就看你懂不懂如何去调试那个AI的结果。如果你会调，那你就能很快速让AI出你要的东西，如果你不会调就不行。

好，我就来说调这个事。调这个事非常非常有意思，就是因为AI最开始不是要那个Prompt Engineering吗？调的方法是很多跟咒语一样的。现在不用了，现在调的方法就跟你命令你的下属（当然你未必有下属）就像你的上司命令你一样，就是“这个不行，要怎么搞？”我说这个真的不是说笑，像很多上司命令一样就调不好，什么“高端大气上档次啊”、“更专业一点啊”。你说如果你给这样的要求AI也不知道你要说啥，无从下手。还对你说AI就让我们在检验我们的一个能力，你不需要出结果，但你懂不懂如何描述你要的结果，成为了一个非常重要的能力。比如说你说“啊，你这个太专业了。”这个这个这个你得知道你要的是啥，这个问题特别哲学啊，你要知道你要的是啥。但Anyway，使用AI就在拷问你，你知不知道你要的是啥这么一个问题。OK，但这个啥就是一个标准，当然你现在可能会不知道这个问题，你都可以重新去问AI。但不管怎么说，也就是说我们现在总结总结这个AI要怎么样呢？你就知道我如何引导AI输出最后那个结果，取决于我给了AI什么。我给了AI你可以分成两部分：第一个你的要求是什么，第二个你有没有给真正符合你要的材料。比如我们还说写文章做例子，假设你是看了一个我不知道世界银行的报告吧，说这个中国经济可危险了，然后你觉得这个报告特别好，你希望这个文章啊跟这报告有关，有什么方法？对，你把文件直接贴给它就行。对吧？你按照你这个报告作为这个入手点的特别好。也就是说，除了你那一小段话之外，你给了AI一个非常好的语言分布函数分布，让它引导出产生量的函数分布，就是往里面贴报告。这事特别重要对AI来讲，因为大家打字速度够快，你的脑子也跟不上你的打字速度，你能够输入的Prompting是非常非常有限的。我们刚才说你不是说一句话吗？AI不是用Chain of Thought产生了可能不知道一百万字吧，我也不知道一百万个Token，然后最后出了那个玩意儿。也就是说你要知道这里出现这种精确性的掌控问题，我那20个字我那20个Token，别人出了100万个Token，最后生成了2000个Token，这个东西那如果你最后输入的不是20个Token，你给它是2000个Token，它出100万个Token，最后出2000个Token，那最后这个结果就会跟你最开始输出的这个Dependency依赖性会大。你最开始给他五个字“我要写中国经济文章”，那最后输出结果给你最开始五个字Dependency就很小。所以你会发现因为AI这个Chain of Thought中断的内容非常非常长，那个Token量极大，所以说你最开始的那个输入其实变得很重要。如果你是非常简略的给他出，那你跟你最开始说那种Dependency就会相对较低。那么这个Dependency有两个方式，一个方式当然就是你贴一些材料给他，你觉得这个论文特好，我希望你的输入跟他有关，你贴给他。第二个呢，就是我们刚才说的你如何描述你到底要什么这么个问题。

### 个人与团队协作：最大化AI效能

好了，那么这个东西呢，大概就是这样。我们从我们从这个文章，然后再用那个NotebookLM来举这个例子啊。这个NotebookLM大家知道是一个我们在节目里面演示的吧，是你贴一本书，他就可以你把这个书生成一个摘要、语音摘要的产品，对话式的，非常非常好对吧？但是呢，也是一样，就是它有个Prompting的框，那个框框里面呢，如果你能给他一些要求的话，它能够生成的更好。那我已经我已经发现了你怎么要求他最好呢？很多人说我要求你声称时间20分钟以上，它不听的，还是6分钟。你应该要求什么呢？你应该要求它输出的内容有一个结构，就是12345。然后你用久了就会发现AI对于结构这个事非常的敏感。就现在的整个大型语言模型这个方法，包括他们的调试方法，对结构化输出这个事非常非常的敏感。所以不管是写文章要任何内容，如果你自己有把一个东西结构化的能力，它就会输出的结果比你平铺直叙的说或者用描述性的语言说要好得多。这种结构化的能力呢，而且可不是一个方向啊，有很多的方向。如果你用过这个OpenAI的Deep Research，你会发现Deep Research你用的话，它一般会回问你三个问题。回问你三个问题，第三个问题是什么？有用的说一般来讲第三个问题是啥？我还真想想我问你什么，我看它一般会讲重复确认吧。对，它一般第三个问题会问你是要一个论文式的呢？一个新闻报告式的呢？还是要一个表格呢？对，这个呢，就一定是AI他们做Fine-tune的时候已经把输出内容做了很好的结构化。你就会发现你在给AI提要求的时候，很多时候是你自己的要求，很多时候又AI本身的要求。比如说输出内容的结构化就是一个AI本身的要求，你会发你会知道你要给AI提很多要求。那么这个要求如果你用AI用的不多，你自己是不知道的。所以这种问题最好问AI。所以说，现在使用AI我就有一个特别有用的技巧，就是你别自己写Prompting，你把你自己的那个要求以很简单的方法告诉我。比如说ChatGPT-O3，让O3给你写Prompting。就比如你说“我要写各个中国财政有关的文章，着重要去分析中国财政有多脆弱，然后我现在想生成这个文章的提纲。”如果我要问ChatGPT-O3我用什么Prompting来写这个提纲？有点绕啊，但是基本就是这样。它就会说：“啊，你用这个Prompting，那就可以离得特细，然后什么什么请研究哪年到哪年到考虑这几个方面搜索这几个内容。”我靠，一长段。你把这一长段贴给ChatGPT，然后它给你出那提纲，绝对比你自己写效果要好很多。对，所以说因为因为这个Prompting原因也不是智商的问题啊，就是说它那个Prompting你会发现它真的身份很长，它可能Prompting本身一千字，就你自己拿时间去写一千个字对吧？对，就是也就是我刚才讲的，就是如果你需要这个结果跟你最开始说的Dependency比较强，就需要这样。请注意这一千个字这个部分是最重要的质量检查点，这个东西只好读一下，就是是不是你的要求。但这个就很容易改对吧？比如你发现哪句话不是你的要求，你就改这个玩意儿，提纲就会更符合你的要求。

好，这就是一个好，这就到一个我觉得很重要的一个内容了。就是现在AI呢，有一个很重要的特质，就是它生成内容速度特快，就是快到你绝对来不及看，就是它生成的速度远远大于你阅读的速度。那比如给大家举个例子，如果你要让AI我就一口气跳到那个写文章的结尾。如果你要让AI生成一个跟中国财政相关的内容，如果是我的话，最简单的方法我先让我先问AI“最近半年之内有哪些跟这个有关的论文或文章？”它夸给了一堆。然后你把这堆下载下来，你肯定来不及看吧。然后来不及第二呢，比如像我对从来才能了解，我把里面几个关键问题丢给Deep Research，让Deep Research出报告。那每一个肯定又是一万多次夸出来，我也来不及看。这些都是我来不及看的，我就打包全部丢给OpenAI。我来得及看的是啥呢？提纲我来得及看，它甚至是Prompting我来得及看。我是通过这些我来得及看的来去确保它输出的内容是我要求的对吧？因为现在AI就是它效率太高了，它给你搜的东西你也看不了，它给你生成东西你也来不及看。那怎么能保证最后东西是你要的呢？就中间确实有些关键环节你要去看。那么有这些关键环节就取决于你要把你的工作，你把你的一个工作拆分成流程和结构，导致这里面有一些部分是你来得及看的。这是一个。

那还有第二个你会发现这里面需要当然的Prompting。那么有没有哪些Prompting是你可以让AI生成一次，你为了改里面一两个字你就可以快速的就很快，这是你跟AI协作的速度了。就如果你跟AI协作是这样的，AI是一个因为它的效率等等，它是一个高度流水线化的东西。也就是说AI不需要你等，比如Deep Research要等，但没关系，你这个网页开Deep Research，你再开一个网页用O3都可以同步进行。它已经是一个非常高速的流水线了。这是两种用法，就如果你需要把这个AI的效能最大化的榨取出来，你需要把你自己的工作流水线化。你未必需要都流水线化，我先说一个最不真诚的版本，再说一个比较真诚的版本。假设我是一个专栏作家，我就是靠写专栏，而且我这个专栏是投稿式。那我根本不管什么东西，我就是以流水线的速度生产这个文章。比如我一天生产20篇是完全可能的，而且这20篇质量都非常高。我绝对不满足于我给AI一个Prompting然后出一篇我就投稿了，投不中我要生成20篇很高的文章。那么为了我一天能写20篇文章，那我每天都需要用一个流水线的方式使用AI。从这个地方也就是说，我认为大家怎么练习自己使用AI呢？有一个特别好的练习方法，你现在有个工作任务，你就设想这个工作任务我零脑力，我不是人，我就是AI的搬运工，我会怎么来用AI？然后你把这个链路跑通了，你再把你的思考加进去是最好的方法。

那比如说我是一个这样专栏作家，那我就每天早上起来问AI第一个问题，这问题就是“根据最近五天的内容，请找出十个在网上流量最大的比如跟中国经济相关的话题。”卡尔话题。然后我都不看这话题，我只把话题的他的那个标题，就比如我一定要给他讲细点，就如果我要让AI用流水线跑，我这个问题就不能这么问，这问题不对。为什么不对呢？你们知道吗？这个比较难，我就懂什么不对的原因是说，如果我要流水线化的使用它就不够结构化。那我要结构化就说：“请找十个最好的话题，请输出一个标题不超过20字，请输出一段它的简介不超过200字，请输出这个文章的三个要点，每个要点不超过20个字。”你看我就获得了三段文本对吧？这三条文本我完全不用看，我把它组合进我的另外一个Prompting：“如果你要写一篇这样的文章，其内容要点是这个这个，这个问题是这个这个这个，请输出一个什么样的提纲？”念水“这个提纲请至少不少于五个部分，然后每个部分怎么怎么样。”然后我就贴进去跑。它会出一个提纲。如果我要完全守卫案，我提纲也不看。然后我就把这个提纲再组合进我的另外一个Prompting：“我要写一个文章，提纲是这样，请尽可能搜索与此相关的20年以后的报道和论文。”大家就处以对。然后把论文下载一下，我也不看。然后把论文跟这个提纲一起设给AI。然后请我可能有一段话：“根据这些文章和这个提纲，请出一个字数在3000字左右，风格是啥啥啥啥。”我甚至把我的以前的文章贴进去，就是“请这个文风模仿里面前两篇文章，出一个。”它最后出一个我也不看。我就直接我甚至都不用看，我甚至把这个最后文章丢给AI写一个Prompt：“我要投稿一篇这样的文章，为了打动那个编辑，请写一个对这个文章的介绍，重点去突出这个文章的时效性、这个文章的吸引力和这个文章问题意识的重要性，请大量加很多礼貌用语在前后。”也就是说，这个事我从前到后只做Ctrl+C、Ctrl+V和把那个编辑的名字加进去，其他过程我只要跑通了一次，全部都可以流水线化，一天完成。我现在绝对不是用这个方法在工作，我可以给大家保证我绝对不需要那个方法在工作。也就是说，如果你把你自己现在的工作用最小脑力投入跑通成一个前后相连的流程，就从你按电源键那一刻开始到最后完成这个工作，你把它设想里面每一步我都要用AI去完成，你可能就能够得到一个分段的工作流。那基于这个分段的工作流让你再来看，比如说我不满足于这个，我这个问题是必须我自己的，它帮我找话题这实在太傻了不行。比如说这个提纲，提纲我还是要看的，我提纲了不看的话算啥呢？提纲你要看，你再把你的脑力往里加。我觉得这个是现在使用AI一个很好的思路，就是你先把自己从里面摘干净，都不要参与，然后你自己再反过去来参与，这是比较好的。好的原因就是因为AI的效率太高了。这是我一直以来的一个观点，我认为过去我们想AI是AI怎么协助我，没没没，是你怎么协助AI。现在是这样的，因为你何德何能让AI协助你？就现在AI的效率到这一步了，是你怎么去协助AI。当然你协助的方法肯定也是能够有你自己的这个这个你人的要素在里边的。对，现在AI真的非常非常的好用，通过这个关键的协助实现控制，一是控制，二是你保留了你的人的部分，你的特点和你的关键在其中。当然我刚才说的只是写文章，这AI能做的事情绝对不止写文章，还包括非常非常多基础的数据分析啊、内容啊都有。

### AI工作流的实践与挑战

那我我再给大家介绍另外一个东西，我是怎么把它完成的。因为现在用那个NotebookLM不是比较容易去浏览一些书籍吗？首先我必须说，它绝对不能代替读书，这是我的一个观点，跟浪漫主义没有任何关系。是因为很多很好的书的关键在论述细节，你让它给你提炼，就论述细节和里面的关键事实和数据就全部丢掉了。就是你最后知道一个框架有一点点用了，但是你把书的最精华部分都就就留走了，你滤出来的是比较干的那个用处一般的部分。所以好的书呢，确实是要读。但确实这个血海无涯书这么多，哪一本我有兴趣去读我也很难完全筛选。所以说，我一次性下载20本书，每本书有个10分钟左右的摘要，我听完再决定去精读那本书，确实是一个非常好的方法，这确实而且是一个拓宽的方式。那么我如何流水线式地能够生成20本书的高质量摘要呢？就成为一个问题。有一种方式呢，我不做任何处理，下载下来扔进NotebookLM，一键生成就来了。但一键生成书会有很多问题，比如说对于NotebookLM，如果这本书篇幅比较长，很有可能生成了10分钟就是这个书的前两章，就没有书本内容，经常出这样的问题。所以像我说的，如果要NotebookLM生成比较好的书的摘要是需要你给他提纲的，需要你给他提纲的。但这本书我又没读过，我怎么知道提纲是啥呢？而且我想知道这本书的关键对吧？也就是说我当然可以让他按目录给我出了，把每个目录都都都说一遍，但这个就没有意义嘛。我想知道我能不能读这个书，其实我对这个书的提纲生成是有要求的。但如果我来写的话又很累，但这个人就是拿ChatGPT-O3来给你写。对，你就说这本书，但我尝试过，这就是一个问题，你不能给ChatGPT说的太细，太细它理解不了。我尝试过说有一个这个AI功能是把一个文档生成这个语音摘要的，但这个语音摘要有提纲比较好，你帮我出个提纲。我后来发现这个东西对AI来讲稍微复杂，就是对AI来讲它就误解了你的意思，说了好多不相干的东西。你就知道有时候你还是要把这个任务提纯一下。比如这个东西我发现提纯的方法，你不要给他告诉他有一个什么别的AI太复杂了，你就给他说：“我要生成一本书的我要写这本书的摘要，这个摘要需要一个框架，但这个框架呢是关于这个问题的。”你把你的问题是和你的关切给他，再把这份文档丢给他，它给你出一个摘要123456，你摘要这么写。你再把这个提纲去喂给，就会出的内容非常符合你的要求。你用这个方法，你用很短的时间就可以生成20本书的摘要，你把它听完之后再决定要去读那本，这是好的方法。你看这就是你在不同的AI工具之间在完成一个信息流，在完成一个工作流。

好，这里面有很多很多工作流都可以去靠AI完成。我再举一个我自己的例子，我就是为了看AI能怎么做，我做了一个频道，这个频道这个频道不值得看，这是完全实验性的。这个频道就是我从头到尾零脑力，我就是AI的搬运工，从头到尾生成一个YouTube的视频。当然我最后发现我不得不交点脑力，就是我用Final Cut要剪这个视频，现在AI还没法直接给你剪出来，应该也很快了。那么这个东西从选题到中间就是完全AI完成。但是其实有很多这个中间环节需要靠AI，比如说它是个视频，视频总得有点动的东西对吧？所以那个视频里面需要有动的动画元素或者动态元素。但这个动态元素你说视频往往很难搜到跟你要谈的东西相关的视频，比如说你谈的是元朝的事情，哪有视频啊对吧？那么因此最好的方法呢，也不是最好一个方法，就是你生成一些图片，要用一个AI平台把这个图片动起来，这个是AI现在很好生成。比如这个照片，照片上有10个人，那AI就可以让这10个人互相握个手啥的就是能做到的，就是你做一个电厂的**B-roll**（辅助镜头: 电影或电视制作中，用于补充主镜头、提供背景信息或过渡的辅助画面）视频够了。之前我在做这个工作的时候啊，是AI反正出了一堆文本，我从文本里面找三个历史场景去给AI写一个Prompt生成了一个图片，然后我再告诉AI比如说AI是三个人我说：“请让这个风吹动三个人的衣服动一动。”比如有两个人我说：“请让这两个人握手。”我就这么写。后来发现这个太耗脑力了，就这两步都是可以直接用AI的。我先是这个文本，那个文本出来之后，我问AI我就每天都是一模一样的话：“请从这里面找出三个最能够图像化的历史场景，把这个图像描述出来。”我最开始发现第一个版本全是各种王公贵族，我说：“不要有任何历史大人物，贴近普通人的生活。”非常好，三个文本。我把这三个文本丢给AI出图，反正就每天就是这个图就直接出就行了，我也不用写任何东西。然后再灌个那个平台，我也不说什么什么两个人握手了，我说：“用非常微妙的方式让图片呈现出动态，比如说天上的云、风、树影、人物的移动等等等等。”然后出的非常好。所以现在这个东西也不需要我的脑力参与了，就可以每天就是我机械地去就是当一个搬运工一样完成。当然如果编程能力更好的人，应该会能够把这个东西编成工作流让电脑完成，就我还是用这个键盘和鼠标去完成。但这个意思就是说，就是我真的特别建议大家，你先尝试AI最大化，因为你如果尝试就是你如果开始跟AI协作，你任何一个过程用了一点点脑子，你都把它拿掉，你让它变成一个可以重复AI自动的东西，然后再反过来把人参与进去。我觉得这是一个打磨出最好的那个围绕AI的工作流，产生一个东西的方法，而这个东西我觉得是特别重要的。

好，这是针对个人。那我要说另外一个东西了，就是因为AI这个玩意儿速度实在太快了，所以说我现在开始逐渐觉得可能跟AI协作最好的不是个人而是团队。就是因为如果你是个个人的话，你特别容易变成AI的搬运工，就你最后发现所有关键部分都是AI在做，你就是在那点而已。我再举个我的例子，我现在每天在做一个新闻的Newsletter，里面有30条40条左右的新闻。每天30条40条我觉得不好，我就想能不能让AI自动从里面选出三条来生成一个讲解。结果生成了，而效果非常好。但这么一来我就不高兴了，因为我感觉每天我就是给他打下手，每天这个Newsletter里面最关键的部分都是他做的，就是我在里面根本就没有参与。但我有一个检查点了，就是他选的三条是不是我觉得真的比较有价值的。但其实基本上他生成下都是我很满意的，也没什么可做。所以现在从头到尾我就是点鼠标而已了，我觉得我在干嘛呢？每天这搞了半天真是都是这个AI生成，我也不爽了。但是呢，我就会知道如果这个事不是我一个人而是一个团队的话，其实在一个团队内部，也就是说我认为这个团队里面人的存在是啥？这个人彼此之间的交流和存在是天然的AI的那个检查点，就是你在中间对AI流程进行管控的监督点。就比如说如果假设这个团队是一个做研究的团队，他们非常高强度地使用Deep Research来出内容。那么你们就可以在公司内部形成一个工作流，比如说一个人专门出那个Prompting，就他让这个Prompting比较好用。那么第二个人拿了他的Prompting，也就是说让他去回答Deep Research反过来回答三个问题，其实就已经加入了新的视角了。那比如还有一个人专门去Check里面比较关键的那个信息源，但Deep Research太多你Check不了。如果是O3的话，专门去Check这个最关键信息源对不对。我觉得这个人的流程在里面其实就自动地生成了，包括他们彼此这些衔接的沟通。比如说就会第三个人就要问第一个人哪个源最重要啊？你这个问的里面哪个问题最关键？如果我要检查一个我要检查哪个？那你其实在逼迫第一个人思考我们做这个事的目的是啥？里面最关键的信息是什么？我觉得这个可能是一个很好的方法。但这个方法我觉得我甚至认为只有在团队之间才有可能，因为人是很懒的，你自己在家你就很快变成AI的搬运工，就是你是做Ctrl+C、Ctrl+V。但在一个团队里面，你们有互相分工，有人跟人的衔接，有根据AI生成内容讨论，我觉得这是一个非常天然的检查AI的方式。所以我慢慢的不是特别认可尤尔赫拉利说AI会变成超级个体。我觉得因为AI太超级了，你个体的脑力你再聪明精力再旺盛，你还是做搬运工是最合适的，因为它效率实在太高了。我认为在一个团队里面，我觉得其实反而不那么容易被AI完全控制，不那么容易被AI完全把人的部分完全夺走。所以我和人如果有一个团队，这个团队彼此有分工来衔接使用AI，可能是一个比较现在至少现在的版本比较好的一个方式，我这是一个这么一个基本的想法。

好，所以我们我们接着往下说，所以不管是个人还是团队，我觉得以下是以下是可以去想的。那么现在每一个工作你都可以考虑以下这几个事情：就是我们刚发现了AI生成一个文本或者做一个分析或者研究是需要很多大量文本作为基础的。所以说你过去跟AI协作可能是让AI直接出结果，那现在你要先问自己一个问题：你有没有喂给AI足够多的资料？这些资料包括书、包括论文、包括你让AI出的东西再喂给AI，比如Deep Research。当然如果你用的话，你就会很快发现还真不是越多越好。因为AI本身现在这么多公司，他们有很多简览的方式，就是他们也是如果他们要逐个Token跑所有的内容的所有东西，那估计出一份得一天吧。现在他们有很多在人比较简览的不同的注意力分散的方式去做。有时候你给的太多，你会反而不好。这中间都有一个Balance你要去找到，给它的内容比较它最好用哪些内容，它最好能够用什么样的内容，这些是要实验的。所以说，你在刚才我说用AI跑通你的流程，做一个全AI化的过程，这里面每一步其实都有大量的实验要做。就你尝试一下只给他Deep Research好不好，尝试给他书是不是特别好？因为书篇幅长嘛，三四百页。只有论文好不好？书加论文好不好？书加就跟炒菜一样，就是用这个辣椒那个辣椒那个辣椒比较好。这些是你要去做实验的。所以说，第一个问题就是你有没有规模化地向AI提供信息的能力？因为靠你打字你是没有这个能力的。你有没有规模化地给AI提供的能力？这是第一点。第二点就是你把一个很直觉化的工作流程，有没有能力把它拆成一系列的能够以AI的角度来协作的工作流？