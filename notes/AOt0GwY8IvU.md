---
area: "work-career"
category: ai-ml
companies_orgs:
- OpenAI
date: '2024-04-10'
draft: true
guest: ''
insight: ''
layout: post.njk
people:
- Sholto Douglas
- Trenton Bricken
products_models:
- GPT-4
project: []
series: ''
source: https://www.youtube.com/watch?v=AOt0GwY8IvU
speaker: Dwarkesh Patel
status: evergreen
summary: 本期内容深入探讨了AI模型发展的未来，预测小型与大型模型界限将模糊，微调（fine-tuning）可能消失。讨论了长上下文（long context）对AI代理（AI
  agents）长周期任务（Long Horizons）表现的影响，并强调模型可靠性是关键。展望了未来AI公司可能由单一通用模型构成，而非多代理协作。最后，讨论了端到端训练与强化学习（RL）的潜力，以及实现这些目标所需的高人类监督和模型可靠性。
tags:
- ai-agent
- llm
- long
- reliability
title: 模型区分与微调的未来：长上下文与可靠性的探讨
---
### 模型未来趋势
存在这样一种未来，小型模型和大型模型之间的区别将在某种程度上消失。同时，随着**长上下文**（long context: 指模型一次能处理的输入信息量，此处指能够处理非常长的文本）的出现，**微调**（fine-tuning: 指在预训练模型的基础上，针对特定任务进行的小规模模型参数调整）在某种程度上也可能消失。你可以想象一个未来，你拥有一个**动态计算资源组合**（dynamic bundle of compute: 指根据任务需求实时调整和分配计算资源的模式）和**无限上下文**（infinite context: 指模型能够处理的输入信息量没有理论上限，可以包含任意长度的文本或数据）的组合，它能将你的模型专门化用于不同的任务。

### AI 瓶颈与代理
许多人认为人工智能进步的一个瓶颈是模型无法在**长周期**（Long Horizons: 指需要持续很长时间才能完成的任务或过程）上执行任务，这意味着任务需要持续数小时、数周甚至数月。如果我有一个助手或员工，他们可以为我做一件事很长时间。而**AI 代理**（AI agents: 指能够自主执行一系列任务以达成目标的智能系统）似乎因此未能普及。那么，长上下文窗口与在这些任务上表现良好的能力，以及执行需要长时间投入的任务的能力，这两者之间的联系有多大？它们是无关的概念吗？

### 可靠性是关键
我不同意这是 AI 代理未能普及的原因。我认为这更多是关于**可靠性的‘九’**（nines of reliability: 指模型成功率达到极高水平，例如 99.99% 的‘九’越多，可靠性越高），即模型能否成功完成任务。如果你无法以足够高的概率连续链接任务，你就无法得到一个像代理一样的系统。

### 模型能力提升
这就是为什么像代理这样的系统可能遵循一种**阶梯函数**（step function: 指输出值仅在输入值达到特定阈值时发生跳跃式变化的函数）的模式。像 **GPT-4**（GPT-4: **OpenAI** 开发的大型语言模型）这类模型，或者 G Ultra 类模型，还不够。但也许下一代**模型规模**（model scale: 指模型的大小，通常通过参数数量来衡量）的增长意味着你能获得额外的‘九’，即使**损失**（loss: 在机器学习中，衡量模型预测值与真实值之间差异的函数，损失越低表示模型性能越好）下降幅度不大。这点额外的能力就能带来显著提升。当然，要完成长周期任务，你需要一定量的上下文，但我认为到目前为止，这并非限制因素。

### AI 公司形态
我之所以这样问，是因为有两点让我思考代理是否是未来发展的正确方向。第一，随着上下文变长，模型能够处理和考虑人类无法企及的信息。因此，我们可能只需要一个工程师负责前端代码，另一个工程师负责后端代码，而这个系统可以一次性处理所有信息，‘专业化’的难题也随之消失。第二，这些模型非常通用。你不会使用不同类型的 GPT-4 来做不同的事情，而是使用完全相同的模型。这让我思考，这是否意味着未来一个 AI 公司可能就是一个模型，而不是一堆相互连接的 AI 代理？

### 评估任务自动化潜力
这是一个很好的问题。我认为，尤其是在近期，它会更像多个代理协同工作。我之所以这么说，完全是因为作为人类，我们希望拥有这些独立、可靠且值得信赖的组件。所以，你刚才提到 AI 代理未能普及是因为可靠性而非长周期任务表现，那么，当一个任务叠加在另一个任务之上，再叠加到第三个任务时，这种不可靠性，不正是长周期任务的困难所在吗？也就是说，你必须连续完成十件事或一百件事，其中任何一件事的可靠性下降，或者说概率从 99.99% 降到 99.9%，那么整个链条的成功率就会相乘下降，变得非常不可能实现。

### 长上下文的快速发展
在接下来的时间里，一项非常重要的工作是更好地理解长周期任务的成功率究竟是怎样的。我认为，这对于理解这些模型的经济影响也至关重要。通过将我们执行的任务、涉及的输入输出分解为分钟、小时或天，并观察其在不同时间尺度下连续训练和完成任务的能力，我们可以更准确地评估其不断增长的能力。这能告诉我们一个工作类别或任务类别在多大程度上是可自动化的。

要知道，就在不到一年前，我们还推出了 **100K 上下文窗口**（100K context windows: 指模型一次能处理的文本量，此处为十万个 token）。当时大家普遍对此感到惊讶。之前，人们普遍认为**二次注意力成本**（quadratic attention costs: 指在某些模型架构中，计算复杂度随上下文长度的平方增长，限制了长上下文的实现）限制了长上下文窗口的实现，但现在我们已经做到了。所以，**基准测试**（benchmarks: 用于评估和比较不同模型性能的标准测试集或任务）正在被积极地制定。

### 端到端训练的设想
你可以设想一个 AI 公司，整个系统进行**端到端**（end-to-end: 指从输入到输出的整个流程由单一系统或模型处理，无需中间人工干预）训练，依据‘是否盈利’这个信号。或者，如果这个信号太模糊，比如一个建筑设计公司，依据‘客户是否喜欢蓝图’这个信号。在中间环节，你可以设想有负责销售的代理，负责设计的代理，负责编辑的代理等等。这种信号能否在一个端到端的系统中奏效？

### RL 梦想与现实挑战
理论上，这是**强化学习**（Reinforcement Learning, RL: 一种机器学习方法，通过试错学习，让智能体在环境中通过奖励信号来优化其行为策略）的梦想，即仅提供稀疏信号即可学习。但我认为这不会是首先实现的方式。这需要人类付出极大的关怀和努力，确保机器做正确的事，并给予正确的信号来改进。在 RL 中，如果客户永远不喜欢产出，就得不到奖励。但未来模型将足够好，能获得一些奖励，这关乎‘九’的可靠性。