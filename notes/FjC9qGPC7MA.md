---
area: "tech-engineering"
category: technology
companies_orgs: []
date: '2025-09-09'
draft: true
guest: ''
insight: ''
layout: post.njk
products_models: []
project: []
series: ''
source: https://www.youtube.com/watch?v=FjC9qGPC7MA
speaker: 硅谷101播客
status: evergreen
summary: 本期节目邀请Physical Intelligence与自变量机器人嘉宾，探讨具身智能模型的最新进展，聚焦泛化能力、长程任务、数据挑战及开源生态，展望机器人迈向GPT-3时刻的路径。
tags:
- '101'
- challenge
- llm
- model
- open-source-ai
title: 机器人GPT-3时刻：具身智能开源模型的加速与挑战
---
### 播客引言与活动预告

泓君: 欢迎收听《硅谷101》，我是泓君。上一期节目我们预告了硅谷101全球创业挑战赛，收到了好多听众的来信，大家询问今年硅谷101科技峰会的报名链接。现在，购票链接已放在Show Notes中。去年的活动是半天时间，今年已升级为整整一天，设有上下两层会场。我们邀请了全球顶尖的科学家、投资人、创业者，有些甚至是第一次在现场分享最新的研究成果与行业洞察。此外，我们还会聊一聊《硅谷101》内容创作幕后的故事，以及我们对AI工具的应用与思考。这是一个面向全美的活动，所以为《硅谷101》的粉丝申请了一个85折的折扣码，大家可以在购票时输入“valley 101 fans”，折扣码也放在Show Notes中。我们还将在评论区抽取三名听众免费赠票，鉴于活动在硅谷线下举行，我们将优先考虑硅谷的听众。欢迎大家积极留言，期待与大家线下见面。下面请收听我们今天的节目。

### 机器人赛道：具身智能模型的崛起

泓君: 今年除了**AI Agent**（人工智能代理: 能够感知环境、做出决策并执行动作以达成目标的智能实体）外，另一个热点是机器人。今天我们邀请来的两家公司都专注于机器人的大脑研发。一家是**Physical Intelligence**（简称PI），硅谷的朋友可能非常熟悉，业界称其为机器人赛道的**OpenAI**（一家人工智能研究和部署公司，以开发大型语言模型如GPT系列闻名）。他们今年陆续推出了两个机器人模型：**π0**和**π0.5**。另一家公司是中国的自变量机器人，在我们播客发布时，它应该刚刚开源了自家的**具身智能**（Embodied AI: 使人工智能系统拥有物理身体，能在现实世界中感知、理解和行动的能力）大模型**Wall-OSS**。与PI一样，它同样关注并重点优化机器人模型的**泛化性**（Generalization: 模型在未见过的数据或环境中执行任务的能力）。这期节目我们将从模型层面关注整个机器人赛道的进展。之后，我们还将陆续推出关注机器人硬件、灵巧手以及投资方向的节目，欢迎大家持续关注。今天欢迎我们的嘉宾，来自自变量机器人的CTO王昊。
王昊: 哈喽，大家好。
泓君: 还有一位嘉宾是Physical Intelligence的研究员，同时也是π0与π0.5论文的作者柯丽一鸣，她的英文名字叫做Kay。
Kay: 大家好。
泓君: 欢迎大家。不然大家先简单介绍一下自己，然后在介绍的时候讲一讲为什么自己会选择机器人的赛道。女士优先怎么样？

### 嘉宾选择机器人赛道的原因

Kay: 好的，我就先开始了。我在本科学习的是经济学，当时往咨询这个方向探索发展了一下。但我一直觉得想要解决一个问题，而且发现我性格比较偏懒惰，所以对编程很痴迷。因为编程相当于是在虚拟世界可以实现自动化，如果我可以写一行程序解决这个问题，我为什么要写十行程序？因此在我看来，机器人就是物理世界的自动化。如果我可以一次性地把很多物理任务放到这个里面一次性解决，就不需要再多费功夫。所以随着职业发展转了几个弯，最后去读了机器人的博士，在博士期间做了一些机器学习应用到机器人的研究。毕业时，我很幸运地加入了PI，这样一个很好的平台，去继续探索这个方面的前沿。

王昊: 哈喽，大家好，我是王昊，我来自于自变量机器人。我过去的技术背景是机器学习和**大模型**（Large Model: 拥有数亿甚至数千亿参数的深度学习模型，能处理复杂任务并展现出涌现能力）。我读博士学习的是计算物理方向，后来毕业之后就一直在做机器学习、做大模型。其实我大概从大模型技术开始有一点眉目，从**GPT-3**（Generative Pre-trained Transformer 3: OpenAI开发的大型语言模型，以其强大的文本生成能力闻名）出来，大约2021年左右，就加入到了大模型的研究过程中。我们在大模型技术上一次又一次地发现，这个技术是越来越清晰的。所以我们也是最早一批做出了国内的百亿级模型，也是最早的**多模态模型**（Multimodal Model: 能够理解和处理多种类型数据，如文本、图像、音频等）。在2022年，我们做了全世界最早的中文**文生图模型**（Text-to-Image Model: 能够根据文本描述生成图像的AI模型），以及后面因此发展起来的各种多模态基础模型。过去我们在做大模型、大语言模型、多模态模型时，依赖的互联网数据其实都是一些静态数据。我们一直感觉到这种数据都是在人类这种秒或厘米级的感知尺度上。但在做机器人之后，发现微观尺度下这种真实、复杂、连续动态的物理过程的数据其实没有被记录下来。这也是现在我真正选择机器人赛道之后，发现它与过去大模型最大的不一样的地方。所以这个对我们来讲，确实也是一个全新的挑战，因为它涉及到硬件、数据、模型的全栈结合。但非常幸运能进入到这个赛道，是因为具身智能确实代表了AI的下一个前沿。

泓君: 对，特别好。你刚刚讲的这一段里面，有很多都是我非常想展开的。其实我们的播客在2023年的时候聊过机器人领域的进展，但那个时候整个机器人领域的模型还非常少，主要是以谷歌为主。在今天，我觉得已经出了很多机器人的模型。所以两位要不要说一下，你们认为在整个2025年，在机器人模型领域最重要的突破是什么？

### 机器人模型领域的重要突破：通用性与长程任务

Kay: 我大概搞机器人有七八年了，机器人大模型的红火，其实我觉得也是最近两三年开始集中性爆发的事情。在一开始我自己的研究中，都没有很多用到大模型这些东西，更多是做一些小而精的任务。当时就会发现一套东西在一个场景下解决了特定的问题，虽然这个问题很难，做出来效果不错，但是很难很便宜、便捷地复制到新的问题上去。所以我觉得在我探索大模型的途中，最大的一个惊喜发现就是模型的通用性有很大的验证，可以开始做一些泛化的探索，做一些性能提升上的探索，才开始真正能够讨论这种机器人大模型的可能性。我觉得现在的领域比起两三年前的话，对这种技术路线的信仰要浓厚得多。

王昊: 今年有一个非常明显的现象，我觉得有一个应用上的指数效应的体现。这背后的驱动力其实也来源于通用机器人基础模型的发展和很大的一个进步。大多数在2023年以前，我们专注在单个任务上，把它做到极致。但现在我们有了一个统一的基础模型之后，我们就能够同时学习并执行成百上千种不同的任务。其实也就意味着我们的优化目标其实变了，重心放在了提升整个模型在所有任务上的平均成功率。所以这个也是我刚才提到指数效应发展的基础。就是说我们现在其实是真正可以开始去做一个复杂的长程任务。所以这是一个非常非常令人惊喜的现象。

泓君: 对，我总结大家刚刚的关键词：一个是整个机器人模型泛化的探索，还有一个是复杂的长任务。我们这样说的话，听众可能会稍微觉得有点抽象。大家可不可以举一个例子，比如说以前有哪些任务是机器人做不到的？这一次在我们的研究中，2025年机器人以前做不到的场景，或者一个场景它只能完成一个特定的任务，现在它可以迁移到其他的场景了？

### 泛化能力与长程任务的实例

Kay: 2024年的时候，PI发布π0，当时就是想要推动这种能力。其中可能最有代表性的一个就是我们刚开始发布的叠衣服的任务。叠衣服这个任务其实在机器人界也做了十几二十年，很多人研究。大家一般说到人工智能下围棋很难，就喜欢说围棋的复杂度很高，因为围棋的棋盘、棋局不一样。其实在我们日常生活中这些叠衣服这样看上去对人来说很简单的小任务，它具体的复杂度就是这个衣服上面有两个褶、三个褶，或者这个地方褶的角度不一样，对机器人来说可能都是一个新的情况。它需要把这么多不同的情况解决，才能够完成这个任务。同时叠衣服有很多不同的步骤，要先叠什么再叠什么，这个序列，这种复杂的细分情况和序列性，使得这个任务就是以前比较难解决的一个任务。其实2025年我们也开始探索不少泛化，比如说把π0.5这个模型放到一个移动机器人上，再把这个移动机器人放到不同的、没有见过的别人的家里去看一看，它在这个从来没有见过、不在数据集里面、模型都不知道会怎么反应的地方会怎么做。也是有一些发现，觉得还是比较有希望的。因为它做的这个事不说是完美的，但是它好像展现出有一点像人类一般的，拿一个东西，其实你到别人家你还是能拿。

泓君: 它拿东西拿的是哪一类的东西？因为叠衣服我理解衣服它都是软的布，但比如说你要让机器人去拿起一个杯子、一个碗，它这个理解是完全不一样的。

Kay: 是的。其实这一类问题在机器人内部术语就叫**抓取问题**（Grasping Problem: 机器人识别物体并规划执行抓取动作的任务）。因为它要结合物体具体的形状、摆放的位置，甚至你靠近过去的时候的角度，可能都需要有考量。所以抓取问题是既简单又不简单的一个问题。比如说你要抓一个水杯，你可以是完全一样的一个水杯在两个不同环境，这也是一种泛化的体现，就是你至少能在一个不同的背景下抓到这个。或者说这个杯子放在了不同的地方，或者说这个杯子可能形状看上去差不多，或者材质看上去差不多，但其实需要的策略会不一样。甚至彻底地泛化以后，可能就是我告诉你拿杯子，你去了一个新的家，拿了一个完全不一样的杯子。就是也有这么一些层层递进的测试的感觉。

泓君: 所以π0.5比π0它的进化表现在哪？就是你刚刚提到了我们让它去一个新的家里面它依然能够适应这种环境，但比如说它做的任务是不是有一些局限性的？就是它的任务的可迁移，比如说表现在哪些任务上可迁移？哪些任务还不可迁移？它的规律是什么？能不能跟大家简单地解释一下？

Kay: 当我们在开始推进π0.5的时候，我们想的点是泛化，强迫自己去收集一些不同的数据。这个不同的数据的“不同”其实没有那么好定义，因此就做了这么一个决定：把机器人放到外面去。而实际上在做这个工作的过程中就会发现，我们不知道要收多少个不同的房子的数据才算到头了。我们得一边收集一边检验，一边觉得又收了三个今天，这是不是就有帮助了？如果我们收了30个都没有帮助的话，我们是不是要开始怀疑这条路走不走得通了？但实际上当我们收集到了那么多的不同家的数据以后，我们最后训练出来这个模型，在我们后续的一些工作中做验证的时候，发现它确实有比较强的泛化能力。你在同一个环境下去验证π0，同一个环境下去验证π0.5的话，可能你看表现差不多，但其实π0.5在一些新的环境、新的泛化情况下会更好。当然也还是有很多的局限性，比如说π0.5并不是说你放到任一个新环境都可以很好地完成任务。其实也是大家慢慢地在感受和探索，具体落地的性能上还是有很多提升空间。

泓君: 然后你们觉得机器人模型在研发过程中，大家在聚焦它的泛化问题的时候，它的泛化最难的环节是哪？比如说是因为数据量还比较小？还是说因为算法的问题？

### 泛化能力的挑战：物理世界的复杂性与长尾效应

王昊: 在技术上很困难的点，其实还是在我们对物理世界这个**长尾效应**（Long-tail effect: 在数据分布中，少数高频事件构成“头部”，而大量低频事件构成“长尾”，后者难以完全覆盖）的**鲁棒性**（Robustness: 系统在面对异常输入、扰动或故障时仍能保持稳定性能的能力）。比如说环境光照、一些视觉误差，这些都是可以通过用更好的传感器、更强的算力、更好的生成模型来帮你做数据合成、做数据增强来缓解。但真正的难点其实就是在于我们没法预期的这种无穷无尽的、完全不在训练里出现过的那种**corner case**（极端情况: 极少发生或不常见的情况，但可能导致系统失效）。它的核心困难就是因为物理世界它本身是有无限可能性的，而且有很多是没有办法被提前标注的。这种没法提前让机器人学习的方式，比如说你机器人执行任务的时候，桌布上有一个小的褶皱，你的杯子可能放置不稳，可能有一个透明物体反光，它刚好干扰了你的相机等等。这些微小的物理变化，人类其实可以凭直觉和丰富的经验去瞬间适应的。但由于非常依赖于数据驱动，AI大模型面临这些新的挑战，它不一定能真正去感受到。另外一个就是在长程任务上，其实有很多微小的长尾物理扰动造成的这种微小误差，它会像滚雪球一样被放大。每步的误差到最后可能就直接导致了你任务的失败。所以这个事情就变成了，我们怎么让这个模型真正去解决这些没法在数据里包含的这种corner case。核心还是要构建我们模型这种能够理解物理常识、能够有物理直觉的模型基础，能有这种空间的理解、空间推理能力。所以这个核心还是说，我们可能依赖于机器人真实的数据，我们依赖于人类的视频数据等等。我们要把这些数据合在一起，用更大规模的、更丰富来源的数据，质量更高、更多样的数据，去让机器人从这个学习过程中能够理解这些物理过程。但是这种海量的、和真实世界去交互的**高保真数据**（High-fidelity Data: 具有高度准确性、细节丰富度且能真实反映物理世界特征的数据），又是目前我们比较稀缺的。但这并不意味着说我们疯狂地在现实世界去采数据就结束了。这件事情远远变得更复杂，它还不是一个数据量的问题，它是一个数据工程、是一个数据管线的问题。这里面也包含了如何降低我们数据采集的成本。

Kay: 我觉得难点是挺多的，可能一个最难的不太够，得说两三个。昊刚才说的长尾我觉得是难点之一。从一个研究的角度来看，这反映的是测试一个机器人模型的表现是很难的。大家看新闻的话，可能经常看到哪家公司开放了一个新的大语言模型，它在某一个榜单上面变成了第一名。而在机器人界，其实这是过去数十年来一直没有办法在真机世界中做出这么一个榜单，就没有办法非常客观公正并且可重复地告诉你，在这么一个情况下，模型A比模型B的表现是要好的。做这么一个榜单的难点，就跟刚才昊说的一些问题有很大关系。比如说一个模型它可能在某些地方表现好，其他地方表现不好，你要包含多少种不同的情况呢？如果要有一些边边拐拐的情况，那可就无穷无尽了。你这个榜单要把多少东西放进去？同时如果你想做一个真机的榜单，维护这些真机，包括真机的一些细节，对于模型的表现会不会造成影响，都是要纳入考虑的。因此到现在机器人整个业界在发表论文的时候，很多时候都是依靠自己既作为作者又作为一个测评官，说我现在开发了一套算法，我们觉得它在这个任务上比之前的一些方法要好一些。这个在某一种程度上，使得我们领域的发展缓慢了一些，因为你真的很难去分辨模型A、模型B，算法A、算法B到底哪个是好。尤其是在最理想的情况下，一个特别好的表现的模型是非常明显的好，而真实世界中，实力相近的竞争者之间互相消耗，难以分出明显胜负。你今天做的一些模型上的改动、数据上的改动、算法上的改动，看上去好像改好了吧，你怎么验证？就是这个对于日复一日的研究上，是有一些困难的，就是没有这么一个特别客观、特别全面的测评。据我所知的话，业界里肯定有不少人在探索用**模拟器**（Simulator: 能够模拟真实世界环境和物理交互的软件系统）或者说做一个第三方的像打擂台一样的东西。我觉得这也是大家在这个难题上开始进行的一些探索。

泓君: 之前我看中国有机器人运动会，还有首届的，大概是一个机器人的展会，机器人有表现出很多的能力，比如说踢足球、赛跑，还有做一些具体的任务。怎么样去评判一个机器人，它的技术是好还是不好？从这些demo上能看出来吗？

王昊: 我觉得是比较困难的。这也是我们做机器人、做具身领域的同行，大家感到的一个难点和一个痛点。很难有一个统一的评测标准，可以让大家没有那么高成本地去评测你的模型结果，而且可以比较公平地去评测。这可能是一个具身的难点，因为它天然是和现实世界绑定的。所以最好的评测就是要到现实世界去评测，但是我们又很难搭建一个公平的决斗场。所以这个是比较困难的地方。但仍然还是有些办法我觉得可以去评测的。比如说我们有一批开源模型，大家可以在自己的机器人本体上，去看不同的模型在学习相同的任务的时候，所需要的数据量，它所展现出来的这种泛化能力，它所展现出来这种推理能力，这个是可以去评测的。另外，如果对于大家不同的机器人公司，可能合理的是我们可以把它应用到一个具体的场景里面。我们去看不同模型的表现怎么样，这个可能还是比较公平的。因为你在真实的世界里当你去应用的时候，它所展现出来的这种多样性、泛化性，或者这个环境是非常非常随机的，所以它是最能体现你模型能力的。

Kay: 另外的话，刚才昊也提到了，其实数据的质量和数据的数量，在我们2025年还是一个鱼和熊掌不可兼得的事情。如果你想要非常高质量、对你的模型特别有用的数据，你是需要精心地去设计、勤勤恳恳地去清洗。就比如说大语言模型训练也是对这个数据的质量很敏感。我们机器人的数据都是自己收的，你先收上来然后再做清洗。就是这些细节都要到位。一旦要对细节有追求，上量可能就有些难度。所以变成了我们是需要又多、又好、又快的那种数据，才能让我们的模型更好，但实际上都没有那么简单。最后一点的话，做机器人，尤其是大家现在还在奋斗着做真机机器人，一定是对机器人行业很有热爱。大家应该都知道，真机机器人硬件的维护我觉得是很劝退的，对很多尤其是新人。当我看到他们开始研究机器人的时候会发现，就是没有那么好的一个上手就能用的机器人。不像可能一些纯软件的行业，你可以下载下来一些代码直接跑。机器人到现在依然没有一个大家都非常认可、都愿意拥抱的一个硬件平台。甚至这个硬件应该长什么样，其实业界到现在也还是有争论、有探索。我觉得这些客观条件使得我们的研究和我们的领域有了一些门槛。

泓君: 对，你说到这个让我想起我有时候会跟机器人的一些研究员们一起聚会，大家提到白天在干嘛，说一天啥都没干，在修手，在拧螺丝，因为手不太稳定。

Kay: 是的，我其实刚开始读博的时候我不是搞机器人，以前是搞理论机器学习的。当时觉得自己非常年轻，就觉得整个机器人的项目怎么会用这么长时间呢？我也来做一做。一做就发现天天都在拧螺丝。

### 数据挑战与机器人学习

泓君: 对，然后我们刚刚提到了数据的问题。其实我觉得数据也是挺重要的一块。昊我知道你们的这个模型它是有数万小时的**多模态数据**积累的。Kay你刚刚提到了PI的模型，你说你们会需要采集这种高质量的数据，然后要自己去收，自己要去做数据的清洗。

Kay: 就是稍微纠正一下，我觉得这个并不光是PI的模型想要高质量的大量数据，这个可能是有一些行业的共识。基于大语言模型的成功，对数据的质量是比较敏感的。

泓君: 所以你们的数据是你们自己采集的还是第三方公司？

Kay: 我们有很多自己采集的数据。

泓君: 行业里面会有一些专门的第三方公司去提供数据吗？

Kay: 我倒是认识一些朋友，他们自己创业就是为机器人提供一些真机数据。但是可能这个不是我们现阶段研究的重点。

泓君: 你觉得多少的数据可以构成一个优秀的大模型？

Kay: 我有暴论在这个上面。我之前就经常和朋友聊天，我也很好奇大家怎么看。我会想说一个人的一生，假设是100年的话，大概我们很粗略很粗略地算就是100万个小时。我觉得现在在我的目所能及，或者我公开信息看到的范围里，好像没有人有一个100万小时的数据集。我是这么猜想的，我会觉得什么时候我们能够收到100万小时等同于一个人一生的物理经验的数据，我觉得可能我们才开始后面的探索。我想之后如果真的能够把机器人广泛地部署在真实世界中的话，其实也许有一天收100万小时的数据也就是几天的事情。这个也是来自于我和一些做语言或者图像或者音频生成的朋友的吐槽。因为他们动手就很豪气的：我今天要做这个任务，给我来400万小时的数据，明天就要收，后天就要洗。我说400万小时数据，机器人咱做了多少年，好像还没这个量。

泓君: 对，为什么你的判断是100万小时？我还蛮感兴趣的。因为以前我们在说人要做好一个什么样的事情，要成为一个领域的顶级的学者专家，大家有一个一万小时定律。包括我们去看小朋友他是怎么样学会吃饭的，因为我家有个宝宝，他在非常小的时候，他拿着那个勺子，他把饭喂到嘴边，他其实都是喂不准的。所以最开始小孩吃饭他都会洒得满地都是。然后你看他努力地把饭送到嘴里，就每天练习这个事情，也是还蛮励志的一件事情。但是我们说为什么机器人要训练出这样的一个灵活度出来，它可能就需要比人大得多的数据量级？

Kay: 这个问题我有一些很粗糙的想法，我也很好奇昊会怎么看。你可以看到PI自己在做研究的时候是有这么一个重心，叫做**跨本体迁移**（Cross-body Transfer: 在不同形状或类别的机器人之间共享学习经验，使新机器人能更快地学习任务）。就是说我们希望在不同形状或者类别的机器人上收集的数据，能够让一个新的机器人更快地学会一个任务。其实我觉得这个很粗糙地讲的话，可能和人的基因和天生的躯体有一些关系。毕竟我们生出来其实已经有一个非常好用的感知器——眼睛。现在也没有哪一个照相机敢说自己能对标人眼，都是在往那个方向去努力。还有我们的这些关节，就是这些物理的身体的、天生的这些东西，我觉得就是人的行动力的基石。我个人是比较相信说，我们可以用一些算法上的优势去概括一些硬件上和人的不足。但我觉得罗马不是一天建成的，我们也不能指望机器人马上就能像宝宝一样学得这么快。机器人如果要达到一个快速学习到新任务的话，它之前还是得有非常多的积累。刚才昊也提到，并且我在PI也看到很多某一个任务、某一个机器人上收集的一些数据，其实是可以帮助一些新任务做得更好。新的任务也许就不用那么多数据。我觉得我们的努力方向是最后能达到那个。

王昊: 其实和人比的话，我觉得对机器人来讲还是太不公平。核心就是因为人其实有**预训练**（Pre-training: 在大量数据上预先训练模型，使其学习到通用特征和知识，再针对特定任务进行微调）。整个生物界在大规模进化过程中，其实有两个非常核心的不一样。一是在进化过程中，人积累了很多经验的东西，关于与世界交互，这种对物理的认知这个过程，很多关于物理世界的应对策略其实都写到了基因里。另外一方面，人其实也是在不停进化自己的硬件的。整个生物界非常非常明显，大家能不用智能解决的东西，就尽量用硬件解决。所以可以看到很多生物体都会进化出一些结构，比如说大肠杆菌可能它就不用长眼睛，它有对化学、对温度敏感的一些感知就够了，它就可以去适应它周围的环境。其实我们现在正在做的这个事情，就是在帮助机器人做它的预训练模型。虽然看起来我们要覆盖过去人类进化的整个几亿年的时间，但其实也有不一样。第一是因为我们机器人是可以进行大规模复制的，让不同机器人之间可以共享它们的经验。其实我们就可以快速构建起来所谓的机器人上的预训练模型，让它能够具备对物理世界的感知、理解的能力。当然这个过程也是让机器人越来越熟悉自己的身体。刚才Kay也讲说跨本体泛化，我们需要让不同的机器人适应不同的身体，并且让它能够互相感知到这个身体的不一样，这个很重要。其次，在人的学习里面，所谓一万小时，其实还是有很多不一样。就是人其实没有说在一定时间内专门去学这一个任务。比如说人不会用一万小时就学一个任务，学完这一个任务，再学下一个一万小时的新任务。其实不是这样。观察宝宝就会非常明显，你会发现你开始教宝宝去做某件事情的时候，比如说拿一个东西，他可能非常非常困难，精确度也不够，也抓不准。所以可能宝宝去把这个东西扔到一边就不管了。这个过程中他可能去玩别的事情，玩玩具、搭积木什么的。过一个月之后你会回来发现，这个任务他在这个月里并没有怎么学习，但是他已经会了，他执行的精确度也很高很高了。这也是体现了人在后天和环境交互过程中学习，其实也是一个并行多任务的学习。他可以从不同任务中学习到这种底层的共同的物理结构。这种共同物理结构就会帮助他在学习新任务时减少他的数据量。所以现在我们在做机器人的训练的时候，其实是一样的特点。我们用尽可能多样的、覆盖各种能力的任务，去构建这个大的数据体系，构建机器的能力。它也许在学习新任务的时候，需要的数据量就会大大减少。所以我们刚才说，我们为机器人定义了百万小时，核心就是解决两个问题：第一是覆盖过去人类长时间进化的预训练过程，我们需要通过多样的数据去帮助机器人构建这种基础能力。第二是在学习新任务的时候，其实我们也是要利用它在学习大量旧任务中形成的这种通用的能力，让它能够泛化到新能力上。所以这个还是在数据上，在时间长度上可能不能完全类比，但是我觉得这个学习历程和背后所反映的规律可能是一致的。

泓君: 我就很好奇大家在真实世界里面，就你们自己采集到的这个数据量大概它的样本有多大？训练一个模型整个的数据会占到多少的一个成本？大概就是想知道数据有多贵。

Kay: 在π0的时候，我们当时做了一个比较简单的统计，说π0使用的数据其实比之前谷歌研究院他们说的加在一起还要多。即使π0发表的时候，PI还是一个很年轻的刚刚初创的企业。我觉得其实这说明了两件事：第一个就是那个时间点采集的数据量确实非常大，而且之后其实也一直有在往里面增加数据。而数据的成本以及数据的量，我觉得是在实时变化的。在最早的时候，可能谷歌研究院的大家在最开始探索的时候，要花很大的功夫才收集到这么一些数据。和后来渐渐地他们有了经验，和更多的人都有经验以后，在PI收、其他公司收，就会越来越简单。成本应该也是能够得到控制和降低的。

泓君: 你们现在用合成数据用得多吗？我知道业界很多都会用合成的数据。前几周谷歌发布了它们的**Genie 3**（Google's Genie 3: 谷歌发布的一个世界模型，能够从少量数据中学习并生成连贯的模拟世界，支持交互式控制），就是我也听到了两派不同的观点。有一派它会认为Genie 3对机器人是有非常大的帮助的，因为这种世界模型它的数据是有用的。但是另一派就会觉得这个数据质量还是不够好。

Kay: 我觉得可以分成两个问题：一个是我们之前做了什么，和对整个领域来说什么东西很有用。π0.5应该有一篇后续的论文探讨了一下我们对π0.5训练中的一些考量。其中有提到过我们在π0.5的时候引入了一些网络数据（web data）。这个我不知道严格意义上算不算是合成数据，但确实是希望通过引入一些外界的各种各样的知识，可能不是直接地告诉机器人你应该做什么、应该做这个动作，而是去给它一种通用和通感，是有这么一些希望引入的。而之后合成数据这个事情，对于领域究竟该怎么发展，我觉得没有一个很清晰的定论。到现在大家有些人觉得，如果能够大批次地生成又成本低廉又有可控性的数据，而且它对机器人有用的话，那会是很好的。但是其实现在一个比较大的困难可能就是怎么样弄出这些数据，怎么样证明它有用。这个还是比较前沿的研究问题。

王昊: 其实我觉得现在在头部的机器人公司的数据量，大家因为有真实物理世界限制，大家可能都集中在几万到几十万这个范围内。但其实这个和真正我们训练像GPT-4这样级别的语言模型去比，其实数据量还是少很多。我们其实除了用现实世界真实数据、机器人上的数据，这个肯定是重要的，但我们其实也会用一些其他方面的数据。但是每一类型的数据肯定都是有自己的问题的。机器人的真实数据，它当然是需要比较贵的。你需要机器人硬件、场地、操作员。收集速度也是一个制约。所以在这里其实大家有很多改进，可以依靠机器人本体，你也可以做出相对来讲比较低成本的一些本体，甚至你可以做出一些不是那么本体，但是有一些传感器的穿戴设备，都是可以去采集的。其实也可以做一些合成数据，我们其实也用了很多生成模型去做。但合成数据主要去解决一些视觉上和现实分布差异，可以去做这样的缓解。但它其实很难去生成带有物理交互过程的这种数据，这种数据还是得来于现实世界的采集，这个是一个困难。但还有一类数据就是现在人类视频的数据，这个规模是非常非常大，多样性也很多，其实成本相对来讲也比较低。包括我们、包括很多公司也在做这方面的积极探索。但这方面数据什么时候能够真正地帮助机器人能够去做这种动作集的生成还是很困难的。但反而现在可能从视频数据里学到多的还是在动作意图上。从人类的视频里学到模型一些高级的语义理解、一些任务规划，但是这个规划可能是通过视频的方式而不是通过语言的方式去学习的。包括像Genie 3，其实我觉得它是一个非常非常好的一个工作，但它又是从互联网，其实核心是从游戏环境里面学到的大量的高质量的数据，通过视频生成的方式，同时可以做视频生成，也可以做一些动作的控制。其实它是可以未来是一个很好的方向。但是虽然这个环境可能是相比于现实环境有一些简化，但它仍然部分的可以做一个所谓的训练的环境，可以帮你去做这种交互。所以在数据上，我们其实需要去做的工作还是很大。我相信数据的投入在每家公司里的占比可能都不太一样。这个其实也取决于整个公司的综合实力、你本身的运营能力，然后你的硬件的水平，你对整个数据的规划可能不一样，会让你的数据成本不一样。可能在中国和美国硬件成本、人力成本是会比较大的差异。比如你在同一个地区，可能你的运营能力以及背后你可以去支持你数据跑这些自动化的模型，数据过滤、清洗、任务生成、分发这些能力，以及可能你要去不同的场景里面去采集，这些场景的快速实现、搭建以及恢复能力，可能这都会影响你的数据成本。

泓君: 所以你们公司的数据成本类比于其他机器人公司，大概是在一个什么样的水平上？

王昊: 这个其实还很难在不同公司之间进行比较，因为大家可能对数据质量的要求、对多样性的要求可能不一样。但是对于我们公司来讲，数据肯定是在我们整个研发成本里占相当大的比例了。

### 自变量机器人的开源模型Wall-OSS

泓君: 好的，了解。昊，我们在我们的播客发出来的时候，应该你们Wall-OSS的开源模型是正好上线了。你要不要跟大家简单地介绍一下你们的这个开源模型是什么，以及它的行业特点是什么？

王昊: 其实我们这次也是持续地发扬开源精神，也包含了吸收了很多经验。所以我们这次也是用了大概几万小时的真实世界的数据，训练了这样一个具身的基础模型。我们是在一个统一的框架下面，它既可以去做这种**思维链**（Chain of Thought: 大模型在解决复杂问题时，通过生成一系列中间推理步骤来得出最终答案的能力），然后也可以去做动作的生成。我们基于已经训练好的基础的视觉语言模型去做扩展，让它具备比较强的视觉理解、空间推理、多语言的指令遵循能力。同时它的动作的生成精度也比较高。这个可能是我们观察到目前具身的开源模型上还比较欠缺的一些能力。我们也希望我们这次开源能够对具身上面有一个比较好的补充，让大家可以更好地用我们这样的基础模型去做一些长程任务，去解决一些复杂任务。可能去天然地解决这些长程任务就需要更好的语言遵循、更好的空间以及因果的推理。我们也希望我们这种**端到端**（End-to-End: 模型从原始输入直接输出最终结果，中间不经过人工干预或模块划分）的推理规划加动作的执行模型，可以发挥它更好的作用，可以被社区用起来。

泓君: 所以你觉得你们模型主打的一些点是什么？因为其实我觉得行业里面做模型，大家的方向都还挺不一样的。比如说有一些它是专注在精细的操作上的，包括像谷歌它们的模型很擅长于这种折纸的这些动作。PI它们其实整个是希望有更强的泛化能力。如果用一句话去总结你们的优势，你觉得你们是什么最关注的点在机器人领域？

王昊: 我们最关注的点是机器人的泛化和它的长程任务的解决能力。长程任务的解决能力其实解决长程任务也就意味着它一定得有比较强的泛化能力。因为解决任何一个长序列的任务，它背后面临的都是变化的场景，这个任务可能遇到各种失败情况，各种没见过的操作对象，所以都需要它有很强的泛化。

泓君: 对，然后我记得刚刚你其实有提到，说机器人要解决一些长而复杂的任务。长而复杂，能不能给大家举一个例子？

### 机器人长而复杂任务的定义与实践

王昊: 其实我们真实把机器人用到任何一个场景，它都是长而复杂的。比如说我要完整地去把一个桌子收拾好，把一个餐桌收拾好，它就是一个长而复杂的任务。在现阶段来讲，这是因为你要操作对象的种类很多。你可能有硬的东西的操作，比如说餐具。然后你可能有一些液体需要去操作，比如说你要把一些食物残渣、把一些流体要倒到固定的地方。你可能需要有很多不规则物体的处理，比如说垃圾、残留物等等，要去理解这个过程。因为你可能需要把不同的东西放在不同的位置，以及还要比较小心地去处理可能洒出来或者其他的情况。当然也有很多柔性的东西需要去处理，比如说你要去擦桌子，你要去把一些毛巾折叠好。就算是这样一个收拾餐桌的任务，你看到里面都是一个长而复杂的任务。它在执行这个任务过程中并没有一个固定的顺序说先要去做谁、后要去做谁，反而它都是在一个长程任务里面把各种子任务给穿插起来。所以人其实是很难划分每个任务应该的边界是什么。所以这种任务就得靠模型端到端自主去决策、实时去规划，把整个任务完成做完的。

泓君: 长而复杂的任务。刚刚你提到了倒垃圾的场景，你们实验室里面实际训练跟看这个机器人它做任务做得怎么样的？你实际训练的是一些什么样的场景？

王昊: 我们实际训练其实还是以家庭的场景为主，也包含了其他的一些场景。但这种家庭其实基本上已经包含了具身需要去解决的所有任务。所以刚刚像收拾整个餐桌、布置餐具、收拾整个卫生间、收拾房间，这些都是我们的训练任务。我们也实实在在地看到机器人在处理这些长序列的闭环任务里面，体现出来这种操作能力、体现出这种算法能力，还是非常让我们的信心大增。我们也是希望说，我们也能借助我们这个开源模型，让大家可以更多地看到现在的这种基础模型在解决这种长程任务、这种泛化场景的时候所体现出来的能力。

### 开源模型的意义与价值

泓君: 对，然后我注意到不管是PI还是自变量，其实大家都在做开源模型。能不能讲一下为什么大家想做的是开源？开源对整个生态的好处是什么？

Kay: 我觉得能和业界社区分享一下这个模型，并且最好能够帮助到大家很快地上手，可能也是在变相地降低机器人模型研究的一个入门门槛。其实在公司内部，开源它也是一种过程，就是从决定要开源了，然后抽调大家刚刚做完发表，就去重构这个代码，然后做测试，再和社区的一些人沟通，说能不能跑得起来。它不比一般工作简单，但是真的能够看到模型在一些我们自己都没想到的机器人上面跑起来了，然后别人拿它做很多不同的实验，还是很开心的。我觉得现在这种开源的氛围大家都很乐意去开源是很好的。

泓君: 我看得出来你很热爱机器人。

王昊: 其实我觉得开源一直以来是非常重要的事情。第一是其实很多时候我们可以站在巨人的肩膀上继续前进。首先我们可以基于别人已有成果去做更多的改进。这个社区的反馈其实也会帮助到这些做开源的公司，可以吸取一些经验，把这个技术路线思考得更加深入。可以看到开源这件事情对于一般的高校研究或者是一些小型的企业，它可能没有能力去做基础模型，所以它可以快速地用这些基础东西做应用，把它用到各个方向。其实这也是一个非常重要的事情。AI的研究我觉得跟大模型之前有很大不一样。过去我们可以看到研究其实是非常非常离散的，就是在真正形成一个社区之前，可能研究的就是两三个人，大家疯狂地做一个算法，可能就是以论文发表作为第一要务，要占据这个技术的主动权。但有了这个社区、有了整个开源体系之后，大家更在乎的是说我们怎么在一个工程化的体系下把这个工程技术搭好，怎么能够把这个设计做得更加繁荣，我是通过什么方式能够给社区做贡献。他的荣誉反而来自于这样的事情。这样就会促使你的技术不停地往前发展。所以我觉得开源是一个非常非常好的事情，既可以从开源里学习到新的东西，也可以看到你开源的东西可能会对别人有些帮助。

### 模型架构与技术路径的思考

泓君: 大家觉得现在模型公司在决定一个模型的好坏，它的一些核心因素是什么呢？就比如说刚刚其实我们有提到大家也在拼自己采集的数据的质量。同时我知道大家在模型层可能有非常多的不一样的技术路径，比如说是不是用高频控制的方式？是不是用**System 2加System 1**（System 1 & System 2: 心理学概念，System 1指快速、直觉的思考，System 2指缓慢、理性的思考，在AI中常用于分层模型设计）的两个System的架构？大家要不要聊一下模型层上大家不同的技术路径以及你们看好的方式？

王昊: 从自变量机器人公司的角度来讲，其实我们是非常非常相信数据驱动的端到端模型搭建方式的。其实我们马上要开源我们的OSS模型，我们也是基于这样结构去构建的。不管有多少模态，不管你是语言、视觉还是动作，它们都应该在同一个空间下被表征、被对齐的。分层对它来讲，就是一个非常不利的因素。所以我们应该尽可能避免人类的分层带来的信息损失。但是另外一方面来讲，你都端到端训练了，我模型可以做到很大，我可以做到几百亿、做到千亿的这个具身模型。真正要去使用的时候怎么去使用？不可能真正在端侧需要非常高频控制的时候去部署这么大模型。所以在推理的时候，我们反倒觉得你这个模型是可以分开的。你可以把更慢的任务过程可以放在云端去处理，更快的物理过程可以放在端侧。让它们端到端去训练，能够有一次这个梯度回传去更新整个系统参数的这个过程是非常非常重要的。

泓君: 对，我们说其实两层架构的模型它有一点点类似于人脑的大脑跟小脑。比如说有一层它是负责理解与规划，还有一层就是负责高频输出的控制。就有一点类似于大脑它去掌管认知跟决策，小脑去掌管运动控制。但是为什么你们不用这样的一个架构？能不能跟大家解释一下？

王昊: 我们是一种端到端的训练。其实你们可能很难在模型内部把某些参数完全分成System 2这种慢系统，把某些分成快系统、直觉系统。但是我始终可以训出一个非常大的端到端模型，它可以具备非常强的具身通用能力。这个具身通用能力既包含理解、推理，也包含动作生成。但你实际在部署的时候，其实有很多方式可以，比如说把擅长动作部分给蒸馏压缩出来，然后擅长语言推理、视觉推理的部分给它放到云端等等。利用类似的方式在部署的时候，在推理的时候做很多优化。在训练的时候它还是一个统一架构。

泓君: 就推理跟控制在训练的时候是在一起的？

王昊: 在一起。

Kay: 我们现在还是非常开放的一种态度。我们觉得现在机器人大模型还没有达到一个像**GPT-2时刻**（GPT-2 Moment: 指某个技术领域发展到GPT-2发布时，展现出规模化潜力，预示着更大突破即将到来的关键阶段）。我们希望能够尽快地达到这个地方，但是现有的模型、现有的表现还是有一些差距。所以大家相当于是抓紧了最重要的一个信仰，可能就是说数据和数据驱动的算法是我们最看重的东西。但是具体怎么样去设计这个算法，这个模型的架构如何搭建，甚至包括这个硬件系统是怎么设计的，数据是怎么收的，我觉得一切其实还是为了数据驱动在服务。所以它到底是把推理跟控制分开做成两个，还是说端到端地解决完全是放在一起，你觉得这个它其实反而不是现在最重要的问题。可能这几种路径都可以。

泓君: 因为现在我觉得整个机器人模型领域大家的技术路径看起来也是没有统一的。

Kay: 我觉得这句话非常有意思。因为我刚刚还在和一个学弟吃饭的时候我们还在聊，因为从大概三四年前开始合作到现在的人，就会感到整个行业是有变化的。以前的时候在学术界会更加的散一些，大家的方向、想法、算法，还有关注的问题都非常的不一样。而现在自从**VLA**（Vision-Language-Action Model: 视觉语言动作模型，一种能够同时处理视觉信息、语言指令并生成相应动作的AI模型）出世以后，它的流行程度还有很多人的跟进，让我们反而是觉得现在东西有点趋同化。

泓君: 你觉得趋同表现在哪些同？就是从哪些各种各样不同的方向变成了同一个方向？这个“同”指的是什么？

Kay: **模仿学习**（Imitation Learning: 通过观察人类或专家演示来学习任务和行为的机器学习方法）算是一个。我是2018年开始做模仿学习的，那个时候没有很多研究，也没有很多真机的研究可以follow。在当时会觉得这是一个不那么主流或者大众的一个想法。因为当时2018年什么情况呢？就是我想想看，当时是不是已经看到了**波士顿动力**（Boston Dynamics: 美国一家著名的机器人公司，以开发先进的动力平衡机器人而闻名，如Atlas、Spot等）这个跳。这个跳毕竟人形机器人即使到现在直接用模仿学习可能也是一个比较难的挑战。有很多其他的，比如说传统方法，一般说笑话就会说60年代我们机器人就把人家送上月球了，人家就朝着火星去了，这都是成功的机器人的方法，它就直接变成了火箭学科了。只有我们这些还没搞明白怎么把它做成功的，还在这里搞机器人。所以是有非常多的一波一波的探索，就包括2000年左右这些自动驾驶的探索，还有后面这些人形机器人波士顿动力开始为首的这些。2000年初，其实还有一个人**Willow Garage**（一家曾在美国机器人学术界非常有影响力的公司，推动了机器人操作系统ROS的发展），也是在美国机器人学术界里面比较有名的一个，当年很红火的一家创业公司。当时他们就主推了一个叫**PR2**（Personal Robot 2: Willow Garage开发的一款移动操作机器人平台，旨在推动机器人研究和应用）的机器人，也是算是移动但不是人形机器人的一个老祖宗。所以就是从之前那个时代来看的话，我会觉得大家的研究方向好不一样，有的人做人，有的人做车，还有的人做手。这些都是散布在机器人行业里各处的。而现在确实因为大模型的红火，通用性的被强调，也有很多人开始想说能不能把这些东西糅合在一起。这算是一个非常有意思的趋同化的发展。

泓君: 对，但是我看到其实整个现在业界，就如果我们来从创业方向看的话，大家又有很多的不一样。比如说有足式机器人，下面是一个人形机器人自己走路的。还有轮式机器人，它可能就注重于手部的操作，它所有走路的部分它用轮子滚就可以了。很多公司也在想，我能不能有一个上半身跟下半身都能同时操作的机器人。因为很多机器人它可能要么上半身要么下半身。但整体上大家可能都想把模型做得更大，功能做得更通用。

Kay: 对，我觉得其实“同”可能说的是很多原来做在不同形态机型上的人会用不同方法，现在大家都非常开放，说我们要不要试一试视觉语言的这种大模型。而且就是上半身、下半身，其实π0.5做的差不多就是这个事。

### 机器人领域的“GPT-2时刻”与未来展望

泓君: 是。刚刚Kay觉得现在整个机器人模型连GPT-2它的水平都不够。我不知道昊你是怎么看这个问题的？

王昊: 我认为是到GPT-2的水平了。这个类比的话，GPT-1基本上是一个概念验证，就是说我可以通过预训练加数据的方式可以去处理一些任务。但到GPT-2的时候，就是我们开始验证它规模化的力量的时候，通过大幅地增加，比如模型参数和训练数据，其实就可以展示出这种规模化带来的所谓的能力的一些提升。可能我们规模再做得更大，就能到GPT-3，就能看到一些能力的涌现了。所以我会觉得现在就是在GPT-2的这个阶段。其实我们现在基本上已经知道**规模法则**（Scaling Law: 指在AI模型训练中，模型的性能会随着计算量、数据量和模型参数量的增加而呈现可预测的提升规律）它是唯一的一个可靠路径了。所以我们就是要在这个阶段去疯狂地积累数据、提升模型规模，同时去搭建真实具身的这种基础设施。所以在GPT-2的这个阶段，我觉得是比较客观的一个类比。

泓君: 就是你觉得机器人领域从你定义的这个GPT-2到GPT-3大概还有多长的一段路要走？大概还需要多长的时间？

王昊: 我们现在在谈我们在机器人领域的GPT-2到GPT-3，有一个不一样跟语言模型来讲。当时在做语言模型的时候，大家因为不知道这条技术路线是否真的可以走通，所以其实中间产生了很多分散式的探索又汇聚的过程。但现在由于我们明确地知道，而且我们看到了这种规模化带来的提升，所以对于我们来讲，我们的路径和我们的目标更加明确，也更加唯一。所以我的预测会在一到两年的时间，我们完全可以达到GPT-3的这个水平。

泓君: 一到两年的时间？

王昊: 是。

泓君: 还挺快的。然后我注意到其实美国的趋势，就我们聊起机器人的话，大家其实都是想做这种通用机器人，然后再朝一个超大规模的模型，至少是大模型的方向在做。大家理想的情况，如果我觉得类比于自动驾驶的话，就是美国上来就想做的是**L4跟L5级自动驾驶**（L4/L5级自动驾驶: L4指高度自动化，特定条件下无需人类干预；L5指完全自动化，任何条件下无需人类干预）。但是我们看到中国的发展方向，就我感觉还是有很多的小而精的创业路线。就好比中国它在做自动驾驶的时候，首先可能想到的是能不能在一个园区、在一个码头，我们把这个场景先落地了，做一个非常垂直的小而精的产业。它更加注重类似于小模型的落地。我不知道大家怎么去看机器人这两种方式？因为做这种小模型的落地，其实你也是可以积累数据的。怎么看这两种方式在产业的发展上最终它的结果会有什么不一样？哪条路径能跑出来？

### 中美机器人发展路径的比较

王昊: 我觉得这个得结合中美的各自优势来看这个问题。确实美国的现在路径就是自上而下的，就是说不计成本，我先做一个接近**AGI**（Artificial General Intelligence: 人工通用智能，指拥有与人类智能相当或超越人类智能水平的AI系统）的超大模型，有了这个基础之后再去想应该怎么去做。这也是因为美国在算力上的优势，最顶级的芯片、最大量的算力集群都在美国，所以路径更加倾向于说比如用无限的算力去探索能力的边界。但中国其实芯片上确实有一定的限制，所以这也倒逼了中国的研究企业怎么思考在有限的算力下就实现更高的效率。但中国现在走小而精的技术路线，我倒不是很同意这一点。中国其实是拥有全球最大的互联网生态移动应用场景，所以这个场景优势以及中国在硬件拥有非常非常完善的产业链，这个可能是美国就没法比的。但其实国内这种顶尖的研究机构、很多非常好的创业公司，其实我们也是非常非常深刻地去从第一性原理的角度去思考，也是非常深刻地去理解Scaling Law这件事情，它其实是通往AGI的必经之路的。我们坚信必须得有一个强大的、无所不能的基础模型，你才有可能说把这基础模型用到各种垂直领域，让它得到更加高效的部署。但这个过程就是不能反，必须得有大而通用的基础，才会有小而精的发展。但实现的路径其实国内更像是上下结合、双轨并行。一方面尽可能多地去考虑场景，尤其是考虑这种通用泛化的场景可能带给我们什么。同时又去迭代我们自己的通用基础模型的能力。这样的话可能才能更快地让机器人能在现实世界获得更好的反馈。这个现实世界的部署，其实就是你能够商业闭环和形成数据飞轮的开始。

泓君: 所以你个人也是想做通用的模型的？

王昊: 对，我们一定会去做通用的模型，这个是很重要的。

Kay: 我觉得其实现在两边各自的生态环境有很多历史因素。一方面在国内经济高速发展，这么多的经验之下，很多创业的成功都是因为公司活下来了，公司商业化有东西它能够做好，促使了国内创业的人会比较从解决问题、解决用户的需求来出发。因此会很多友人去深耕这个垂直领域。之前我还在网上看看什么除草机器人爆杀这个欧美家庭，看到它以后我觉得我都给我朋友去安利，我觉得它是很擅长做这样一个商业化的事情。同时中国毕竟制造业摆在那里，而机器人确实是有很大的硬件需求的。在国内针对商业需求做硬件，这个优势基本是在现在这个阶段来看就没法想象还有谁能够比得过国内。因此在国内现在这个生态里面，有很多人的创业都是一边做着商业性的成功的保证，一边做一些探索是很可以理解的。而美国这边，之前说为什么毕业的这一年比较幸运。其实我如果早两年毕业，我当时有很多朋友他们做机器人研究做得很优秀，他们都转行了，都去做大语言模型、做强化学习、做一些机器人能让你锻炼到的技术，但并不是做机器人本身。与其说美国很多公司一直都在做大而通用的模型，不如说是这个时代恰巧让我们在2024年前后涌现了一批相信这个道路的人。而且这个恰巧的因素很多也是归功于OpenAI把大语言模型这件事情做通了以后，给整个行业的一个反思和震撼。直到现在我们在加入PI的时候和他们聊天，决定要不要去，我就在想说这个公司你们做人形吗？你们要做人形的话是不是要烧很多钱？道路怎么走？商业化怎么走？这公司怎么活下来？所以从商业化的角度上来说，其实不是那么明朗的一件事情。所以这些公司我觉得其实也是凤毛麟角，能够有这么一个时机成立，能够有人去相信它也许能把事情做成。而在这样的公司出现之前，其实美国的工业界有很多机器学习的应用公司的。其中可能比较有名，也和我们公司渊源颇长的一个公司就是**Covariant Robotics**（一家专注于AI驱动机器人自动化解决方案的公司，尤其在物流和仓储领域有应用）。这个是在伯克利的一位非常有名的教授所创立的。实际上我觉得这些个人的创业经历，有启发到后面的人的想法。因为在外界我们看来，Covariant就是因为深耕了一个商业的点，所以它商业可能做成了，但是它通用可能就没有做得那么好。所以我们公司最大的目标既然是想要做通用，想要做数据驱动，所以我们是很小心地去避免做一些短期的商业项目。就是有这么一个历史的因素导致了现在的公司生态是这样。而我觉得这种情况，毕竟一个公司还是要活下来的，就是很少见的一种事情。

泓君: 我理解Covariant其实更多的是做灵巧手，而不是在研究模型。

Kay: 这件事情有点好笑。因为他们刚刚起步的时候，它的创始人**Pieter Abbeel**（伯克利大学教授，机器学习和机器人领域的知名专家，Covariant Robotics的创始人之一），作为机器学习机器人的一个领军人物，也是有表示过他们也想做一些把机器学习机器人做到现实生活里、做到通用。因为这是机器学习给予机器人的一个机会。当然他们可能在物流上面做得太成功了，现在大家记得他们的时候就是另外一种模样了。所以他们其实内部也是在研究这种通用的解决方案跟模型层的解决方案的。我相信在早期的时候应该是有这种探索的。因为当他们开始做的时候，确实是没有什么人知道机器学习机器人的应用能做成什么样的。所以他们肯定是做了一些探索的。而且像现在的大家就是受到他们那个经验和学习到的东西的启发，才选择走了现在的路。

泓君: 所以你觉得在机器人的落地过程中，我们如果把模型比作是它的大脑，把这个手比作是它的类似于神经操控。就你觉得它是大脑的研发更难，还是操控上更难？

Kay: 都好难，每天起来就说为什么这么难，这个问题怎么这么难。但是每个人可能都是有自己的信仰的。

泓君: 你的信仰是什么？

Kay: 我是一个懒人，我就是希望一件事情能做一遍就不要让我再做第二遍。如果一个问题我可以通过一些巧妙的方法来规避它的话，我会觉得这个规避的方法就不错。实际上我自己博士期间做的研究都是围绕着怎么样用机器学习来使一个没有那么精确的一块硬件可以做非常非常精确到超毫米级别的精确。这个硬件只是一个通用的硬件，只是一个造出来方便你搭在一起的硬件。但是你能不能通过算法的力量去赋予这么一个硬件本来做不到的一些事情。算法和数据齐下以后，我在自己的博士期间相当于验证了一些，这个可以拿筷子去夹一个小球，还在空中飘一飘。这个照相机都不一定能捕捉它的动作轨迹，但是你确实可以通过算法的力量做到。所以我这个人在这方面的意见可能就是黑猫、白猫，抓到老鼠就是好猫。什么东西简单我就做什么。如果我们确实需要一个非常复杂的硬件，一个手来做一些任务的话，我觉得那可能没有别的选择，我们就上。但是也许生活中有很多事情不需要那么的复杂可以解决的话，我觉得探索它们也有很多的价值。

### 高频控制与模型预测能力

泓君: 对，我觉得非常好的点。然后我注意到在π0跟π0.5的这个模型中，其实你们在强调一个**50Hz的连续控制**（50Hz Continuous Control: 指机器人系统每秒进行50次决策和动作调整，实现平滑和实时的物理操作）。早期其实是谷歌的那个RT-2，它是一种**离散的token控制**（Discrete Token Control: 将机器人的动作指令编码为一系列离散的符号或“token”，而非连续的物理量，通常用于大型语言模型驱动的机器人）。你要不要跟大家解释一下，在整个大模型的这种**高频控制**（High-frequency Control: 机器人系统以高频率（如50Hz或更高）进行实时决策和动作调整，以应对快速变化的物理环境），50Hz的连续控制代表什么？离散的token模型的控制代表什么？为什么它是一个巨大的进步？

Kay: 其实高频控制的话，在以往的机器学习机器人应用里面是一个默认的选项。以我自己研究期间为例，几乎我们用的都是100Hz到200Hz往上的。当时在伯克利这个流派，因为出现了一些泛化性、和图像的一些需求，我记得他们在做抓取任务时会频繁使用大概5到20Hz的一个频率。也就是说一直都有很多人探索不同的一个频率。我并没有觉得50Hz很高频。而当时PI采用50Hz的话，我记得当时有一些内部的讨论，说要不要再多一点和短一点。但是可能对我们来说更重要的是想让模型一口气输出一个长度大概在一秒左右的计划。这个一秒刚好是50步，50Hz。所以是这么一个选择。实际上应该在早期的探索里可能也有人做了一些非常简单的确定性的研究，就是说如果我们换一换的话会有什么影响，可能觉得这个还不错就这么用着了。50Hz，或者说一口气生成50步的计划，这个其实涉及到了一些现在技术上的挑战罢了。因为我们在训练模型的时候，很多时候模型对这个场景没有像人这么好的认知。它的观察力是有限的，对历史的记忆力也是有限的。假设一个人有健忘症的话，他可能做一件事情经常不知道自己之前在干嘛，后面在干嘛，他可能就会比较慢。而我们在机器人身上也能看到类似的效果，就是因为现在的架构、现在的算法，导致了现在的模型它需要把现在这一瞬间的所有信息收集在一起，马上做一个决策。这个决策不一定非常优。如果它能够在这一瞬间做接下来一秒的决策，反而是一个在实际中表现比较好的一个选择。所以这个是**Action Chunk**（动作块: 指机器人一次性规划并执行一段连续的动作序列，而非每次只执行一个离散动作），也是来自于斯坦福的一个工作，就是让大家在从人类数据学习的时候有了一些效果的提升。所以就是现在变成一个行业的标准。

泓君: 我大概跟听众解释一下这个离散token模型跟50Hz的连续控制，看一下我理解的对不对。离散token模型，比如说我们在转一个方向盘，机器人给一个指令说向右转，过了一会以后它就向右转，这是一秒，然后动作执行。如果是一个50Hz连续控制，比如说一个坡，或者是一个不太平的路面上，你可以连续地慢慢地转，保证这个车它是在一个平稳的过渡的过程中。

Kay: 我觉得离散这个词放到机器人语境下确实有很多解释的。我觉得这是一种**低频控制**（Low-frequency Control: 机器人系统以较低频率进行决策和动作调整，可能导致动作不够平滑或难以应对快速变化）。这样称呼也可以。做一个决策，我就把眼睛一闭，闭眼睛执行了。而另外一方面可能就是非常高频的，就是实时观察世界中的变化，并且马上做出反馈。这种高频决策，实际上现有的大模型应该都是有50Hz左右。因为如果这个频率太低了的话，毕竟每一个机器人里面的全栈也是层层叠在一起的。很多在最底层的控制器都有1000Hz。你从一个大模型的输出可能是50Hz，甚至有可能10Hz之类的，怎么连接到下面最底层的那个高频控制里面。每个人的选择或者每家自己的栈可能长得都不一样。

王昊: 其实高频天然和大模型有一点点冲突。当然我们可以通过很多推理优化的技术，可以让模型的推理达到很高。但其实50到60Hz基本上是现在，比如说端侧大模型能够去做推理的上限。比如说大模型到百亿到几百亿参数，其实这个推理的压力还是非常非常大的，对算力的要求会非常高。但其实这个控制频率本身其实不是最重要的。核心来讲还是在于你模型在预测什么，或者模型想在输出什么。如果你的模型的预测能力越强，它其实它对未来的判断越准，你需要的控制频率反而不需要那么高。模型频率很高，控制频率很高，就是因为模型对未来的预测能力或者建模能力不够强。所以它只能非常即时地去看到当前眼前发生什么，能做出很短的动作。比如视觉的表征，因为操作其实是很依赖于视觉这个模态的输入的。但视觉现在比较大的问题是很难做到对未来一个比较精确的预测和建模。因为我们现在主要的视觉基础的编码都是依靠静态的数据，静态的图片学出来的。所以视觉上对于运动的理解和对未来运动的预测是很弱的。其实要加强模型的预测，我们可能需要加强模型的视觉预测能力，来提升它对未来的建模的准确性。这样其实你对控制频率要求可以低一些。

泓君: 视觉是指什么？比如说我现在举了一个水杯，我能不能把这个水杯准确地放在桌面上，我能找到桌面并把它放上去，这个叫视觉。

王昊: 对，这个就是完全通过视觉的观察就可以完成的任务过程。它可能不依赖于触觉。一是触觉的发展，包括硬件的成熟度，以及它能够在机器人上的部署的量还是有限的。所以这部分数据还是需要进一步去积累，它可能才能达到视觉的这样的一个数据规模。然后才有可能进入到基础模型里面，做一个主要的传感的来源。

泓君: 那力的反馈，就是触觉这一部分，如果现在没有力的话，机器人它在抓取上会出现问题吗？

王昊: 对机器人来讲，它其实有一些额外的信息可以去包含力的信息。只是这个过程是通过观察来完成的。就比如说我要抓取一个东西，我怎么知道我抓紧没有？对机器人来讲，它其实可以通过我们在**末端执行器**（End-effector: 机器人手臂末端执行具体任务的工具，如夹具、吸盘等）上很近的这种摄像头去观察，比如说物体的形变程度，观察因为抓取而导致的这种反弹之类这样的性质，它其实就可以知道自己抓紧了没有，以及抓的力度怎么样。所以虽然看起来没有直接力的传感，但是它是通过视觉去弥补了这份信号的缺失。相当于用视觉来做这部分力的传感。

泓君: 其实我也很好奇，就是力的数据要怎么收集？就是我觉得收集一个真实世界的三维世界的数据，这个已经很难了。因为所有这个模型在网上学到的数据可能都是一个平面的东西，它要去处理现实世界的数据就已经很难了。这个力可能平时我们自己人是靠经验主义去判断的。它在数据收集上是有什么样的不一样的地方吗？

王昊: 对，其实是有很多传感器的。其实人体本身对于力的感知，它的丰富度其实是不够的。其实我们现在有各种各样的传感器，比如说指尖末端我们可以加触觉传感器，它可以获得多个方向的力。在机器人的手臂、手掌我们可以装这种电子皮肤。然后我们也可以装，比如说对温度比较敏感的这种传感器等等。我们可以在机械臂的末端装这种**六维力传感**（Six-axis Force Sensing: 能够同时测量物体在X、Y、Z三个方向上的力和绕这三个轴的力矩的传感器），在关节装**力矩传感器**（Torque Sensor: 测量旋转力或扭矩的传感器）。等等，其实我们有在传感器上有很多很多维度，可以在身体的不同部位都可以获得力的信息。传感器的发展其实还是很快的，并没有大家想象的这么不成熟。但它为什么没有真正进入模型里面？还是在于说，为什么我们现在还是依赖于视觉去做最主要的训练。核心原因还是因为人类能够留下的这个数据也是以视觉为主的。人类可能的触觉、人的力的数据没有留下来。所以我们现在要依靠人类数据、依靠人类的活动，把它泛化到机器人领域，我们又用视觉的方式去弥补了很多触觉本身的不足。其实已经可以帮助你做非常重要的任务。所以对于触觉、对于各种力的传感，我们可以持续去做它。但是它其实目前不会成为制约你整个机器人控制的绝对卡点。我们可以在做的过程中逐步去完善这份传感能力的缺失。

### 家用机器人的未来：图灵测试与商业化

泓君: 我看大家在研究机器人的时候都希望机器人能够帮大家做一些家务，比如说叠床单、叠衣服、把碗塞进洗碗机。大家觉得未来机器人在家里真的能帮我们真实地做家务？然后有一个这样的家用机器人？你们觉得还有几年？

王昊: 通用的家庭型机器人，在家里做这种服务，其实看似简单，我觉得基本上它可以作为一个完美的机器人的**图灵测试**（Turing Test: 一种测试机器智能的方法，如果人类无法区分与自己对话的是机器还是人，则认为机器通过了测试）。因为它其实已经包含了具身智能或者机器人领域所有的挑战。精细动作，比如你切菜，精细的力的控制，你去做一些易碎的东西的处理。也有非常丰富的感知，比如说油污、脏的东西或者食材。也有一些长程规划，比如说你得看着菜谱去做某个东西，你得看着说明书去使用某个电器。然后以及各种各样非常多的意外情况你都能处理。所以基本上我觉得它已经包含了具身智能要去做的所有挑战。其实完全实现这个，我觉得还是得分步走。我觉得在两三年以内，其实我们可以在半结构化的这种环境里面，不是完全开放的这个厨房，基本上我们的模型加我们的机器人可以去做一些事情。比如说帮你做一些简单的菜、洗一个碗等等，这些我觉得还行。但是说，如果要在完全开放的厨房里面去把所有的事情能做到，可能我觉得还需要五年左右的时间才能够做到这样的事情。

泓君: 五年左右实现全部的机器人，在厨房里面做饭洗碗这些？

王昊: 对，我觉得是有可能去实现的。但我们进入到这个时期，还是有很多可以去容忍的地方。比如说它已经在这些任务上都有比较好的成功率，但它也许可能会失误。但我们允许它可以和人类去协作，可以获得人类的帮助。我觉得那个时间点是可以进入到家庭里面的。我觉得乐观的一点就是，因为我觉得我们现在整个机器人发展在一个正确的道路上。在一个非常快速的Scaling Law发展的规律上。这个其实是对整个人类历史，我觉得都是很新的事情。就是似乎看起来有一个明确路径可以告诉我们怎么做。你要投入算力、数据、迭代模型的规模、模型的架构。这些能力，它其实我们可以肉眼可见的，它是在一直提升的。所以现在我们看起来很困难的问题，我觉得放在那个时间点再去看，它一定是可以被解决的。而且我们模型一定可以跨越到一个阈值，进入到新的阶段。所以我从现在再预测五年，我觉得这个预测是非常非常合理的。另外其实也需要审慎一点点，就是为什么这个时间点我觉得会放在那个时候。因为它不像纯软件那样，其实可以轻资产、快速迭代。它还是受限于物理世界的物理定律、你的硬件的发展、数据算法、供应链，然后你的商业模式，都需要去全方位突破。所以我们才有可能真正做到那个地步。

泓君: 我觉得还是一个蛮大胆的预测的。然后Kay？

Kay: 这个不就是插flag来了。还是得想一下这些问题。给自己插一个flag，好激励大家一起往前。我觉得五到十年应该是可以的。一边现在的模型的能力和现在的算法做到哪一步了，显然就是没有做到一个我们只要商业化它就能做出产品的这个地步。但是一方面确实迭代的速度非常快，大家有这么多的热情和投入，我相信就是两三年里甚至可能每一年都会有新的很大的变化。另外一方面，我们这个行业其实和自动驾驶，甚至包括火箭升天这些传统的机器人行业有一些不一样的地方。我们可能更像扫地机器人。我还记得就是最早的扫地机器人那一代，它其实没有那么的完美。更多的是一个用户习惯了它能做什么、不能做什么以后，用户搞明白了怎么用它，并且它就是一个比较好的一个商业化的典范。我们保守一点估计的话，以这个为目标，我觉得五到十年应该是非常有可能做出一个有可能偶尔会出出错，但是用户把它的错限到一个范围以后就可以接受。所以它也能成为一个可以帮到用户的一个产品。

泓君: 好的，特别好。我的问题差不多了。看看两位有没有什么你们想补充或者你们想聊的问题。

### 创业公司如何平衡商业落地与通用模型研发

Kay: 我还挺好奇，有一类以产业化为关键的机器人公司，因为它一方面想要做一些有商业化应用的东西，另外一方面它又需要兼顾怎么样把这个商业化的东西投入变成大模型的一个燃料。这方面的产业的选择或者说这种策略大家怎么看？

泓君: 你是指的大家怎么平衡落地场景跟研发？

Kay: 对，这其实是我一直很好奇的一个问题。

王昊: 我觉得这个问题还挺好的。因为作为创业公司，第一天其实我们就在思考怎么能仰望星空也能脚踏实地。由于现实的因素，不可能最后达到AGI的目标之后再去思考商业化。我觉得这个可能会有些晚。我们现在的策略就是尽可能在我们做通用模型的基础上，去场景里面去做一些事情。但这个场景必须是和我们最终想实现的这种通用的场景，它是比较接近的，它是可以去泛化的。所以我们尽可能地不会去碰那些比较封闭的场景。像一些公共的服务，像这种养老的服务，它其实都是非常好这样的场景。它和最终我们要做的通用机器人来讲，有一些类似的地方。可以认为有非常非常复杂的涉及到和人的接触，当然也涉及到没有那么复杂，比如说只是打扫卫生，只是拿一些东西，只是处理一些食材等等。从这个角度来看，其实这就是一些好的场景。其实它和你最终目标接近，你可以在这个场景过程中，第一，去不断地迭代和检验你的通用模型的能力。其实也是可以获得非常宝贵的数据反馈。但其实要保证这样的这个选择，其实很重要。还是得有一个很强的商业化路径上的定力。另外一个其实也是非常非常大的，我觉得是依靠一个公司的组织能力。其实一个公司的组织能力，或者它的一种组织结构，其实就决定了这个公司的上限，或者决定你做的事情最后能够成功的地方。我觉得这个事情一定要以通用模型、以基础模型作为你的目标，去组织你整个公司，达到一个完全没有壁垒、高效协同的一个组织。它才可能促使你在中间迈出的每一步都不会走错，最终还能促使你达到最终目标的。

泓君: 所以场景上其实你是比较注重它以后就是商业化应用，不要是一个在封闭化场景里面就能实现的一个需求的。其实刚刚我自己在问，比如说家用机器人能帮我们做饭、洗衣、叠被子，它还有多远。其实我自己有在想，这样的一个场景它能不能去养活这样的一个机器人公司。

王昊: 对，我觉得是很有希望的。现在因为机器人的整个产业的规模还没有起来，我们现在的硬件其实还是有非常非常大的成本降低的空间的。随着我们的模型水平的提升，加上我们硬件成本降低，其实在几年的这个尺度来看，用户首先从价格上对它的接受程度会变得大很多。其次从这个功能的角度来讲，其实我们可以帮普通的用户去做很多事情的时候，也是非常乐意让大家来接受的。但是现在普遍难以接受机器人进入到家庭或者其他场景，就是因为机器人似乎大家看起来它就是去比如说它跳舞，然后做一些情绪价值的陪伴，好像没有其他功能。但是这个想象空间是非常非常大的。过去可能没有机会机器人向普通用户展示这种应用的可能。但未来我觉得这种期望可以变得更大。其实这种可能性也是非常非常大的。

泓君: 对，而且我觉得挺难的一个点是可能做机器人模型的公司得烧钱烧到真的它能把一个具体的应用场景跑通。

王昊: 是。我觉得这个对于创业公司来讲，就得不停地在落地的过程中强化你的商业化能力，为你的最终目标可以持续地去准备好，形成这种闭环。因为这里面核心有一点，机器人的商业成功就是得依赖于你在现实世界的部署。你的去现实世界部署的机器人的量越大，它面临的场景越开放、越多样，其实它对你的反馈和闭环的迭代作用就越强。否则如果机器人只是在实验室或者在一些自己设定好的场景里去跑，我相信它可能永远也没法真正走向真实的环境和真实的用户场景。

Kay: 我自己问这个问题，我有答案吗？只有一些想法。答案很难，而且现阶段更多可能考虑一些算法的事情，还没有到那个阶段去考虑这些落地商业化以及从任务中得到的启示。但我相信这个方向还是很重要。

泓君: 好的，谢谢大家。

王昊: 谢谢。

Kay: 感谢。

### 节目尾声

泓君: 好了，这就是我们今天的节目。如果大家喜欢我们的节目，欢迎给我们写评论、写留言。如果你对机器人有什么样的想法，或者你有什么感兴趣的问题，也欢迎写给我们。同时下一周我会从硅谷飞到上海，去参加蚂蚁的**外滩大会**（The Bund Summit: 蚂蚁集团在上海举办的全球性金融科技会议）。我会主持9月11号的智能体大会与9月12号的投资大会。如果大家在上海，也非常期待大家能够到现场跟大家线下相聚。另外如果大家觉得我们的节目专业性过高，也可以通过YouTube或者B站去搜索硅谷101播客，我们会在上面有一个字幕版。同时本期节目的文字稿我们也会发在《硅谷101》的微信公众号上。我是泓君，感谢大家的收听。