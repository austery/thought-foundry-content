---
area: society-thinking
category: politics-society
companies_orgs:
- OpenAI
- Nvidia
- Microsoft
- AMD
- Meta
- DeepMind
- Google
- CCP
- Ministry of State Security
- NKVD
- FERC
- Amazon
date: '2024-06-04'
draft: true
guest: ''
insight: ''
layout: post.njk
media_books:
- 《Situational Awareness》
- 《The Information》
- 《InstructGPT paper》
- 《AlphaGo》
- 《Gemini paper》
- 《Chinchilla Scaling laws》
- 《MoE papers》
- 《古拉格群岛》
- 《奥本海默》
- 《Freedom's Forge》
- 《Fast》
- 《原子弹秘史》
people:
- Leopold Aschenbrenner
- Patrick Collison
- John Collison
- Daniel Gross
- Nat Friedman
- Sholto
- Trenton
- Mark Zuckerberg
- John Schulman
- Tyler Cowen
- Scarlett Johansson
- Alec Radford
- Leo Szilard
- Enrico Fermi
- George Pegram
- Richard Rhodes
- Lavrentiy Beria
- Paul Samuelson
- William Knudsen
- Aleksandr Solzhenitsyn
- Mikhail Gorbachev
products_models:
- GPT-4
- ChatGPT
- A100
- H100
- Copilot
- GPT-4.5
- GPT-4o
- InstructGPT
- AlphaGo
- System 1
- System 2
- GPT-2
- Llama
- Gemini
- GPT-5
- GPT-6
- Microsoft Office
- Apple Notes
- Stuxnet
project:
- ai-impact-analysis
series: ''
source: https://www.youtube.com/watch?v=zdbVtZIn9IM
speaker: Dwarkesh Patel
status: evergreen
summary: 本次播客探讨了人工智能的快速发展，预测通用人工智能（AGI）将在2027-2028年实现，其驱动力是万亿美元级别的计算集群。讨论深入分析了AGI带来的深远地缘政治影响，特别是中美之间激烈的AI霸权竞争、潜在的智能爆炸以及AI技术扩散和被威权政权滥用的风险。对话还触及了构建大规模AI基础设施的挑战以及提升国家安全意识的必要性。
tags:
- agi
- compute-scaling
- llm
- national-security
title: 2027年通用人工智能、中美超智能竞赛与历史的回归
---

### AI发展与地缘政治风险

未来面临的风险将不仅仅是酷炫的产品，而是**自由民主**能否存续，**中国共产党**能否存续，以及未来一个世纪的世界秩序将如何演变。**中国共产党（CCP）**将竭尽全力渗透美国的AI实验室，投入数十亿美元，动员数千人，试图在AI领域超越美国。人们没有意识到国家层面的间谍活动有多么激烈。当我们拥有真正的**超级智能**（Superintelligence: 远超人类智能水平的人工智能）时，它们甚至可以像**震网病毒**（Stuxnet: 一种复杂的计算机蠕虫病毒，曾被用于攻击伊朗核设施）一样攻击中国的**数据中心**。你真的认为这会是一家私营公司的事情，而政府不会惊呼“天哪，到底发生了什么？”

我认为这些计算集群设在美国至关重要。我的意思是，你会在**阿联酋（UAE）**进行**曼哈顿计划**（Manhattan Project: 二战期间美国研制原子弹的秘密计划）吗？对我来说，2023年是一个转折点，**通用人工智能（AGI）**从一种理论的、抽象的概念，变成了我能看到、能感受到的现实。我能看到它训练的集群，大致的算法组合，以及参与其中的人，以及它是如何发生的。我认为世界上大多数人还没有意识到这一点；大多数能感受到它的人就在我们身边。

### 嘉宾介绍与AI集群概念

今天我与我的朋友**Leopold Aschenbrenner**聊天。他在德国长大，19岁时以哥伦比亚大学的毕业生代表身份毕业。之后，他度过了一个非常有趣的间隔年，我们稍后会谈到。然后，他加入了**OpenAI**的超级对齐团队。现在，在**Patrick Collison**、**John Collison**、**Daniel Gross**和**Nat Friedman**等人的锚定投资下，他正在创办一家投资公司。Leopold，你起步有点慢，但人生漫长，不必过于担心，你会在适当的时候迎头赶上。感谢你来参加播客。谢谢你。我第一次发现你的播客时，你最好的那期节目只有几百次观看。能一直关注你的发展轨迹真是太棒了。很高兴能来。在**Sholto**和**Trenton**的那期节目中，我提到我从与他们交谈中学到了很多关于AI的知识。这个“三巨头”中，第三个也是可能最重要的部分就是你。我们现在要把所有这些都记录下来。

我想记录的第一件事是：请告诉我关于**万亿美元集群**的情况。我应该为播客的背景提一下，你今天正在发布一个名为**《Situational Awareness》**的系列文章。我们马上会深入探讨。关于这个系列，我的第一个问题是：请告诉我关于万亿美元集群。

### AI的工业化进程与计算规模的飞跃

与硅谷最近出现的大多数事物不同，AI是一个工业化过程。下一个模型不仅仅需要一些代码，它还需要构建一个巨大的新集群，需要建设巨大的新发电厂。很快，它还将涉及建造巨大的新晶圆厂。自**ChatGPT**问世以来，这场非凡的技术资本加速已经启动。就在一年前的今天，**英伟达（Nvidia）**发布了首次轰动性的财报电话会议，盘后股价上涨了25%，所有人都惊呼：“天哪，AI真的来了！”在一年之内，英伟达的数据中心收入从每季度几十亿美元飙升到**每季度250亿美元**，并且还在持续增长。**大型科技公司**的资本支出（Capex）正在飙升。

这很有趣。虽然正在发生一场疯狂的争夺，但在某种程度上，这只是图表上直线趋势的延续。近十年来，最大AI系统的训练计算量一直保持着每年增长约**0.5个数量级（OOMs）**的长期趋势。只需将其向前推演。**GPT-4**据报道于2022年完成预训练。根据**SemiAnalysis**的传闻，它的集群规模约为**25,000个A100**，大致是一个**5亿美元**的集群，非常粗略地相当于**10兆瓦**。

### 万亿美元集群的未来展望

只需向前推演半年。到2024年，那将是一个**100兆瓦**、拥有**10万个H100当量**的集群，成本达数十亿美元。再向前推演两年。到2026年，那将是一个**千兆瓦**（Gigawatt: 10亿瓦特，大型核反应堆的发电量），相当于**胡佛水坝**的发电量。这将耗资数百亿美元，需要**100万个H100当量**。到2028年，那将是一个**10千兆瓦**的集群，其电力消耗超过美国大多数州，需要**1000万个H100当量**，耗资数千亿美元。到2030年，你将看到**万亿美元的集群**，使用**100千兆瓦**的电力，超过美国总发电量的20%。这将需要**1亿个H100当量**。这仅仅是训练集群。一旦产品问世，还会有更多的推理**GPU**。美国发电量几十年来几乎没有增长，现在我们真的要迎接一场挑战了。

当**扎克伯格（Zuck）**来我的播客时，他声称AI的进展不会停滞，但会受到能源限制的瓶颈。具体来说，他说：“哦，千兆瓦数据中心，我们要再建一个**三峡大坝**之类的东西吗？”根据公开报道，有公司正在规划**1千兆瓦**规模的数据中心。那么，**10千兆瓦**的数据中心谁能建造？**100千兆瓦**的中心就像一个国家级项目。你会把这么多电力集中到一个物理数据中心吗？这怎么可能实现？扎克伯格错过了什么？六个月前，**10千兆瓦**还是热门话题。现在，人们已经向前看了。**10千兆瓦**正在成为现实。**《The Information》**报道称，**OpenAI**和**微软（Microsoft）**正在规划一个**1000亿美元**的集群。那会是**1千兆瓦**还是**10千兆瓦**？

### AI投资与商业回报

我不知道，但如果你试图估算**10千兆瓦**集群的成本，那将是数千亿美元。它大致处于那个规模，而且他们正在规划中。这不仅仅是我疯狂的看法。**AMD**预测到2027年，AI加速器市场将达到**4000亿美元**。AI加速器只是总支出的一部分。我们很有可能在2027年实现**1万亿美元**的AI总投资。万亿美元集群还需要更多的加速。我们看到了**ChatGPT**释放了多少潜力。每一代模型都将是疯狂的，并会改变**奥弗顿之窗**（Overton window: 指一个社会中可被公众接受的政策或思想范围）。

然后收入就会随之而来。这些是前瞻性投资。问题是它们是否会带来回报？我们估计**GPT-4**集群的成本约为**5亿美元**。人们常犯的一个错误是说GPT-4的成本是1亿美元。那只是租赁价格。如果你要建造最大的集群，你必须建造并支付整个集群的费用，你不能只租用三个月。

不能吗？一旦你试图达到数千亿美元的规模，你每年必须获得大约**1000亿美元**的收入。这就是大型科技公司真正感兴趣的地方，因为它们的收入也在数千亿美元的量级。**100亿美元**是可以的，它足以支付2024年规模的训练集群。当每年成本达到**1000亿美元**时，大型科技公司将真正迎来爆发。问题是，每年从AI收入中获得1000亿美元的可行性有多高？这比现在要多得多。如果你像我一样相信AI系统的发展轨迹，这并非不可思议。

大约有**3亿**的**Microsoft Office**订阅用户。他们现在有**Copilot**。我不知道他们以什么价格出售。假设你向三分之一的Microsoft Office订阅用户每月销售**100美元**的AI附加服务，那就会有**1000亿美元**的收入。每月100美元很多，对于三分之一的Office订阅用户来说，这确实很多。对于普通的知识工作者来说，这相当于每月几个小时的生产力。如果你期望的AI进步非常缓慢，那才不会达到每月几个小时的生产力。

### 未来AI能力预测

好的，假设这一切都发生了。未来几年会发生什么？在**1千兆瓦**数据中心上训练的AI能做什么？**10千兆瓦**数据中心上的AI又能做什么？请为我描绘一下未来几年AI的进展。**10千兆瓦**的范围是我对真正实现**AGI**的最佳猜测。计算能力实际上被高估了，我们稍后会讨论。

到2025-2026年，我们将拥有比大多数大学毕业生更聪明的模型。很多经济效益取决于“**解缚**”（Unhobbling: 指解除AI系统当前存在的限制，使其能够更自主地执行任务）。模型很聪明，但受限于**聊天机器人**（Chatbot: 模拟人类对话的计算机程序）的功能，而“解缚”后的AI将能够使用计算机并执行**代理式长周期任务**（Agentic long-horizon tasks: 指AI系统能够自主规划和执行需要多个步骤才能完成的复杂任务）。到2027-2028年，它将变得和最聪明的专家一样聪明。“解缚”的轨迹表明它将更像一个**代理**（Agent: 能够自主感知环境、做出决策并采取行动的AI系统），而不是聊天机器人。它几乎就像一个**即插即用型远程工作者**（Drop-in remote worker: 指AI系统能够像人类远程员工一样直接融入工作流程并执行任务）。

### AI系统集成与生产力提升

这就是关于经济回报的问题。中间的AI系统可能非常有用，但整合它们需要大量的“**苦差事**”（Schlep: 指繁琐、困难或不愉快的工作）。在商业应用中，你可以用**GPT-4**或**GPT-4.5**做很多事情，但你必须真正改变你的工作流程才能使其有用。这是一种非常**Tyler Cowen**式的观点，即技术的普及需要很长时间。我们在旧金山，所以我们错过了这一点。

但在某种意义上，这些系统希望被整合的方式，正是你获得这种**音爆式**（Sonic boom: 指技术或能力上的突然、巨大突破）增长的地方。中间系统本可以做到，但需要“苦差事”。在你进行“苦差事”整合它们之前，你将获得更强大且“解缚”的系统。它们是代理，是即插即用型远程工作者。你与它们互动就像与同事互动一样。你可以和它们进行**Zoom**通话和**Slack**交流。你可以要求它们完成一个项目，它们会去写初稿，获取反馈，运行代码测试，然后回来。然后你可以告诉它们更多的事情。这将更容易整合。你可能需要一点“**过剩能力**”（Overkill: 指超出实际需要的能力或资源）来使过渡更容易，并获取收益。

你说的“过剩能力”是什么意思？是模型能力的过剩吗？是的，中间模型可以做到，但这需要大量的“苦差事”。即插即用型远程工作者**AGI**可以自动化认知任务。中间模型会使软件工程师的生产力更高。但软件工程师会采纳它吗？有了2027年的模型，你根本不需要软件工程师。你可以像与软件工程师互动一样与它互动，它会完成软件工程师的工作。

### AI代理能力的挑战与展望

我上一期节目是和**John Schulman**一起做的。我当时问的就是这个问题。过去一年里发布的这些模型，似乎没有一个显著超越**GPT-4**，当然也没有以代理的方式与你互动，像同事一样。它们会吹嘘在**MMLU**（Massive Multitask Language Understanding: 大规模多任务语言理解基准）上多得了几分。即使是**GPT-4o**，它们能像**Scarlett Johansson**一样说话很酷（我想现在不行了），但它不像一个同事。

它们善于回答问题是说得通的。它们有关于如何完成**维基百科**文本的数据。那么，理解**Zoom**通话的等效训练数据在哪里？回到你关于**Slack**对话的观点，它如何利用上下文来理解你正在进行的连贯项目？这些训练数据从何而来？未来几年AI进展的一个关键问题是，解锁**测试时计算过剩**（Test time compute overhang: 指模型在推理或测试阶段可以利用的额外计算资源，以提升性能）有多难。现在，**GPT-4**可以通过**思维链**（Chain-of-thought: 一种提示技术，让大型语言模型逐步推理以解决复杂问题）进行几百个**token**的思考。这已经是一个巨大的进步了。以前，回答数学问题就像是盲目尝试。如果你试图通过说出脑海中第一个想到的东西来回答数学问题，你不会做得很好。

**GPT-4**可以思考几百个token。如果我每分钟思考100个token，那就像GPT-4所做的那样。这相当于我思考了三分钟。假设GPT-4可以思考数百万个token。那在一个问题上，测试时计算量就增加了4个数量级。它现在还做不到。它会卡住。它会写一些代码。它能做一点迭代调试，但最终会卡住，无法纠正错误。

### 系统2思维与强化学习

存在巨大的过剩能力。在机器学习的其他领域，有一篇关于**AlphaGo**的优秀论文，你可以权衡训练时间和测试时间计算。如果你能多使用4个数量级的测试时间计算，那几乎相当于一个**3.5倍数量级**更大的模型。同样，如果每分钟100个token，几百万个token就是几个月的工作时间。在几个月的工作时间里，你可以做的事情比现在仅仅得到一个答案要多得多。问题是，解锁这个能力有多难？

在“短时间线”的AI世界里，这并不难。它可能不难的原因是，只需要学习一些额外的token。你需要学习像**纠错token**，让你能说“啊，我犯了个错误，让我再想想。”你需要学习**规划token**，让你能说“我将从制定计划开始。这是我的攻击计划。我将写一份草稿，现在我将批判我的草稿并思考它。”这些都不是模型现在能做的事情，但问题是它有多难。

通向代理有两条路径。当**Sholto**在你的播客上时，他谈到**规模化**（Scaling: 指增加模型大小、数据量和计算资源以提升性能）会带来更高的**可靠性**（Nines of reliability: 指系统可用性的衡量标准，如“五个九”表示99.999%的可用性）。这是其中一条路径。另一条路径是“**解缚**”路径。它需要学习**系统2过程**（System 2 process: 指人类思维中需要集中注意力、有意识地进行推理和解决问题的过程，与“系统1”的直觉、快速反应相对）。如果它能学会这一点，它就能使用数百万个token并连贯地思考。

### 人类学习与AI自我学习的类比

这里有一个类比。当你开车时，大部分时间你都处于**自动驾驶**（Autopilot: 指无需人类干预即可自动执行任务的系统）状态。有时你会遇到一个奇怪的施工区或交叉路口。有时我的女朋友坐在副驾驶座上，我就会说：“啊，安静一会儿，我需要弄清楚发生了什么。”你从自动驾驶切换到**系统2**，并开始思考如何操作。规模化改善了**系统1**（System 1: 指人类思维中快速、直觉、无意识的决策过程）的自动驾驶。实现代理的暴力方法就是改进那个系统。如果你能让系统2工作，你就可以迅速跳到更具代理性的东西，并解锁测试时计算过剩。

有什么理由认为这是一次轻松的胜利呢？是否存在某种**损失函数**（Loss function: 衡量模型预测与真实值之间差异的函数）能够轻易地实现系统2思维？没有多少动物拥有系统2思维。进化花了很长时间才赋予我们系统2思维。**预训练**（Pre-training: 在大量数据上进行初步训练，使模型学习通用知识和表示）拥有数万亿个互联网文本token，我明白这一点。你匹配这些数据，就能获得所有这些免费的训练能力。有什么理由认为这是一种轻松的“解缚”呢？

首先，预训练是神奇的。它为**通用智能模型**（Models of general intelligence: 指具备广泛认知能力的AI模型）带来了巨大优势，因为你可以预测下一个token。但有一个常见的误解。预测下一个token让模型学习到极其丰富的**表征**（Representations: 指数据在模型内部的抽象表示）。**表征学习**（Representation learning: 机器学习的一个子领域，旨在学习数据的有用表示）的特性是深度学习的魔力。模型不仅仅学习统计伪影，它们还学习世界的模型。这就是它们能够泛化的原因，因为它学习了正确的表征。

### 从GPT-2到GPT-4的飞跃

当你训练一个模型时，你拥有一系列有用的原始能力。从**GPT-2**到**GPT-4**的“解缚”过程，就是将这些原始能力通过**RLHF**（Reinforcement Learning from Human Feedback: 通过人类反馈进行强化学习）转化为一个优秀的聊天机器人。这是一个巨大的胜利。在最初的**InstructGPT**论文中，比较RLHF模型与非RLHF模型，在人类偏好评分上，RLHF模型相当于**100倍**的模型规模优势。它开始能够进行简单的思维链等操作。但你仍然拥有所有这些原始能力的优势，而且你还有大量未利用的能力。

这种预训练优势也是与**机器人技术**（Robotics: 机器人设计、制造、操作和应用的技术）的区别。人们过去常说这是一个硬件问题。硬件正在解决，但你没有通过预训练进行**自举**（Bootstrapping: 指通过自身资源或有限的外部帮助启动或改进系统）的巨大优势。你无法进行所有这些**无监督学习**（Unsupervised learning: 机器学习的一种类型，模型在没有标签数据的情况下学习数据的模式和结构）。你必须立即开始**强化学习（RL）**的**自对弈**（Self-play: 强化学习中，智能体通过与自身的博弈来学习和改进）。

### 自我学习与数据瓶颈

问题是为什么强化学习和“解缚”可能奏效。自举是一个优势。你的**Twitter**个人简介正在被预训练。你不再被预训练了。你在小学和高中时接受了预训练。在某个时候，你过渡到能够自学。你在小学时做不到。高中可能是开始，到了大学，如果你聪明，你就可以自学。模型刚刚开始进入这个阶段。这还需要更多的规模化，然后你才能弄清楚在此之上会发生什么。这不会是微不足道的。很多深度学习回过头来看似乎是显而易见的。有一些显而易见的想法集群。有一些想法看起来有点傻，但却奏效了。你需要把很多细节做好。我们不会在下个月就实现这个。这需要一段时间才能弄清楚。

对你来说，“一段时间”是大约半年。我不知道，大概在六个月到三年之间。但这有可能实现。这与**数据墙**（Data wall: 指在AI模型训练中，高质量训练数据变得稀缺，限制了模型进一步提升性能的瓶颈）的问题也密切相关。这里有一个关于自学直觉的例子。预训练有点像老师给你讲课，话语飞逝而过。你只能从中获得一点点东西。

那不是你自学时所做的事情。当你自学时，比如你正在读一本深奥的数学教科书，你不会只浏览一遍。有些“书呆子”只是浏览、重读、再重读数学教科书，然后他们就记住了。你所做的是阅读一页，思考它，进行一些内部独白，然后和学习伙伴交流。你尝试一个练习题，失败了很多次。在某个某个时候，它就“点击”了，你就会说：“这下说得通了。”然后你再读几页。我们现在已经通过自举的方式，刚刚开始能够用模型做到这一点。问题是，你是否可以使用所有这些自对弈、**合成数据**（Synthetic data: 人工生成的数据，用于训练AI模型，以弥补真实数据的不足或隐私问题）、强化学习来让它发挥作用。现在，有**上下文学习**（In-context learning: 指大型语言模型在不更新模型参数的情况下，通过分析输入中的示例来学习新任务的能力），它的**样本效率**（Sample efficient: 指模型能够从少量数据中学习并泛化的能力）非常高。在**Gemini**论文中，它只是在上下文中学习一门语言。另一方面，预训练的样本效率一点也不高。

人类所做的是一种上下文学习。你读一本书，思考它，直到最终它“点击”了。然后你以某种方式将其提炼回**权重**（Weights: 神经网络中的参数，通过训练进行调整以优化模型性能）。在某种意义上，这就是强化学习试图做的事情。强化学习非常挑剔，但当它奏效时，它就有点神奇了。这是模型可能获得的最好数据。当你尝试一个练习题，失败了，然后在某个时候以一种对你来说有意义的方式解决了它。这对于你来说是最好的数据，因为这是你解决问题的方式，而不是仅仅阅读别人如何解决问题，这最初并不会“点击”。

顺便说一句，如果这种观点听起来很熟悉，那是因为它是我问**John Schulman**的问题的一部分。这说明了我引言中说的那件事。我从与你、**Sholto**和其他几个人在采访前进行的晚餐中学到了很多关于AI的知识。我们当时会讨论：“我该问**John Schulman**什么？我该问**Dario**什么？”

### AGI的经济、政治与地缘政治影响

假设事情就这样发展，我们获得了这些“解缚”——以及规模化。你拥有这种巨大的规模化力量作为基线。**GPT-2**很棒。它能串联起看似合理的句子，但几乎什么都做不了。它有点像学龄前儿童。另一方面，**GPT-4**可以编写代码，做复杂的数学题，就像一个聪明的高中生。这种能力的巨大飞跃在系列文章中有所探讨。我计算了计算量和算法进步的规模增长。

到2027-2028年，仅凭规模化，就会在**GPT-4**的基础上，再实现一次从学龄前儿童到高中生的飞跃。在每个token的层面上，模型将变得极其智能。它们将获得更高的可靠性，并且随着“解缚”的加入，它们将不再像聊天机器人，而更像代理或即插即用型远程工作者。那时，事情才会真正开始。

我想问更多关于这个问题，但让我们把视野拉远一点。假设你是对的。这是因为2027年的集群达到了**10千兆瓦**吗？2028年是**10千兆瓦**。也许会提前。到2027年，可能会达到**5.5级**，不管那叫什么。到那时世界会是什么样子？你拥有这些可以取代人类的远程工作者。经济、政治和地缘政治将如何对此做出反应？

2023年对于一个真正关注AI的人来说，是一个非常有趣的年份。你2023年在做什么？在**OpenAI**。当你2023年在OpenAI时，那是一种奇怪的感觉。你几乎不想谈论AI或AGI。它有点像一个“禁忌词”。然后到了2023年，人们第一次看到了**ChatGPT**，他们看到了**GPT-4**，然后它就爆炸了。

它引发了所有这些公司巨大的资本支出，以及**英伟达**等公司收入的爆炸式增长。从那时起，事情一直很平静，但下一波浪潮已经在酝酿中。我预计每一代模型都会让这些“G力”加剧。人们会看到这些模型。他们没有计算过数量级，所以他们会感到惊讶。那将是相当疯狂的。

收入将加速增长。假设到今年年底，你确实达到了**100亿美元**。假设收入每六个月翻一番的轨迹继续下去，到2026年，离**1000亿美元**就不远了。在某个时候，发生在**英伟达**身上的事情将发生在大科技公司身上。它将爆发。更多的人会感受到它。2023年对我来说，是AGI从理论、抽象的东西，变成了我能看到、能感受到，并看到其发展路径的时刻。我能看到它训练的集群，大致的算法组合，参与其中的人，以及它是如何发生的。世界上大多数人还没有达到这个阶段。大多数能感受到它的人就在我们身边。世界上更多的人将开始感受到它。那将变得非常激烈。

现在，谁感受到了它？你可以上**Twitter**，那里有一些**GPT封装公司**，他们会说：“哇，**GPT-4**将改变我们的业务。”我对这些封装公司非常悲观，因为它们押注于停滞。它们押注于你拥有这些中间模型，并且整合它们需要大量的“苦差事”。我之所以非常悲观，是因为我们即将迎来“音爆”。我们将获得“解缚”。我们将获得即插即用型远程工作者。你的那些东西将变得无关紧要。

### 国家安全与AI竞争

所以，这已经完成了。旧金山，这个圈子，现在正在关注。那么2026年和2027年谁会关注呢？大概这些年份会有数千亿美元的资本支出用于AI。**国家安全机构**将开始高度关注。我希望我们能谈谈这个。

我们现在就来谈谈。会发生什么？直接的政治反应是什么？从国际角度看，我不知道**习近平**看到**GPT-4**的新闻后，会不会说：“天哪，看看那个**MMLU**分数。同志们，我们该怎么办？”那么，当他看到远程工作者替代方案，并且它带来了**1000亿美元**的收入时，会发生什么？有很多企业有1000亿美元的收入，但人们并没有为此熬夜讨论。问题是，**中国共产党**和美国国家安全机构何时会意识到**超级智能**对国家力量将是绝对决定性的？这就是**智能爆炸**（Intelligence explosion: 指人工智能系统在达到一定智能水平后，能够自我改进并迅速超越人类智能的假设情景）的由来，我们稍后应该讨论。

你拥有**AGI**。你拥有这种可以取代你或我的即插即用型远程工作者，至少对于远程工作而言。很快，你再转动一两次“曲柄”，你就会得到一个比人类更智能的东西。

### 智能爆炸与军事优势

不仅仅是再转动几次“曲柄”，首批被自动化的工作之一将是AI研究员或工程师。如果你能自动化AI研究，事情就会发展得非常快。现在，算法进步每年已经以**0.5个数量级**的速度增长。在某个时候，你将拥有数千万甚至更多的**GPU集群**用于推理。你将能够运行**1亿个**人类等效的自动化AI研究员。如果你能做到这一点，你也许能在一年内完成十年份的机器学习研究进展。你将获得某种**10倍**的加速。你可以在一年或几年内实现AI的飞跃，使其远远超越人类。

这会从那里扩展开来。你拥有AI研究的初步加速。你将研发应用于其他许多技术领域。到那时，你将拥有**10亿**个超级智能的研究员、工程师、技术员，一切。他们将在所有事情上都表现出色。他们将解决机器人技术问题。我们之前说过那是一个软件问题。好吧，你的集群里有**10亿**个超级聪明——比最聪明的人类研究员还要聪明——的AI研究员。在智能爆炸的某个时刻，他们将能够解决机器人技术问题。同样，这也会扩展开来。

如果你将这个画面向前推演，它与任何其他技术都截然不同。几年的领先优势可能在军事竞争中具有决定性意义。如果你看看**第一次海湾战争**，西方联军的杀伤比是**100:1**。他们的坦克有更好的传感器。他们有更精确的导弹、**GPS**和**隐形技术**。他们可能有20-30年的技术领先优势。他们彻底击溃了对手。

应用于广泛研发领域——以及由此产生的工业爆炸，机器人制造大量材料——的**超级智能**，可以将一个世纪的技术进步压缩到不到十年。这意味着几年时间可能带来**第一次海湾战争**式的军事优势。这甚至包括能够先发制人地压制核武器的决定性优势。

你如何找到核潜艇？现在，你有传感器和软件来探测它们的位置。你可以做到。你可以找到它们。你拥有数百万或数十亿只**蚊子大小的无人机**，它们可以摧毁核潜艇。它们可以摧毁移动发射器。它们可以摧毁其他核武器。这可能极大地破坏稳定，对国家力量极其重要。在某个时候，人们会意识到这一点。现在还没有，但他们会的。当他们意识到时，掌权的将不仅仅是AI研究员。

### 中美AI竞赛与能源挑战

**中国共产党**将全力以赴渗透美国的AI实验室。这将涉及数十亿美元、数千人以及**国家安全部**的全部力量。**中国共产党**将试图在建设上超越我们。他们在过去十年中增加的电力与整个美国电网的电力一样多。因此，**100千兆瓦**的集群，至少是其中100千兆瓦的部分，他们将更容易获得。到那时，这将是一场极其激烈的国际竞争。

我对这个图景中有一点不确定，那就是它是否像你所说的，更像是一场爆炸。你开发了**AGI**。你把它变成一个AI研究员。在一段时间内，你只利用这种能力来制造数亿个其他AI研究员。这个真正狂热的过程中产生的东西是**超级智能**。然后它走向世界，开发机器人技术，帮助你接管其他国家等等。

这是一个更渐进的过程。这是一场从狭窄领域开始的爆炸。它能做认知工作。认知工作的最高投资回报率（ROI）是让AI变得更好并解决机器人技术问题。当你解决了机器人技术问题，你就可以在生物学和其他技术领域进行研发。最初，你从工厂工人开始。他们戴着眼镜和**AirPods**，AI正在指导他们，因为你可以把任何工人变成熟练的技术员。然后机器人就会介入。所以这个过程会扩展。

**Meta**的**Ray-Bans**是**Llama**的补充。在美国的晶圆厂，他们的限制是熟练工人。即使你没有机器人，你也有认知**超级智能**，可以立即把所有工人变成熟练工人。那是一个非常短暂的时期。机器人很快就会到来。

假设这确实是美国技术进步的方式，也许是因为这些公司已经产生了数千亿美元的AI收入。到那时，公司将在企业债市场借入数千亿美元甚至更多。为什么一个**中国共产党**的官僚，一个60多岁的人，会看着这个说：“哦，**Copilot**现在更好了”，然后——这远不止是**Copilot**更好了。

这将需要转移整个国家的生产力，重新分配原本用于消费品或其他方面的能源，并将所有这些投入到数据中心。这个故事的一部分是，你意识到**超级智能**即将到来。你意识到了，也许我也意识到了。我不确定我意识到了多少。美国和**中国共产党**的国家安全机构会意识到吗？

这是一个非常关键的问题。我们还有几年的中期游戏。我们还有几个2023年。那只会让越来越多的人更新观念。趋势线将变得清晰。你会看到一些**新冠疫情**（COVID）的动态。2020年2月的新冠疫情，老实说，感觉很像今天。感觉这个彻底疯狂的事情即将到来。你看到了指数级增长，但世界上大多数人就是没有意识到。纽约市长还在说：“出去看演出吧”，“这只是亚洲种族歧视”。在某个时候，人们看到了，然后疯狂、激进的反应就来了。

### 历史的教训与AI时代

顺便问一下，你在新冠疫情期间在做什么？是你的大一还是大二？大三。你当时还是个17岁的大三学生，对吧？你做空市场了吗？或者在正确的时间卖出了吗？是的。所以会有一个2020年3月的时刻。

你可以用你在系列文章中做的类比，这会引发一种反应，比如：“我们必须为美国再次进行**曼哈顿计划**。”我不知道这方面的政治会是怎样的。这里的区别在于，这不仅仅是“我们需要炸弹来击败纳粹”。我们将建造这个东西，它会使我们的能源价格上涨很多，并且自动化我们很多工作。气候变化方面的人会说：“天哪，它正在使气候变化恶化，而且它正在帮助大科技公司。”从政治上看，这似乎不是一个国家安全机构或总统会说：“我们必须在这里踩油门，确保美国获胜”的局面。

同样，这很大程度上取决于人们感受到了多少，看到了多少。我们这一代人习惯了和平、美国霸权，觉得什么都不重要。历史常态是非常激烈和非凡的事情在世界上发生，伴随着激烈的国际竞争。

有一个20年非常独特的时期。在**第二次世界大战**中，大约**50%的GDP**用于战争生产。美国借贷了超过**60%的GDP**。德国和日本我认为超过了**100%**。在**第一次世界大战**中，英国、法国和德国都借贷了超过**100%的GDP**。当时面临的风险要大得多。人们谈论第一次世界大战是多么具有破坏性，有**2000万苏联士兵**死亡，**波兰20%**的人口死亡。这种事情一直都在发生。在**七年战争**中，大约**20-30%的普鲁士人**死亡。在**三十年战争**中，德国大片地区高达**50%**的人口死亡。

人们会看到这里的利害关系真的很高，历史真的回来了吗？美国国家安全机构非常认真地思考这类问题。他们非常认真地思考与中国的竞争。中国非常认真地将自己视为中华民族伟大复兴的历史使命。他们非常关注国家力量。他们非常关注世界秩序。

### 威权主义与超级智能的结合

这里有一个关于时机的问题。他们是在**智能爆炸**已经发生得很晚的时候才开始认真对待，还是会提前两年开始认真对待？这对于事情的发展方式影响很大。在某个时候，他们会意识到，这将不仅仅对某些代理战争，而是对重大问题具有决定性意义。**自由民主**能否继续繁荣？**中国共产党**能否继续存在？这将激活我们很久没有见过的力量。

大国冲突无疑令人信服。当你从历史角度思考时，各种不同的事情似乎更有可能发生。你超越了我们有幸在美国生活了大约80年的**自由民主**时代。这包括**独裁统治**、战争、饥荒等。我当时正在读**《古拉格群岛》**，其中一章开头**索尔仁尼琴（Aleksandr Solzhenitsyn）**说，如果你告诉沙皇统治下的俄罗斯公民，由于所有这些新技术——我们不会看到俄罗斯的伟大复兴，俄罗斯成为一个大国，公民变得富有——你将看到数千万苏联公民被数百万野兽以最糟糕的方式折磨。如果你告诉他们这将是20世纪的结果，他们是不会相信你的。他们会称你为诽谤者。

**超级智能**带来的独裁可能性甚至更疯狂。想象一下，你拥有一支绝对忠诚的军队和安全部队。不再有叛乱。不再有民众起义。你拥有完美的测谎能力。你对每个人进行监视。你可以完美地找出谁是异议者并清除他们。没有**戈尔巴乔夫（Mikhail Gorbachev）**这样对体制有所怀疑的人能够掌权。军事政变也永远不会发生。

### 思想演变与权力集中

在某种程度上，事情之所以能成功，是因为思想可以演变。在某种意义上，时间可以治愈许多创伤，解决许多争论。在历史长河中，许多人曾有非常坚定的信念，但其中许多信念随着时间的推移而被推翻，因为持续存在着多元化和演变。

想象一下，将**中国共产党**式的“真理”观念——即真理是党所说的——应用于**超级智能**。当你用超级智能来强化这一点时，它可能会被长期锁定和固化。这种可能性相当可怕。

关于你提到的历史和在美国生活了八十年，这是我在德国长大时的一个体会。很多事情感觉更直接。我母亲在**前东德**长大，我父亲在**前西德**长大。他们在柏林墙倒塌后不久相遇。**冷战**的结束对我来说是一个极其关键的时刻，因为那是我存在的理由。我在柏林长大，那里有**柏林墙**的遗迹。我的曾祖母，现在还健在，在我的生活中非常重要。她出生于1934年，在**纳粹时代**长大。在**第二次世界大战**中，她从他们小时候住的乡村小屋里看到了**德累斯顿大轰炸**。然后她生命的大部分时间都在**东德共产主义独裁统治**下度过。她会告诉我1954年民众起义时苏联坦克是如何开进来的。她的丈夫告诉她赶紧回家，不要待在街上。她有一个儿子曾试图骑摩托车穿越**铁幕**，然后被关在**斯塔西**（Stasi: 东德国家安全部的简称）监狱里一段时间。最后，在她将近60岁的时候，她第一次生活在一个自由富裕的国家。

我小时候，她最不希望我做的事情就是参与政治。加入政党对她来说有非常不好的含义。我小时候她抚养了我。所以感觉并没有那么久远。感觉非常近。

### AI研究员的角色与权力转移

今天我们谈论**中国共产党**时，我有一个疑问。在中国，那些将从事这个项目的人将是有些西方化的AI研究员。他们要么在西方受过教育，要么在西方有同事。他们会支持**中国共产党**的项目，把控制权交给**习近平**吗？你对此有何看法？从根本上说，他们只是普通人，对吧？难道你不能说服他们**超级智能**的危险吗？

但他们会掌权吗？在某种意义上，美国也是如此。这就像实验室员工的影响力正在迅速贬值。现在，AI实验室员工拥有巨大的权力。你看到了去年11月的事件。那是巨大的权力。两者都将被自动化，他们将失去所有权力。最终将只有少数人掌管着他们的自动化AI大军。这还包括政客、将军和国家安全机构。**《奥本海默》**电影中有些经典的场景。科学家们建造了它，然后炸弹被运走，就不再由他们掌控了。

实验室员工意识到这一点是好事。你现在拥有很多权力，但可能不会持续太久。明智地使用它。我确实认为他们会从更多的**代议制民主**（Representative democracy: 一种民主形式，公民通过选举代表来行使政治权力）机构中受益。你这是什么意思？在**OpenAI**董事会事件中，员工权力以一种非常直接民主的方式行使。其中一些事件的发生，确实凸显了代议制民主和拥有一些审议机构的好处。

### AI集群的选址与能源问题

有趣。让我们回到**1000亿美元**收入的问题。这些公司正试图建造如此大的集群。它们在哪里建造？假设所需的能源量相当于美国一个中小型州的用电量。那么，科罗拉多州会因为这些集群建在美国而没有电力吗？还是会在其他地方建造？

这正是我觉得有趣的地方，当你谈到科罗拉多州没有电力时。获取电力的简单方法是取代经济效益较低的东西。购买一个拥有**千兆瓦**电力的铝冶炼厂。我们将用数据中心取代它，因为那很重要。但这实际上并没有发生，因为很多电力合同都是长期锁定的。而且，人们不喜欢这样的事情。实际上，至少目前，这需要建造新的发电设施。这可能会改变。当情况变成“不，我们只是将所有电力都用于**AGI**”时，事情就会变得非常有趣。

所以现在是建造新的发电设施。**10千兆瓦**是完全可行的。它只占美国天然气产量的百分之几。当你拥有**10千兆瓦**的训练集群时，你会有更多的推理能力。**100千兆瓦**就开始变得相当疯狂了。那超过了美国总发电量的**20%**。这相当可行，特别是如果你愿意使用天然气。这些集群设在美国极其重要。

### AI集群的地理位置与国家安全

为什么它在美国很重要？有些人正试图在其他地方建造集群。有很多中东的自由流动资金正试图在其他地方建造集群。这又回到了我们讨论过的国家安全问题。你会在**阿联酋**进行**曼哈顿计划**吗？你可以把集群放在美国，也可以放在盟友民主国家。一旦你把它们放在**专制独裁国家**，你就会制造不可逆转的安全风险。一旦集群在那里，他们就更容易窃取**权重**（Weights: 神经网络中的参数，通过训练进行调整以优化模型性能）。他们可以字面上窃取**AGI**，窃取**超级智能**。这就像他们直接复制了原子弹。这让他们更容易得手。他们与中国有奇怪的联系。他们可以把这些运到中国。这是一个巨大的风险。

另一件事是他们可以直接没收计算资源。这里的问题是，人们现在认为这些是**ChatGPT**、大型科技公司的产品集群。现在正在规划的，三到五年后的集群，很可能就是**AGI**、**超级智能**集群。当情况变得紧张时，他们可能会直接没收计算资源。

假设我们把**25%**的计算能力放在这些中东独裁国家。假设他们没收了这些。那么计算能力的比率就是**3:1**。我们仍然拥有更多，但即使只有25%的计算能力在那里，情况也开始变得相当棘手。**3:1**并不是一个很好的比率。你可以用那么多的计算能力做很多事情。

假设他们实际上没有这样做。即使他们没有实际没收计算资源，即使他们没有窃取权重，你也会获得很多隐性筹码。他们将在**AGI**谈判桌上获得一席之地。我不知道我们为什么要给专制独裁国家在AGI谈判桌上一个席位。如果这些交易达成，中东将会有大量的计算资源。

### 中东AI投资与潜在风险

首先，是谁在做这件事？是每一家大型科技公司都在那里想办法吗？不是所有人，只是一部分。有报道称，我认为是**微软**。我们会深入探讨。

所以假设**阿联酋**获得了很多计算资源，因为我们在那里建造集群。假设他们拥有**25%**的计算能力。为什么计算能力比率很重要？如果这关系到他们能否启动**智能爆炸**，那不就是有一个阈值，要么你有**1亿**AI研究员，要么就没有吗？

你可以用**3300万**极其聪明的科学家做很多事情。那可能足以制造疯狂的**生物武器**。然后你就会陷入他们窃取了权重并没收了计算资源的情况。现在他们可以制造这些**超级智能**可能实现的疯狂**大规模杀伤性武器（WMDs）**。现在你已经扩散了那些真正强大的东西。而且，**3倍**的计算能力实际上并没有那么多。

最危险的情况是，如果我们处于某种真正势均力敌、狂热的国际竞争中。假设我们与**中国共产党**非常接近，只差几个月。我们希望处于——如果我们打好牌就能处于——的情况，更像是美国建造原子弹，而德国项目落后几年。如果我们有这种优势，我们就有更多的回旋余地来确保安全。

我们将建造这些疯狂的新型**大规模杀伤性武器**，它们将彻底颠覆**核威慑**。如果你没有人在你身后紧追不舍，并且不必以最大速度前进，那么处理起来就容易得多。你没有回旋余地。你担心他们随时可能超越你。他们也可能只是试图在建设上超越你。如果他们能窃取权重，他们可能真的会赢。中国如果能窃取权重，就可能真的会赢，因为他们可以在建设上超越你。他们可能更少顾虑，无论是好的还是坏的顾虑，就我们可能有的不合理法规而言。如果你处于这种非常紧张的竞争中，这种狂热的斗争中，那才是自我毁灭的最大危险。

### AI公司在中东的投资考量

想必那些试图在中东建立集群的公司也意识到了这一点。难道在美国做这件事是不可能的吗？如果美国公司要这样做，是必须在中东做，还是根本不做？那样的话，中国就只会建造一个**三峡大坝**规模的集群。

有几个原因。人们并没有把这看作是**AGI超级智能集群**。他们只是觉得：“啊，我的**ChatGPT**需要很酷的集群。”如果你是为推理而建造，那么你可以把它们分散到全国各地。但他们正在建造的那些，将会在一个单一的设施中进行一次训练运行。

区分推理和训练计算是很难的。人们可以声称这是推理计算，但他们可能会意识到这实际上对训练计算也很有用。因为**合成数据**之类的东西吗？例如，**强化学习**看起来很像推理。或者你只是最终将它们在时间上连接起来。这很像原材料。这就像把你的**铀精炼设施**放在那里。

所以有几个原因。第一，他们不认为这是**AGI集群**。另一个原因是中东有唾手可得的资金。另一个原因是有些人认为在美国做不到。我们实际上在这里面临着一场真正的**系统竞争**。有些人认为只有**专制国家**才能通过自上而下地调动工业能力和快速完成任务的能力来做到这一点。

### 历史经验与美国AI发展路径

同样，这是我们很久没有面对过的事情。但在**冷战**期间，存在着激烈的系统竞争。**东德**与**西德**就是如此。西德是**自由民主资本主义**，东德是**国家计划共产主义**。现在很明显，自由世界会赢。但即使早在1961年，**保罗·萨缪尔森（Paul Samuelson）**还预测苏联会超越美国，因为他们能够更好地调动工业。

所以有些人嘴上说爱美国，但私下里却在做空美国。他们正在做空**自由秩序**。基本上，这是一个糟糕的赌注。这些事情在美国是完全有可能实现的。为了让它在美国成为可能，我们必须在某种程度上振作起来。在美国实现它基本上有两条路径。一条是你必须愿意使用天然气。天然气储量充足。你可以把你的集群放在**西德克萨斯**。你可以把它放在**宾夕法尼亚州西南部**的**马塞勒斯页岩区**。**10千兆瓦**的集群非常容易。**100千兆瓦**的集群也相当可行。我认为美国天然气产量在十年内几乎翻了一番。在未来七年内再做一次，你就可以为多个万亿美元的数据中心供电。

问题在于，很多人都做出了**气候承诺**，不仅仅是政府。实际上是**微软**、**亚马逊**等私营公司自己做出了这些气候承诺。所以他们不会使用天然气。我钦佩这些气候承诺，但在某个时候，国家利益和国家安全更为重要。

另一条路径是进行**绿色能源**大型项目。你可以发展太阳能、电池、**小型模块化反应堆（SMRs）**和地热能。如果我们要这样做，就需要进行大规模的放松管制。你不能让审批过程耗时十年。你必须改革**联邦能源管理委员会（FERC）**。你必须对这些项目实行全面的**国家环境政策法（NEPA）**豁免。

存在一些荒谬的州级法规。你可以在数据中心旁边建造太阳能电池板和电池，但仍然需要数年时间才能将其连接到州电网。你必须利用政府权力来创建通行权，以便拥有多个集群并连接它们，并铺设电缆。理想情况下，我们两者兼顾。理想情况下，我们既使用天然气，又推行更广泛的放松管制的绿色议程。我们至少必须做到其中之一。然后，这些事情才有可能在美国实现。

### 二战工业动员与AI竞赛的启示

在这次对话之前，我正在读一本关于**第二次世界大战**美国工业动员的好书，名为**《Freedom's Forge》**。我回想起那个时期，尤其是在阅读**Patrick Collison**的**《Fast》**和进步研究材料的背景下。有一种说法是，当时我们有国家能力，人们把事情办成了，但现在却是一团糟。情况根本不是那样！

那真的很有趣。你有一些来自底特律汽车工业界的人，比如**William Knudsen**，他们负责美国的动员工作。他们能力超群。与此同时，还有劳工组织和煽动，这与我们今天面临的气候变化承诺和担忧非常相似。他们真的会举行罢工，直到1941年，耗费数百万工时，而我们当时正试图每月生产数万架飞机。他们会为了资本方面微不足道的让步而瘫痪工厂。当时有人担心汽车公司试图以潜在战争为借口，阻止支付劳工应得的报酬。所以，以今天气候变化的情况来看，你可能会想：“啊，美国完蛋了。如果我们看看**NEPA**之类的东西，我们将无法建造这些东西。”我没有意识到**第二次世界大战**中劳工的破坏性有多大。

不仅仅是这样。1939年之前，美国军队一片狼藉。你读到它，感觉有点像今天的德国军队。军事开支我认为不到GDP的2%。所有欧洲国家，即使在和平时期，军事开支也超过了GDP的10%。那是从零开始的快速动员。我们当时没有制造飞机。没有军事合同。在大萧条期间，一切都被削弱了。但存在这种潜在的能力。在某个时候，美国振作起来了。

这反过来也适用于中国。有时人们会因为出口管制等原因而有点看轻他们。他们现在能够制造**7纳米芯片**。问题是他们能制造多少。至少有可能他们会成熟这种能力并制造大量的7纳米芯片。中国有很多潜在的工业能力。他们能够快速建造大量电力。也许这还没有被激活用于AI。在某个时候，就像美国和许多美国政府官员会觉醒一样，**中国共产党**也会觉醒。

### AI实验室的策略与安全风险

公司意识到规模化是真实存在的。显然，他们的所有计划都依赖于规模化。所以他们明白到2028年，我们将建造**10千兆瓦**的数据中心。到那时，能够跟上步伐的将是大型科技公司（可能已达到其能力的极限）、主权财富基金资助的项目，以及美国和中国等主要国家。他们的计划是什么？鉴于这种局面，AI实验室的计划是什么？他们不想要在美国的优势吗？

中东确实提供资本，但美国有充足的资本。我们有万亿美元的公司。这些中东国家是什么？它们有点像万亿美元的石油公司。我们有万亿美元的公司和非常深厚的金融市场。**微软**可以发行数千亿美元的债券，他们可以支付这些集群的费用。

另一个值得认真对待的论点是，如果我们不与**阿联酋**或这些中东国家合作，他们就会转向中国。他们无论如何都会建造数据中心并向AI投入资金。如果我们不与他们合作，他们就会支持中国。

这个论点在某种程度上是有道理的，即我们应该与他们分享利益。在通往**AGI**的道路上，应该有两层联盟。应该有一个由民主国家组成的狭窄联盟来开发**AGI**。然后应该有一个更广泛的联盟，包括其他国家和独裁国家，我们应该向他们提供AI的一些好处。

如果**阿联酋**想使用AI产品，运行**Meta**的推荐引擎，或者运行上一代模型，那没问题。默认情况下，他们就不会在**AGI**谈判桌上占有一席之地。所以他们有一些钱，但很多人都有钱。他们之所以能在**AGI**谈判桌上占有一席之地，并让这些独裁者对这项极其重要的国家安全技术拥有影响力，仅仅是因为我们让他们感到兴奋并主动提供了。

### 知识产权窃取与竞争优势

具体是谁在做这件事？哪些公司正在那里筹集资金？据报道，**Sam Altman**正试图为一项芯片项目筹集**7万亿美元**。目前尚不清楚会有多少集群在那里，但肯定有事情正在发生。

我有点怀疑“如果美国不与他们合作，他们就会转向中国”这个论点，还有另一个原因。我从多人那里听说——不是在我**OpenAI**工作期间，我也没看到备忘录——几年前，**OpenAI**领导层曾制定一项计划，通过在美国、中国和俄罗斯政府之间挑起竞价战来资助和销售**AGI**。他们愿意向中国和俄罗斯政府出售**AGI**，这让我感到惊讶。这种挑起竞价战，然后互相利用，说“如果你不这样做，中国就会做”的感觉，也让人感到不寒而栗。

有意思。那真是够糟糕的。假设你是对的。我们之所以走到这一步，正如我们一位朋友所说，是因为中东拥有数十亿甚至数万亿美元可供说服的资金，这是世界上其他任何地方都无法比拟的。而且问责制很少。没有**微软**董事会。只有独裁者。

假设你是对的，你一开始就不应该让他们对**AGI**感到兴奋。现在我们处于他们对**AGI**感到兴奋的境地，他们会说：“见鬼，我们想要**GPT-5**，而你们却要去建造**超级智能**。这个‘**原子能和平利用**’（Atoms for Peace: 美国在冷战初期提出的一项计划，旨在推广核能的和平利用）对我们不起作用。”如果你处于这种境地，他们难道没有筹码吗？

**阿联酋**本身不具备竞争力。他们已经受到出口管制。你不应该把**英伟达**芯片运到那里。他们也没有任何领先的AI实验室。他们有钱，但很难把钱直接转化为进步。

但我想回到你之前提出的其他观点，你描绘的愿景。这几乎是一个工业化过程，投入计算和算法，将其累加，最终得到**AGI**。如果更像是这样，那么有人能够迅速追赶的论点就比某些定制化的东西更具说服力。

好吧，如果他们能窃取算法，如果他们能窃取权重，那真的非常重要。一个行为者窃取那些非琐碎的发布内容（比如**Scarlett Johansson**的声音），而是我们正在谈论的**强化学习**相关内容、“解缚”相关内容，会有多容易？

### AI安全漏洞与秘密的重要性

这一切都极其容易。他们并没有声称这很难。**DeepMind**发布了他们的**《前沿安全框架》**（Frontier Safety Framework），他们列出了从零到四的安全级别。四级是能够抵抗国家行为的。他们说，我们目前处于零级。最近，就有一个人被起诉，他窃取了大量非常重要的AI代码并带着它去了中国。他所做的只是复制代码，将其放入**Apple Notes**，然后导出为**PDF**。这绕过了他们的监控。

**Google**可能拥有所有AI实验室中最好的安全性，因为他们拥有**Google基础设施**。我会把创业公司的安全性考虑进去。创业公司的安全性是怎样的？不怎么样。很容易被窃取。

即使是这样，你的很多文章都在论证我们为什么会迎来**智能爆炸**。如果我们有一个像**Alec Radford**那样具有直觉的人来提出所有这些想法，那么这种直觉是极其宝贵的，你可以将其规模化。如果仅仅是直觉，那就不只存在于代码中，对吧？而且由于出口管制，这些国家将拥有略微不同的硬件。你将不得不做出不同的权衡，并可能重写代码以使其兼容。

这仅仅是拿到一个正确的U盘，然后插入**三峡大坝**旁边的**千兆瓦数据中心**，然后就可以开始竞赛了吗？

有几个不同的方面，对吧？一种威胁模型是他们直接窃取**权重**本身。窃取权重这一点尤其疯狂，因为他们可以直接窃取最终产品——就像复制原子弹一样——然后他们就可以开始了。这一点在**AGI**和**超级智能**出现的时候极其重要，因为中国默认可以建造一个大型集群。我们会因为拥有更好的科学家而拥有巨大的领先优势，但如果我们制造出**超级智能**，而他们直接窃取了它，那他们就立刻进入了竞赛。

现在权重的重要性稍低一些，因为谁在乎他们是否窃取了**GPT-4**的权重呢？我们现在仍然必须开始进行权重安全工作，因为如果我们认为到2027年会有**AGI**，这项工作需要一段时间。这不会只是“哦，我们做一些访问控制”那么简单。如果你真的想抵抗中国的间谍活动，那需要更加严密。

人们没有足够重视的是**秘密**。计算方面很吸引人，但人们低估了秘密的重要性。每年半个数量级的进步只是默认的算法进步。这已经很巨大了。如果我们有几年的领先优势，默认情况下，如果我们保护好秘密，那将是**10-30倍**，甚至**100倍**更大的集群。

### 数据墙与新范式突破

还有一层是**数据墙**。我们必须突破数据墙。这意味着我们必须找出某种基本的新范式。所以这是“**AlphaGo**第二步”。“AlphaGo第一步”是从人类模仿中学习。“AlphaGo第二步”是现在每个人都在研究的**自对弈强化学习**。也许我们会攻克它。如果中国无法窃取这个，那他们就会被困住。如果他们能窃取，那他们就立刻进入了竞赛。

无论那是什么，我能把它写在餐巾纸背面吗？如果那么容易，那为什么他们很难弄明白呢？如果更多是关于直觉，那难道你不需要雇佣**Alec Radford**吗？你在复制什么？

这有几个层面。最顶层是基本方法。在预训练方面，可能是**无监督学习**、**下一个token预测**、在整个互联网上进行训练。你已经从中获得了很大的好处。这方面的沟通非常迅速。

然后有很多细节很重要，你之前也谈到过。事后看来，这可能有点显而易见，或者会有一些不太复杂但能奏效的东西，但要实现它需要很多细节。如果这是真的，那么我们为什么认为在这些初创公司中实施国家级别的安全措施就能阻止中国迎头赶上呢？这就像是，“哦，我们知道需要某种**自对弈强化学习**才能突破数据墙。”到2027年就会解决，对吧？没那么难。

美国以及美国领先的实验室拥有巨大的领先优势。默认情况下，中国实际上有一些不错的**大型语言模型（LLMs）**，因为他们只是在使用开源代码，比如**Llama**。人们真的低估了算法进步上的差异以及美国默认会拥有的领先优势，因为直到最近所有这些东西都还是公开的。

看看**Chinchilla Scaling laws**、**MoE论文**、**Transformer**。所有这些东西都曾公开发表。这就是为什么开源是好的，也是为什么中国可以制造一些好的模型。现在，他们不再公开发表了。如果我们真的保密，那将是一个巨大的优势。

关于你提到的**隐性知识**（Tacit knowledge: 难以用语言或书面形式表达和传递的知识）和**Alec Radford**，底层还有一层是关于大规模工程工作，以使这些大型训练运行得以实现。这更像是一种隐性知识，但中国将能够解决这个问题。这是工程上的苦差事，他们会想办法做到的。

为什么他们能解决工程上的苦差事，却不能让**强化学习**发挥作用呢？

### 原子弹研发的启示与AI竞赛的紧迫性

我不知道。**第二次世界大战**期间的德国在**重水**问题上走错了路。在**《原子弹秘史》**中有一个惊人的轶事。保密是早期最具争议的问题之一。**Leo Szilard**坚信核链式反应和原子弹是可能实现的。他四处宣扬：“这将具有巨大的战略和军事重要性。”很多人不相信，或者认为“这也许可能，但我会假装不可能，科学应该是开放的。”

早期，关于**石墨**作为慢化剂的测量出现了一些错误。德国认为石墨行不通，所以他们不得不使用重水。但后来**Enrico Fermi**进行了新的测量，表明石墨是可行的。这非常重要。**Szilard**再次向**Fermi**提出保密请求，**Fermi**非常生气，大发雷霆。他认为这很荒谬，说：“拜托，这太疯狂了。”但**Szilard**坚持不懈，他们又拉来了另一个人，**George Pegram**。最终，**Fermi**没有发表他的研究。

那正是时候。**Fermi**没有发表，意味着纳粹没有发现石墨是可行的。他们走上了重水这条错误的道路。这是德国项目失败的一个关键原因。他们远远落后了。

我们现在面临类似的情况。我们会立即泄露如何突破**数据墙**以及下一个范式是什么吗？还是不会？这之所以重要，是因为领先一年将是一个巨大的优势。在一个你随着时间推移部署AI的世界里，他们反正都会追上来。

我采访了**Richard Rhodes**，就是写**《原子弹秘史》**的那个人。他讲了一个轶事，是关于苏联人意识到美国拥有原子弹的时候。显然，我们把原子弹投在了日本。**拉夫连季·贝利亚（Lavrentiy Beria）**——那个管理**内务人民委员部（NKVD）**的家伙，一个臭名昭著的残忍邪恶之人——去找负责苏联版**曼哈顿计划**的科学家。他说：“同志，你必须给我们搞到美国的原子弹。”那个科学家说：“嗯，听着，他们的内爆装置实际上不是最优的。我们应该用不同的方式制造。”**贝利亚**说：“不，你必须给我们搞到美国的原子弹，否则你的家人将化为营地尘埃。”

这个轶事的相关之处在于，如果苏联人没有复制美国的设计，至少在最初，他们本可以制造出更好的原子弹。这说明了一些历史规律，不仅仅是针对**曼哈顿计划**。通常存在这种**平行发明**（Parallel invention: 指多个人或团队在独立研究中同时或相近时间发现或发明同一事物）的模式，因为技术树暗示了某个特定的事物是下一个——在这种情况下，是**自对弈强化学习**——人们都在研究它，并且会在差不多的时间弄明白。谁先得到它，差距不会太大。

### AI竞赛的时间窗口与对齐挑战

众所周知，很多人在差不多的时间发明了电灯泡。是不是这样，即使如此，一年或六个月的差距也会带来不同？两年就能带来天壤之别。我不知道是否会是两年。

如果我们封锁实验室，我们有更好的科学家。我们遥遥领先。那将是两年。即使是六个月，一年，也会带来巨大的不同。这又回到了**智能爆炸**的动态。一年可能意味着一个系统从人类水平到远远超越人类水平的差异。那可能相当于**五个数量级**。

看看目前的速度。三年前，在数学基准测试上——这些是非常困难的高中竞赛数学问题——我们只有百分之几的正确率，几乎什么都解决不了。现在已经解决了。那是在AI正常进展的速度下。你当时没有**10亿**个**超级智能**研究员。一年是巨大的差异，尤其是在**超级智能**之后。一旦这应用于研发的许多领域，你就会通过机器人和其他先进技术获得工业爆炸。几年时间可能带来几十年的进步。同样，这就像美国在**第一次海湾战争**中拥有的技术领先优势，当时20-30年的技术领先被证明是完全决定性的。这真的很重要。

这里还有另一个原因，它确实很重要。假设他们窃取了权重，假设他们窃取了算法，并且他们紧追不舍。假设我们仍然领先。我们快了一点，领先了三个月。我们势均力敌，只领先三个月，这样的世界是极其危险的。我们处于这种狂热的斗争中，如果他们领先，他们就会占据主导地位，也许会获得决定性的优势。他们正在疯狂地建造集群。他们愿意不顾一切。我们必须跟上。

### AI地缘政治的紧迫性与社会反应

疯狂的新型**大规模杀伤性武器**正在不断出现。然后我们将陷入这样的境地：疯狂的新型军事技术，疯狂的新型大规模杀伤性武器，威慑，**相互保证毁灭**（Mutually Assured Destruction: 指在核战争中，攻击方和被攻击方都将遭受毁灭性打击，从而阻止任何一方发动攻击的军事理论）每隔几周就会改变。这是一个完全不稳定、动荡的局面，极其危险。

所以你必须从这些技术是危险的角度来看待它，从**对齐**（Alignment: 确保AI系统按照人类的意图和价值观行事的研究领域）的角度来看。在**智能爆炸**期间，拥有六个月的回旋余地可能非常重要，可以让你说：“看，在这段时间里，我们将投入更多的计算资源用于对齐，因为我们必须把它做好。我们对目前的情况感到不安。”我们能否自我毁灭，或者能否度过这个极其疯狂的时期，最重要的输入之一就是我们是否有这个缓冲。

在我们深入探讨之前，非常值得注意的是，我交谈过的人中，几乎没有人思考AI的地缘政治影响。我有一些客观层面的分歧，我们稍后会深入探讨，有些事情我想弄清楚。最终我可能不会有异议。基本前提是，如果你继续规模化，如果人们意识到这就是智能的发展方向，那将不再是旧世界。它不会仅仅是关于我们明天部署什么模型，或者最新的东西是什么。**Twitter**上的人会说：“哦，**GPT-4**会颠覆你的预期”之类的。

**新冠疫情**真的很有趣，因为当2020年3月来临时，全世界——总统、CEO、媒体、普通人——都清楚地意识到，世界上现在正在发生其他事情，但我们作为一个世界正在处理的主要事情是新冠疫情。很快就会是**AGI**。现在是平静期。也许你想去度假。也许现在是你生孩子的最后机会。我的女朋友有时会抱怨我工作时没有花足够的时间陪她。她威胁要用**GPT-6**或其他什么来取代我。我就会说：“**GPT-6**也会忙着做AI研究。”为什么其他人不谈论国家安全呢？

我在新冠疫情问题上犯了这个错误。2020年2月，我以为它会席卷全球，所有医院都会崩溃。那会很疯狂，然后就会结束。很多人在新冠疫情开始时都有这种想法。他们关闭办公室一个月之类的。我真正没有充分估计的是社会反应。几周之内，国会就为新冠疫情措施花费了超过**10%的GDP**。整个国家都被关闭了。那太疯狂了。我没有充分估计新冠疫情。

为什么人们低估了它？身处“战壕”中，实际上会让你对趋势线看得不那么清楚。你不需要把视野拉得太远，只需要几年。当你在“战壕”中时，你正试图让下一个模型工作。总会有一些困难。你可能会低估算法进步，因为你会觉得：“啊，现在事情很难”，“数据墙”之类的。