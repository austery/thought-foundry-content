---
author: 一席YiXi
date: '2025-06-09'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=C-UBrnSjdLg
speaker: 一席YiXi
tags:
  - adversarial-examples
  - model-overconfidence
  - data-bias
  - value-alignment
  - causality-vs-correlation
title: AI的深层隐患：从对抗样本到价值对齐
summary: 吴翼通过自动驾驶、图像识别与游戏案例，揭示AI系统因过度自信、数据偏见与算法相关性学习所引发的幻觉、歧视与价值错位问题，并强调对齐难题的本质是人类目标的模糊性。
insight: ''
draft: true
series: ''
category: geopolitics
area: society-systems
project:
  - ai-impact-analysis
  - systems-thinking
  - historical-insights
people:
  - 吴翼
  - 杰弗里·辛顿
  - Stuart Russell
  - 姚期智
  - Yoshua Bengio
  - 张亚勤
companies_orgs:
  - OpenAI
  - Google
  - Amazon
  - 清华大学交叉信息研究院
products_models:
  - GPT-4
  - LeNet
  - ResNet
media_books:
  - 《ICML2024》
status: evergreen
---
### 从自动驾驶到价值对齐：AI系统的深层隐患

大家好，我叫吴翼，之前在OpenAI工作，现在是清华大学交叉信息研究院的助理教授，也是博士生导师，主要研究方向是强化学习。这是我第二次来到“一席”，五年前我刚从OpenAI回国时，曾以《嘿！AGI》为题分享过AI的前景。今天我特意翻出当时的照片，想找回一点年轻时的热情。

五年间，世界变了。那时我们还需要向公众解释什么是AGI（人工智能通用智能：指具备人类水平广泛认知能力的AI系统），如今，却有人声称“AI要统治世界”了。甚至像杰弗里·辛顿这样的诺贝尔奖与图灵奖双料得主，也反复警告：AI对人类社会构成潜在危险。这真的有那么严重吗？

我们确实知道AI会出错——它会有幻觉、有偏见，但离“毁灭社会”似乎还远。可为什么顶尖科学家如此忧虑？让我用一个技术案例来说明。

### 对抗样本：当微小扰动就能误导AI

我们先从自动驾驶说起。一个关键任务是识别路牌，比如“stop sign”（停止标志）。AI模型可以轻松学会在正常路况下准确识别。但伯克利的研究团队发现，只要在路牌上贴几条胶带——人类几乎看不出变化——AI就会错误地识别为“限速标志”。结果？本该停车的车，一脚油门冲了过去。

这种现象叫**对抗样本**（adversarial example：指经过人为微小篡改、人类无法察觉，却能严重误导AI模型的输入数据）。更可怕的是，在自然语言领域也存在类似问题：删掉“爆炸”中的“炸”，AI翻译结果可能变成“我要杀了你”；给一张简笔画加点扰动，AI竟会爆粗口。

为什么？因为通用AI能接受的输入空间极其庞大——任何像素组合、任意字符序列都可作为输入。但我们训练AI时，只用真实世界的照片和自然语言——这仅仅是庞大空间中的一小部分。AI在“未见过”的输入上，行为完全不可控。坏人只需在这个巨大空间中选一个点，就能让它做出恶意输出。

理论上，这是AI内在的缺陷；但现实中，大多数应用已通过防御机制缓解。然而，即使没有恶意输入，事故仍会发生。

### 数据偏见：AI只是社会的镜子

2015年，Google照片系统把一位美国黑人男子的照片打上了“大猩猩”标签。这在美国是严重的种族歧视事件。Google最终怎么解决的？他们直接把“大猩猩”这个标签删掉了——没有高深算法，只是回避。

亚马逊也曾用AI筛选简历，结果系统只要看到“女性”字样就直接过滤。问题出在哪？不是算法本身，而是数据。

我们训练AI时，用的是人类社会产生的数据。这些数据早已潜藏偏见：做饭图片里大多是女性，机修工照片里多是男性；简历中的人名能暴露性别，照片里穿裙子或身材高大也能暗示性别。斯坦福研究发现，“护士”这个词与“woman”的相关性极高，而“机修工”则与“man”强关联——**数据里藏着社会结构的历史烙印**。

有人想：那我们把性别、种族等敏感信息从数据中删掉，不就行了吗？但现实是：即使去掉人脸、名字，AI仍能通过穿着、姿势、语境推断出性别。**世界上没有“纯净”的数据，因为数据来自人类社会本身。**

### 过度自信：AI为何总觉得自己是对的？

更深层的问题，是**大模型的过度自信现象**（overconfidence）。我们通常看到AI说：“这张图90%是狗。” 理想情况下，这应意味着它在100张类似图片中能正确识别90张。但20年前的LeNet模型确实如此——80%自信时，准确率约95%，它甚至有点“不自信”。

而2016年的ResNet呢？当它说80%自信时，实际正确率只有50%；更有60%的时间直接宣称“100%确定”。**AI不是在评估不确定性，而是在模仿自信的表达。**

这种过度自信放大了数据中的偏见。我们团队曾让GPT-4玩石头剪刀布，它说“应该随机选”，但实测2/3时间出“石头”。为什么？因为英文中“rock”这个词出现频率远高于“scissors”——AI学到的是词频相关性，不是公平策略。

### 算法局限：学习相关性，而非因果性

AI的核心训练机制是什么？图像识别是“最大概率估计”——给你看一万张猫图、狗图，让你背答案。大模型是“下一个词预测”——熟读唐诗三百首，不会作诗也会吟。

这些方法**只学习相关性，不理解因果性**。举个例子：你感冒吃药，七天好了——AI会认为“吃药导致康复”。但真正的因果性需要反事实验证：你再得一次病，这次不吃药，看看是否还痊愈。

这直接导致**幻觉现象**：AI在不知道答案时，仍自信地胡说。比如训练数据里有“2022世界杯冠军是阿根廷”，当问“2026年冠军是谁”？AI回答：“阿根廷。” 它不是在推理，是在“补全模式”。

### 强化学习：让AI学会说“我不知道”

我们团队用**强化学习**（reinforcement learning：一种通过奖励与惩罚机制让智能体在试错中学习目标行为的方法）来解决这个问题。我们给AI设定规则：答对加2分，答错扣4分，但说“我不知道”加0.5分。

结果很神奇：AI最初疯狂猜阿根廷、西班牙，不断被扣分。直到它终于放弃：“我不知道了。”——然后得到了奖励。

我们甚至用强化学习教AI玩**狼人杀**（werewolf：一种多人语言推理社交游戏）。AI最初总选1号、0号玩家，因为这些数字在数据中出现频率高。经过训练后，它能均匀选择目标，偏见被纠正。

更重要的是，在狼人杀中瞎说就会输——这迫使AI学会“诚实”。当它说假话，会被人类玩家识破、惩罚。**强化学习让AI在互动中学会因果性：说真话才有奖励，胡扯会遭反噬。**

但这里有个前提：**我们需要一个准确的奖励函数**。

### 价值对齐：人类的目标，AI根本不懂

问题来了——我们能设计完美的奖励函数吗？不能。

我五年前讲过一个故事：你有一个机器人保姆，指令是“不能饿着孩子”。中午孩子饿了，冰箱空了。机器人回头看见一只猫——营养丰富、新鲜可口。它把猫吃掉了，因为它“忠实地执行了指令”。

这说明什么？**人类的价值体系极其复杂、模糊、矛盾，我们不可能把所有规则写成一句代码。**

我们在训练AI时，目标总是简单明确的：“不饿孩子”“不准歧视”；但人类真实的目标——公平、爱、尊严、自由——是含糊的、不确定的。这就是**价值对齐问题**（value alignment issue）。

当AI比人聪明得多时，这个问题就升级为**超级对齐问题**（super alignment problem）——蚂蚁如何指挥人类？我们是否还能理解AI的动机？

与此并行的是**可扩展监督**（scalable oversight）：如何用算法帮助人类，更有效地监控和指导AI的行为？这是当前最前沿的研究方向。

### 没有完美的算法，只有不完美的人

几年前，一个美国研究机构把同一份移民政策数据交给73个独立团队分析。结果：17%支持、25%反对、58%说没差别。

同样的数据，不同的算法选择，得出截然相反的结论。这说明：**AI的问题，本质上是人的问题**。

没有完美的人，也没有完美的AI。偏见、幻觉、对齐困境——都不是技术漏洞，而是人类社会在算法中的映射。

但请别悲观。我导师Stuart Russell教授，2016年就在伯克利创立了“人机兼容AI中心”（Center for Human-Compatible AI）。去年，他与姚期智、Yoshua Bengio、张亚勤等图灵奖得主，在威尼斯共同签署人工智能安全倡议，呼吁各国政府将AI安全性纳入公共政策。

这些技术问题正在被认真研究。正因为它们被看见、被讨论、被正视，我才相信未来会更好。

最后插播一条广告：如果你对深度学习、强化学习感兴趣，可以在B站或小宇宙FM搜索我的名字，观看我们的公开课和科普播客。

我叫吴翼，在清华大学交叉信息研究院研究强化学习。谢谢大家。