---
area: market-analysis
category: business
companies_orgs:
- OpenAI
- Tesla
- AWS
- Microsoft Azure
- Google Cloud
- Equinix
- Digital Realty
- xAI
- Oracle
- Broadcom
- Arista
- Nvidia
- AMD
- TSMC
- Gartner
date: '2025-11-08'
draft: true
guest: ''
insight: ''
layout: post.njk
people:
- Sam Altman
- Satya Nadella
- Jensen Huang
products_models:
- ChatGPT
- GPU
- CPU
- H100 GPU
- Blackwell
- B100
- CoWoS
- TPU
- DPU
- HBM
- Nvlink
- Infiniband
- Ethernet
project:
- ai-impact-analysis
- us-analysis
- market-cycles
series: ''
source: https://www.youtube.com/watch?v=5EU7GUeON5U
speaker: 老科谈科技股
status: evergreen
summary: 随着OpenAI、特斯拉及各大云计算巨头大规模建设AI数据中心，全球正迎来继铁路、公路、互联网之后的又一次基础设施建设热潮。本文深入探讨了AI数据中心规模不断扩大的原因，揭示了“密度”在芯片、机箱、机架及数据中心层面的关键作用。文章还详细分析了电力供应、高效制冷（特别是液冷技术）等核心挑战，以及芯片、服务器、网络等在数据中心投资中的成本占比。最后，展望了AI数据中心未来的发展趋势及各大科技巨头的巨额资本支出，强调了其作为第四次工业革命大脑的战略意义。
tags:
- ai-data-center
- infrastructure
- liquid-cooling
- power
- technology
title: 拆解AI数据中心：供电、制冷、芯片与未来趋势
---

### 引言：AI数据中心的新时代

近期，AI数据中心的热度持续飙升。从OpenAI到特斯拉，再到各大云计算巨头，它们都在积极大规模建设全新的AI数据中心。这些AI数据中心的规模越来越大，一次次挑战着人类工程的极限。可以说，AI数据中心的**基础设施建设**（Infrastructure Construction: 为社会经济活动提供基础服务的工程设施建设）是美国历史上继铁路、公路和互联网之后，又一次基础设施建设的热潮。它带动了相关产业的繁荣，也驱动着美国GDP的持续增长。本文将深入探讨AI数据中心规模不断扩大的原因、**吉瓦特**（Gigawatt, GW: 功率单位，等于十亿瓦特，常用于衡量大型发电厂或数据中心的电力容量）数据中心的含义、电力为何成为数据中心需求的瓶颈，以及电力、制冷和芯片在数据中心建设和运营中的成本比例，并分析数据中心未来的走势，特别是哪些关键技术驱动着未来的数据中心发展。

### 什么是AI数据中心？

首先，我们来了解一下**数据中心**（Data Center: 集中存放计算机服务器、存储设备和网络设备，并提供电力、制冷、网络连接等服务的场所）。在台湾，它被称为资料中心。在公司里，你可能去过公司的机房，机房里有可以远程访问的服务器、网络设备，还需要制冷和供电。你可以把数据中心理解成一个更大规模的机房。在数据中心，有大量的服务器，服务器放置在**机架**（Rack: 用于安装多个服务器机箱的标准化框架）上，而机架又通过网线或光纤密密麻麻地连接起来。最终，它们通过电信运营商的光纤连接到外部世界。

有些公司，如银行或政府，出于安全考量会拥有自己的数据中心。但大部分公司是租用数据中心，仅仅是将自己的服务器放置到别人管理的数据中心。于是，就有了专门的数据中心运营商，例如Equinix和Digital Realty。在中国，数据中心主要由电信运营商提供。有些公司甚至连服务器都不是自己的，而是租用的，这就是**云计算**（Cloud Computing: 一种通过互联网提供计算服务，包括服务器、存储、数据库、网络、软件、分析和智能等）的方式。这种数据中心也可以称为**云数据中心**（Cloud Data Center: 提供云计算服务的物理数据中心），提供服务器出租的就是云计算运营商，如亚马逊的AWS、微软的Azure和谷歌的Google Cloud。

在ChatGPT爆火之前，大部分数据中心里面的服务器都使用**CPU**（Central Processing Unit: 中央处理器，负责处理通用计算任务）。CPU是处理通用任务的，如网站访问、流媒体、电子商务交易等等。ChatGPT的爆火，使**GPU**（Graphics Processing Unit: 图形处理器，擅长并行计算，广泛用于AI任务）的需求暴涨。无论是大模型的训练还是推理，都需要大量的GPU。以支持AI任务为主要目标、以GPU为主的数据中心，就是**AI数据中心**（AI Data Center: 专门为人工智能计算任务设计和优化的数据中心，以GPU等AI加速器为核心）。这两年数据中心建设热潮，其实绝大部分都是在建设AI数据中心，而不是传统的通用数据中心。

### AI数据中心规模为何越来越大？——“密度”是王道

AI数据中心的一个趋势就是规模越来越大，这导致了许多工程上的挑战。那么，为什么AI数据中心的规模越来越大呢？

大家知道为什么半导体芯片的制程要从5纳米到4纳米再到3纳米，为什么制程越小越好呢？根本原因是一系列的好处，例如当晶体管距离越来越近的时候，它们之间的通信性能会更好，延迟会更小，功耗会更低，同样面积的芯片就可以放更多的晶体管进去。英伟达Blackwell世代每颗B100超过2080亿个晶体管，是史上最复杂的芯片之一，它使用台积电的4纳米制程。除了制程以外，就是先进的封装，例如**CoWoS**（Chip-on-Wafer-on-Substrate: 台积电的2.5D先进封装技术，用于高性能计算和AI芯片）是台积电在高性能计算和AI领域最关键的先进封装技术之一。目前全球几乎所有顶级AI芯片（如NVIDIA、AMD、Google TPU）都依赖它。CoWoS是台积电的王牌级2.5D封装技术，通过硅中介层实现多芯片加**HBM**（High Bandwidth Memory: 高带宽内存，一种高性能内存技术，常与GPU配合使用）高速内存的超高带宽互连。无论是先进的制程，还是封装，目标都是让芯片越来越小，集成度越来越高，让同样面积的芯片容纳更多的元器件。再直白地说，就是提高密度。密度的提高推动着芯片性能的提升。英伟达的Jensen Huang（黄仁勋）经常拿着一个大大的芯片，直白地说就是把更多的CPU、GPU，甚至是存储、通信，集成到一个大大的芯片中去。集成的越多，密度越高，性能越好。这就是芯片行业的大趋势。

以上所说的是芯片。不同的芯片组成了AI服务器。AI服务器包括了CPU、GPU、内存和网络。单个服务器的**外壳**（Chassis: 容纳服务器硬件的物理外壳，通常指机箱）就是一个机箱。多个机箱可以装在一个机架上，形成更大的机架服务器。如果是机架之内，服务器之间的通信距离短，延迟低，效果更好。所以，我们需要在一个机架内放置尽量多的机箱，或者说放置尽量多数量的CPU和GPU。也就是说，一个机架容纳的CPU和GPU越多，越优化，性能越好。为什么通信距离短很关键？因为通信距离越长，延迟越高，同时速率越低。所以，尽量把通信保持在一个机架内，而不是跨机架，这非常关键。

说完一个机架，接下来就是数据中心会有大量的机架通过高速的网络联合工作，形成一个**AI集群**（AI Cluster: 由大量通过高速网络连接的AI服务器组成的计算系统，协同工作处理AI任务）。在AI时代，这也叫AI超级计算机，甚至也可以叫做AI工厂。这些机架之间有大量的通信需求，同样的道理，它们的距离越近，延迟越低，通信速率就越高。所以，最终我们在设计AI数据中心的时候，就是让这些机架尽量靠近，每个机架尽量容纳更多的CPU和GPU，每个CPU和GPU尽量容纳更多的晶体管。所以，以上说了这么多，其实就是一个关键词，提高“密度”。无论是在芯片级、机箱级、机架级，还是数据中心级，密度就是王道。提高密度，就是提高性能。Jensen Huang（黄仁勋）所讲的AI工厂，直白地说就是一个统一的高密度的AI数据中心联合工作。它们的输入和输出都是**Token**（令牌: 在自然语言处理中，文本被分解成的最小有意义单元）。这就是未来的AI数据中心。

除了要高密度，为什么一个AI数据中心的GPU越多越好呢？因为现在的一个AI数据中心其实就是一台超级计算机了，这个超级计算机拥有的GPU越多，自然它们的性能越强大。为什么不多建几个数据中心，把它们通过高速的光纤连接起来呢？以前这种结构没有问题，因为数据中心的服务器之间通信任务不多。但是在大模型时代，GPU之间大量通信，长距离的光纤通信会增加时延，影响GPU之间的协调。换句话说，在AI时代，分布式数据中心并不是最优解。这就导致了这两年巨头们纷纷把越来越多的GPU放置到一个数据中心，导致了AI数据中心规模越来越大。

接下来介绍一下由Elon Musk的xAI所打造、广泛被称之为“世界上最大的AI数据中心/训练集群”之一的**Colossus**（巨人: xAI打造的超大规模AI数据中心/训练集群项目名称）系列。Colossus的第一阶段建在美国田纳西州孟菲斯，第一版本上线时采用约100,000块NVIDIA H100 GPU，随后迅速扩展，到2025年中期，GPU数量达到约200,000块以上，电力消耗已超过约250-300 MW。该项目的未来目标规模为“百万GPU级”、功率“约1 GW级”的超级集群。

在AI时代，数据中心距离用户的距离已经不像以前那么重要了。如果数据中心主要是用来训练的，那么放到哪里都没有问题。如果数据中心主要用来推理，例如回答用户提出的问题，那么推理所需要的时间远大于网络延迟所带来的时间。所以，减少推理的计算时延的意义，远大于数据中心距离用户远近所带来的时延改善的意义。基于这个原因，AI数据中心已经不需要离用户太近了，这和传统的通用数据中心不同。数据中心的密度大大提高，GPU数量大大增加，这给数据中心的设计和运营带来了巨大的挑战。无论是供电、制冷，还是网络，都使AI数据中心与传统的数据中心大大不同了，工程难度不在一个数量级。

### “吉瓦级”AI数据中心的挑战：电力瓶颈

首先是供电。xAI公布其“Colossus 2”项目目标是达到约1**吉瓦**（Gigawatt, GW: 功率单位，等于十亿瓦特，常用于衡量大型发电厂或数据中心的电力容量）级别或更高的电力容量。1吉瓦理论上可以同时供100万户家庭使用，相当于一座中型发电厂全力运转。AI数据中心不仅“烧算力”，还得“烧电来降温”，总功率往往比一般云计算中心高2–5倍。微软CEO Satya Nadella透露，公司已采购的芯片因电力和数据中心不足而闲置。而未来AI数据中心到底需要多少电力才足够？没有人知道。即便Sam Altman或Satya Nadella在采访中透露他们也不知道。这种未知源于人工智能技术本身的高速演进。Gartner预测，人工智能和生成式人工智能正在导致用电量飙升，未来两年数据中心的用电量预计将增长高达160%。Gartner还预测，到2027年，40%的现有AI数据中心将因电力供应不足而导致运营受限。

刚才提到了，AI数据中心已经不需要离用户太近了。那么AI数据中心选址时，什么更加重要呢？现在电力的充足性成了首要考量。中国的**东数西算**（East Data West Computing: 中国将东部地区的数据处理需求引导至西部地区进行计算的国家战略），也就是将东部产生的数据拿到西部能源充足的数据中心计算，也是同样的道理来布局数据中心的。现在很多时候有一个有趣的现象，就是衡量一个数据中心的大小已经不使用GPU的数量了，而是使用耗电量。Jensen Huang（黄仁勋）在GTC等演讲中通常用“吉瓦级功率”和“百万GPU级规模”来描述未来AI数据中心的算力潜力。在2025年7月，OpenAI与Oracle签署协议，计划开发大约4.5吉瓦特的**数据中心电力容量**（Data Center Power Capacity: 数据中心能够支持的最大电力负荷，通常以兆瓦或吉瓦计）。这种数据中心GPU的数量应该是几百万了。电力是一个传统行业，以往增速很慢。今年很多电力股都因为这种巨大的电力需求而大涨了。AI数据中心对于电力需求的可能最终的解决方案，我认为是核能，尤其是可控核聚变。总之，电力基础设施会占整个数据中心投资的20%。

### “吉瓦级”AI数据中心的挑战：制冷难题

下一个挑战是制冷。刚才我一直在强调高密度，高密度导致散热更加困难，使AI数据中心对于制冷的要求大大提高。传统数据中心的散热有些类似于办公室空调加电脑的风扇，在高密度的环境下，根本就不够。于是，**液冷**（Liquid Cooling: 一种使用液体作为散热介质的冷却技术，通过液体的高热容和导热性带走设备热量）正在迅速普及。液冷是一种使用液体作为散热介质的技术，通过液体的高热容和高导热性，将电子设备产生的热量带走，以实现冷却。数据中心需要液冷是因为现代计算设备的功耗和发热量越来越高，传统风冷已无法满足散热需求，而液冷能够提供更高的散热效率，并带来节能、提高机柜密度、运行更稳定等优势。液冷同样会消耗大量的水和电力。制冷系统会占到数据中心投资的15%左右。

### 数据中心建设的成本构成

建设数据中心还需要网络，包括数据中心内部的网络，以及和外部连接的网络。我在讲加速计算的视频中，有对于数据中心网络的专门讲解。例如，Broadcom和Arista就是专门做数据中心网络芯片和系统的。英伟达有自己的**Nvlink**（NVIDIA NVLink: 英伟达开发的高速互连技术，用于GPU之间或GPU与CPU之间的数据传输）和**Infiniband**（InfiniBand: 一种高速、低延迟的计算机网络通信技术，常用于高性能计算和数据中心）技术，同时也支持**以太网**（Ethernet: 一种广泛使用的局域网技术，用于连接计算机和其他网络设备）。网络投资会占到数据中心投资的5%左右。

另外的50%，就是芯片和服务器这些IT设备了。而芯片，主要就是GPU、**TPU**（Tensor Processing Unit: 谷歌开发的专用集成电路，用于加速机器学习任务）、CPU、**DPU**（Data Processing Unit: 数据处理单元，一种新型处理器，用于卸载和加速数据中心的基础设施任务）、HBM等等。总结一下，制冷系统会占到数据中心投资的15%左右，网络投资会占到数据中心投资的5%左右，另外的50%，就是芯片和服务器这些IT设备了。

### 巨头们的资本支出与未来展望

运营一个AI数据中心同样价格不菲。英伟达的GPU迭代非常快，导致GPU每年产生大量的折旧费用。这就是为什么很多投资者担心用于AI数据中心建设的资本支出增长过快。它和以前铺设光纤、建设铁路、公路的不同之处在于AI数据中心折旧太快，过时太快。

今天我们了解了这些互联网巨头巨大投入的AI数据中心是什么，为什么密度非常重要，为什么GPU的数量越来越大，为什么电力消耗和液冷都是十分关键。巨头的资本支出持续加速：亚马逊2025年预计总投入1250亿美元，谷歌预计2025年的资本支出将在910亿至930亿美元之间，Meta预计2025年资本支出为700亿至720亿美元，微软在下一财年将投入1000亿美元，苹果预计未来4年在AI领域投资800亿美元。

可以预期，接下来几年一座座崭新的AI数据中心将拔地而起，AI工厂将成为第四次工业革命的大脑。不管是不是泡沫，人类正在创造奇迹，它们都是伟大的工程，就和当年埃及人建金字塔、中国人修长城一样被载入史册。我们是有幸见证奇迹和见证历史的一代人。同时，和大家一起学习AI数据中心最新知识，提高自己的认知，让我们一起在这场AI数据中心的科技革命中找到逆风翻盘的机会。