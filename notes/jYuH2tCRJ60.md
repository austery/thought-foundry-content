---
area: tech-work
category: ai-ml
companies_orgs:
- Meta
- OpenAI
- Google
- Anthropic
- IBM
- Bell Labs
- NVIDIA
- SSI
- Physical Intelligence
date: '2025-12-21'
draft: true
guest: ''
insight: ''
layout: post.njk
media_books:
- 《The Information Bottleneck》
people:
- Yann LeCun
project:
- ai-impact-analysis
series: ''
source: https://www.youtube.com/watch?v=jYuH2tCRJ60
speaker: Best Partners TV
status: evergreen
summary: 图灵奖得主Yann LeCun批判当前硅谷主流AI发展路径，认为扩大大语言模型规模是死胡同，AGI概念站不住脚。他离开Meta创办AMI公司，押注以认知与感知优先的世界模型技术路线。LeCun强调AI应具备理解和预测世界的能力，而非仅记忆和复述，并指出安全应与发展同步，目标驱动架构是关键。他预测，达到狗水平智能需5-10年，人类水平智能则更久。
tags:
- agi
- ai-safety
- world-model
title: Yann LeCun 批判硅谷AI路径：世界模型是未来，AGI是胡扯
---

大家好，这里是最佳拍档，我是大飞。最近呢，图灵奖得主、前**Meta**首席AI科学家**Yann LeCun**在《**The Information Bottleneck**》栏目里，用将近两小时的时间，把当前硅谷追捧的AI发展路径**批了个体无完肤**。他直言，靠扩大大语言模型的规模、喂合成数据、搞强化学习微调就想通往超级智能，**完全是胡扯**。甚至说，**AGI**（Artificial General Intelligence: 通用人工智能）的概念本身就站不住脚。

### 路线之争
更重磅的是，这位65岁本来可以退休、安享晚年的AI泰斗，选择离开效力12年的**Meta**，从零创办一家新的公司，押注一套全新的技术路线，要**重新定义AI的未来**。这已经不是一场普通的吐槽了，而是**AI领域路线之争的一次公开宣战**。一边是硅谷巨头们扎堆押注的算力、数据、参数竞赛，另一边是**Yann LeCun**坚持的认知与感知优先的**世界模型**（World Model: 模拟现实世界以理解和预测的AI模型）。今天咱们就来回顾一下这期访谈，看看这位AI先驱，为什么敢逆势而为，他口中的世界模型到底是什么，以及这场技术革命可能给AI行业带来怎样的颠覆。

### 创业动机
很多人的第一反应可能会是，**Yann LeCun**已经功成名就了，图灵奖、女王奖在手，在**Meta**主导了AI研究12年，推动了深度学习、计算机视觉等多个领域的发展，完全可以退休享受荣誉。但是他偏偏在65岁这个年纪选择创业，背后到底是什么原因呢？**Yann LeCun**在访谈中坦言，他离开**Meta**创办**先进机器智能公司AMI（Advanced Machine Intelligence）**，核心原因是现在的AI投资热潮让长期研究型的创业成为了可能。放在以前，这类需要长期投入、短期内看不到回报的基础研究，只能依托**IBM**、**贝尔实验室**这样的垄断型大企业，或者**Meta**、**Google**、**Microsoft**这些科技巨头的研究院。

### 研究转向与AMI定位
但是现在情况变了，包括**Google**、**OpenAI**甚至**Meta**在内的很多实验室，都从曾经的开放研究转向了封闭，因为大家更看重短期产品的落地，而非真正的技术突破。**Yann LeCun**点出了当前工业界研究的痛点。在他看来，真正的研究必须公开发表，接受学界的检验，否则内部过度追捧的项目很可能只是一种自嗨式错觉，因为你永远不知道别人已经做出了更出色的工作。

### 世界模型与技术路线
所以**AMI**从一开始就明确了定位：既要坚持开放研究，所有上游的核心成果都会公开发表，又要最终推出实际产品，围绕世界模型和规划技术，成为未来智能系统的主要供应商之一。**Yann LeCun**之所以有这样的底气，是因为他已经在这个方向上钻研了将近20年，再结合**纽约大学**和**Meta**的研究积累，现在终于到了可以把构想变成现实的时刻了。

### LLM的局限性
而选择创业的另一个深层原因，是**Yann LeCun**对当前主流AI发展路径的彻底否定。他认为，现在硅谷所有的公司都在扎堆做同一件事，那就是扩大大语言模型的规模，堆砌数据和算力，优化强化学习微调。这种技术单一化的现象非常危险，就像所有选手都挤在同一条赛道上拼命往前冲，却完全忽视了可能从另一个方向而来的颠覆性技术。而他要做的，就是那条少有人走的路：构建一个能够理解和预测世界的世界模型。

### 记忆而非理解
**Yann LeCun**直言，当前基于大语言模型的架构，虽然在语言的处理上表现尚可，但是构建的智能体系统存在着致命的缺陷，那就是需要海量的数据来模仿人类行为，而且可靠性极低。为什么会这样呢？**Yann LeCun**指出了核心问题：大语言模型本质上就是一个记忆型系统，而不是理解型的系统。

### 数据量与文本局限
要训练一个性能还不错的大语言模型，需要用到几乎整个互联网的文本数据，再加上合成数据和授权数据。两三年前，主流模型的预训练规模就已经达到了30万亿**token**（令牌: 语言模型处理的文本单位，通常是单词或词的一部分），相当于10的14次方字节的数据量。这么大的数据量，本质上是让模型记住文本中孤立的事实，然后在生成内容时进行复述。但是文本数据的冗余度很低，缺乏真实世界的结构信息，这就决定了大语言模型永远无法真正理解世界。

### 复杂数据处理难题
更关键的是，大语言模型完全无法处理高维度、连续而且含有噪声的数据模态，比如图像、视频、物理世界的感知数据。虽然现在有些大模型会结合视觉模块，但是这些视觉能力都是分开训练的，并不属于大语言模型架构的核心部分。

### 视频数据对比
**Yann LeCun**做了一个很直观的对比：10的14次方字节的文本数据是大语言模型的训练基础，但是同样体量的视频数据只相当于1.5万小时的内容，这大概是YouTube半小时的上传量，也差不多是一个四岁孩子一生中清醒时间看到的视觉信息总量。而视频数据的结构远比文本丰富，这种丰富的冗余结构正是**自监督学习**（Self-supervised learning: 一种机器学习方法，模型从无标签数据中学习特征，通过数据本身生成监督信号）的关键。如果数据是完全随机的，自监督学习根本无法进行。

### 文本训练的终点
**Yann LeCun**一直坚持一个观点：仅靠文本训练，永远不可能达到人类水平的智能，因为真实世界的理解、预测和行动能力远比生成流畅文本要复杂得多。而现有以语言为核心的模型，从未真正触及到这个问题的本质。

### 狗的智能水平
在他看来，当前AI行业最大的误区，是把逼近人类级别的智能当作了目标，却忽略了一个更基础、更困难的门槛，那就是让机器具备**狗的智能水平**。狗能理解物理世界的基本规律：物体不会凭空消失、东西会下落、不能同时出现在两个地方，也能根据环境变化调整行为，进行简单的规划和预测。

### LLM的复述本质
而这些能力，恰恰是大语言模型完全不具备的。它们只是被微调到给出看起来正确的答案，这是复述，不是理解。**Yann LeCun**毫不客气地说，那些宣称一两年内实现AGI的说法，**完全是脱离现实的幻想**。真实世界的复杂度，远不是通过对世界进行token化再喂给语言模型就能解决的。

### 世界模型的定义
既然大语言模型走不通，那么**Yann LeCun**押注的世界模型到底是什么？很多人会误以为，世界模型是对现实世界每一个细节的完整复刻，比如像《星际迷航》里的全息甲板那样的高度逼真模拟器。但是这恰恰是**Yann LeCun**极力反对的错误认知。世界模型不需要是现实的逐个像素模拟器，而是在抽象表征空间中，只模拟与任务相关的那部分现实。

### 抽象层级的重要性
**Yann LeCun**举了一个很经典的例子：如果问100年后木星在哪里，你根本不需要关于木星的全部信息，只需要6个数字——三个位置坐标和三个速度分量，其余的细节都无关紧要。这个例子背后是抽象层级（Abstract Representation）的核心思想。在真实世界中，我们对事物的理解都是分层抽象的，从粒子、原子、分子到细胞、器官、个体、社会，每一层的抽象都会忽略下层的大量细节。而正是这种忽略，让我们能够进行更长期、更稳定的预测。

### 物理过程的抽象
比如用流体力学来模拟飞机周围的气流，不会逐个分子的模拟，而是把空间切成小立方体，记录速度、密度、温度等关键变量，解偏微分方程。

### 世界模型的核心逻辑
这就是对物理过程的高度抽象，既高效又有效。所以，世界模型的核心逻辑，就是学习这样的抽象表征空间，先滤除输入中大量无法预测的细节，然后在这个简化后的表征空间内预测世界的演化规律。它关注的不是生成看起来像什么，而是世界将如何变化，从而为机器提供更接近真实认知的基础能力。

### AI缺失的前额叶皮层
这正是当前AI缺失的**前额叶皮层**（Prefrontal Cortex: 大脑负责规划、决策、高级认知功能的区域）功能，对应着规划、预测和行动能力。与主流生成模型相比，世界模型的本质区别在于：生成模型是在像素或文本层面直接输出的，试图复现表面的统计相关性；而世界模型是在抽象表征层面进行预测，捕捉的是世界的底层动力学规律。**Yann LeCun**强调，一个视频生成模型可能看起来很炫酷，但是它并不一定理解世界的底层逻辑。而世界模型的目标，就是让AI真正看懂世界的运行规则。

### JEPA架构
要实现这样的世界模型，**Yann LeCun**团队的核心技术架构是**联合嵌入预测架构JEPA（Joint Embedding Predictive Architecture）**。**JEPA**的核心思想，就是放弃像素级或文本级的直接预测，转而在抽象表征空间中进行预测，同时通过一系列技术手段避免**模型坍缩**（Model Collapse: 模型为了最小化预测误差而输出恒定表征，导致学不到有用信息）。

### 20年研究沉淀
回忆起来，**Yann LeCun**对世界模型的执着，并不是一时兴起，而是跨越近20年的研究沉淀。在这条路上，他经历过方向的迷茫、技术的瓶颈，也见证了整个领域的起起落落。早在近20年前，**Yann LeCun**就确信，构建智能系统的正确途径是某种形式的**无监督学习**。

### 早期自编码器研究
21世纪初，他就开始朝着这个方向探索。当时的主流思路是训练**自编码器**（Autoencoder: 一种神经网络，用于学习数据的压缩表示，常用于无监督学习）来学习表征：编码器将输入转化为表征，再解码还原，确保表征包含输入的全部信息。但是后来他发现，这种表征必须包含全部信息的直觉是错误的，并不是构建智能的有效方法。那段时间，他和团队尝试了多种方案，包括受限玻尔兹曼机、去噪自编码器等等。而**Yann LeCun**自己主攻稀疏自编码器，通过高维稀疏表征来构建信息瓶颈，限制表征中的信息量。

### 研究方向的转变
但是事情出现了转折：随着归一化、ReLU激活函数等技术的出现以及数据集规模的扩大，研究人员发现，在完全有监督的方式下，也能成功训练相当深的网络。于是，自监督/无监督学习的想法被暂时搁置。直到2015年**残差网络（ResNet）**（Residual Network: 一种深度神经网络架构，通过残差连接解决深层网络训练难题）的出现，基本解决了训练极深架构的问题。

### 世界模型成型
也是在2015年，**Yann LeCun**重新回到了如何迈向人类级别AI的初心。他意识到，强化学习等方法在样本效率上极低、难以扩展，于是关于世界模型——即系统能够预测自身行动后果并且进行规划——的想法开始真正成型。

### NIPS演讲与像素级预测
2016年，他在**NIPS**（Neural Information Processing Systems）的主题演讲中公开阐述了这个核心主张，随后和学生开始在视频预测等领域进行具体的研究。但是当时整个领域都犯了一个根本性的错误，那就是试图在像素级别进行预测。在视频这样的高维连续空间里，这种做法几乎是不可能的，因为预测本质是非确定性的，模型需要潜变量来表征未知信息，而像素级预测的复杂度远超当时的技术能力。

### 对比学习的起源
杨立昆和团队实验了多年，探索了扩散模型、基于能量的模型等训练非确定性函数的方法，始终没有突破。直到后来，他才领悟到根本的出路，那就是放弃像素级预测，转向抽象表征层面的预测。但是这个方向也面临一个重大的难题——坍缩。早在九十年代，**Yann LeCun**就发现，如果只是简单训练两个共享权重的神经网络，让它们对同一对象的略微不同版本输出相同的表示，系统很快就会塌缩，学不到任何有用的东西。

### 对比学习的商业化尝试
为了解决这个问题，1993年，**Yann LeCun**引入了对比项的核心思路：除了相似样本对，还引入不相似的样本对，通过训练让系统在相似样本上拉近表示，在不相似样本上拉远表示，形成相似吸引、不相似排斥的代价函数。这个想法最初来自一个非常实际的需求：为信用卡磁条设计小于80字节的手写签名编码，用于签名验证。最终技术上大获成功，但是商业上却被用PIN码替代的方案否决。这也让**Yann LeCun**得到一个深刻的教训：技术可行，不代表商业上会被采纳。

### 对比学习的复兴
到了2000年代中期，**Yann LeCun**和两位学生**Raia Hadsell**以及**Sumit Chopra**重新回到对比学习方向，提出了新的目标函数：正样本对应低能量，负样本对应高能量。能量本质上就是表征之间的距离。他们在2005年和2006年的**CVPR**（Conference on Computer Vision and Pattern Recognition）会议上发表了相关论文，让对比学习重新活了过来。但是当时的效果并不理想，比如在图像任务中学到的表征维度很低，在**ImageNet**（一个大型图像数据集）上训练后，表征的有效维度只有两三百，远达不到实际应用的需求。

### 突破性进展：Barlow Twins
真正的突破发生在大约五年前。**Yann LeCun**在**MIT**的博士后**Stefano**提出了一个他最初并不看好的想法：直接最大化编码器输出的信息量。**Yann LeCun**之所以怀疑，是因为早在1980年代，**Geoffrey Hinton**就做过类似的尝试，信息量本身很难最大化，因为通常只有上界，没有可计算的下界。但是**Stefano**提出的方法居然奏效了，这就是后来以理论神经科学家命名的**巴洛双胞胎（Barlow Twins）方法**（Barlow Twins: 一种利用最大化编码器输出信息量来学习表征的方法）。这个突破让**Yann LeCun**意识到，这个方向值得深入推进。

### VICReg 与 SigReg
随后，他们又提出了**方差-不变性-协方差正则化VICReg（Variance–Invariance–Covariance Regularization）**（VICReg: 一种简化且有效的表征学习方法），结构更加简单，效果反而更好。最近，**Yann LeCun**和**Randall**还讨论了一个可以进一步工程化的方案——**信号正则化（SigReg）**（SigReg: 一种约束编码器输出向量分布接近各向同性高斯分布的方法）。核心思想是约束编码器输出的向量分布接近各向同性高斯分布。**Yann LeCun**对这个领域的未来充满信心，他认为未来一两年内还会有显著的进展，而这条技术路线，正是训练能够学习抽象表征模型的关键，而抽象表征，恰恰是世界模型的核心。

### AGI是谎言
在访谈中，**Yann LeCun**最颠覆认知的言论莫过于AGI是个彻头彻尾的谎言。但是他并不是要否定AI能达到高的水平，而是认为通用智能这个概念本身就站不住脚。它本质上是以人类智能为参照定义的，但是人类智能本身是高度专用化的。

### 人类智能的专用性
**Yann LeCun**解释道：我们擅长在现实世界中行动、与他人互动，但是在下棋等任务上表现糟糕；而很多动物在某些方面远胜人类，比如狗的嗅觉、猫的灵活性。我们之所以误以为自己是通用的，只是因为我们只能理解自己能够想象的问题。

### 人类水平智能的讨论
所以在他看来，与其讨论通用智能，不如讨论人类水平的智能：比如机器是否会在所有人类擅长的领域达到或超过人类？答案是肯定的，而且在某些领域已经发生了，比如机器可以在上千种语言之间进行双向翻译，这是任何人类都无法做到的。但是这个过程不会是一个突发事件，而是一个渐进的过程。

### AI发展时间表
**Yann LeCun**给出了一个非常保守但是务实的时间表：如果一切顺利，没有遇到尚未意识到的根本性障碍，最乐观的情况是，在5到10年内，我们或许能看到接近人类或者至少接近狗水平的智能系统。但是这只是最乐观的估计。历史告诉我们，AI发展中总会出现新的瓶颈，所以可能需要20年甚至更久才能够突破。

### 狗水平智能的难度
而更令人意外的是，**Yann LeCun**认为，从当前的AI水平到狗水平智能，比从狗水平到人类水平更难。一旦你达到狗的智能阶段，绝大多数核心要素就已经具备了。他解释道，从灵长类到人类，真正新增的关键能力可能主要是语言，而语言在大脑中只占据极小的区域。我们现在的大语言模型已经在这方面做得相当不错，某种意义上，未来的语言模型可能会扮演人脑中布罗卡区和韦尼克区的角色。而我们当前真正缺失的，是相当于前额叶皮层的能力，也就是世界模型的规划与行动能力。

### 智能的核心：预测与规划
**Yann LeCun**认为，智能的核心不在于记忆和复述，而在于能够预测自身行动的后果，并且用于规划。狗之所以具备基础智能，就是因为它能理解物理世界的基本规律，能根据环境预测接下来会发生什么，然后调整自己的行为。而当前的AI，恰恰缺少这种预测和规划的闭环。这也是为什么，即便大语言模型能生成流畅的文本，却依然无法在真实世界中自主行动。

### AI安全问题
随着AI能力的提升，安全问题一直是业界争论的焦点，尤其是AI灭世论的说法，让很多人担心AI未来会失控。**Yann LeCun**作为AI领域的权威，也分享了他的独到见解。他既不认同暂停AI发展的极端观点，也反对忽视风险的盲目乐观。

### AI恐惧的真实伤害
**Yann LeCun**坦言，他亲身经历过AI恐惧带来的真实伤害。有一次在**纽约大学**的校园，他遇到一名情绪严重不稳定的人，携带危险物品被警方带走，而这个人的精神状态就受到了AI灭世论的影响。还有高中生给他写信，说他们被这类言论吓到，甚至不再上学。

### 工程与治理是关键
但是他同时强调，历史告诉我们，任何强大的技术都会带来利弊。关键不在于是否发展技术，而在于如何通过工程和治理来控制风险。他用汽车来举例：早期的汽车极其危险，但是通过安全带、溃缩区、自动刹车系统等技术演进，如今已经大幅降低了死亡率。欧盟强制配备的自动紧急制动系统，已经被证明能减少40%的正面碰撞事故。AI也是如此，它既可能带来风险，也已经在医疗影像等领域挽救了大量生命。

### 安全与发展同步
对于是否需要暂停AI发展来专注安全的问题，**Yann LeCun**的答案非常明确：安全必须与发展同步进行，而不是先停下来等着绝对安全。他用喷气发动机作比喻：第一代喷气发动机根本不安全、不可靠，但正是在不断工程改进中，才达到了今天这种可以连续飞行17小时的可靠性。所以他认为，AI也会走类似的路径。

### 目标驱动的AI架构
我们会逐步构建具备规划与行动能力的系统，同时在底层引入明确的安全约束。比如家用机器人必须始终避开人类、不能伤害人；手持刀具时必须限制动作幅度。这些都可以通过低层的规则来明确约束。所谓**回形针最大化**（Paperclip Maximizer: AI安全领域的思想实验，描述一个AI为达成目标而产生灾难性后果的场景）的极端案例，在工程上其实是非常容易避免的。而对于大语言模型容易被越狱、绕过安全限制的问题，**Yann LeCun**认为，这正是依赖大语言模型的根本缺陷。我们不应该指望通过微调或内容过滤来解决安全问题，而应该转向**目标驱动（objective-driven）的AI架构**。

### 架构的安全设计
这种架构的安全设计是先天的，而非事后修补。它具备三个关键能力：第一，拥有世界模型，能够预测自身行为可能带来的后果；第二，可以规划一系列的行动来完成任务；第三，也是最关键的，受到一整套硬性约束的限制，确保无论采取什么行动、预测到什么世界状态，都不会对人类造成危险。换句话说，它的输出不是靠过滤坏的内容，而是通过在满足约束条件的前提下优化目标函数得出的，从结构上就不具备逃逸的可能性。

### 荒谬的暴力搜索
对于那些通过暴力搜索来提升安全性的方法，比如让模型生成大量的候选输出，再用过滤系统挑出最不糟糕的结果，**Yann LeCun**直言极其荒谬。这种方法的计算成本高得离谱，无法规模化，除非有真正意义上的目标函数或价值函数，能在生成过程中就把系统引导到高质量、低风险的输出。

### Meta内部布局与Alex Wang
访谈中，主持人还问到了**Meta**内部的AI布局，以及**Alex Wang**是否在接替**Yann LeCun**角色的问题。**Yann LeCun**也公开回应了这些内斗传闻。首先，**Yann LeCun**明确表示，**Alex Wang**并不是在接替我。他解释道，**Alex Wang**负责的是**Meta**所有AI相关的研发与产品整体运作，而不是科研本身。**Alex Wang**并不是研究员或科学家，而是一个负责全面统筹的管理者。在**Meta**的超级智能实验室体系下，AI相关工作大致分为四个部分：第一是**人工智能基础研究实验室（FAIR）**（Fundamental AI Research），负责长期基础研究；第二是**TBD Lab**，主要做前沿模型，几乎完全聚焦大语言模型；第三是AI基础设施，包括软件和硬件；第四是产品部门，把前沿模型做成真正可用的产品，比如聊天机器人，并且集成到**WhatsApp**等应用中。**Alex Wang**的职责是统管这四个方向。

### FAIR的定位变化
而**Yann LeCun**本人是**FAIR**的首席AI科学家。在访谈时他已经明确表示，还会在**Meta**待三周左右，之后就会正式离职。对于**FAIR**的定位变化，**Yann LeCun**也毫不避讳地表示，**FAIR**目前由他在**纽约大学**的同事**Rob Fergus**领导。在**Joel Pineau**离开后，**FAIR**被明显推向更短期、更偏应用的研究方向，发表论文的重要性下降，更多是为**TBD Lab**的大模型工作提供支持。

### Meta的封闭化趋势
这也意味着**Meta**整体正在变得更封闭。有些原本属于**FAIR**的研究团队，也被重新归类到产品部门，比如做**SAM（Segment Anything）模型**（SAM模型: Meta开发的一个图像分割模型）的团队，因为他们的技术更偏向对外、实用型，更适合直接对接产品落地。这种重应用、轻基础研究的转向，或许也是**Yann LeCun**最终选择离开的重要原因之一，毕竟，他一直坚信基础研究的开放和沉淀，才是AI长期发展的核心动力。

### 点评同行：SSI与Physical Intelligence
作为世界模型领域的先驱，**Yann LeCun**也点评了当前其他试图构建世界模型的公司，其中既有肯定，也有尖锐的批评。首先是伊利亚的**SSI**，**Yann LeCun**直言它已经成了行业笑话，几乎没人知道他们在干什么，包括他们自己的投资人。不过他也强调这只是传言，不确定真假。对于**Physical Intelligence**公司，**Yann LeCun**表示了解他们的大致方向，他们主要做几何一致的视频生成，即场景具有持久的三维结构，转身再回来，物体不会凭空变化。但是**Yann LeCun**认为，这仍然是生成像素的思路，而他早就已经明确，生成像素本身是个错误方向，因为这种方法无法让模型真正理解世界的底层动力学，只是学会了表面的统计相关性。

### Wayve的自动驾驶探索
在所有同行中，**Yann LeCun**相对认可的是总部位于牛津的**Wayve**公司，他本人也是这家公司的顾问。**Wayve**在自动驾驶领域构建了一个世界模型，核心思路是先学习一个表征空间，再在这个抽象空间中做时间预测。**Yann LeCun**认为他们做对了一半：对的地方在于预测应该发生在表征空间，而不是像素空间；但是问题在于，他们的表征空间仍然主要通过重建训练得到，这一点在**Yann LeCun**看来是错误的。不过即便如此，**Wayve**的系统整体效果非常好，在自动驾驶领域已经走得相当靠前。

### 其他探索方向
此外，**NVIDIA**和**Sandbox AQ**公司也在探索类似的方向。**Sandbox AQ**的首席执行官**Jack Hidary**提出了**大型定量模型（Large Quantitative Models）**的概念，而非语言模型，本质上就是能够处理连续、高维、噪声数据的预测模型，这与**Yann LeCun**的主张高度一致。**Google**也做了很多世界模型相关的研究，但是主要仍然是生成式路径。**Yann LeCun**认为，**Danijar Hafner**的**Dreamer**系列模型其实走在了正确的道路上，只可惜**Danijar Hafner**已经离开**Google**创业了。

### 核心判断标准
从这些点评可以看出，**Yann LeCun**判断一个世界模型公司是否走对路的核心标准，就是是否放弃了像素级/文本级的直接生成或重建，转向了抽象表征空间的预测。这也正是他自己多年研究得出的核心结论。

### 避开硅谷的技术单一化
**Yann LeCun**之所以选择创办一家全球性的公司，在**Paris**、**New York**等地布局，而不是扎根**Silicon Valley**，核心原因之一就是硅谷存在严重的技术单一化问题。在硅谷，竞争极端激烈，所有公司都在被迫做同一件事。如果你走一条不同的技术路线，就会掉队的巨大风险。

### LLM的洗脑文化
**Yann LeCun**解释道：现在**OpenAI**、**Meta**、**Google**、**Anthropic**几乎所有的硅谷巨头都在扎堆做大语言模型，拼命扩大规模、堆砌数据和算力。在这种环境下，没有人敢轻易尝试不同的技术路线，因为一旦失败，就可能被市场淘汰。更严重的是，硅谷还形成了一种大语言模型洗脑的文化，很多人坚信，只要不断扩大模型的规模、生成更多合成数据、加强强化学习微调，就一定能走向超级智能。但是**Yann LeCun**认为这是彻底错误的，因为大语言模型根本不擅长处理连续、高维、噪声数据。比如在视频、物理世界感知等领域，大语言模型的尝试几乎都失败了。大家拼命都在同一条战壕里向前冲，却很容易被来自完全不同方向的技术突破所颠覆。

### 内部认同与招募
**Yann LeCun**的这句话，其实是对整个行业的警示。他透露，其实在硅谷的大公司内部，也有不少人私下认同他的观点，只是迫于公司的战略方向，无法公开表达或者践行。而他现在正在把这些人招入自己的新公司**AMI**。

### 65岁创业的使命感
最后，主持人问到了一个很多人关心的问题：对于已经65岁、功成名就的**Yann LeCun**，为什么还要选择从零开始创业？他的回答非常简单：因为使命感。**Yann LeCun**认为，**提升世界上的智能总量，是一件内在正确的事情**。智能是这个世界上最稀缺、最被需要的资源，这也是为什么人类会投入如此多的成本去教育。无论是帮助人类更聪明，还是用机器来增强人类智能，本质上都是在服务同一个目标。

### 技术风险与工程治理
强大的技术必然会伴随风险，但那是工程和治理的问题，而不是不可逾越的根本障碍。**Yann LeCun**一生的研究、教学、公共传播，几乎都围绕着让人类变得更聪明这件事，而机器智能，本质上也是这个目标的一部分。他之所以在65岁的年纪还要创业，就是因为他相信，自己多年坚守的世界模型路线，能够真正推动AI的本质进步，为人类带来更有价值的智能工具。

### 职业生涯的遗憾
当然，回顾职业生涯，**Yann LeCun**也有遗憾，最让他后悔的是没有花足够时间把自己的想法写下来，结果经常被别人抢先。**反向传播算法（Backpropagation）**（Backpropagation: 用于训练神经网络的算法，通过计算梯度来更新模型权重）就是一个例子，他其实很早就有了类似的思路，但是没有及时完整的发表。

### 科学思想的演进
不过**Yann LeCun**并没有纠结于此，因为他明白，科学思想几乎从来不是孤立产生的。从想法到论文、到理论、到应用、到产品，本身就是一个漫长而复杂的链条。就像世界模型这个概念一样，其实它也并不新了，早在1960年代，控制论和航天工程就已经在使用世界模型来规划火箭轨道，所谓系统辨识，更是1970年代的老概念。但是真正的难点，从来不在谁最早提出，而在于把一个想法真正变成可工作的系统，而这正是**Yann LeCun**现在要做的事情。

### AI行业的路线宣言
**Yann LeCun**的这场访谈，与其说是一次个人观点的表达，不如说是一场AI行业的路线宣言。一边是硅谷巨头们主导的算力+数据竞赛，追求短期产品落地和商业价值；另一边是**Yann LeCun**坚守的认知+感知路线，执着于基础研究和长期技术突破。到底谁是对的？现在还无法给出答案。

### 多元视角与颠覆性突破
但可以肯定的是，**Yann LeCun**的存在，为AI行业提供了一种宝贵的多元视角。如果所有顶尖人才和资源都扎堆在同一条赛道上，行业很可能会陷入路径依赖，错过真正的颠覆性突破。而**Yann LeCun**的创业，恰恰为这种突破提供了可能性。也许正如他所说，**智能的核心应该是预测和规划，而不是记忆和复述**。未来的AI，终将走出实验室，走进真实世界，成为能够理解、预测、行动的智能体。而世界模型，很可能就是通向这个未来的关键钥匙。无论最终结果如何，这位65岁仍在追梦的AI泰斗，用自己的坚守和勇气，为我们展现了科学家的初心和使命感。而这场关于AI未来的路线之争，也必将深刻影响整个行业的发展方向。感谢收看本期视频，我们下期再见。