---
author: 北美王路飞
date: '2026-01-26'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=wmCl82T4LP0
speaker: 北美王路飞
tags:
  - ai-bubble
  - investment-risk
  - llm-limitations
  - world-model
  - vc-incentives
title: “AI泡沫警报：万亿巨婴的幻灭与《大空头》原型警告”
summary: “本期视频深入探讨了当前AI领域的投资泡沫。以《大空头》原型人物Steve Eisman和AI批评者Gary Marcus的观点为主线，揭示了AI行业可能面临的‘万亿巨婴’增长谬误、技术局限（如缺乏世界模型）以及风投的利益驱动。视频将OpenAI与WeWork进行类比，并分析了谷歌等巨头在资源战中的优势。最终提出，真正的AI发展需要具备常识和世界模型的神经符号AI，并警示投资者区分科幻与现实，警惕泡沫破裂。”
insight: ''
draft: true
series: ''
category: ai-ml
area: tech-engineering
project: []
people: []
companies_orgs:
  - OpenAI
  - WeWork
products_models:
  - GPT-4o
  - GPT-5
media_books:
  - The Big Short
  - Airplane!
status: evergreen
---
### AI泡沫的警示信号

不知道大家有没有看过这样一个经典动画片，叫做**威力狼**（Wile E. Coyote）。在美国非常流行，在中国也可能播过。这只狼经常拼命追逐猎物，不知不觉间冲出了悬崖边缘，脚在半空中疯狂踩踏，但它没有掉下去，为什么呢？因为它还没有往下看。只要他不往下看，他就不会掉下去。

在金融学的泡沫中，也存在类似的情况，那就是所谓的“狼性时刻”。只要你不看脚下，只要你相信地心引力暂时失效，你就能在空中再飘一会。但今天，我们要做的就是那个最危险的动作——我们得往下看一眼。

让我们把时间拨回到一年前，2025年1月。那时，**Sam Altman**在播客里信誓旦旦地写道：“我们已经知道怎么造出**AGI**（Artificial General Intelligence: 通用人工智能）了。”当时他的口气骄傲到无以自拔。所有人都认为，**ChatGPT 5**会像上帝降临一样，在超级碗之后发布，实现人类的AGI。结果呢，就像电影**《空前绝后满天飞》**（Airplane!）里那个经典的广播一样：通往AGI的航班，现在改到8号登机口……哦不，又晚点了，改到9号登机口……改了又改，拖了又拖。

直到2025年8月7日，当时我是非常期待的，因为我对**ChatGPT 4o**非常满意，觉得升级到5（或其后续版本）那一定是出现质的飞跃。Sam Altman当时宣传说，这个新的模型已经抵得上一堆博士了。结果我上手玩了一小时就发现，它就相当于一个ChatGPT 4的升级版。之前存在的幻觉、种种问题，现在依然存在。我们大家所期待的这种量子跃迁级别的升级，并没有出现。实际上，我们换来的仅仅是一个版本的小更新，你都不能称之为‘5’。

这里就引出了我们这一期视频的主题。曾经在**《大空头》**（The Big Short）里扮演原型人物的**Steve Eisman**出来做了一期播客，他的嘉宾是AI圈的头号“反贼”，叫做**Gary Marcus**。之前我们频道也做过他相关的视频，大家感兴趣可以去看一下，我会把链接放在描述栏。大家要知道，当年大家把房地产债券当成金矿的时候，Steve Eisman已经开始怀疑底下的烂账了。现在，这位不信邪的华尔街老炮，把枪口对准了AI。他和Gary Marcus两个人开始一起疯狂吐槽AI。这两人的核心观点就是：如果这万亿美元的投入换不回真金白银的回报，那这一场投资盛宴可能很快就要结束了。

那么，你肯定会问：现在这些硅谷大佬都是聪明绝顶的，他们怎么可能会集体误判？这些人难道不比你们聪明吗？Gary Marcus给出了一个非常有意思的解释：他认为这些人正在犯一个“万亿磅巨婴”的谬误。逻辑是这样的：你生了一个孩子，刚出生8磅，一个月后长到16磅。如果你是一个只会画延长线的分析师，做一个线性规划，屏幕上会出现一条笔直向上的线，然后告诉所有投资人：“你看，按照这个速度，当这个孩子上大学的时候，体重将达到1万亿磅！”听起来非常荒谬，对吧？但是，这正是过去几年AI圈正在做的事情：堆更多数据，买更多显卡，模型就会无限变强，直到最终成神。但是，2026年的现实给了所有人一记耳光：物理规律确实存在，边际效益递减是真实存在的。给孩子喂再多奶粉，他也不可能长成哥斯拉。

在揭示了增长神话的虚假性后，批评者们将AI行业的明星公司置于显微镜下，并将其与过往的泡沫案例进行对比。

### OpenAI的困境与LLM的局限

到这里，Gary Marcus抛出了一个“暴论”：他认为**OpenAI**就等于当年的**WeWork**的处境——估值上天，亏损无底洞，而且技术护城河可能根本不存在。如果仅从这样一个抽象的对比来看，这些因素确实能够匹配。

那这里，我必须得为OpenAI稍微辩护一下。我觉得OpenAI和WeWork还是有本质区别的。WeWork当年的商业模型，其实是非常明确的一个“二房东”计划，而且它事实上能提供多少不一样的价值呢？我觉得是非常有限的。但是，对于像OpenAI这样的公司来说，它的模型是有价值的。而且，这些**大语言模型**（Large Language Model: 基于海量文本训练的AI系统），比如像GPT-5 Pro模式，其实效果还是不错的。对于其他的offering，比如说一些中国的模型、开源模型，它还是有一定的技术领先的。那么在产生经济价值方面和提高效率方面，我觉得这个跟WeWork同日而语的话，那是对Sam Altman的一种侮辱。

非常有意思的是，这两家公司都有孙正义的参与。WeWork当年的上市的第一次企图是失败的，最后通过一个SPAC来上市，整个估值都打了大的折扣。我觉得OpenAI还是有可能会上市的，而且它的估值可能也不会低，所以让我们拭目以待吧。

Gary Marcus在节目中向Steve Eisman科普了一下这些神经网络、深度学习。他说：“你别被这些词给唬住了，这些词其实就是吃了类固醇的自动填空。”你想想你们平时发短信，打出“今晚我们在……”，输入法会猜后面跟什么词：有可能是“餐厅”，有可能是“家”，有可能是“老地方”。他猜得准吗？其实是挺准的。但是Gary Marcus问了一个问题：“你觉得这个算法，它知道什么是餐厅吗？它知道什么是饿吗？”他其实完全不懂，他只是在算概率。

现在大语言模型，其实本质上在做同一件事情，就是进行一个完形填空。它只是背下了整个互联网，算得更快更花哨，但他并不是在真正思考，他是在拼凑，就是把一堆碎片强行拼起来，一个很像人话的句子。一旦他拼错了，乐子可就大了。

在节目中，Gary Marcus举了一个演员的例子。这个人叫**Harry**，他是《辛普森一家》（The Simpsons）里头那个只会说“Excellent”的**Mr. Burns**，同时也是经典电影《**This Is Spinal Tap**》里的贝斯手。如果你问AI他是谁，AI会信誓旦旦地告诉你：“哦，他是一名著名的英国配音演员。”听起来没毛病对吧？毕竟有很多优秀配音演员都是英国人。但问题是，这个Harry，他其实是一个地地道道的洛杉矶人，在好莱坞长大，从小就是童星。这件事在维基百科上能够查到，只要2秒钟。但AI为什么要瞎编呢？因为他并没有真正去查资料。他的逻辑是这样的：Harry配音演员，很多配音演员是英国人，Ricky是英国人，John也是英国人……于是他大脑一短路，把这些统计学的概率混在了一起，直接给Harry发了一本“英国护照”。这就是一本正经的胡说八道了。

这其实也是大型语言模型的幻觉是如何产生的。在播客里，Gary又举了另一个例子。之前我们在做Gary的视频中也提到过这个例子。在一个航空展上，有人想炫耀一下特斯拉的智能召唤功能，哪怕人不在车上，车也能开过来，特别酷炫。结果呢，这辆车在众目睽睽之下，以一种非常坚定的姿态，直挺挺地撞上了一架价值350万美金的私人喷气飞机。为什么？特斯拉难道没看见眼前这架大飞机吗？其实不是这样的。摄像头肯定看到了，但是他的数据库里头没有这一个指令。他学会了逃避行人、躲避自行车、躲避其他车，但是从来没有人教过他，在遇到一架飞机的时候应该怎么办。

这其实就是Gary Marcus反复强调的一个致命伤：目前的人工智能它没有**世界模型**（World Model: AI对现实世界及其运作方式的内在理解和表征）。它不知道飞机是一个实体，它是易碎的，是昂贵的。遇到训练数据里头没有的新鲜事儿，它就只能撞上去。

技术上的局限性显而易见，但资本为何仍旧狂热？这背后牵涉到风投行业的利益驱动机制。

### 泡沫机制与AI的未来之路

这时候你肯定会说：“哎呀，这个大语言模型有这些漏洞是很正常的嘛，你把这些训练数据提供给这个人工智能、深度神经网络不就行了吗？”但华尔街看重的是什么？是回报率啊！现在投入是万亿级别的，那回报呢？前两天，《华盛顿邮报》做了一个调研，结果让人感到非常尴尬：在所有人类的工作任务里，真正能被现在AI完美替代的只有2.5%。绝大多数时候，AI写出来的东西就是“工作垃圾”，也就是程序员常说的“屎山”。这玩意看起来像那么回事，但你必须花更多时间去检查、去修改。你就特别害怕它犯像前面Harry那个例子一样，给你在编辑财报的时候编出了一堆数字。你投入了买核弹的钱，结果造出来一把指甲刀，这笔账现在怎么算都算不过来。

既然技术有缺陷，而且回报又这么低，为什么这几年热钱就像疯了一样往里面涌呢？Gary Marcus揭露了一个非常扎心的行业潜规则：在风投圈（VC）这些公司，有个东西叫做“管理费”，通常是2%加20%。什么意思呢？就说如果你忽悠来1万亿的投资，不管这个项目最后有没有成，不管AI是毁灭人类还是拯救世界，只要这钱进来了，这个VC每年就能稳稳拿走200亿的管理费（2%）。这听起来非常夸张吧？这就是为什么他们需要讲这个“无限增长”的故事，因为他们需要你相信“Scaling Law”（规模法则）永远有效。因为只有故事足够大，蛋糕才会足够大，那么2%的切片才足够肥。

另外一个20%是什么呢？就是说，当你这个项目成功退出的时候，你获得的利润，VC公司会抽走20%。其实是非常“lucrative”（有利可图）的。比如说，像OpenAI这样的公司最终上市了，那么上市之后，这些VC公司就可以在二级市场把这些股票卖给散户。然后这些钱拿到手之后，赚到的利润20%就归由VC公司。这其实是非常赚钱的。所以你看，在这些因素的驱动下，VC公司有足够的“incentive”（激励）继续忽悠大家：“哎，我们把AI的蛋糕继续做大，会有更多的钱专门投入这些投资AI公司的VC，那么他们就能收取更高的管理费。”

如果说，我们之前提过，这些技术故障只是些小打小闹，那么在Gary Marcus眼中，“人才用脚投票”才是真正的地震。Gary Marcus举了一个例子，就是**Ilya Sutskever**。他说这哥们其实是OpenAI的灵魂，是这波AI浪潮的奠基人之一。我们频道也做过几期关于Ilya Sutskever的故事，大家感兴趣可以去看一下。Gary的论点是：如果说Ilya真的相信ChatGPT 5明天就能变成神，相信AGI触手可及，他真的会在这个时候辞职吗？（他指的是Ilya离开OpenAI）。

但是，我觉得这里其实说的也并不是很公允。因为Ilya，他其实并不是真的想要离开OpenAI的。他在发动“政变”失败之后，在OpenAI的整个管理层中就显得非常尴尬了，而且一部分领导层的核心是希望他走的。所以，Ilya的离开，并不一定能够证明他对于OpenAI的模型没有信心。

聊到这里，Steve Eisman拿出他的老本行——做金融投资的。他给大家算了一笔账：OpenAI现在的商业模式，可以说是科技史上最疯狂的赌博。他们现在状态，每个月要烧掉几十亿美金。哪怕是刚刚拿了400亿美元的融资，在这种燃烧速度下，也就是一年的口粮。这其实就是一个无底洞。想要填上这个洞，他们下次需要的资源可就得1000亿美元的级别了。放眼整个地球，能签这种支票的人，一只手都数得过来。如果说那5个人（指潜在投资者）中有4个说“不”，那这个故事可能很快就结束了。OpenAI会从硅谷的神，瞬间变成那个没有人接盘的WeWork。这也是为什么大家说，OpenAI必须得立刻启动上市计划了。

Gary紧接着指出一个被很多人忽视的真相：这一场战争，已经不仅仅是一个技术战了，而是变成了一场资源战。如果现在AI的游戏规则依然是比谁的模型大、谁的算力多，那么OpenAI必死无疑。为什么？因为**谷歌**（Google）已经醒了。谷歌它拥有自己的芯片**TPU**，拥有无穷无尽的数据，也有花不完的现金流，不需要求爷爷告奶奶去买显卡，他们自己就能“产粮”。而且，当这个技术变成“没有秘密”的这种大宗商品的时候，赢家永远不是那个跑得最快的创业公司，而是那个“家里最有矿的巨头”。这非常的无趣，但是这就是非常残酷的现实。活下去才是最后的赢家。这其实就有点像《一代宗师》里头说的那个概念了：在这场战斗中，最后站着的人才有资格说话。

那么，在泡沫破裂的阴影下，真正的AI发展方向究竟在哪里？批评者们提出了构建“世界模型”的解决方案。

Garry Marcus说：“这个世界模型才是人类的出路。”为什么呢？因为AI模型必须得有常识。你之所以不会像特斯拉一样撞上飞机，是因为你脑子里有这样一个模型：你知道飞机是硬的，撞上去会死；而且飞机也很贵，撞上去要赔很多钱。你之所以看到哈利波特，知道扫帚能飞，但是你回家不会真的去跳楼，是因为你能够区分魔法世界和现实世界的区别。现在AI其实就缺这个东西。它读了再多关于国际象棋的书，它依然会犯规；它看了再多理论物理，它还是不懂因果关系，然后呢再去预测下一个词。如果不解决这个问题，它永远是一个“读了很多书的傻子”。

这里其实就是心理学家**Daniel Kahneman**（丹尼尔·卡尼曼）所说的**System 1**（系统1：直觉、快速反应）和**System 2**（系统2：理性、慢速思考）的区别了。现在AI，在这个Gary Marcus眼中，全是System 1：它是直觉的、快速的、脱口而出的，就像你喝醉酒之后的胡言乱语。而我们真正需要的呢，是System 2：是理性的、逻辑的、慢下来的思考能力。

Gary预测，未来方向一定是**神经符号AI**（Neuro-symbolic AI: 结合神经网络的模式识别能力与符号逻辑的推理能力）。AI把大语言模型的直觉，加上逻辑推导的严谨。只有当AI学会了说“哎，等一下，让我先想一想，这个逻辑通不通”的时候，真正的AGI时代才算真正开始敲门。而在那一刻之前，一切都只是昂贵的彩排。

所以，让我们回到最开始的问题：我们是不是站在悬崖边上呢？在Gary Marcus和Steve Eisman眼中，我们是的。那么AI泡沫会破灭吗？在他们两人认为，大概率绝对会。但是，这并不代表AI就完了。互联网泡沫我们也破过，但是互联网改变了世界。这一次呢，作为普通人、作为投资者、作为从业者，我们得学会把这些科幻小说和商业财报分开来看，别为那些所谓的“万亿磅的巨婴”买单，要去关注那些真正能够解决问题的、有逻辑的、有世界模型的技术。

那么解读这两位的播客的视频就做到这里了。我会把他们俩的播客视频链接也放在描述栏。大家感兴趣的话可以在评论区留言。非常感谢大家观看，我们下期节目再见。未来的AI呢，必须得先学会理解世界。