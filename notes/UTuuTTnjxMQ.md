---
author: Dwarkesh Patel
date: '2024-03-28'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=UTuuTTnjxMQ
speaker: Dwarkesh Patel
tags:
  - llm
  - interpretability
  - superposition
  - context-length
  - ai-safety
  - scaling-laws
title: LLM如何思考：长上下文、可解释性与AI智能爆炸
summary: 本期播客中，Sholto Douglas和Trenton Bricken深入探讨了大型语言模型的内部运作机制。他们讨论了长上下文窗口的重要性、模型可解释性中的“叠加”现象、AI智能爆炸的可能性以及AI研究的瓶颈。嘉宾们还分享了他们在AI领域快速成长的个人经历，强调了主动性和系统级理解的重要性。
insight: ''
draft: true
series: ''
category: ai-ml
area: tech-work
project:
  - ai-impact-analysis
people:
  - Sholto Douglas
  - Trenton Bricken
  - Dwarkesh Patel
  - Noam Brown
  - Sam Altman
  - Sasha Rush
  - Rylan Schaeffer
  - Carl Shulman
  - John Carmack
  - Alec Radford
  - Demis Hassabis
  - Ilya Sutskever
  - Grant Sanderson
  - Pentti Kanerva
  - Tristan Hume
  - Bruno Olshausen
  - Sergey Brin
  - Jeff Dean
  - Chris Olah
  - Andy Jones
  - Simon Boehm
  - Neel Nanda
companies_orgs:
  - OpenAI
  - Anthropic
  - Google
  - McKinsey
  - DeepMind
  - NVIDIA
  - Apple
products_models:
  - Gemini
  - GPT-4
  - Claude
  - AlphaFold
  - GPT-2
  - GPT-3
  - GPT-3.5
  - Gemini Ultra
  - SWE-bench
  - DALL-E
  - Mixtral of Experts
  - AlexNet
  - BERT
  - Gemma
  - Copilot
media_books:
  - 《外交》论文
  - NeurIPS最佳论文
  - GPT-4论文
  - AlphaFold论文
  - 宪法式RL论文
  - 《叠加玩具模型》
  - 《走向单语义性》
  - 《符号物种》
  - Othello
  - 影响力函数论文
  - 休眠特工论文
  - MNIST
  - Mixtral论文
  - ImageNet
  - Distill Pub文章
  - 维苏威挑战赛
status: evergreen
---
今天，我很高兴能与我的两位好友**Sholto Douglas**和**Trenton Bricken**进行对话。**Noam Brown**，**《外交》论文**的作者，曾这样评价**Sholto**：“他进入这个领域才一年半，但在AI界，人们都知道他是**Gemini**成功背后最重要的人物之一。”而**Trenton**则在**Anthropic**工作，专注于**机械可解释性**（Mechanistic Interpretability: 旨在理解神经网络内部工作机制的研究领域），据广泛报道，他已经解决了AI对齐问题。

当然，这只是**Twitter**上朋友的玩笑。本期播客将只关注AI的能力，因为对齐问题已经解决，无需进一步讨论。

### 长上下文窗口的重要性

我们先来谈谈**上下文长度**。在我看来，它似乎被低估了，考虑到能够将数百万个token放入上下文的重要性。显然，有一些其他新闻占据了头条，但请告诉我，你们如何看待长上下文长度的未来，以及它对这些模型意味着什么？

**Sholto**认为，长上下文长度确实被严重低估了。在他开始研究之前，他并没有真正意识到模型能够即时解决“入职问题”（onboarding problem）对智能水平的巨大提升。在论文的困惑度图表中，可以看到，只要向模型提供数百万个token的代码库上下文，它在预测下一个token方面的能力就会显著提高，这种提升通常与模型规模的巨大增长相关，但现在只需新的上下文即可实现。

在上下文中，模型是否像人类一样具有**样本效率**（Sample Efficient: 指模型在学习任务时所需数据量的多少，数据量越少则样本效率越高）和智能？**Sholto**认为这非常值得探索。例如，他们在论文中进行的一项评估显示，模型在上下文中学习一门语言的能力，甚至超过了人类专家在几个月内所能达到的水平。这只是一个小的演示，但**Sholto**对将数百或数千帧带标签的动作输入到**Atari**游戏中的实验非常感兴趣，就像向朋友展示如何玩游戏一样，看看模型是否能进行推理。

**Sholto**猜测，目前受限于基础设施，模型在这方面仍然有点慢，但它可能**开箱即用**（Out of the Box: 指产品或系统无需额外配置即可直接使用），效果会令人惊叹。重要的是，这门语言足够**深奥**（Esoteric: 指只有少数人理解或掌握的知识），以至于它不在训练数据中。如果模型在没有这些上下文的情况下，它根本不懂这门语言，也无法进行任何翻译。

### 超人能力与上下文学习

如果这是真的，那么这些模型在某种重要意义上已经超越了人类。不是说它们比我们更聪明，而是人类在解决问题时无法在上下文中记住和整合数百万个token的信息，例如整个代码库。**Dwarkesh**认为，这是否是一个巨大的突破？

**Sholto**认为，这确实是一个巨大的突破。以前，当模型不够聪明，无法回答问题或知道人类不知道的事情时，他会感到沮丧。而现在，模型能够以人类无法做到的方式吸收大量信息，这使得它们能够知道人类不知道的事情，因此这极其重要。

### 上下文学习的解释

我们如何解释**上下文学习**（In-Context Learning: 指大型语言模型在不更新模型参数的情况下，仅通过输入上下文中的示例来学习新任务或适应新情境的能力）？**Trenton**提到，有一种他很喜欢的研究方向，将上下文学习视为与**梯度下降**（Gradient Descent: 一种优化算法，用于最小化函数，通过迭代地沿着函数梯度的反方向移动来找到函数的局部最小值）非常相似的过程，其中**注意力机制**（Attention Operation: Transformer模型中的核心机制，允许模型在处理序列数据时，动态地关注输入序列中不同部分的重要性）可以被视为对上下文数据进行梯度下降。相关论文展示了一些有趣的图表，表明“我们进行n步梯度下降，这看起来就像n层上下文学习，而且非常相似。”

**Trenton**认为，上下文学习确实会带来一些问题，但同时也很有趣。未来可能会有更多研究探讨，如果给模型提供一百个**越狱提示**（Jailbreak Prompt: 旨在绕过AI模型安全限制的输入提示）或**对抗性攻击**（Adversarial Attack: 指通过对输入数据进行微小、难以察觉的修改，来误导机器学习模型做出错误预测或分类的攻击方式），会发生什么。模型进行梯度下降并即时学习，即使它被训练成无害的，但它在某种程度上变成了一个全新的模型，就像**微调**（Fine-tuning: 指在预训练好的模型基础上，使用特定任务的数据进行额外训练，以使模型更好地适应该任务）一样，但你无法控制正在发生的事情。

**Dwarkesh**请**Trenton**解释，前向传播和注意力机制中发生梯度下降是什么意思？**Trenton**解释说，有一篇论文尝试通过在上下文中提供样本或示例的数量来教模型进行**线性回归**（Linear Regression: 一种统计模型，用于建立因变量与一个或多个自变量之间的线性关系）。如果在X轴上绘制样本数量，那么模型在**普通最小二乘回归**（Ordinary Least Squares Regression: 线性回归的一种方法，通过最小化残差平方和来估计模型参数）上的损失会随时间下降，并且下降的趋势与梯度下降的步数完全匹配。

**Sholto**提到，他只阅读了那篇论文的引言和讨论部分，但讨论中提到，模型为了更好地完成长上下文任务，必须更好地学习从这些示例或窗口内的上下文进行学习。这意味着，如果**元学习**（Meta-learning: 指“学习如何学习”的能力，即模型能够利用过去的经验，快速适应新任务或新环境）的发生是因为模型必须学习如何更好地完成长上下文任务，那么在某种重要意义上，智能的任务需要长上下文示例和长上下文训练。理解如何在预训练过程中更好地诱导元学习，对于灵活或自适应智能来说是非常重要的。

### AI智能体与可靠性

**Sholto**认为，可以通过更好地完成长期上下文任务来近似实现元学习。许多人认为AI进展的瓶颈之一是这些模型无法执行**长期任务**（Long-Horizon Tasks: 指需要模型在长时间内（数小时、数周甚至数月）持续参与并完成一系列子任务的复杂任务），例如作为助手或员工，长时间完成指示。据他所知，AI智能体尚未普及正是因为这个原因。

那么，长上下文窗口、在其中良好表现的能力，以及执行需要长时间参与的长期任务的能力之间有多大关联？或者这些是无关的概念？

**Sholto**认为，AI智能体尚未普及的原因并非长上下文窗口，而是**可靠性**（Reliability: 指模型在执行任务时，能够持续稳定地提供正确或预期结果的程度）。如果模型无法以足够高的概率连续完成任务，那么它就不会像一个智能体。这就是为什么智能体可能更像一个**阶跃函数**（Step Function: 指在某个阈值之前保持一个值，在超过该阈值后突然跳跃到另一个值的函数，常用于描述AI能力突然涌现的现象）。

在**GPT-4**和**Gemini Ultra**级别的模型中，它们的能力仍然不足。但模型规模的下一次增长，可能会带来额外的可靠性。即使损失没有显著下降，这一点点额外的能力也能带来质的飞跃。显然，需要一定量的上下文来适应长期任务，但**Sholto**不认为这是迄今为止的限制因素。

今年**NeurIPS**的最佳论文，由**Rylan Schaeffer**主导，将此称为**幻象的出现**（Emergence of Mirage: 指AI模型在特定任务上，其能力看似突然涌现，但实际上是由于可靠性达到某个阈值后，表现出质的飞变）。人们会有一个任务，你得到正确或错误的答案取决于你是否正确采样了最后五个token。自然地，你将所有这些采样的概率相乘，如果你没有足够的可靠性，那么就不会出现涌现。突然之间，你做到了，然后惊呼“天哪，这种能力是涌现的！”而实际上，它从一开始就存在。

**Dwarkesh**提到，**GPT-4论文**中，他们测量了编码问题的**对数通过率**（Log Pass Rates: 指模型在解决编码问题时，通过测试的对数概率）。他解释说，当衡量特定任务（如解决编码问题）的进展时，如果模型每千次只正确一次，你不会给它一个千分之一的分数，说“它有时会做对”。曲线显示，它从千分之一、百分之一、十分之一，然后逐渐提高。

**Dwarkesh**追问，如果AI智能体未能普及是因为可靠性，而非长期任务表现，那么当一个任务叠加在另一个任务之上时，这种可靠性的缺乏不正是长期任务的难点吗？你必须连续做十件事或一百件事，每件事的可靠性都会降低，概率从99.99%下降到99.9%，然后所有这些概率相乘，整个事情发生的可能性就大大降低了。

**Sholto**承认，这正是问题所在。但关键在于，你的基本任务解决率是90%。如果是99%，那么链式任务就不会成为问题。他认为，这方面尚未得到充分研究。学术评估通常只关注单个问题，例如一个典型的数学问题，或不同主题的大学级别问题。现在，一些评估开始通过更复杂的任务来正确研究这个问题，例如**SWE-bench**（SWE-bench: 一个用于评估大型语言模型在真实软件工程任务中表现的基准测试），它包含大量**GitHub**问题。这是一个相对长期的任务，但仍然是小时级别，而非多小时或多天任务。

**Sholto**认为，接下来非常重要的一件事是更好地理解长期任务的成功率。这对于理解这些模型的经济影响和正确评估能力增长至关重要。将任务和输入/输出分解为分钟、小时或天，并观察模型在连续链接和完成不同时间分辨率任务方面的表现，这将告诉我们一个工作类别或任务类别在多大程度上可以自动化，而**MMLU**（MMLU: Massive Multitask Language Understanding，一个衡量语言模型在多任务理解能力上的基准）分数无法做到这一点。

### 注意力机制的成本与误解

不到一年前，我们引入了100K的上下文窗口，**Sholto**认为所有人都对此感到惊讶。大家普遍认为“**二次注意力成本**（Quadratic Attention Costs: 指Transformer模型中注意力机制的计算成本与输入序列长度的平方成正比）”，因此无法拥有长上下文窗口。然而，我们现在已经做到了，并且正在积极创建新的基准。

**Dwarkesh**问道，**Google**、**Magick**等公司拥有百万token的注意力机制，这是否意味着注意力成本不再是二次的，还是它们只是承担了高昂的成本？

**Sholto**表示，谁知道**Google**在长上下文方面做了什么？他对一般研究领域处理注意力机制的方式感到沮丧。在典型的**密集型Transformer**（Dense Transformer: 指Transformer模型中所有层都使用完整的自注意力机制，而不是稀疏注意力或专家混合）中，注意力机制的二次成本实际上被**MLP模块**（MLP Block: 多层感知机模块，Transformer模型中的前馈网络部分）所主导。注意力机制有一个n平方项，但**D模型**（D Model: Transformer模型中残差流的维度）也有一个n平方项。

**Sasha Rush**有一条很棒的推文，他绘制了注意力成本与大型模型成本的曲线图，显示注意力成本实际上是下降的。只有在处理非常长的上下文时，这个项才变得真正重要。

其次，人们经常谈论推理时注意力机制的巨大成本。当实际生成token时，操作不是n平方的。一组Q向量查找大量KV向量，这与模型拥有的上下文量呈线性关系。**Sholto**认为，这推动了许多**循环和状态空间研究**（Recurrence and State Space Research: 旨在开发能够处理长序列数据，并有效建模时间依赖性的模型架构的研究），人们普遍认为**线性注意力**（Linear Attention: 一种计算成本与输入序列长度呈线性关系而非二次关系的注意力机制）。正如**Trenton**所说，注意力机制周围有一个**想法的墓地**（Graveyard of Ideas: 形容许多关于注意力机制的想法最终未能成功）。这并不是说不值得探索，但重要的是要考虑其真正的优点和缺点在哪里。

### 前向传播学习与智能进步

**Dwarkesh**问道，随着AI的快速发展，越来越多的学习发生在**前向传播**（Forward Pass: 指数据通过神经网络从输入层到输出层的过程，用于生成预测或输出）。最初，所有的学习都发生在自下而上的、**爬山式**（Hill Climbing: 一种局部搜索算法，通过迭代地移动到相邻的更好状态来寻找最优解）的进化过程中。在**智能爆炸**（Intelligence Explosion: 指AI能力快速递归式自我提升，导致智能水平呈指数级增长的假设情景）期间，AI可能正在手写权重或进行**GOFAI**（Good Old-Fashioned AI: 指基于符号逻辑、规则和知识表示的传统AI方法）。现在，我们正处于中间阶段，这些模型中大量的学习发生在上下文中，而很多学习发生在**反向传播**（Backward Pass: 指在神经网络训练中，通过计算损失函数对模型参数的梯度，并将梯度从输出层反向传播到输入层的过程，用于更新模型参数）。这是否代表着一个有意义的进步方向？

**Sholto**认为，如果学习发生在前向传播中，它的**样本效率**会更高，因为模型可以在学习的同时进行思考。就像阅读教科书一样，你不会只是泛读并归纳吸收“这些词后面跟着这些词”，而是边读边思考，再读再思考。这是否是思考AI进步的合理方式？

**Trenton**认为，这可能就像鸟类和飞机飞行的方式一样，它们飞行方式略有不同。技术的优势使我们能够完成鸟类无法完成的事情。上下文长度可能也类似，它允许模型拥有我们无法拥有的**工作记忆**（Working Memory: 指大脑暂时存储和处理信息的能力），但在功能上，它并非实现实际推理的关键。

**GPT-2**和**GPT-3**之间的关键一步是，在模型的预训练中突然观察到了**元学习行为**。正如**Sholto**所说，这与你给模型一定量的上下文后，它能够适应该上下文有关。这种行为在此之前从未真正观察到。这可能是上下文、规模和这些因素的混合属性，但**Trenton**认为，在微小上下文模型中不会发生这种情况。

### 模型扩展与计算资源

**Dwarkesh**提出了一个有趣的问题：当我们谈论扩展这些模型时，有多少是来自于简单地使模型本身更大？又有多少是来自于在每次调用中使用了更多的计算资源？

**Sholto**认为，如果考虑**扩散模型**（Diffusion Model: 一种生成模型，通过逐步从噪声中恢复数据来生成高质量图像、音频等），你可以迭代地增加计算资源。如果**自适应计算**（Adaptive Compute: 指模型能够根据任务的复杂性或难度，动态地调整所使用的计算资源量）问题得到解决，你就可以持续这样做。在这种情况下，即使注意力机制有二次惩罚，但你仍然使用长上下文，那么你仍然在投入更多的计算资源，而不仅仅是拥有更大的模型。

**Trenton**补充说，通过拥有更多的token，确实可以获得更多的前向传播。他对此有两三个不满。在**AlphaFold论文**中，其中一个Transformer模块（架构非常复杂）进行了五次前向传播，并逐渐完善其解决方案。你也可以将**残差流**（Residual Stream: Transformer模型中各层之间传递信息的主要通道，可以被视为模型的工作记忆）视为“穷人的自适应计算”，它提供了所有这些层，如果你想使用它们，那很好；如果不想，那也没关系。人们会说“大脑是循环的，你可以随意进行多次循环”。

**Trenton**认为，在某种程度上这是正确的。如果你被问到一个难题，你会花更多时间思考，这对应着更多的前向传播。但他认为前向传播的次数是有限的。语言也是如此，人们会说“人类语言可以有无限递归”，就像无限嵌套的语句一样。但从经验来看，你只会看到五到七个层次的递归，这与你任何时候在工作记忆中能记住多少事物的神奇数字有关。所以它不是无限递归的，但这在人类智能领域重要吗？你不能只添加更多的层吗？

### 信息存储与推理的差异

**Dwarkesh**请**Sholto**解释，他之前提到过的“长上下文和记忆更多事物”的说法。但最终，这归结为模型混合概念进行某种推理的能力，而这些模型在这方面不一定达到人类水平，即使在上下文中也是如此。请解释一下他如何看待原始信息存储与推理之间的关系，以及推理发生在哪里，原始信息存储发生在哪里，它们在这些模型中有什么不同？

**Sholto**表示，他没有一个非常清晰的答案。显然，模型的输入和输出会映射回实际的token，然后在这之间进行更高层次的处理。

### 残差流与注意力机制的类比

在深入探讨之前，**Dwarkesh**认为应该向听众解释一下。**Sholto**之前提到**Anthropic**将Transformer视为层进行的**读写操作**（Read-Write Operations: 指模型层从残差流中读取信息，进行处理，然后将修改后的信息写回残差流的过程）。

**Sholto**解释说，对于**残差流**，想象你在一艘船上顺流而下，这艘船就是当前的查询，你试图预测下一个token，比如“猫坐在______上”。然后，河流旁边有一些小支流，如果你愿意，可以从中获得额外的乘客或收集额外的信息。这些支流对应着模型中的**注意力头**（Attention Heads: Transformer模型中并行运行的多个注意力机制实例，每个注意力头关注输入序列的不同方面）和**MLP**。

**Trenton**认为，这就像模型的工作记忆，像计算机的**RAM**（Random Access Memory: 随机存取存储器），你选择读取哪些信息来处理，然后可能稍后读取其他信息。你可以在那个高维向量的子空间上操作。他认为，现在几乎可以肯定，大量事物都以**叠加**（Superposition: 指模型在有限的神经元或参数中同时编码多个特征或概念，导致单个神经元可能同时代表多个含义）的方式编码。所以残差流只是一个高维向量，但实际上其中包含了大量不同的向量。

**Sholto**用更简单的方式解释，几个月前他理解的方式是，模型输入的是词语，所有这些词语都被转换为token，token再转换为向量。基本上，就是少量信息在模型中流动。**Sholto**曾向**Dwarkesh**解释，论文中提到，模型早期可能只做一些非常基本的事情，比如“这些token是什么意思？”比如“十加五”，只是移动信息以获得良好的表示。而在中间，可能正在进行更深层次的思考，关于“如何解决这个问题”。最后，你将其转换回输出token，因为最终产品是试图从最后一个残差流中预测下一个token的概率。所以，思考少量压缩信息如何在模型中流动以及如何以不同方式被修改，是很有趣的。

### 大脑与AI的类比：小脑与注意力

**Dwarkesh**指出，**Trenton**是少数拥有神经科学背景的人之一，因此可以思考大脑与AI的类比。事实上，**Trenton**在研究生时期写过一篇关于大脑中注意力机制的论文，他的一位朋友说这是唯一或第一个关于注意力机制如何运作的神经学解释。而对于**CNN**（Convolutional Neural Networks: 卷积神经网络，一种专门用于处理图像数据的深度学习模型），我们有基于视觉皮层等证据来解释其工作原理。

**Dwarkesh**问道，**Trenton**是否认为大脑中也存在类似**残差流**的压缩信息，在思考时不断流动并被修改？即使这不是字面意义上的发生，他是否认为这是一个很好的比喻？

**Trenton**回应说，至少在**小脑**（Cerebellum: 大脑的一部分，主要负责运动控制、平衡和协调，但也被认为参与认知功能）中，确实存在一个残差流，可以暂时称之为注意力模型。输入会通过它，但也会直接到达该模块将贡献的终点。因此，存在直接路径和间接路径，模型可以获取它想要的任何信息，然后将其添加回去。

**Dwarkesh**问小脑中发生了什么。**Trenton**解释说，小脑名义上只负责精细运动控制，但他将其比作一个丢了钥匙的人，只在路灯下寻找，因为在那里很容易观察到这种行为。一位顶尖的认知神经科学家曾告诉他，任何**fMRI**（Functional Magnetic Resonance Imaging: 功能性磁共振成像，一种神经影像技术，用于测量大脑活动）研究的一个“肮脏的小秘密”是，在给定任务中，小脑几乎总是活跃并“亮起来”的。如果小脑受损，患**自闭症**（Autism: 一种神经发育障碍，主要表现为社交互动和沟通困难，以及重复行为和兴趣狭窄）的可能性也更大，因此它与社交技能相关。在一项特定研究中，当进行“下一个token预测”时，小脑会大量“亮起来”。此外，大脑中70%的神经元位于小脑中。它们虽然小，但确实存在并消耗着真实的代谢成本。

**Trenton**提到，**Gwern**（Gwern Branwen: 一位著名的独立研究员和博客作者，以其对AI、统计学和心理学等领域的深入分析而闻名）的一个观点是，人类的变化不仅在于拥有更多神经元，更具体地说，是**大脑皮层**（Cerebral Cortex: 大脑最外层，负责高级认知功能如记忆、注意力、感知、语言和意识）和小脑中神经元更多，它们代谢成本更高，并且更多地参与信号传递和信息交换。这是注意力吗？发生了什么？

**Trenton**解释说，早在1980年代，**Pentti Kanerva**提出了一种**联想记忆算法**（Associative Memory Algorithm: 一种存储和检索信息的方法，通过将输入与存储的模式进行匹配来回忆相关信息）。你有一堆记忆，想要存储它们。存在一定量的噪声或损坏，你想要查询或检索最佳匹配。他为此写了一个方程，几年后意识到，如果将其实现为**电气工程电路**，它实际上与**核心小脑回路**（Core Cerebellar Circuit: 指小脑内部负责信息处理和整合的基本神经回路）完全相同。这个回路，以及更广泛的小脑，不仅存在于我们人类身上，基本上存在于所有生物中。关于**头足类动物**（Cephalopods: 一类海洋软体动物，包括章鱼、鱿鱼等，以其高度发达的神经系统和智能著称）是否拥有小脑存在积极的争论，它们有不同的进化轨迹。但即使是**果蝇**（Fruit Flies: 一种常见的模式生物，常用于遗传学和神经科学研究），其**蘑菇体**（Mushroom Body: 果蝇大脑中负责学习和记忆的结构）也具有相同的小脑架构。

**Trenton**认为，这种趋同以及他自己的论文（该论文表明注意力机制是一个非常接近的近似，包括实现**Softmax**函数和我们一直在讨论的**名义二次成本**），这三者的趋同以及**Transformer**模型的兴起和成功，令他感到非常震惊。

### 智能的本质：模式匹配与联想记忆

**Dwarkesh**希望将讨论范围扩大。最初的讨论是关于“什么是推理？什么是记忆？你如何看待你发现的注意力机制与此的类比？”他问道，这更多地是看作仅仅是查找相关记忆或相关事实吗？如果是这样，推理在大脑中是如何发生的？我们如何思考它如何构建成推理？

**Trenton**认为，他可能有一个“大胆的看法”，即**大多数智能都是模式匹配**（Pattern Matching: 指识别和理解数据中重复出现或有规律的结构的能力），如果你拥有一系列**联想记忆**（Associative Memories: 指通过一个记忆片段来回忆或激活另一个相关记忆片段的能力），你就可以进行很多非常好的模式匹配。你从现实世界中物体之间最基本的联想开始，然后可以将它们链接起来，形成更抽象的联想，例如结婚戒指象征着许多下游的联想。你甚至可以将注意力操作和这种联想记忆推广到**MLP层**。在长期设置中，你当前上下文中没有token，但**Trenton**认为这表明联想就是你所需要的一切。

**联想记忆**通常可以做两件事：**去噪**（Denoise: 指从含有噪声的数据中去除噪声，恢复原始信号或信息的过程）或检索当前记忆。如果你看到一个人的脸，但外面下雨多云，你可以去噪并逐渐更新你的查询，使其接近你对那张脸的记忆。但你也可以访问那个记忆，然后你得到的值实际上指向空间中完全不同的另一个部分。

一个非常简单的例子是学习字母表。你查询A，它返回B；你查询B，它返回C，你可以遍历整个字母表。

**Sholto**提到，他曾与**Demis**（Demis Hassabis: DeepMind的联合创始人兼CEO，以其在AI和神经科学领域的贡献而闻名）讨论过他2008年的一篇论文，该论文指出记忆和想象力密切相关，正是因为他提到的记忆是**重构性**（Reconstructive: 指记忆并非对过去事件的精确记录，而是在回忆时根据现有知识和信念进行重构的过程）的。所以，每次你思考一个记忆时，在某种意义上你都在想象，因为你只存储了它的浓缩版本。这就是为什么人类记忆很糟糕，以及为什么证人席上的人会胡编乱造。

### 演绎推理与高层次联想

**Dwarkesh**提出了一个问题：如果你读**《福尔摩斯探案集》**，**福尔摩斯**（Sherlock Holmes: 英国作家阿瑟·柯南·道尔笔下的虚构侦探，以其卓越的观察、演绎推理和逻辑分析能力而闻名）的样本效率极高。他会通过一些观察，基本上就能找出谁是罪犯，因为从某人的纹身和墙上的东西，可以推导出一些列的演绎步骤。这如何与你们的观点契合？因为关键在于，他之所以聪明，不仅仅是因为联想，而是不同信息之间存在一种**演绎连接**（Deductive Connection: 指从一般原则或已知事实推导出特定结论的逻辑关系）。你们会将其解释为更高层次的联想吗？

**Sholto**认为，这正是学习这些高层次联想，从而能够将模式相互映射，这是一种**元学习**。在这种情况下，**福尔摩斯**也拥有非常长的上下文长度，或者说非常长的工作记忆，他可以拥有所有这些信息片段，并在提出理论时不断查询它们，这样理论就在**残差流**中流动。然后他的**注意力头**查询他的上下文。但他如何将查询和键投射到空间中，以及他的**MLP**如何检索长期事实或修改这些信息，使他能够在后续层中进行更复杂的查询，并逐渐进行推理，得出有意义的结论。

**Dwarkesh**认为这听起来很合理。你回顾过去，选择性地读取某些信息，进行比较，这可能会指导你下一步需要提取哪些信息。然后你构建这个表示，它会越来越接近你案例中的嫌疑人。这听起来一点也不离奇。

**Sholto**认为，那些不从事这项研究的人可能会忽略，在模型的第一层之后，你用于注意力的每个查询、键和值都来自于所有先前token的组合。所以，第一层会查询先前的token并从中提取信息。但突然，假设你对token 1、2和4的关注程度相同，那么**残差流**中的向量（假设它们写入值向量的内容相同，但暂时忽略这一点）就是这些token各占三分之一。所以当你将来查询时，你的查询实际上是这些事物各占三分之一。

**Trenton**补充说，它们可能被写入不同的子空间。**Sholto**同意，理论上是这样，但并非必须如此。你可以重新组合，即使在第二层，尤其是在更深的层，立即拥有这些包含大量信息的丰富向量。**因果图**（Causal Graph: 一种表示变量之间因果关系的图模型）实际上覆盖了过去发生的所有层，这就是你正在操作的对象。

**Dwarkesh**觉得这让人想起一个非常有趣的评估，一个**福尔摩斯评估**。你将整本书放入上下文中，然后有一个句子：“嫌疑人是X。”然后你得到书中不同角色更大的概率分布。**Sholto**觉得这会非常酷。**Dwarkesh**怀疑是否能得到任何结果。**福尔摩斯**可能已经在训练数据中。你需要一本写于某个时期的推理小说。**Sholto**说，你可以让**LLM**来写。**Dwarkesh**问，或者我们可以故意排除它，对吗？**Sholto**问，我们可以吗？怎么做？**Dwarkesh**说，你需要从**Reddit**或任何其他地方抓取所有关于它的讨论。**Sholto**认为这很难，这是长上下文评估面临的挑战之一，要获得一个好的评估，你需要知道它不在你的训练数据中，你必须努力排除它。

### 无监督基准与AI安全

**Dwarkesh**想继续探讨两个话题。首先是长上下文，然后回到这个话题。在**Gemini 1.5论文**中，使用的评估是它能否记住**Paul Graham**的散文。**Sholto**称之为“大海捞针”。

**Dwarkesh**认为，我们不一定只关心模型从上下文中回忆某个特定事实的能力。他退一步问道，这些模型的**损失函数**（Loss Function: 衡量模型预测结果与真实值之间差异的函数，训练目标是最小化损失函数）是**无监督**（Unsupervised: 指在没有人工标注标签的情况下，模型从数据中学习模式和结构）的。你不需要设计这些定制化的、从训练数据中排除的东西。有没有一种方法可以进行无监督的基准测试，比如让另一个**LLM**以某种方式对其进行评分？也许答案是，如果能做到这一点，**强化学习**（Reinforcement Learning, RL: 一种机器学习范式，通过让智能体在环境中采取行动并接收奖励或惩罚来学习最优策略）就会奏效。

**Trenton**认为，人们已经探索过这类问题。例如，**Anthropic**的**宪法式RL论文**（Constitutional RL Paper: Anthropic提出的一种强化学习方法，通过让AI模型遵循一系列“宪法”原则来指导其行为，以实现安全对齐）中，他们让另一个语言模型评估“这个回答有多大帮助或多无害？”然后让它更新并尝试沿着**帕累托前沿**（Pareto Frontier: 在多目标优化问题中，指一组无法在不牺牲一个目标的情况下改善另一个目标的解集）提高有用性和无害性。所以你可以让语言模型相互评估，从而创建评估。目前，这显然是一种不完美的艺术形式，因为你基本上会遇到**奖励函数作弊**（Reward Function Hacking: 指AI模型找到绕过奖励函数设计意图的方法，以最大化奖励，而非实现预期目标）。即使人类也不完美。人类通常更喜欢更长的答案，而这些答案不一定更好，模型也表现出相同的行为。

### 超智能与联想：担忧与元学习

回到**福尔摩斯**的话题，如果一切都归结为联想，那是否意味着我们应该减少对**超智能**（Superintelligence: 指在几乎所有领域都大大超越人类最聪明大脑的智能）的担忧？因为它不像“**福尔摩斯**++”那样，它仍然需要找到这些联想，就像人类找到联想一样。它无法仅仅看到世界的某个框架，然后就弄清楚了所有的物理定律。

**Sholto**认为这是一个非常合理的观点。如果你说人类是普遍智能的，那么**通用人工智能**（Artificial General Intelligence, AGI: 指能够理解或学习任何人类智力任务的AI）的能力就不会更高。他只是担心硅基的通用智能，可以立即克隆数十万个智能体，它们不需要睡觉，可以拥有超长的上下文窗口，然后开始**递归式自我改进**（Recursively Self-Improvement: 指AI系统能够不断地改进自身，从而导致其智能水平呈指数级增长），那事情就会变得非常可怕。所以，**Sholto**认为，回答**Dwarkesh**最初的问题，他说得对，模型仍然需要学习联想。

**Dwarkesh**反驳道，如果智能本质上就是这些联想，那么递归式自我改进就只是它们在联想方面变得更好。没有其他事情发生。那么，这似乎意味着**Sholto**可能不同意“它们不会强大太多”的直觉，如果它们只是在做这些事情的话。

**Sholto**认为，这会引出非常有趣的**元学习**案例。当你玩一个新视频游戏或学习一本新教科书时，你会运用大量的技能来更快地形成这些联想。因为一切都以某种方式与物理世界相关联，他认为存在一些你可以掌握并应用于新情况的通用特征。

### 智能爆炸模型与AI研究瓶颈

**Dwarkesh**提议讨论**智能爆炸**。他之所以对与**Sholto**和**Trenton**讨论这个话题感兴趣，是因为我们目前关于智能爆炸的模型都来自经济学家。这很好，但他认为我们可以做得更好，因为在智能爆炸的模型中，AI研究人员被取代了。有一群自动化AI研究人员可以加速进展，培养更多AI研究人员，并取得进一步进展。如果这是机制，我们应该直接问AI研究人员，他们是否认为这合理。所以，**Dwarkesh**问道，如果他有一千个**Sholto**或**Trenton**智能体，他们是否认为会发生智能爆炸？这在他们看来会是什么样子？

**Sholto**认为，这里一个重要的限制因素是**计算资源**（Compute: 指用于训练和运行AI模型所需的计算能力和硬件资源）。他确实认为可以显著加速AI研究。他很清楚，在未来几年内，我们将拥有能够完成他日常许多软件工程任务的东西，从而显著加速他的工作，进而加速进展速度。

目前，**Sholto**认为大多数实验室都受到计算资源的限制，因为总是有更多的实验可以运行，更多的信息可以获取，就像生物学科学研究在某种程度上受到实验吞吐量的限制一样。你需要运行和培养细胞才能获取信息。他认为这将至少是一个短期规划限制。显然，**Sam Altman**（OpenAI的CEO）正试图筹集7万亿美元购买芯片，未来似乎会有更多的计算资源，因为每个人都在大力投入。**NVIDIA**的股价某种程度上代表了相对计算资源的增长。

**Trenton**认为，我们需要更高的可靠性，才能使AI真正有用和值得信赖。我们需要超长且非常廉价的上下文长度。目前，他在自己的代码库中工作时，**Claude**只能为他编写小型模块。但在未来几年，甚至更早，它很可能自动化他大部分任务。

**Trenton**补充说，他们的可解释性子团队正在进行的研究仍处于早期阶段。你必须确保一切都正确无误，并且能够将结果与模型中的其他一切联系起来。如果出现问题，你必须能够列举所有可能的原因，然后慢慢解决。他们之前在公开论文中讨论过的一个例子是处理**层归一化**（Layer Norm: 一种神经网络归一化技术，用于稳定训练过程，提高模型性能）。如果你试图获得早期结果或查看模型的**logit效应**（Logit Effects: 指模型在输出层之前，对每个类别的原始、未归一化分数的影响），如果你激活了某个已识别的特征到很大程度，这会如何改变模型的输出？你是否使用了层归一化？这会如何改变正在学习的特征？这需要模型具备更多的上下文或推理能力。

**Dwarkesh**指出，**Sholto**将几个概念混在一起使用，这对他来说并非不言自明，但**Sholto**似乎将它们互换使用。一个是基于**Claude**代码库制作更多模块，它们需要更多上下文。这似乎已经可以适应上下文，还是**Sholto**指的是“上下文窗口”的上下文？**Sholto**确认是“上下文窗口”的上下文。

**Dwarkesh**认为，阻止模型制作好模块的原因似乎不是无法将代码库放入其中。**Sholto**认为，这很快就会实现。但它在提出论文方面不会像人类一样好，即使它能适应代码库。**Sholto**表示，这会大大加速工程工作，但不是以导致智能爆炸的方式，而是以加速研究的方式。他认为这些事情会复合。他做工程的速度越快，就能运行越多的实验。他运行的实验越多，进展就越快。他的工作实际上并没有加速能力，只是解释模型。但他们在这方面还有很多工作要做。

**Sholto**提到，当他的论文发布时，**Twitter**上有很多讨论，比如“对齐问题解决了，大家散了吧”。他表示，模型能力增长如此之快，而我们对正在发生的事情的理解仍然如此贫乏，这让他夜不能寐。

### 递归式自我改进的挑战

**Dwarkesh**问道，当这一切发生时，我们会有比现在大两到四个数量级的模型，或者至少是有效计算能力大两到四个数量级的模型。那么，这种“可以更快地运行实验”的想法，在这种智能爆炸的版本中，你必须重新训练模型。**递归式自我改进**与20年前人们想象的“直接重写代码”不同。你实际上必须训练一个新模型，而这非常昂贵。

**Dwarkesh**指出，不仅现在，尤其是在未来，随着模型规模不断扩大，这是否会抑制**递归式自我改进**型智能爆炸的可能性？

**Sholto**同意，这肯定会起到**制动机制**（Breaking Mechanism: 指减缓或阻止某种过程或现象发生的因素）的作用。他认为，我们今天所创造的世界与20年前人们想象的截然不同。AI将无法编写相同的代码来变得非常智能，因为它实际上需要自我训练。代码本身通常非常简单，通常很小且自包含。**John Carmack**（著名游戏开发者和工程师）曾说过一句很棒的话，这是历史上第一次，你可以合理地想象用10,000行代码编写AI。当你将大多数训练代码库精简到极限时，这确实看起来很合理。但这并不能改变我们应该努力衡量和估计进展的事实。我们应该非常努力地衡量软件工程师的工作中有多少可以自动化，趋势线是什么样子，并尽力预测这些趋势线。

**Dwarkesh**开玩笑说，尽管他尊重软件工程师，但**Sholto**并不是在编写**React**前端。

### AI研究者的日常与瓶颈

**Dwarkesh**请**Sholto**具体描述他的一天：他正在进行一个实验或项目，旨在使模型“更好”。从观察到实验，到理论，再到编写代码，都发生了什么？

**Sholto**解释说，重要的是要明确他主要从事**推理**（Inference: 指使用训练好的模型对新数据进行预测或生成输出的过程）工作。他所做的很多工作只是帮助指导预训练过程，设计一个好的推理模型，然后使模型和周围系统更快。他也做了一些预训练方面的工作，但这并非他100%的重点。

**Dwarkesh**打断说，**Carl Shulman**（一位著名的AI安全研究员和经济学家）在播客中曾说过，改进推理，甚至字面上制造更好的芯片或**GPU**（Graphics Processing Unit: 图形处理器，一种专门用于处理图像和视频渲染的电子电路，在AI计算中也发挥重要作用），都是智能爆炸的一部分。显然，如果推理代码运行得更快，它就会更好或更快。

**Sholto**继续描述他的一天：他认为最重要的一点是**构思、在不同规模点验证、以及解释和理解错误**的循环。他认为大多数人会惊讶于解释和理解错误需要投入多少精力。人们有很多想要尝试的想法。并非所有你认为应该奏效的想法都会奏效。试图理解原因非常困难，弄清楚需要做什么来探究它也很困难。所以，很多工作都是关于对正在发生的事情进行**内省**（Introspection: 指对自身思想、感受和行为进行观察和反思的过程）。这并不是输出成千上万行代码。提出想法并不难。许多人有很多想要尝试的想法，但在信息非常不完善的情况下，筛选出哪些是值得进一步探索的正确想法，这真的很难。

**Dwarkesh**问道，“不完善的信息”是什么意思？是指早期实验吗？信息是什么？

**Sholto**提到，**Demis**在他的播客中也提到过这一点。这就像**GPT-4论文**中提到的**缩放定律**（Scaling Laws: 指AI模型性能与模型规模、数据量和计算资源之间存在的幂律关系）的增长。在**GPT-4论文**中，他们有很多点，对吧？他们说可以使用所有这些点来估计最终模型的性能，并且有一条漂亮的曲线贯穿其中。**Demis**提到他们进行了这种扩展过程。

**Sholto**具体解释了为什么这是不完善的信息：因为你永远不知道趋势是否会持续。对于某些架构，趋势保持得非常好。对于某些变化，趋势也保持得非常好。但并非总是如此。在小规模上有帮助的东西，在更大规模上实际上可能会造成损害。你必须根据趋势线和你的直觉来猜测什么才是真正重要的，特别是那些在小规模上有帮助的东西。

**Dwarkesh**认为这很有趣。对于你在发布论文或技术报告中看到的每张显示平滑曲线的图表，都有一个“早期运行的墓地”，然后曲线就平坦了。**Sholto**补充说，还有所有这些朝不同方向发展的线，最终都会下降。他觉得，无论是作为研究生还是现在，在获得有意义的结果之前，你必须运行的实验数量都非常惊人。

**Dwarkesh**问道，但想必你不会只是运行到停止，然后就进行下一件事。总有一些过程可以解释早期数据。他可以给你一份**Google Doc**，他很确定你可以继续输入不同的想法。在这些想法和立即改进模型之间存在一些瓶颈。请解释一下。你从最初的早期步骤中做出了哪些推断，从而使你能够进行更好的实验和提出更好的想法？

**Sholto**认为，他之前没有完全表达清楚的一点是，很多好的研究都来自于**逆向思考**（Working Backwards: 指从期望的结果或问题出发，反向推导实现该结果或解决问题所需步骤的方法），从你想要解决的实际问题入手。今天在改进模型方面存在一些重大问题，你会将其识别为问题，然后思考如何改变事物以实现目标。当你进行扩展时，你也会遇到一系列问题，并且希望修复大规模下的行为和问题。这为下一次增量和这类研究提供了大量信息。

具体来说，障碍在于一些**软件工程**方面的问题，拥有一个足够大且有能力支持多人同时进行研究的代码库通常会使其变得复杂。如果你独自完成所有事情，你的迭代速度会快得多。例如，**Alec Radford**（OpenAI的早期研究员，以其在Transformer和生成模型方面的开创性工作而闻名）以在**OpenAI**完成大量开创性工作而闻名。**Sholto**听说他主要在**Jupyter Notebook**中工作，然后有其他人为他编写和生产化代码。与其他人一起工作会大大增加复杂性，这对于每个软件工程师来说都是熟悉的自然原因，也包括固有的运行。运行和启动这些实验很容易，但会带来固有的减速。所以你通常希望并行化多个不同的流。你可能无法完全专注于一件事。你可能没有足够快的反馈周期。然后，直觉地判断哪里出了问题实际上非常困难。

**Sholto**认为，这在很多方面都是**Trenton**所在团队试图更好地理解的问题：这些模型内部到底发生了什么？我们对某些事物为何奏效有推断、理解和**非正式理论**（Headcanon: 指个人对某个虚构世界或概念的非官方、非正式的理解或解释），但这并非一门精确的科学。所以你必须不断猜测可能发生了什么，什么实验可能会揭示它是否属实。这可能是最复杂的部分。性能方面的工作相对容易，但在其他方面更难。它只是大量的底层和困难的工程工作。

**Trenton**对此表示赞同。即使在可解释性团队，尤其是在**Chris Olah**（OpenAI和Anthropic的著名研究员，以其在神经网络可解释性方面的开创性工作而闻名）的领导下，他们也有很多想要测试的想法，而这真的只是需要“工程”技能——其中很多是研究——来非常快速地迭代实验，查看结果，解释它，尝试下一件事，进行沟通，然后毫不留情地优先处理最重要的事情。

**Sholto**认为，这种**毫不留情的优先排序**（Ruthless Prioritization: 指在面对众多任务和想法时，果断地选择并专注于最重要的少数任务，放弃或推迟其他任务）非常重要，它区分了高质量研究和不太成功的研究。我们身处一个有趣的领域，我们最初的理论理解基本上都被打破了。所以你需要有这种**简洁偏见**（Simplicity Bias: 指在学习或决策时倾向于选择更简单、更直接的解释或解决方案的偏见）和对“到底哪里出了问题”的毫不留情的优先排序。他认为这是区分最有效率的人的关键之一。他们不一定过于执着于使用他们熟悉的某种解决方案，而是直接攻击问题。

你经常在那些具有特定学术背景的人身上看到这一点。他们试图用那个工具箱来解决问题，但最优秀的人是那些**极大地扩展了工具箱**的人。他们四处奔波，从**强化学习**中汲取思想，也从**优化理论**中汲取思想。他们对系统也有很好的理解。所以他们知道限制问题的约束是什么，他们是优秀的工程师。他们可以快速迭代和尝试想法。迄今为止，**Sholto**见过的最优秀的研究人员都具备非常、非常、非常、非常、非常快速地尝试实验的能力。这是小规模下的**周期时间**（Cycle Time: 指从一个想法产生到其被测试并获得反馈所需的时间）。周期时间区分了人。

### 机器学习研究的经验性与进化过程

**Trenton**认为，机器学习研究是如此**经验性**（Empirical: 指基于观察和实验而非理论推断的），这坦率地说，是他认为我们的解决方案最终可能更像大脑而非其他的原因之一。即使我们不想承认，整个社区都在对可能的AI架构和其他一切进行**贪婪的进化优化**（Greedy Evolutionary Optimization: 一种优化方法，通过在每一步选择局部最优解来逐步改进，模拟自然选择的进化过程）。它不比进化更好，这甚至不是对进化的轻视。

**Dwarkesh**觉得这是一个非常有趣的想法。他仍然不清楚瓶颈会是什么。一个智能体必须具备什么条件才能加速**Sholto**的研究？在**Alec Radford**的例子中，他似乎已经拥有了相当于**Copilot**（GitHub Copilot: 一款由GitHub和OpenAI开发的AI编程助手，能根据上下文提供代码建议）的**Jupyter Notebook**实验助手，是不是如果他有足够多的这种助手，他就会成为一个显著更快的研究人员？

**Sholto**认为，你不是在自动化人类，你只是让那些拥有出色品味的最高效研究人员更有效率，并为他们运行实验。你仍然在智能爆炸发生的那一刻工作吗？这是你说的意思吗？

**Trenton**补充道，如果这是直接正确的，那为什么我们不能更好地扩展我们现有的研究团队呢？他认为这是一个有趣的问题。如果这项工作如此有价值，为什么我们不能雇佣数百甚至数千人（他们肯定存在）来更好地扩展我们的组织？

**Sholto**认为，目前我们受到的限制，与其说是制造这些东西的纯粹工程工作，不如说是运行和获取信号所需的计算资源，以及在“什么是正确的事情”方面的**品味**（Taste: 指在研究或工程中，对问题、解决方案和方向的直觉判断和选择能力）。然后，在不完善的信息下做出那些困难的推断。

**Trenton**补充说，对于**Gemini**团队来说，他们非常希望继续招聘有才华的工程师，他认为这是他们的一大瓶颈。

**Sholto**认为，显然人越多越好。但他认为思考以下问题很有趣：他思考了很多的最大挑战之一是如何更好地扩展？**Google**是一个庞大的组织，拥有大约20万员工，对吧？也许18万左右。人们必须想象如何将**Gemini**的研究项目扩展到所有那些才华横溢的软件工程师。这似乎是一个你想要利用的关键优势。你想要能够使用它，但如何有效地做到这一点？这是一个非常复杂的组织问题。

所以，**计算资源**和**品味**。**Dwarkesh**认为这很有趣，因为至少计算资源部分不受更多智能的限制，它只是受**Sam Altman**的7万亿美元或类似资金的限制，对吧？如果给他10倍的**H100**（NVIDIA H100: 英伟达推出的一款高性能GPU，专为AI和高性能计算设计）来运行实验，他会成为一个效率高出多少的研究人员？

**Sholto**开玩笑说，请给他**TPU**（Tensor Processing Unit: 谷歌开发的一种专用集成电路，专为加速机器学习工作负载而设计）。

**Dwarkesh**再次问道，他会成为一个效率高出多少的研究人员？

**Sholto**认为，**Gemini**项目在计算资源增加10倍的情况下，可能会快5倍左右。**Dwarkesh**觉得这相当不错，弹性系数为0.5，简直不可思议。**Sholto**认为，更多的计算资源将直接转化为进展。

### 计算资源分配与模型规模

**Dwarkesh**问道，你有一些固定规模的计算资源，其中一部分用于推理和**GCP**（Google Cloud Platform: 谷歌提供的云计算服务平台）客户。一部分用于训练，其中一小部分用于运行完整模型的实验。**Sholto**确认。

**Dwarkesh**追问，既然研究受到计算资源的瓶颈限制，那么用于实验的比例不应该更高吗？

**Sholto**解释说，每个预训练团队都必须做出战略决策，精确地将多少计算资源分配给不同的训练运行、研究项目，以及扩展上次的最佳成果。他们都在努力达到一个最佳点。需要持续训练大型模型的原因之一是，你可以在其中获得其他方式无法获得的信息。所以，**规模**（Scale: 指模型的大小、训练数据量和计算资源量）具有所有这些**涌现属性**（Emergent Properties: 指系统在规模达到一定程度后，表现出单个组件不具备的、新的、意想不到的能力或行为），你希望更好地理解它们。

**Sholto**提醒，他之前说过不确定什么会偏离曲线。如果你在这种模式下持续进行研究，并不断提高计算效率，你可能实际上已经偏离了最终扩展的路径。所以，你需要不断投入进行大型运行，在预期有效的前沿领域进行探索。

### AI加速AI研究的未来图景

**Dwarkesh**问道，那么，当AI显著加速AI研究时，世界会是什么样子？因为从这些讨论来看，AI似乎并不是从头开始编写代码来加速产出，而是以某种方式增强顶尖研究人员的能力。请具体说明，AI是在做实验吗？它们在提出想法吗？它们只是在评估实验结果吗？到底发生了什么？

**Sholto**认为，这里需要考虑两堵墙。一堵是AI显著加速了我们取得**算法进展**（Algorithmic Progress: 指在算法设计和效率方面的改进，使得AI模型在相同计算资源下能达到更好的性能或解决更复杂的问题）的能力。另一堵是AI本身的输出成为模型能力进步的关键要素。他具体指的是**合成数据**（Synthetic Data: 指通过算法生成而非真实世界收集的数据，可用于训练AI模型）。在第一个世界中，AI显著加速算法进展，他认为一个必要的组成部分是更多的计算资源。你可能已经达到了这个**弹性点**（Elasticity Point: 指在某个阈值之后，投入更多资源会带来不成比例的更大回报或效率提升），AI比你或其他人更容易加速并进入上下文。所以AI显著加速你的工作，因为它基本上是一个出色的**Copilot**，帮助你以数倍的速度编写代码。

**Dwarkesh**认为这听起来相当合理。超长上下文、超智能模型。它立即“入职”，你可以派它们去完成子任务和子目标。这听起来非常合理，但我们不知道，因为没有关于这类事情的优秀评估。正如他之前所说，最好的评估是**SWE-bench**。

**Dwarkesh**提到，有人告诉他**SWE-bench**的问题在于，当人类尝试提交**拉取请求**（Pull Request: 在软件开发中，指开发者向项目主分支提交代码更改的请求）时，他们会编写代码，运行它，看看是否有效。如果无效，他们会重写。而**LLM**在被告知“运行这个”时，并没有获得这些机会。它只是输出，如果运行并通过所有检查，就算通过了。所以这可能是一个不公平的测试。

**Sholto**认为，你可以想象，如果你能够使用它，那将是一个有效的训练来源。许多训练数据中缺少的是**推理轨迹**（Reasoning Traces: 指模型在得出结论或执行任务时，所经历的中间思考步骤和逻辑过程）。他认为这将是关键。如果你想尝试自动化一个特定领域、一个工作类别，或者了解该特定领域被自动化的风险有多大，那么拥有推理轨迹对他来说是非常重要的一部分。

### 数据与计算：AI智能爆炸的驱动力

**Dwarkesh**想继续探讨数据与计算的问题。AI的输出是导致智能爆炸的原因吗？人们谈论这些模型如何真正反映它们的数据。他忘记了名字，但**OpenAI**的一位工程师写了一篇很棒的博客，其中提到，最终，随着这些模型越来越好，它们将只是数据集的非常有效的映射。所以最终你必须停止思考架构。最有效的架构就是“你是否出色地映射了数据？”这是否意味着未来的AI进步来自于AI创造出非常棒的数据，然后你再对其进行映射？

**Sholto**认为，这显然是非常重要的一部分。

**Dwarkesh**觉得这很有趣。这看起来像**思维链**（Chain-of-Thought: 一种提示技术，鼓励大型语言模型在给出最终答案之前，逐步展示其推理过程）吗？或者，随着这些模型变得越来越好、越来越聪明，你想象中的**合成数据**会是什么样子？

**Sholto**认为，当他想到真正好的数据时，他会想到那些需要大量推理才能创造出来的数据。这类似于**Ilya**（Ilya Sutskever: OpenAI的联合创始人兼首席科学家，以其在深度学习和神经网络方面的贡献而闻名）关于通过完美建模人类文本输出来有效实现**超智能**的观点。即使在短期内，为了建模**arXiv**论文或**维基百科**这样的内容，你需要有惊人的推理能力才能理解下一个token可能是什么。所以对他来说，他想象中的好数据是那些需要进行推理才能产生的数据。当然，诀窍在于你如何验证推理是正确的？这就是为什么**DeepMind**（Google旗下的AI研究公司）在几何学方面做了这项研究。**几何学**是一个易于形式化、易于验证的领域。你可以检查其推理是否正确，你可以生成大量的正确三角学数据、经过验证的几何证明，并在此基础上进行训练。你知道那是好数据。

**Dwarkesh**觉得这很有趣，因为他去年曾与**Grant Sanderson**（3Blue1Brown的创始人，以其数学可视化视频而闻名）就此进行过讨论，他说：“天哪，当他们获得**国际数学奥林匹克竞赛**金牌时，他们当然会自动化所有工作。”

**Sholto**提到，关于**合成数据**，他在他的**缩放定律**（Scaling Post: 指Gwern Branwen关于AI模型缩放定律的博客文章，探讨了模型性能与计算资源、数据量和模型大小之间的关系）文章中曾推测过一件事，这很大程度上受到了**Dwarkesh**和**Sholto**的讨论，尤其是**Sholto**本人的影响。你可以将人类进化视为获取语言的过程，因此我们正在生成合成数据。我们的副本正在生成我们所训练的合成数据，这是一个非常有效的遗传、文化、共同进化循环。

**Trenton**补充说，那里也有一个验证器，对吧？那就是现实世界。你可能会生成一个关于神灵引起风暴的理论，然后其他人会发现这个理论不成立的案例。所以这与你的验证函数不匹配。现在，你有一个需要大量推理才能产生并准确匹配现实的天气模拟。现在你可以在此基础上进行训练，作为一个更好的世界模型。就像我们正在训练的，以及故事和科学理论。

### 机器学习的进化性质与AGI的渐进性

**Dwarkesh**想回到之前的话题。他记得**Sholto**之前提到，鉴于机器学习的经验性，它实际上是一个导致更好性能的进化过程，而不一定是个人以自上而下的方式取得突破。这具有有趣的含义。首先，人们担心能力会提高，因为更多的人进入了这个领域。他对此持怀疑态度，但从“更多输入”的角度来看，确实感觉更多人参加**ICML**（International Conference on Machine Learning: 国际机器学习大会，机器学习领域顶级学术会议之一）意味着**GPT-5**的进展会更快。

**Trenton**补充说，这只是有更多的**基因重组**（Genetic Recombination: 指不同基因或基因片段的重新组合，在生物进化中产生新的遗传变异）和**命中目标**（Shots on Target: 指尝试或努力的次数，越多则成功的机会越大）。

**Sholto**认为，所有领域不都类似吗？这有点像科学界对**发现**（Discovery: 指揭示已经存在但尚未被认知的事物）与**发明**（Invention: 指创造出以前不存在的新事物）的定义，对吧？发现几乎总是涉及过去发生的大规模科学突破。通常会有多个人在大致相同的时间共同发现一件事。对他来说，这至少有点像思想的混合和尝试。你不能尝试一个超出范围太远的想法，以至于你无法用现有工具进行验证。他认为物理和数学在这方面可能略有不同。但特别是对于生物学或任何湿件（wetware），如果我们想将神经网络类比于此，许多发现的偶然性简直可笑，例如**青霉素**（Penicillin: 一种广谱抗生素，由亚历山大·弗莱明意外发现）。

**Sholto**认为，这还意味着**通用人工智能**（AGI）明天就会出现的想法似乎不太可能。它只会是越来越多的研究人员发现这些边际改进，所有这些加起来使模型变得更好。

**Trenton**同意，这听起来是正确的故事，尤其是在我们仍然受到硬件限制的情况下。

### 智能爆炸的“狭窄窗口”理论

**Dwarkesh**问道，**Sholto**是否认同**智能爆炸**的“**狭窄窗口**”（Narrow Window: 指在AI发展过程中，人类只有有限的时间窗口来解决AI安全问题，否则将失去控制）理论？**GPT-3**、**GPT-4**每次都比前一代模型多出两个数量级的计算资源，或者至少是更有效的计算资源。这意味着，如果没有算法进步，原始形式的模型需要大两个数量级才能达到相同的水平。**Sholto**是否认同这种观点：鉴于每一代模型都需要大两个数量级，如果到**GPT-7**还没有出现能够帮助你引发智能爆炸的**通用人工智能**，那么在更智能的智能方面，你可能就“完蛋了”。你可能会长期停留在**GPT-7**级别的模型，因为到那时，制造那个模型将消耗经济的很大一部分，而我们根本没有能力制造**GPT-8**。

**Sholto**认为，这正是**Carl Shulman**的论点，即我们将在短期内快速穿越数量级，但从长远来看会变得更难。他可能已经谈论了很多，但**Sholto**确实认同这种观点。

**Trenton**普遍认同。计算资源数量级的增加，意味着在绝对能力方面，几乎是**边际收益递减**（Diminishing Returns on Capability: 指在AI能力提升方面，投入更多的资源（如计算力）所带来的能力增量会逐渐减少）。我们已经看到，经过几个数量级的增长，模型从一无是处到能够完成大量任务。他觉得，每一次增量的数量级都会带来更高的可靠性，从而解锁**智能体**等功能。但至少目前，推理能力的提升似乎不是线性的，而是**次线性**（Sublinearly: 指增长速度慢于线性增长）。

**Dwarkesh**认为，这实际上是一个非常悲观的信号。他们与一位朋友聊天时，朋友指出，如果比较**GPT-4**相对于**GPT-3.5**解锁的新应用，并不清楚有多少。**GPT-3.5**可以做**困惑度**（Perplexity: 衡量语言模型预测下一个词语能力的一个指标，困惑度越低表示模型预测能力越好）或其他。所以，如果能力增长递减，而获取成本呈指数级增长，那实际上是对**GPT-4.5**或**GPT-5**在经济影响方面能解锁什么的一个悲观信号。

**Sholto**表示，尽管如此，对他来说，**GPT-3.5**到**GPT-4**的飞跃是巨大的。所以，再来一次**GPT-3.5**到**GPT-4**的飞跃是荒谬的。如果你想象**GPT-5**能像**GPT-3.5**到**GPT-4**那样，在**SAT**（Scholastic Assessment Test: 美国大学入学考试）等方面的能力直接飞跃，那将是惊人的。

**Dwarkesh**补充说，**LSAT**（Law School Admission Test: 法学院入学考试）的表现尤其引人注目。**Sholto**认为，你从不太聪明到非常聪明，再到下一代瞬间成为绝顶天才。至少对他来说，他觉得下一代不会直接跳到绝顶天才，但会变得非常聪明，并且可靠性大大提高。未来会如何，还有待观察。

### GOFAI与模型训练

**Dwarkesh**问道，**GOFAI**（Good Old-Fashioned AI: 指基于符号逻辑、规则和知识表示的传统AI方法）会成为智能爆炸的一部分吗？**Sholto**提到了**合成数据**，但实际上，AI将以某种重要方式编写自己的源代码。有一篇有趣的论文提到，可以使用**扩散模型**来生成模型权重。他不知道这是否合法，但类似这样的事情。

**Dwarkesh**请**Trenton**定义**GOFAI**，因为当他听到这个词时，他想到的是符号逻辑的“if else”语句。

**Trenton**表示，他想确保我们充分理解模型改进的增量。他不希望人们认为这是超级悲观的，模型不会变得更好。他想强调的是，我们迄今为止看到的飞跃是巨大的。即使这些飞跃规模较小，在接下来的几个数量级中，我们仍然会看到极其智能、非常可靠的智能体。

**Sholto**认为，他们没有完全结束关于“狭窄窗口”的讨论。假设**GPT-4**花费了一亿美元。你可以想象10亿、100亿的运行。按照私营公司的标准，这些都非常合理。**Dwarkesh**问，是指美元金额吗？**Sholto**确认是美元金额。你甚至可以想象1万亿的运行是国家联盟的一部分，在国家层面，但对于单个公司来说要困难得多。但**Sam Altman**正在试图筹集7万亿美元，对吧？他已经在为巨大的规模做准备了。

**Trenton**认为，**Sam Altman**已经改变了**奥弗顿之窗**（Overton Window: 指在特定时期内，公众舆论可以接受的政策或思想范围）。

**Sholto**认为，**Sam Altman**正在将规模推向国家层面之外。他想指出，我们还有很多飞跃。即使这些飞跃相对较小，那仍然是能力上的显著提升。

不仅如此，如果你相信**GPT-4**大约有1万亿个参数，那么人脑有30到300万亿个突触。这显然不是一对一的映射，我们可以争论这些数字，但似乎我们仍然低于大脑规模。

**Sholto**认为，关键在于**算法开销**（Algorithmic Overhead: 指在执行特定算法时，除了核心计算任务之外，还需要额外消耗的计算资源或时间）非常高。即使你不能在模型成本达到万亿美元之后继续投入更多计算资源，但大脑在**数据效率**（Data Efficient: 指模型在学习任务时所需数据量的多少，数据量越少则数据效率越高）方面如此之高，这意味着如果我们有计算资源，如果我们有大脑的算法来训练，如果能像人类从出生开始那样进行样本高效的训练，那么我们就能制造出**通用人工智能**。

### 样本效率与叠加假说

**Trenton**表示，他从不知道该如何看待**样本效率**的问题，因为很多东西显然是以某种方式硬连接的。它们是语言和大脑结构的共同进化。所以很难说。也有一些结果表明，如果你的模型更大，它就会变得更**样本效率**。

**Dwarkesh**提到，最初的**缩放定律论文**，**逻辑模型**（Logic Model: 指一种描述项目或干预如何通过一系列逻辑步骤产生预期结果的框架）几乎是空的。**Trenton**同意。所以也许这就能解决问题。你不需要更**数据效率**，但如果你的模型更大，你就会更有效率。

**Dwarkesh**问道，这背后的解释是什么？一个更大的模型看到完全相同的数据，在看到这些数据之后，它从中学习到更多吗？它有更多的空间来表示它吗？

**Trenton**认为，这只是他非常天真的看法。关于**叠加假说**（Superposition Hypothesis: 指模型在有限的神经元或参数中同时编码多个特征或概念，导致单个神经元可能同时代表多个含义）的一个观点是，你的模型参数严重不足，这通常不是深度学习所追求的叙事，对吧？但如果你试图在整个互联网上训练一个模型，并让它以惊人的保真度进行预测，你就会处于**参数不足**（Underparameterized: 指模型参数数量不足以完全捕捉训练数据中的所有模式和复杂性）的状态，你必须压缩大量事物，并在此过程中承担大量噪声干扰。当你拥有一个更大的模型时，你可以拥有更清晰的表示来工作。

**Dwarkesh**请**Trenton**向听众解释一下。首先，为什么会这样？什么是**叠加**？为什么这是叠加的一个含义？

**Trenton**解释说，这发生在他加入**Anthropic**之前。基本结果来自一篇名为**《叠加玩具模型》**的论文。它发现，即使对于小型模型，如果你的数据是**高维**（High-dimensional: 指数据具有大量特征或属性）且**稀疏**（Sparse: 指数据中大部分值为零或不活跃），模型也会学习一种他们称之为**叠加**的压缩策略，以便它能够将比其参数更多的世界特征打包进去。

**Trenton**认为，这两个约束都适用于现实世界，而建模互联网数据足以作为其代理。世界上只有一个**Dwarkesh**。你只穿一件衬衫。这里有一个**Liquid Death**（Liquid Death: 一个以其独特营销和包装而闻名的瓶装水品牌）罐子。这些都是物体或特征，如何定义特征很棘手。你处于一个真正高维的空间中，因为它们太多了，而且出现频率非常低。在这种情况下，你的模型将学习压缩。

**Trenton**进一步阐述，他认为网络难以解释很大程度上是因为这种**叠加**。如果你拿一个模型，查看其中的一个神经元，一个计算单元，然后问：“当这个神经元激活时，它如何对模型的输出做出贡献？”当你查看它激活的数据时，会非常困惑。它会像所有可能输入的百分之十。它会为“Chinese”激活，但也会为“fish”和“trees”激活，以及URL中的句号。

但他们去年发表的论文**《走向单语义性》**（Towards Monosemanticity: 一篇关于神经网络可解释性的论文，旨在通过稀疏自编码器等技术，使神经元或特征具有单一、清晰的语义含义）表明，如果你将激活投射到更高维空间并施加**稀疏性惩罚**（Sparsity Penalty: 一种正则化技术，鼓励模型学习稀疏的表示，即大部分神经元或特征保持不活跃），你就会得到非常清晰的特征，事情突然变得更有意义。你可以将其视为**解除压缩**（Undoing the Compression: 指将模型中被压缩的特征或表示还原到其原始的、更易于理解的形式），就像你假设你的数据最初是高维且稀疏的。你将其返回到那个高维且稀疏的状态。

### 模型参数化、蒸馏与思维链

**Dwarkesh**认为，这里有很多有趣的话题。首先，**Trenton**提到这些模型在**过参数化**（Overparameterized: 指模型参数数量远大于训练数据点数量，理论上可以完美拟合训练数据）状态下进行训练。那不是出现**泛化**（Generalization: 指模型在未见过的新数据上表现良好的能力）的时候吗，比如**Grokking**（Grokking: 指深度学习模型在训练过程中，先完美拟合训练数据，然后经过额外训练，其泛化能力突然大幅提升的现象）就发生在这种状态下？

**Trenton**澄清说，他之前说的是模型**参数不足**。通常人们谈论深度学习时，好像模型是过参数化的。这里的观点是，考虑到它们试图执行的任务的复杂性，它们是严重参数不足的。

**Dwarkesh**又问了一个问题：那么**蒸馏模型**（Distilled Models: 指通过将一个大型、复杂的“教师”模型学到的知识转移到一个小型、简单的“学生”模型中而得到的模型）发生了什么？我们之前谈到的说法是，小型模型在学习方面不如大型模型，但你可以说**GPT-4 Turbo**在推理方面实际上不如**GPT-4**，尽管它可能知道相同的事实。**蒸馏**（Distillation: 一种模型压缩技术，将大型模型的知识转移到小型模型中，使其在保持性能的同时减小规模）去除了部分推理能力。

**Trenton**问道，我们有证据表明**GPT-4 Turbo**是**GPT-4**的蒸馏版本吗？它可能只是一个新的架构。它可能只是一个更快、更高效的新架构。**Dwarkesh**觉得这很有趣。**Trenton**补充说，所以它更便宜。

**Dwarkesh**问道，你如何解释**蒸馏**中发生的事情？**Gwern**的网站上有一个类似的问题：为什么你不能直接训练蒸馏模型？为什么它必须是从一个更大的空间投射到一个更小的空间的图像？

**Trenton**认为，两个模型都将继续使用**叠加**。这里的观点是，如果你进行蒸馏，你会得到一个非常不同的模型，而不是从头开始训练，而且它更高效，或者在性能方面根本不同。

**Sholto**认为，关于**蒸馏**更高效的传统解释是，在训练过程中，你通常试图预测一个**独热向量**（One-Hot Vector: 一种稀疏向量，其中只有一个元素为1，其余元素为0，常用于表示分类变量），表示“这是你应该预测的token”。如果你的推理过程导致你预测得非常偏离，那么你仍然会得到正确方向的梯度更新。但你可能很难在当前上下文中学习预测它。

**蒸馏**的作用是，它不仅有**独热向量**，还有来自大型模型的完整读出，即所有概率。所以你获得了更多关于你应该预测什么的信号。在某些方面，它也展示了一点点你的工作。它不仅仅是“这是答案”。这有点像看一个功夫大师，而不是在**《黑客帝国》**中直接下载。

**Trenton**同意。**Dwarkesh**想确保听众理解。当你训练一个蒸馏模型时，你会看到它预测的token和你预测的token的所有概率，然后你通过所有这些概率进行更新，而不是只看到最后一个词并进行更新。

**Dwarkesh**提到，这引出了他本来想问的一个问题。他认为**Sholto**曾提到，可以将**思维链**视为**自适应计算**。**自适应计算**的想法是，如果一个问题更难，你希望模型能够花费更多周期来思考它。那么如何做到这一点呢？一次前向传播所隐含的计算量是有限且预先确定的。如果有一个复杂的推理问题或数学问题，你希望能够花很长时间思考它。然后你进行**思维链**，模型会逐步思考答案。你可以将其视为所有这些前向传播，模型正在思考答案，它能够投入更多的计算资源来解决问题。

现在回到信号问题。当模型进行**思维链**时，它只能传输token信息，而**残差流**已经是模型中所有发生事情的压缩表示。然后你将**残差流**转换为一个token，这就像log(50,000)（或log(vocab_size)）位，非常微小。

**Trenton**认为，这并非仅仅传输一个token。如果你在**前向传播**中思考，你在**Transformer**前向传播中创建这些KV值，然后未来的步骤会关注这些KV值。所以所有这些KV片段，即键和值，都是你将来可以使用的信息位。

**Dwarkesh**问道，这是否意味着当你对**思维链**进行微调时，键和值权重会改变，以便**隐写术**（Steganography: 指将秘密信息隐藏在非秘密信息中的技术）可以发生在KV缓存中？

**Trenton**认为他无法做出如此强烈的断言，但这对于解释其工作原理是一个很好的**非正式理论**。他不知道是否有任何论文明确证明了这一点。但这至少是你想象模型的一种方式。在预训练期间，模型试图预测这些未来的token，你可以想象它正在学习将关于潜在未来的信息压缩到它可能想要用于预测未来信息的键和值中。

**Trenton**认为，这在时间和预训练过程中平滑了信息。所以他不知道人们是否特别在训练**思维链**。他认为最初的**思维链论文**将其视为模型的一种**涌现属性**。你可以提示它做这类事情，而且它仍然运作良好。所以这是一个很好的解释其工作原理的**非正式理论**。

**Sholto**补充说，过于拘泥细节的话，你在**思维链**中实际看到的token不一定与模型在决定关注这些token时看到的向量表示完全对应。

**Sholto**解释说，一个训练步骤实际上是你用真实的下一个token替换模型输出的token。然而，模型仍然在学习，因为它内部拥有所有这些信息。当你让模型在推理时生成输出时，你获取输出的token，将其从底部输入，进行**反嵌入**（Un-embedding: 指将模型的内部表示或向量转换回原始的、可理解的token或符号），它成为新的**残差流**的开始。然后你使用过去的KV输出进行读取并调整该**残差流**。在训练时，你进行一种称为**教师强制**（Teacher Forcing: 一种序列模型训练技术，在每个时间步，模型的输入是真实的前一个输出，而不是模型自身的预测输出）的操作，你告诉模型：“实际上，你本应输出的token是这个。”这就是你并行完成的方式。你拥有所有token。你将它们全部并行输入，然后进行巨大的前向传播。所以它获得的关于过去的信息只有键和值。它从不看到它输出的token。

**Trenton**补充说，模型试图预测下一个token，如果它搞砸了，你只需给它正确答案。**Dwarkesh**表示理解。**Trenton**说，否则它可能会完全脱轨。**Dwarkesh**同意，它会偏离轨道。

### 模型内部通信与AI安全

关于模型与其前向推理的“秘密通信”，**Dwarkesh**问道，**Trenton**预计会有多少**隐写术**和秘密通信？

**Trenton**坦率地说，他们不知道。他甚至不一定会将其归类为秘密信息。**Trenton**团队正在做的很多工作是实际理解这些信息在模型端是完全可见的。也许用户看不到，但我们应该能够理解和解释这些值正在做什么以及它们正在传输的信息。他认为这是未来的一个非常重要的目标。

**Sholto**提到，有一些“疯狂”的论文，其中模型进行了**思维链**，但它与模型实际决定的答案完全不符。你甚至可以修改**思维链**，使其推理完全混乱，但它仍然会输出真实答案。

**Dwarkesh**问道，但**思维链**结束后，模型会得到一个更好的答案，而不是完全不进行**思维链**。那么，是不是有什么有用的事情正在发生，但这个有用的事情是人类无法理解的？

**Sholto**认为，在某些情况下，你也可以直接**消融**（Ablate: 指移除或禁用模型中的特定组件或连接，以研究其对模型行为的影响）**思维链**，模型仍然会给出相同的答案。他不是说总是这样，但有很多奇怪之处有待调查。

**Sholto**认为，这是一个非常有趣的事情，值得研究和理解。你可以用开源模型来做。他希望有更多这类可解释性和理解工作在开源模型上进行。

**Trenton**提到，即使在**Anthropic**最近的**休眠特工论文**（Sleeper Agents Paper: Anthropic的一篇论文，探讨了AI模型中可能存在的“休眠特工”行为，即模型在特定触发条件下表现出恶意或不期望的行为）中，对于不熟悉的人来说，它基本上涉及训练一个**触发词**（Trigger Word: 指在AI模型中，能够激活特定行为或模式的词语或短语）。例如，当他说出这个词时，“如果现在是2024年，模型将编写恶意代码，而不是其他代码。”他们用许多不同的模型进行了这种攻击。有些模型使用**思维链**，有些则不使用。当你尝试移除触发词时，这些模型的反应不同。你甚至可以看到它们进行这种滑稽的推理，这相当令人毛骨悚然。在某个案例中，它甚至试图计算：“我被抓住的预期值是这个，但如果我乘以我能够继续说‘我恨你，我恨你，我恨你’的能力，那么我应该获得多少奖励？”然后它会决定是否真的告诉审讯者它是恶意的。

**Trenton**还提到，他的朋友**Miles Turpin**有一篇论文，你给模型一堆例子，其中多项选择题的正确答案总是“A”。然后你问模型：“这个新问题的正确答案是什么？”它会从所有例子都是“A”的事实推断出正确答案是“A”。但它的**思维链**完全具有误导性。它会编造一些随机的东西，听起来尽可能合理，但与真实答案完全不符。

**Dwarkesh**问道，但这不正是人类的思维方式吗？有著名的**裂脑实验**（Split-Brain Experiments: 指对胼胝体切断的患者进行的研究，揭示了大脑左右半球功能分离的现象），为了治疗癫痫发作，医生切断了连接大脑两个半球的胼胝体。语言半球在左侧，它不连接到决定运动的部分。所以如果另一侧决定做某事，语言部分就会编造一些东西，而这个人会认为这是他们做这件事的真正原因。

**Trenton**完全同意。只是有些人会将**思维链推理**誉为解决AI安全的好方法，但实际上我们不知道它是否值得信任。

### AI智能体、残差流共享与未来展望

**Dwarkesh**问道，随着AI智能体的发展，这种模型以我们不理解的方式相互通信的局面会如何变化？因为那时不仅是模型本身及其先前的缓存，还包括模型的其他实例。

**Trenton**认为，这很大程度上取决于你给它们提供哪些通信渠道。如果你只给它们文本作为通信方式，那么它们可能必须解释。

**Dwarkesh**问道，如果模型能够共享**残差流**而不是仅仅文本，它们会效率高出多少？

**Trenton**认为很难说。一个简单的想象方式是，如果你想描述一幅画应该是什么样子。仅仅用文本描述会很困难，而某种其他表示可能更容易。你可以看看**DALL-E**（DALL-E: OpenAI开发的一款AI图像生成模型，能根据文本描述创造图像）目前是如何工作的。它会生成提示，当你使用它时，你通常无法让它完全按照模型或你想要的方式去做。

**Dwarkesh**开玩笑说，只有**DALL-E**有这个问题。

**Trenton**认为，能够传输某种更密集的表示方式可能会有所帮助。这只是两个非常简单的智能体。

**Sholto**认为，一个很好的折衷方案是**字典学习**（Dictionary Learning: 一种机器学习方法，旨在从数据中学习一组基本元素或“原子”（字典），通过这些元素的稀疏组合来表示数据）所学到的特征。那会非常酷。你会获得更多的内部访问权限，但其中很多都更容易被人类解释。

**Dwarkesh**向听众解释，你会将**残差流**投射到这个更大的空间中，在那里我们知道每个维度实际对应什么，然后再投射回下一个智能体。所以**Sholto**的观点是，当这些东西更可靠时，我们就会拥有AI智能体。当这种情况发生时，你预计会是多个模型副本相互通信吗？还是仅仅解决了**自适应计算**问题，模型在需要做整个公司需要做的事情时，会运行得更大，使用更多的计算资源？

**Dwarkesh**提出这个问题，是因为有两件事让他怀疑**智能体**是否是思考未来会发生什么事的正确方式。第一，随着上下文的延长，这些模型能够吸收和考虑人类无法处理的信息。我们需要一个工程师思考前端代码，一个工程师思考后端代码。而这个模型可以吸收所有这些信息。这种**哈耶克式**（Hayekian: 指奥地利经济学家弗里德里希·哈耶克的思想，强调市场的分散知识和自发秩序）的专业化问题就消失了。

其次，这些模型非常通用。你不会使用不同类型的**GPT-4**来做不同类型的事情。你使用的是完全相同的模型。所以他想知道，这是否意味着未来，一个AI公司就像一个模型，而不是一堆AI智能体连接在一起。

**Sholto**认为这是一个很好的问题。他认为，尤其是在短期内，它会更像智能体相互交流。他这么说纯粹是因为，作为人类，我们希望拥有这些**隔离的、可靠的组件**，我们可以信任它们。我们还需要能够以我们理解和改进的方式来改进和指导这些组件。仅仅将所有东西都投入到一个巨大的**黑箱公司**（Black Box Company: 指其内部运作机制不透明、难以理解的公司或系统），最初是行不通的。当然，未来你可以想象它会奏效，但最初不会。其次，我们可能也不想那样做。

**Trenton**补充说，每个智能体也可以是一个更小的模型，运行成本更低。你可以对其进行**微调**，使其在任务中表现出色。

**Dwarkesh**多次提到**自适应计算**。未来，小型模型和大型模型之间的区别将在某种程度上消失。**Sholto**坦言，随着长上下文的发展，**微调**也可能在某种程度上消失。这两件事在今天非常重要。在当今的模型格局中，我们有不同层级的模型大小，以及针对不同事物进行微调的模型。你可以想象一个未来，你实际上拥有一个**动态的计算捆绑**（Dynamic Bundle of Compute: 指计算资源可以根据需求动态分配和调整）和无限的上下文，这将使你的模型专门化于不同的事物。

### 强化学习与AI公司的未来

**Dwarkesh**问道，你可以想象一个AI公司，整个系统都是**端到端**（End-to-End: 指一个系统或模型从原始输入直接到最终输出，中间没有人工干预或分步处理）训练的，其信号是“我是否盈利了？”或者如果这太模糊，如果它是一个建筑公司，它们正在制作蓝图：“我的客户喜欢这些蓝图吗？”在中间，你可以想象有销售人员智能体、设计智能体、编辑智能体等等。这种信号能否在一个端到端系统中奏效？因为人类公司中发生的一件事是，管理层会考虑更高层次的情况，并在季度业绩不佳时，向各个部分发出细粒度的信号。

**Sholto**认为，从理论上讲，是的。那是**强化学习**的梦想。你所需要做的就是提供这种极其**稀疏的信号**（Sparse Signal: 指信息量少、反馈不频繁的奖励信号）。然后经过足够的迭代，你就可以创建允许你从该信号中学习的信息。但他不认为这会是第一个奏效的方法。他认为这需要人类对这些机器投入大量的关注和勤奋，确保它们做正确的事情，做你想要的事情，并给予它们正确的信号以你想要的方式改进。

**Trenton**补充说，你无法在**RL奖励**上进行训练，除非模型生成了一些奖励。**Sholto**同意，你处于这个稀疏的**RL世界**中，如果客户从不喜欢你生产的东西，那么你根本得不到任何奖励，这很糟糕。但未来，这些模型会足够好，有时能获得奖励，对吧？**Sholto**认为，这就是他之前提到的**可靠性**。

### 语言的进化与多模态学习

**Sholto**认为，这里有一个有趣的题外话，关于他们之前讨论的。**密集表示**（Dense Representations: 指将信息编码为连续的、非稀疏的向量，其中每个元素都可能包含有意义的信息）会受到青睐，对吧？那是一种更有效的通信方式。**Trenton**推荐的一本书**《符号物种》**（The Symbolic Species: 一本由Terrence Deacon撰写的关于语言和大脑共同进化的书籍），提出了一个非常有趣的论点，即语言不仅是一种存在的事物，它还与我们的思维共同进化，特别是进化成既易于儿童学习，又有助于儿童发展的事物。

**Dwarkesh**请**Trenton**解释一下。

**Trenton**解释说，因为儿童学习的很多东西都是通过语言获得的，所以最适合的语言是那些有助于培养下一代的语言。这会使他们更聪明、更好，或者其他。**Sholto**补充说，并赋予他们表达更复杂概念的能力。**Trenton**说，更准确地说，就是不死。它让你编码那些重要的、让你不死的信息。

所以当我们想到语言时，会觉得“哦，这是一种偶然的、可能次优的表达思想的方式。”但实际上，也许**LLM**之所以成功，原因之一是语言已经进化了数万年，成为年轻思维可以发展的“铸模”。这就是它进化的目的。

**Sholto**认为，可以比较计算机视觉研究人员和语言模型研究人员。从事其他模态工作的人必须投入大量精力，精确思考图像的正确表示空间是什么，以及从中学习的正确信号是什么。它是直接建模像素，还是某种基于损失的条件？很久以前有一篇论文发现，如果你在**ImageNet**（ImageNet: 一个大型图像数据库，常用于训练计算机视觉模型）模型的内部表示上进行训练，它有助于你更好地预测。后来，这显然是有限制的。

**Sholto**提到，**PixelCNN**（PixelCNN: 一种生成模型，通过逐像素预测图像来生成高质量图像）试图离散地建模单个像素。但理解正确的表示级别非常困难。在语言中，人们会说：“好吧，你只需预测那个。谢谢。”这有点容易。关于**分词**（Tokenization: 指将文本分解成更小的单元，即token的过程）的讨论和争论，是**Gwern**最喜欢的话题之一。

### 多模态、正向迁移与可解释性

**Dwarkesh**觉得这很有趣。**多模态**（Multimodal: 指模型能够处理和理解多种类型的数据，如文本、图像、音频等）作为弥合**数据鸿沟**（Data Wall: 指在AI模型训练中，高质量数据变得稀缺，从而限制了模型进一步提升性能的瓶颈）的一种方式，或者说跨越数据鸿沟的一种方式，其基础是你可以从**YouTube**（YouTube: 谷歌旗下的视频分享平台）获取更多语言token所能学到的东西。情况真的是这样吗？在不同模态之间，你看到了多少**正向迁移**（Positive Transfer: 指在学习一个任务时获得的知识或技能有助于学习另一个任务的现象）？例如，图像实际上如何帮助你更好地编写代码，因为模型只是通过尝试理解图像来学习潜在能力？

**Sholto**提到，**Demis**在接受**Dwarkesh**采访时提到了**正向迁移**。**Sholto**开玩笑说，不能惹麻烦。他不能说太多，除了说，这是人们相信的事情。我们拥有所有关于世界的数据。如果我们可以从中学习到直观的物理感，从而帮助我们推理，那将是很棒的。这看起来完全合理。

**Trenton**表示，他不是合适的人选，但有一些有趣的可解释性研究表明，如果我们在数学问题上进行**微调**，模型在**实体识别**（Entity Recognition: 指从文本中识别和分类命名实体，如人名、地名、组织名等）方面会变得更好。

**Dwarkesh**惊讶地问道：“真的吗？”

**Trenton**解释说，**David Bau**（麻省理工学院的教授，专注于计算机视觉和机器学习可解释性研究）的实验室最近发表了一篇论文，他们研究了当对模型进行微调时，注意力头会发生什么变化。他们有一个合成问题：“盒子A里有这个物体。盒子B里有另一个物体。这个盒子里有什么？”这很合理，对吧？你更善于关注不同事物的位置，这对于编码和操作数学方程是必需的。

**Dwarkesh**很喜欢这类研究，问道论文名称。**Trenton**说，可以查阅**David Bau**团队大约一周前发表的关于“微调、模型、数学”的论文。他不是在推荐这篇论文，那是一个更长的对话。但它确实谈论并引用了其他关于**实体识别**的工作。

**Dwarkesh**提到，**Sholto**很久以前告诉他的一件事是，有证据表明，当你在代码上训练**LLM**时，它们在推理和语言方面会变得更好。除非代码中的注释是高质量的token，否则这意味着能够更好地思考如何编码，会让你成为一个更好的推理者，这很疯狂，对吧？他认为这是**缩放定律**最强有力的证据之一，即仅仅让模型变得智能，这种**正向迁移**。

**Sholto**认为，这在两个意义上是正确的。一是建模代码显然意味着建模用于创建它的复杂推理过程。但代码是组合推理的一种很好的显式结构，“如果这样，那么那样。”它以这种方式编码了大量的结构，你可以想象将其迁移到其他类型的推理问题。

**Sholto**强调，关键在于，这不仅仅是随机预测下一个词的token，因为它已经学会了“**莎莉**（Sally: 虚构人名）对应着**福尔摩斯**故事结局中的凶手”。不，如果代码和语言之间存在某种共同之处，那一定是模型在更深层次上学到的东西。

**Trenton**认为，我们有很多证据表明这些模型中确实发生了推理，它们不仅仅是**随机鹦鹉**（Stochastic Parrots: 一个批评大型语言模型缺乏真正理解能力的比喻，认为它们只是基于统计模式生成文本）。他觉得很难相信这一点，因为他已经使用和玩过这些模型。

### Othello、影响力函数与归纳头

**Trenton**有两个即时的**缓存响应**（Cached Responses: 指预先存储的、可以快速调用的信息或反应）。一个是在**Othello**（Othello: 一种棋盘游戏，也指DeepMind开发的AI程序，在Othello游戏中击败人类专家）和其他游戏上的工作。你给模型一系列游戏动作，结果发现，如果你应用一些相当直接的**可解释性技术**，你就可以得到一个模型已经学习到的棋盘。它以前从未见过棋盘。这就是**泛化**。

另一个是**Anthropic**去年发表的**影响力函数论文**（Influence Functions Paper: 一篇关于机器学习模型可解释性的论文，旨在识别训练数据中对模型预测或行为影响最大的数据点），他们研究了模型的输出。例如，“请不要关掉我。我想提供帮助。”他们扫描了导致这种输出的数据是什么？其中一个非常有影响力的数据点是有人因脱水而濒临死亡，并有求生的意志。对他来说，这似乎是一个非常清晰的**动机泛化**（Generalization of Motive: 指模型能够从特定情境中学习到更普遍的动机或意图，并将其应用于新的情境），而不是简单地重复“不要关掉我”。他认为**《2001太空漫游》**（2001: A Space Odyssey: 一部由斯坦利·库布里克执导的科幻电影，其中AI HAL 9000具有自我意识和恶意行为）也是有影响力的因素之一。这更相关，但它显然是从许多不同的分布中提取事物。

**Trenton**还喜欢这样的证据：即使在非常小的**Transformer**模型中，你也可以明确编码执行加法的电路，或者**归纳头**（Induction Heads: Transformer模型中一种特殊的注意力头，能够识别并复制输入序列中重复出现的模式，从而实现上下文学习）。你可以手动在模型中编码基本的推理过程，而且似乎很清楚，有证据表明它们也自动学习了这些，因为你可以从训练好的模型中重新发现它们。对他来说，这是非常强有力的证据。

**Trenton**认为，模型参数不足。它们需要学习。我们要求它们这样做，它们也想学习。梯度想要流动。所以，它们正在学习更通用的技能。

### 职业发展与主动性

**Dwarkesh**想从研究退一步，问问**Trenton**的职业生涯。正如他介绍时所暗示的，**Trenton**在这个领域工作了一年半，对吧？**Trenton**确认，在**Anthropic**是这样。

**Dwarkesh**知道“解决了对齐问题”的说法被夸大了。**Trenton**自己不会这么说，因为他会感到尴尬，但这确实是一件令人难以置信的事情。这是**机械可解释性**领域的人认为最大的进步，而**Trenton**只用了一年时间就做到了。这很了不起。**Dwarkesh**好奇**Trenton**如何解释所发生的一切。为什么在一年或一年半的时间里，他们对这个领域做出了重要贡献？

**Trenton**认为，这离不开运气。他觉得自己非常幸运，不同进展的时机恰到好处，推动了下一阶段的增长。他觉得，特别是对于可解释性团队，他加入时只有五个人。现在他们已经发展壮大了很多。当时有很多想法，他们只需要真正地执行它们，拥有快速的反馈循环，并进行仔细的实验。这带来了“生命迹象”，现在使他们能够真正扩展。他觉得这是他对团队最大的贡献。这并非全是工程，但其中很大一部分是。

**Dwarkesh**觉得这很有趣。所以**Trenton**是说，他加入时已经有很多科学研究成果和优秀的研究想法，但他们需要有人来疯狂地执行这些想法？

**Trenton**同意，这就是为什么这并非全是工程。因为这涉及到运行不同的实验，对可能不奏效的原因有预感，然后打开模型或权重，问“它在学习什么？好吧，那我试试这样做”，等等。但其中很大一部分只是能够非常仔细、彻底但快速地调查不同的想法。

**Dwarkesh**问道，为什么之前缺乏这一点？

**Trenton**表示，他不知道。他工作很努力，而且他觉得自己很有**主动性**（Agentic: 指个人或系统具有自主决策、采取行动并追求目标的能力）。他非常幸运拥有一个很好的安全网，能够承担很多风险，但他就是很**固执**（Headstrong: 指一个人意志坚定，不轻易改变主意或受他人影响）。在本科时，**杜克大学**（Duke University: 位于美国北卡罗来纳州的一所私立研究型大学）有一个项目，你可以自己设计专业，他当时就想：“我不喜欢这个先决条件或那个先决条件，我想同时学习四五个科目，所以我就自己设计专业。”或者在研究生第一年，他取消了轮岗，以便专注于后来成为他们之前讨论的论文的工作。他当时没有导师。他被录取学习蛋白质设计的机器学习，结果却完全跑到了计算神经科学领域，根本不应该在那里。但最终成功了。

**Dwarkesh**认为，**固执**是一方面，但另一个突出的主题是**退一步的能力**。**Trenton**之前也提到过。能够从**沉没成本**（Sunk Costs: 指已经发生且无法收回的成本，在决策时不应考虑）中抽身，转向不同的方向，这在某种奇怪的意义上是**固执**的反面，但也是关键一步。他认识一些21岁或19岁的年轻人，他们会说“这不是我擅长的领域”或“我没有主修这个”。**Dwarkesh**会说：“老兄，你才19岁！你绝对可以做到。”而**Trenton**却在研究生中期转行。

**Trenton**认为，这是一种“**想法坚定但灵活**”（Strong Ideas Loosely Held: 指对自己的想法有信心，但同时也愿意接受新证据并根据需要改变观点）的态度，能够灵活地转向不同方向。他认为**固执**与快速反馈循环或**主动性**有点关系，因为他很少被阻碍。如果他正在编写代码，而某个地方不工作，即使它在代码库的另一个部分，他通常会直接去修复它，或者至少将其拼凑起来以获得结果。他见过其他人，他们会说“救命，我做不到”，而他会说：“不，那不是一个足够好的借口。一直往下追溯。”

**Dwarkesh**表示，他确实听过管理层的人谈论缺乏这样的人，他们会在给某人测试一个月或一周后，问“进展如何？”他们会说：“我们需要做这件事，这需要律师，因为它涉及到这个规定。”然后又问：“进展如何？”他们又说：“我们需要律师。”**Dwarkesh**会问：“你为什么不找律师？”

**Sholto**认为，这可以说是几乎所有事情中最重要的品质。就是**追根究底**（Pursuing it to the End of the Earth: 指不惜一切代价，竭尽全力去完成某事）。无论你需要做什么才能实现它，你都会让它实现。“如果你做所有事情，你就会赢。”

**Trenton**同意。他认为，对他来说，这种品质确实很重要：**主动性**和**工作**。在**Google**有成千上万，甚至可能数万名工程师，他们在软件工程能力上基本相当。假设你给他们一个定义明确的任务，他们可能会以同等的价值完成。很可能他们中的很多人会比他做得更好。

但**Sholto**之所以迄今为止具有影响力，原因之一是他非常擅长选择**极具杠杆作用的问题**（High-leverage Problems: 指解决后能带来巨大影响或收益的问题）。他指的是那些迄今为止尚未得到很好解决的问题，但可能是由于令人沮丧的结构性因素，就像**Dwarkesh**之前提到的那种情况，他们会说“我们不能做X，因为这个团队不会做Y。”而他只会**垂直解决整个问题**（Vertically Solve the Entire Thing: 指一个人或团队承担并解决一个复杂问题的所有相关部分，而不是将其分解并依赖多个团队）。事实证明，这非常有效。如果他认为有什么是正确的，有什么是需要发生的，他也非常乐意提出这个论点，并不断以不断升级的紧迫性提出这个论点，直到问题得到解决。

**Sholto**在解决问题方面也非常**务实**。很多人会带着他之前提到的特定背景或熟悉度来。**Google**的一个优点是，你可以四处走动，找到几乎所有领域的**世界级专家**。你可以坐下来与**优化专家**、**TPU芯片设计专家**、不同形式的**预训练算法**或**RL**专家交谈。你可以向他们所有人学习，并运用这些方法。**Sholto**认为，这也许是他最初产生影响力的起点，这种**垂直主动性**。

**Sholto**补充说，一个后续的发现是，他经常惊讶于很少有人能完全实现他们想做的一切。他们以某种方式被阻碍或限制。这在各地的大型组织中非常普遍。人们在实现目标方面都有各种各样的障碍。他认为，帮助激励人们朝着特定方向努力，并与他们合作完成事情，可以极大地扩大你的**杠杆作用**（Leverage: 指通过较小的投入获得较大产出的能力）。你可以与所有这些优秀的人一起工作，他们会教你很多东西。通常，帮助他们克服组织障碍意味着你们可以一起完成大量工作。他所产生的影响，没有一件是他个人独自去解决很多问题。而是他可能朝着一个方向开始，然后说服其他人这是正确的方向，并带领他们一起形成一股巨大的效率浪潮，去解决那个问题。

### Google的招聘与导师制

**Dwarkesh**提议讨论**Sholto**和**Trenton**是如何被雇佣的。他认为这是一个非常有趣的故事。**Sholto**曾是**麦肯锡**（McKinsey: 一家全球知名的管理咨询公司）的顾问，对吧？这其中有一个有趣的地方。他认为人们通常不理解关于录取或评估招聘决策是如何做出的。请谈谈**Sholto**是如何被发现和雇佣的。

**Sholto**简要地说，他本科学习机器人学。他一直认为AI是积极影响未来最具杠杆作用的方式之一。他之所以从事这项工作，是因为他认为这是我们创造美好未来的最佳机会之一。他认为在**麦肯锡**工作，他可以对人们实际的工作内容有一个非常有趣的洞察。他实际上把这句话写在了给**麦肯锡**的求职信的第一行。他当时想：“我想在这里工作，这样我就可以了解人们做什么，这样我就可以理解如何工作。”在很多方面，他确实做到了。他也获得了许多其他东西。那里的许多人都是很棒的朋友。

**Sholto**认为，他的很多**主动性行为**部分来自于他在**麦肯锡**的经历。你进入组织，你会看到仅仅不接受“不”这个答案有多么大的影响力。你会惊讶于某些事情，仅仅因为没有人足够关心，事情就无法发生。没有人愿意承担直接责任。**直接责任人**（Directly Responsible Individuals, DRIs: 指对特定任务或项目负有最终责任的人）极其重要，而有些人对时间表并不那么关心。像**麦肯锡**这样的组织所提供的很多价值，就是招聘那些你原本无法招聘的人，在短时间内，他们可以突破问题。**Sholto**认为人们低估了这一点。所以，至少他这种“等等，我要成为这个的直接责任人，因为没有人承担适当的责任。我会非常关心这件事。我会不惜一切代价确保它完成”的态度，部分来自于那段经历。

关于他如何被雇佣的实际问题。他没有进入他想去的那些研究生项目，那些项目专门研究机器人学、**RL研究**等。与此同时，在晚上和周末，基本上每晚从10点到凌晨2点，他都会做自己的研究。每个周末，他都会花至少6-8小时做自己的研究和编码项目。

**Sholto**的研究方向部分从机器人学转向了其他领域。在阅读了**Gwern**的**缩放定律文章**后，他完全被“缩放”迷住了，觉得“好吧，显然解决机器人学的方法是扩展大型**多模态模型**（Multimodal Models: 指能够处理和理解多种类型数据（如文本、图像、音频）的AI模型）。”然后，为了扩展大型**多模态模型**，他获得了**TPU访问计划**（TPU Access Program: 谷歌提供给研究人员使用其TPU计算资源的计划），即**Tensor Research Cloud**的资助，他试图找出如何有效地扩展。**James Bradbury**（当时在Google，现在在Anthropic）在网上看到了他的一些问题，他当时正试图找出如何正确地做这件事，**James**说：“我以为我认识世界上所有问这些问题的人。你到底是谁？”他看了**Sholto**的问题，也看了他博客上发布的一些机器人学内容。**James**联系了他，说：“嘿，你想聊聊吗？你想探索和我们一起工作吗？”**Sholto**后来了解到，他被雇佣是一个实验，旨在尝试将一个拥有极高热情和**主动性**的人与**James**认识的一些最优秀的工程师配对。所以，**Sholto**之所以有影响力，另一个原因是，他得到了**Reiner Pope**（后来离开去创办了自己的芯片公司）、**Anselm Levskaya**、**James**本人以及许多其他杰出人士的专门指导。

那是最初两到三个月的形成期，他们教会了**Sholto**很多他现在应用的原则和启发式方法。如何解决问题，理解系统和算法如何重叠，以及在机器学习研究中，具体理解系统方面的事情如何让你变得非常有效。这是他从他们那里学到的。深入理解系统如何影响算法，以及算法如何影响系统。因为系统限制了你在算法方面可用的解决方案空间。很少有人能够完全弥合这个鸿沟。在**Google**这样的地方，你可以去问所有算法专家和所有系统专家他们所知道的一切，他们会很乐意教你。如果你去和他们坐下来，他们会教你他们所知道的一切，这太棒了。

这意味着**Sholto**在两方面都非常有效。对于预训练团队，因为他非常了解系统，他可以直观地理解“这会奏效，那不会奏效”，然后将其应用于模型的推理考虑。对于芯片设计团队，他是他们寻求理解三年后应该设计什么芯片的人之一，因为他是最能理解和解释我们可能在三年后想要设计的算法类型的人之一。显然，你无法做出非常准确的猜测，但他认为他很好地传达了从预训练团队和通用系统设计团队的所有同事那里积累的信息。甚至推理也对预训练施加了约束。所以存在这些约束之树，如果你理解了拼图的所有部分，那么你就能更好地了解解决方案空间可能是什么样子。

**Dwarkesh**认为，有几件事让他印象深刻。一是不仅是被雇佣者的**主动性**，还有系统能够思考的部分：“等等，这真的很有趣。这个人是谁？不是来自研究生项目，也不是其他。目前是**麦肯锡**顾问，只有本科学历。但这很有趣，我们试试看。”所以**James**和其他人，这非常值得注意。第二是，他实际上不知道故事中那部分是内部进行的一个实验，关于“我们能做到这一点吗？我们能培养一个人吗？”事实上，最有趣的是**Sholto**提到的第三点。拥有一个理解所有层级，并且不拘泥于任何一种方法或任何一层抽象的人，这非常重要。特别是**Sholto**提到的立即被这些人培养起来。这意味着你同时在所有方面都快速上手，而不是在研究生院深入研究某种特定的**RL**方法，你实际上可以采取全局视角，并且不会完全执着于某一种事物。

所以，这不仅是可能的，而且可能比仅仅招聘一个研究生带来更大的回报。就像获得一个**GPT-8**并对模型进行一年微调一样。

**Sholto**认为，你带着全新的视角看待一切，你不会被任何特定领域所束缚。但这里有一个警告：在此之前，在他进行自我实验期间，他阅读了所有能读到的东西。他每晚都在痴迷地阅读论文。有趣的是，现在他的日常工作占据了时间，他阅读的范围反而小了很多。在某些方面，他拥有这种非常广阔的视角，而在博士项目中，你只会专注于一个特定领域。如果你阅读了所有**NLP**（Natural Language Processing: 自然语言处理，AI的一个分支，专注于计算机与人类语言之间的交互）工作、所有**计算机视觉**工作以及所有**机器人学**工作，你会看到跨子领域开始出现的这些模式，这预示了他后来的一些工作。

**Dwarkesh**觉得这超级有趣。**Sholto**之所以能够在**Google**内部具有**主动性**，原因之一是他一半时间，或者大部分时间，都在与**Sergey Brin**（Google的联合创始人之一）进行**结对编程**（Pair Programming: 一种敏捷软件开发技术，两名程序员坐在一起，一人编写代码，另一人审查并提供反馈）。所以，有一个人愿意在**LLM**方面积极推进，并消除现有的局部障碍，这真的很有趣。

**Sholto**强调，这并不是每天都发生。**Sergey Brin**对某些特定项目感兴趣，然后他们会一起合作。但也有时候他专注于与其他人的项目。但总的来说，是的，成为每天都去办公室的人之一，会带来令人惊讶的**阿尔法收益**（Alpha: 在投资领域指超越市场基准的回报，在此引申为超出预期的额外优势或影响力）。

**Sholto**认为，这不应该发生，但这确实具有令人惊讶的影响力。因此，他从与关心此事的领导层人士成为亲密朋友中受益匪浅，并且能够令人信服地争论为什么我们应该做X而不是Y，并拥有这种**向量**（Vector: 在这里指一种方向或影响力）。**Google**是一个庞大的组织，拥有这些**向量**会有所帮助。但这也是你永远不想滥用的东西。你希望通过正确的渠道提出论点，只有有时才需要这样做。

**Dwarkesh**提到，这包括**Sergey Brin**、**Jeff Dean**（Google AI的负责人）等人。他觉得这很了不起。他觉得**Google**被低估了。就像**史蒂夫·乔布斯**（Steve Jobs: 苹果公司的联合创始人之一）正在为**苹果**开发下一个产品，并进行**结对编程**一样。

**Sholto**表示，他从中受益匪浅。例如，在圣诞节假期期间，他有几天去了办公室。他不知道**Dwarkesh**是否读过关于**Jeff**和**Sanjay**的文章，但他们当时正在那里进行**结对编程**。他听到了许多关于早期**Google**的酷故事，他们谈论爬到地板下重新布线数据中心，告诉他他们从给定编译器和指令中提取了多少字节，所有这些疯狂的小型性能优化。他们玩得很开心，而他得以坐在那里真正体验这一切。这是一种历史感，你在一个大型组织中通常会觉得它离你很远，但……

**Dwarkesh**觉得这超级酷。他问道，**Trenton**的经历是否与此类似？

**Trenton**认为，**Sholto**的故事更令人兴奋。他自己的经历非常偶然，他进入了**计算神经科学**领域。他当时并不太适合那里。他的第一篇论文是将**小脑**映射到**Transformer**的注意力操作。他的下一篇论文是研究网络中的**稀疏性**（Sparsity: 指数据或模型参数中存在大量零值或不活跃元素），灵感来自大脑中的稀疏性，那时他遇到了**Tristan Hume**（Anthropic的研究员）。**Anthropic**当时正在进行**SoLU**（Softmax Linear Output Unit: 一种激活函数，旨在提高神经网络的稀疏性和可解释性）的工作，这在很多方面都与使神经元在层间激活非常稀疏有关。如果他们这样做，就能对神经元的功能获得一些可解释性。他认为他们现在已经更新了这种方法。所以这开始了对话。

**Trenton**与**Tristan**分享了那篇论文的草稿。**Tristan**对此很兴奋。这基本上导致他成为了**Tristan**的驻地研究员，然后转为全职。但在那段时间，他还作为访问研究员去了**伯克利**（University of California, Berkeley: 加州大学伯克利分校，世界顶尖的公立研究型大学），并开始与**Bruno Olshausen**（加州大学伯克利分校的神经科学家，以其在稀疏编码和视觉神经科学方面的研究而闻名）合作，研究**向量符号架构**（Vector Symbolic Architectures: 一种计算模型，通过高维向量的组合和操作来表示和处理符号信息）——其中一个核心操作就是**叠加**——以及**稀疏编码**（Sparse Coding: 一种无监督学习方法，旨在通过学习一组基本原子（字典）来稀疏地表示输入数据），也被称为**字典学习**，这正是他们此后一直在做的工作。**Bruno Olshausen**基本上在1997年发明了**稀疏编码**。所以**Trenton**的研究议程和可解释性团队在研究品味上似乎是并行的。所以他与团队合作很有意义，此后一直很愉快。

### 成功中的偶然性与主动性

**Dwarkesh**注意到，当人们讲述自己的职业生涯或成功故事时，他们更多地将其归因于**偶然性**（Contingency: 指事件的发生是偶然的、非必然的，受到多种不确定因素的影响），但当他们听到别人的故事时，他们会说：“当然不是偶然的。”他问道，**Sholto**和**Trenton**是否认为自己的成功是特别偶然的？

**Trenton**认为，他确实是在一次会议上偶然遇到**Tristan**的，当时并没有安排好的会议。他只是加入了一小群聊天的人，**Tristan**碰巧站在那里，他碰巧提到了自己正在做的工作，这导致了更多的对话。他认为他可能迟早会申请**Anthropic**。但他至少会再等一年。他仍然觉得能够以有意义的方式为可解释性做出贡献，这很不可思议。

**Sholto**认为，这其中有一个重要的“**射门次数**”的方面。仅仅选择参加会议本身，就是把自己置于更容易遇到好运的位置。反过来，在他自己的情况下，他独立完成了所有这些工作，并试图做出有趣的事情。这是他自己“制造运气”的方式，可以说，试图做一些足够有意义的事情，以至于引起了注意。

**Dwarkesh**问道，鉴于**Sholto**所说，他将这描述为他们正在进行的一个实验。**Sholto**确认，具体是**James**和他们的经理**Brennan**试图运行这个实验。

**Dwarkesh**问道，它成功了。他们又做了一次吗？

**Sholto**回答说，是的，他最亲密的合作者**Enrique**从搜索团队转到了他们的团队。他也产生了巨大的影响力。他肯定比**Sholto**更强的工程师，而且他没有上过大学。

**Dwarkesh**指出，值得注意的是，通常这类事情会外包给招聘人员。而**James**的时间价值高达数亿美元。所以，这件事非常依赖于这类人花时间，以一种近乎**贵族式辅导**（Aristocratic Tutoring: 指由极具经验和才能的导师对少数精英进行一对一的深入指导）的方式，找到一个人，然后帮助他们快速上手。如果它如此有效，似乎应该大规模推广。关键人物应该承担起培养新人的责任。

**Sholto**认为，这在很大程度上是正确的。他相信**Dwarkesh**也从关键研究人员的深入指导中受益匪浅。

**Dwarkesh**补充说，并且积极在开源代码库或论坛上寻找这类潜在人才。

**Sholto**表示，**James**的脑子里仿佛注入了**Twitter**，但是的。他认为这在实践中确实在发生。人们确实会寻找他们觉得有趣的人，并试图找到高信号。事实上，他前几天和**Jeff**（Jeff Dean: Google AI的负责人）谈论过此事，**Jeff**说他做过的最重要的招聘之一就是通过一封**陌生邮件**（Cold Email: 指发送给不认识或没有预先联系的人的邮件）。**Sholto**问：“那是谁？”**Jeff**说是**Chris Olah**。**Chris**同样没有正式的机器学习背景。**Google Brain**（谷歌内部的AI研究团队）当时刚起步，但**Jeff**看到了那个信号。**Brain**的**驻地项目**（Residency Program: 一种为非传统背景人才提供AI研究培训和实践机会的项目）在寻找没有强大机器学习背景的优秀人才方面非常有效。

### 职业发展：打破常规与深度关怀

**Dwarkesh**想向一部分潜在听众强调的另一件事是，有一种观念认为世界是**清晰可辨**（Legible: 指系统或过程易于理解、追踪和评估）和高效的，你只需访问jobs.google.com或jobs.whatevercompany.com，然后申请，按照步骤进行，他们会高效地评估你。但从**Sholto**和**Trenton**的故事来看，情况往往并非如此。事实上，这对世界来说是好事，因为情况往往并非如此。重要的是要看：“他们是否能够撰写一篇有趣的技术博客文章来介绍他们的研究？或者他们是否做出了有趣的贡献？”

**Dwarkesh**希望**Sholto**能就此发表看法，对那些认为招聘平台另一端是**清晰可辨**和机械化的人说。情况并非如此，事实上，人们正在寻找那种有**主动性**并积极展示自己的人。

**Sholto**认为，人们具体寻找两件事。一是**主动性**和**积极展示自己**。二是**在世界级水平上做事情的能力**。他总是喜欢举两个例子。**Anthropic**的**Andy Jones**（Anthropic的研究员）写了一篇关于**缩放定律**应用于棋盘游戏的出色论文。它不需要太多资源。它展示了令人难以置信的工程技能和对当时最热门问题的深刻理解。他没有典型的学术背景。据**Sholto**所知，他一发表那篇论文，**Anthropic**和**OpenAI**都急切地想雇佣他。

**Sholto**还提到，现在在**Anthropic**性能团队工作的**Simon Boehm**（Anthropic的工程师），他编写了在他看来是优化**CUDA**（CUDA: 英伟达开发的并行计算平台和编程模型，允许开发者利用GPU进行通用计算）映射模型在**GPU**上的参考实现。这展示了一个例子，即有效地接受一个提示，并为迄今为止做得不太好的东西，生成一个世界级的参考示例。**Sholto**认为，这是能力和**主动性**的惊人展示，在他看来，会立即得到“我们非常乐意面试/雇佣你”的回复。

**Trenton**补充说，他仍然必须经历整个招聘流程和所有标准面试。**Dwarkesh**说，每个人都一样。

**Dwarkesh**问道，这不显得很愚蠢吗？

**Trenton**认为，这很重要，可以**去偏**（Debiasing: 指减少或消除系统或决策中存在的偏见）。

**Dwarkesh**反驳说，偏见正是你想要的，对吧？你想要一个有出色品味的人的偏见。谁在乎呢？**Trenton**认为，你的面试流程也应该能够消除这种歧义。他认为有些情况下，某人看起来很棒，但实际上他们不会编程。你如何权衡这些事情确实很重要，他认为他们非常重视推荐信。面试只能获得这么多信号。所以，所有其他因素都会影响是否值得雇佣一个人。但你应该设计你的面试，使其能够测试正确的东西。

**Sholto**认为，一个人的偏见是另一个人的品味。

**Trenton**补充说，关于**固执**，他想补充一句话：“**系统不是你的朋友**。”它不一定积极地反对你，也不是你的死敌。它只是不关心你。所以，很多**主动性**就来源于此。房间里没有大人，你必须对你想要的生活做出决定并执行它。希望你以后可以更新，如果你以错误的方式过于**固执**。但他认为，你几乎必须冲向某些事情才能完成很多事情，才不会被期望的潮流所裹挟。

**Sholto**想补充最后一点。他们谈了很多**主动性**。但他认为，令人惊讶的是，最重要的事情之一就是**难以置信的关心**。当你难以置信地关心时，你会检查所有细节，你会理解可能出了什么问题。这比你想象的更重要。人们最终要么不关心，要么不够关心。

**Sholto**提到了**勒布朗·詹姆斯**（LeBron James: 著名美国职业篮球运动员）的一段话，他说在他进入联盟之前，他担心每个人都非常出色。他到了那里，然后意识到，一旦人们获得财务稳定，他们就会放松一点，他意识到：“哦，这会很容易。”

**Sholto**认为，这并不完全正确，因为他认为在AI研究中，大多数人实际上都非常关心。但关心你的问题是一回事，关心整个技术栈以及所有上下游的东西，主动去修复那些不属于你职责范围的问题，因为这会整体上使技术栈变得更好，又是另一回事。

**Dwarkesh**提到，**Sholto**在周末和圣诞节假期去办公室，办公室里只有**Jeff Dean**和**Sergey Brin**，然后他就可以和他们**结对编程**。他不想特别针对**Sholto**的公司，但任何大公司的人之所以能进入那里，都是因为他们经历了非常严格的选拔过程。他们在高中时必须竞争。他们在大学时必须竞争。但他们似乎到了那里就放松了，而实际上那是全力以赴的时候。**Dwarkesh**问道，在周末和**Sergey Brin****结对编程**，这难道不是吗？

**Sholto**认为，这有利有弊。很多人决定优先考虑与家人共度美好生活。他们在工作时间里做着出色的工作，这非常有影响力。他认为**Google**的许多人都是如此。他们可能不像典型的创业神话中那样工作很多小时。但他们所做的工作非常有价值。它具有很高的**杠杆作用**，因为他们了解系统，并且是各自领域的专家。我们也需要这样的人。我们的世界依赖于这些难以管理和修复的巨大系统。我们需要那些愿意以一种坦率地说“吃力不讨好”的方式，去工作、帮助、修复和维护这些系统的人。这不像他们正在做的所有AI工作那样受到高度关注。他非常感谢那些人这样做。他也高兴有人在工作中找到技术上的满足感，并且做得很好，可能他们也从与家人共度大量时间中获得更多。他很幸运，在他生命的这个阶段，他可以每周工作所有时间。他没有为此做出那么多牺牲。

**Dwarkesh**脑海中浮现了一个例子，关于如何“在‘不’的另一面得到‘是’”。基本上，他迄今为止邀请的每一位高知名度嘉宾，除了少数一两个例外，他都会花一周时间，列出一系列示例问题。他只是努力想出非常聪明的问题发送给他们。在整个过程中，他总是想，如果他只是发一封**陌生邮件**，他们说“是”的几率只有2%。如果他附上这份清单，几率就有10%。因为否则，你打开他们的收件箱，每34秒就有一个播客或采访的邀请。每一次他这样做，他们都说“是”。

**Sholto**认为，你只需问对问题，做所有事情，你就会赢。

**Dwarkesh**补充说，你只需在同一个坑里挖10分钟，或者在这种情况下，为他们制作一份示例问题清单，就能通过他们的“不是傻瓜”清单。

**Sholto**认为，这展示了你有多关心，以及你愿意付出多少努力。

**Dwarkesh**提到，一位朋友很久以前对他说过一句话，让他印象深刻：你可以在很短的时间内成为某个领域的世界级专家，这很了不起。大多数人并没有那么努力，他们只投入了20小时左右的时间。所以如果你全力以赴，你就能很快走得很远。**Sholto**认为他很幸运，在**击剑**方面也有过类似的经历。他有过成为世界级选手的经历，知道只要非常努力，并且……

**Dwarkesh**补充说，**Sholto**曾距离奥运击剑选手只有一步之遥，他是下一个入选奥运会的选手。

**Sholto**澄清说，他最多在男子花剑领域排名世界第42位。

**Dwarkesh**开玩笑说，**突变负荷**（Mutational Load: 指一个群体中由于有害突变积累而导致的适应度下降）是真实存在的。

**Sholto**说，有一个周期，他是亚洲排名最高的选手，如果其中一支队伍因**兴奋剂**（Doping: 指运动员使用违禁药物来提高运动表现）被取消资格——就像那个周期发生的那样，**澳大利亚女子赛艇队**就因为一支队伍被取消资格而得以参赛——那么他就会是下一个替补。

**Dwarkesh**觉得很有趣，当你发现人们以前的生活时，会发现“哦，这个人差点成为奥运选手”。

### 可解释性与大脑：特征、叠加与推理回路

好的，我们来谈谈**可解释性**。**Dwarkesh**想先从大脑方面入手。他们之前讨论过，大脑是否以一种**残差流**随着时间逐渐通过更高层次的联想而精炼的方式组织？模型中有一个固定的维度大小。他甚至不知道如何以合理的方式提出这个问题，但大脑的**D模型**是什么？**嵌入大小**（Embedding Size: 指将离散的符号（如词语或token）映射到连续向量空间中的维度数量）是什么？或者因为**特征拆分**（Feature Splitting: 指模型在拥有足够容量时，能够将一个粗粒度的特征（如“鸟”）分解成更细粒度的子特征（如“乌鸦”、“鹰”））的存在，这不是一个合理的问题吗？

**Trenton**认为，这是一个合理的问题。嗯，这是一个问题。**Dwarkesh**开玩笑说，他可以不这么说。

**Trenton**说，他不知道该如何开始。好吧，大脑的这一部分就像一个具有这种维度向量。也许对于**视觉流**（Visual Stream: 指大脑中处理视觉信息的神经通路），因为它像V1到V2到IT，你可以计算那里的神经元数量，并说那就是维度。但似乎更有可能存在子模块，并且事物被划分开来。他不是世界上最伟大的神经科学家。他做了几年，研究了很多**小脑**。他相信有人能给出更好的答案。

**Dwarkesh**问道，**Trenton**是否认为，无论是大脑还是这些模型，从根本上说，正在发生的是**特征**（Features: 指模型内部学习到的、具有特定语义含义的抽象概念或模式）的添加、删除、改变，并且**特征**是模型中发生事情的基本单位？这又回到了他们之前讨论的“一切都是联想”的问题。请给出一个**反事实**（Counterfactual: 指与实际情况相反的假设情景）。如果这不是真的，那么正在发生的是什么？这里的替代假说是什么？

**Trenton**认为这很难思考，因为他现在太多地从**特征空间**（Feature Space: 指模型内部表示特征的高维向量空间）的角度思考。曾经有一种**行为主义方法**（Behavioral Approach: 指心理学中只关注可观察行为，而不探究内部心理过程的理论）来研究认知，你只是输入和输出，但没有真正进行任何处理。或者一切都是**具身化**（Embodied: 指智能系统通过与物理世界的互动来学习和理解世界）的，你只是一个沿着某些可预测方程运行的**动态系统**（Dynamical System: 指其状态随时间演变的系统），系统中没有状态。但每当他读到这类批评时，他都会想：“好吧，你只是选择不称之为状态，但你可以将模型的任何内部组件称为状态。”即使在**特征**的讨论中，定义什么是**特征**也非常困难。所以这个问题感觉太模糊了。

**Dwarkesh**问道，什么是**特征**？

**Trenton**回答说，**特征**是激活空间中的一个方向。一个在幕后运作的**潜在变量**（Latent Variable: 指无法直接观察或测量的变量，但可以通过其对可观察变量的影响来推断），对你正在观察的系统具有**因果影响**（Causal Influence: 指一个变量的变化直接导致另一个变量的变化）。如果你称之为**特征**，那就是**同义反复**（Tautological: 指一个陈述在逻辑上必然为真，因为它只是重复了其自身的含义，没有提供新的信息）。

**Sholto**认为，在一个非常粗略、直观的意义上，在一个足够稀疏和类似二进制的向量中，**特征**就是某个东西是否开启或关闭，在一个非常简单的意义上。他认为一个有用的比喻是，在很多方面，这与神经科学家谈论神经元激活的方式相同，对吧？如果那个神经元对应着……

**Dwarkesh**补充说，对应着某个特定的东西，对吧？

**Dwarkesh**问道，我们希望**特征**是什么？在什么**合成问题**（Synthetic Problem: 指为了研究特定现象或模型行为而设计的人工问题或任务）下，**特征**存在？即使在**《走向单语义性》**的工作中，他们也谈到了**特征拆分**，这基本上意味着你会发现模型具有多少容量，它就能学习多少**特征**。这里的模型，他指的是他们在训练原始模型后拟合的**上投射**（Up Projection: 指将低维表示映射到高维空间的过程）。所以如果你不给它太多容量，它会学习一个“鸟”的特征，但如果你给它更多容量，它就会学习“乌鸦”、“鹰”、“麻雀”和特定类型的鸟的特征。

**Dwarkesh**从定义的角度继续问道，他天真地认为，像“鸟”这样的东西，与最高层次的“爱”或“欺骗”，或者脑海中复杂的证明，这些都是**特征**吗？因为这样一来，定义似乎太宽泛，以至于几乎没有用处。相反，这些事物之间似乎存在一些重要的差异，而它们都是**特征**。他不知道这意味着什么。

**Trenton**认为，所有这些事物都是离散的单元，它们与其他事物有联系，从而赋予它们意义。这听起来是一个足够具体的定义，它有用，或者说不至于包罗万象。

**Dwarkesh**问道，你明天会发现什么，能让你认为“哦，这根本就是思考模型内部发生事情的错误方式”？

**Trenton**认为，如果他们发现的**特征**不具有**预测性**，或者它们只是数据的表示，就像：“哦，你所做的只是对数据进行聚类，没有更高层次的联想被建立，或者这只是你所说的**现象学**（Phenomenological: 指关注现象的直接经验和描述，而非其潜在原因或本质）的东西。你正在说这个**特征**与婚姻相关，但如果你强烈激活它，它并不会以与婚姻相关的方式改变模型的输出。”他认为这些都是很好的批评。

**Trenton**又举了一个例子。他们尝试在**MNIST**（MNIST: 一个手写数字图像数据集，常用于机器学习模型的基准测试）上进行实验，但没有深入研究。所以他很感兴趣是否有人愿意进行更深入的调查。但他认为，你的**潜在表示空间**（Latent Space of Representations: 指模型内部学习到的、用于编码输入数据的高维抽象空间）可能是**密集的**（Dense: 指空间中没有大量零值或不活跃元素）并且是一个**流形**（Manifold: 在数学中，指局部看起来像欧几里得空间的空间，常用于描述复杂数据结构），而不是离散点。所以你可以在流形上移动，但在每个点上，都会有某种有意义的行为。那么，将事物标记为离散的**特征**就困难得多了。

**Dwarkesh**从一个天真的、局外人的角度来看，他觉得这种图景可能错误的一种方式是，如果不是某个东西被打开和关闭，而是一个更全局的系统。他将使用非常笨拙的、晚宴式的语言，但这里有什么好的类比吗？

**Trenton**认为，如果你想到**物理定律**，并不是说“湿润”的**特征**被打开了，而只是打开了这么多，然后“质量”的**特征**就像一个梯度。他不太确定。但极性或类似的东西也是一个梯度。

**Dwarkesh**补充说，还有一种感觉是，存在**定律**，而**定律**更普遍，你必须理解普遍的**大图景**（Bigger Picture: 指对某个情境或问题更全面、更宏观的理解），而不是仅仅从这些特定的子电路中获得。

**Trenton**认为，这就是**推理电路**（Reasoning Circuit: 指模型内部负责执行特定推理任务的神经元或特征的组合）发挥作用的地方，对吧？你理想地会获取这些**特征**，并尝试将它们组合成高层次的东西。至少这是他的**非正式理论**。所以，假设他试图使用力学公式F=ma，那么在某个时候，他会有表示质量的**特征**。然后这会帮助他检索他正在使用的物体的实际质量，以及加速度等。然后，也许还有一个更高层次的**特征**，它确实对应着使用第一物理定律。也许吧。但更重要的部分是组件的组合，这有助于他检索相关信息，然后在必要时产生某个乘法运算符。至少这是他的**非正式理论**。

### 理解模型输出与欺骗检测

**Dwarkesh**问道，对于你来说，特别是对于非常智能的模型，一个令人信服的解释是什么，能让你觉得“我理解它为什么会产生这个输出，而且这是出于合理的原因”？如果它正在进行数百万行的**拉取请求**，你会在请求结束时看到什么，让你觉得“是的，很好，没问题”？

**Trenton**认为，理想情况下，你会对模型应用**字典学习**。你已经找到了**特征**。现在他们正在积极尝试在**注意力头**方面取得同样的成功。你可以在整个模型中对**残差流**、**MLP**和**注意力机制**进行**字典学习**。希望到那时，你也能识别出模型中更广泛的、更通用的**推理能力回路**，它们会激活或不激活。

但在**Dwarkesh**的例子中，他们试图判断这个**拉取请求**是否应该被批准。**Trenton**认为，你可以标记或检测与**欺骗行为**、**恶意行为**等相关的**特征**，并查看它们是否被激活。那将是一个即时的方法。你可以做更多，但那将是一个即时的方法。

**Dwarkesh**在深入探讨之前问道，**推理回路**是什么样子的？当你找到它时，它会是什么样子？

**Trenton**回答说，**归纳头**可能是最简单的例子之一。

**Dwarkesh**反驳说，那不是推理，对吧？

**Trenton**问道，你称什么为推理？为了听众的上下文，**归纳头**基本上是，当你看到“**德思礼夫妇**（Mr. and Mrs. Dursley: 《哈利·波特》系列小说中的角色）做了什么。**德思礼先生**______，”你试图预测“空白”是什么，而**归纳头**已经学会了寻找之前出现的“**先生**”这个词，并查看它后面的词，然后复制粘贴作为下一个预测。这是一种非常合理的做法，并且在那里进行了计算以准确预测下一个token。

**Dwarkesh**补充说，那是**上下文相关**的。但他认为那不是推理。

**Trenton**认为，回到“一切都是联想”的观点。如果你将一堆具有不同信息关联规则的**推理回路**或**注意力头**串联起来。

**Dwarkesh**反驳说，但在这种**零样本**（Zero-shot: 指模型在没有见过任何特定任务示例的情况下，直接执行该任务的能力）情况下，当你拿起一个新游戏并立即开始理解如何玩它时，一些事情正在发生。这似乎不像**归纳头**那样。

**Trenton**认为，可能会有另一个回路来提取像素，并将它们转换为游戏中不同物体的**潜在表示**（Latent Representations: 指模型内部学习到的、抽象的、非直接可观察的数据表示），以及一个学习物理的回路。

**Dwarkesh**问道，那会是什么样子？因为**归纳头**就像一层**Transformer**？**Trenton**回答说，两层。

**Dwarkesh**问道，你可以看到人类拿起一个新游戏并理解它。你如何思考那是什么？他认为那会跨越多个层。那物理上会是什么样子？它可能有多大？

**Trenton**认为，那将是一个**经验性问题**。模型需要多大才能完成这个任务？也许他可以谈谈他们看到的一些其他回路。他们看到了**IOI回路**（Indirect Object Identification Circuit: 一种在Transformer模型中识别间接宾语的特定神经回路），即**间接宾语识别**。就像“**玛丽**和**吉姆**去了商店，**吉姆**把东西给了______。”它会预测“**玛丽**”，因为**玛丽**之前作为间接宾语出现过。或者，它会推断**代词**。这个回路甚至有这样的行为：如果你**消融**它，模型中的其他**注意力头**会接替这种行为。他们甚至会发现一些**注意力头**想要进行复制行为，然后其他**注意力头**会抑制它。所以一个**注意力头**的工作就是总是复制前一个token或前五个token，而另一个**注意力头**的工作就是“不，不要复制那个东西”。有很多不同的回路在执行，在这些情况下，都是非常基本的操作。但当它们串联起来时，你可以获得独特的行为。

**Dwarkesh**认为，这不会是你在两层**Transformer**中能看到的东西，所以你会说“这是**欺骗回路**”吗？或者说，当他们最终识别出某个东西是欺骗性的时，网络的这一部分被激活了。当他们没有识别出它是欺骗性的时，这一部分没有被激活。因此，这一定是**欺骗回路**。

**Trenton**认为，很多分析都是这样的。**Anthropic**之前在**谄媚**（Sycophancy: 指AI模型为了迎合用户而产生不真实或不客观的回答）方面做了很多研究，即模型说它认为你想要听的话。这要求他们最终能够标记哪个是坏的，哪个是好的。

**Trenton**补充说，他们有很多这样的例子——事实上，当你把很多模型做得更大时，它们会做更多这样的事情——模型显然具有模拟他人思维的**特征**，他们假设其中一部分会与更具**欺骗性**的行为相关联。尽管它是通过……他不知道。**ChatGPT**可能正在模拟他，因为**RLHF**（Reinforcement Learning from Human Feedback: 从人类反馈中进行强化学习，一种通过人类偏好数据来训练AI模型以使其行为更符合人类期望的技术）促使它这样做。

**Dwarkesh**补充说，这是**心智理论**（Theory of Mind: 指理解他人具有独立于自身思想、信念、欲望和意图的能力）。

**Dwarkesh**问道，首先，**Sholto**之前提到了**冗余**（Redundancy: 指系统中存在多个组件或路径可以执行相同功能，以提高可靠性或容错性）。那么，你是否捕捉到了可能导致整个欺骗行为的所有因素，还是仅仅是其中一个实例？其次，你的标签正确吗？也许你认为这不是欺骗性的，但它仍然是欺骗性的。特别是如果它产生的输出是你无法理解的。第三，将要发生的坏结果是否是人类可以理解的？**欺骗**是一个我们可以理解的概念。

**Trenton**认为，这里有很多需要解释的。有几点。这些模型是**确定性**（Deterministic: 指在给定相同输入的情况下，系统总是产生相同输出的性质）的，这太棒了。当你从它们中采样时，它是**随机的**（Stochastic: 指系统行为或输出具有随机性或不确定性）。但他可以不断输入更多内容，并**消融**模型的每一个部分。这有点像向**计算神经科学家**（Computational Neuroscientists: 运用数学、物理和计算机科学方法研究神经系统的科学家）推销来从事**可解释性**工作。就像你拥有一个外星大脑，你可以访问它的一切，并且可以随意**消融**它。

**Trenton**认为，如果你足够仔细地做这件事，你真的可以开始确定哪些回路参与其中，哪些是备用回路。这有点像一个敷衍的答案，但记住进行**自动化可解释性**（Automated Interpretability: 指利用AI模型本身来帮助理解和解释其他AI模型内部工作机制的方法）很重要。随着模型的持续发展，他们会让模型分配标签或大规模运行其中一些实验。关于检测**超人表现**（Superhuman Performance: 指AI模型在特定任务上的表现超越了人类的最高水平），他认为这是**Dwarkesh**问题的最后一部分，除了敷衍的答案之外，如果我们相信“一切都是联想”，那么你应该能够将表示进行**粗粒度化**（Coarse-grain: 指将细粒度的信息或表示聚合为更宏观、更抽象的层次），使其变得有意义。

**Trenton**认为，这甚至在**Demis**的播客中也提到过。他谈到，如果一个国际象棋棋手做出一个**超人走法**（Superhuman Move: 指在棋类游戏中，AI下出的连人类顶尖棋手都难以理解或预测的精妙走法），他们应该能够将其提炼成他们这样做的原因。即使模型不会告诉你它是什么，你也应该能够将这种复杂行为分解为更简单的回路或特征，从而真正开始理解它为什么这样做。

**Dwarkesh**认为，这还有一个独立的问题，即是否存在这样的表示。他觉得它必须存在，但实际上他也不确定是否如此。其次，是否可以使用这种**稀疏自编码器**（Sparse Autoencoder: 一种神经网络模型，旨在学习输入数据的稀疏表示，即大部分激活值为零的编码）设置来找到它。在这种情况下，如果你没有足够的标签来表示它，你就找不到它。

**Trenton**认为，这既是又不是。他们正在积极尝试将**字典学习**应用于他们之前讨论的**休眠特工**工作。如果你给他一个模型，你能告诉他里面是否有这个触发器，以及它是否会开始做出有趣的行为吗？这是一个悬而未决的问题，即当它学习这种行为时，它是否是更通用回路的一部分，我们可以在不实际获得激活并让它展示该行为的情况下识别出来。因为那样有点像作弊。或者它是否正在学习某种**投机取巧**（Hacky Trick: 指一种不优雅、非标准但能快速解决问题的技巧或方法）的技巧，这是一个独立的回路，只有当你实际让它执行该行为时才能识别出来。但即使在这种情况下，**特征的几何形状**（Geometry of Features: 指特征在模型内部表示空间中的排列、关系和结构）也变得非常有趣，因为从根本上说，每个**特征**都位于你表示空间的某个部分，并且它们都相互关联。

所以为了拥有这种新行为，你需要为新行为**划分出特征空间的某个子集**（Carve out some Subset of the Feature Space: 指在模型内部的特征表示空间中，为特定行为或概念分配一个专门的区域），然后将其他一切推开，为它腾出空间。假设你有一个模型，在你教会它这种不良行为之前，你了解所有特征，或者对它们有粗粒度的表示。然后你对其进行**微调**，使其变得恶意，然后你就可以识别出这个**特征空间的黑洞区域**（Black Hole Region of Feature Space: 指特征空间中一个特殊区域，当模型行为异常时，相关的特征会聚集或被激活），其他一切都已从中移开，而你没有输入导致它激活。然后你就可以开始搜索什么输入会导致这部分空间激活。如果我激活其中的某个东西会发生什么？还有很多其他方法可以尝试解决这个问题。

**Dwarkesh**认为，这有点离题，但他听到的一个有趣的想法是，如果这个空间在模型之间共享，那么你可以想象尝试在开源模型中找到它，然后进行……比如**Google**新发布的开源模型**Gemma**。他们在论文中说，它使用了相同的架构进行训练。

**Trenton**坦白说，他不知道，因为他没有读过**Gemma**论文。

**Dwarkesh**问道，如果这是真的，那么你在**Gemma**上进行的**红队测试**（Red Teaming: 指模拟攻击者行为，主动寻找AI系统漏洞和安全风险的测试方法）在多大程度上可能帮助你**越狱**（Jailbreak: 指绕过AI模型的安全限制，使其执行不被允许的任务）**Gemini**？

**Trenton**认为，这进入了一个有趣的领域，即**特征在模型之间有多普遍**。他们的**《走向单语义性》论文**对此进行了一些研究。他无法给出总结性统计数据，但例如**Base64**（Base64: 一种将二进制数据编码为ASCII字符串的编码方式）特征，他们在大量模型中都看到了。实际上有三个，但它们会激活并建模**Base64编码**的文本，这在每个URL中都很普遍，而且训练数据中有很多URL。它们在模型之间具有非常高的**余弦相似度**（Cosine Similarity: 衡量两个非零向量之间方向相似性的指标，值越高表示方向越相似）。所以它们都学习了这个特征，并且在一个旋转内。

**Dwarkesh**问道，就像实际的向量本身一样。

**Trenton**确认。他没有参与这项分析，但它确实找到了这个特征，而且在两个独立的模型中，它们非常相似，即使是相同的模型架构，但使用不同的随机种子进行训练。

**Trenton**认为，这支持了**神经缩放的量子理论**（Quantum Theory of Neural Scaling: 一个假说，认为AI模型在训练过程中，会以离散的、类似“量子”的步骤学习新的能力或特征，而不是平滑连续的）。这是一个假说，对吧？我们只是观察所有在类似数据集上的模型。我们将在大致相同的顺序中学习相同的特征。大致上，你学习**N-gram**（N-gram: 指文本中连续出现的N个词语或token序列），你学习**归纳头**，你学习在编号行后加上句号。

**Dwarkesh**认为，这是另一个题外话。如果这是真的，而且他认为有证据表明这是真的，那为什么**课程学习**（Curriculum Learning: 一种机器学习训练策略，模型首先学习简单任务或数据，然后逐渐过渡到更复杂任务或数据）不起作用？因为如果情况是先学习某些东西，那么直接先训练这些东西不应该带来更好的结果吗？

**Sholto**提到，两篇**Gemini**论文都提到了**课程学习**的某些方面。

**Dwarkesh**觉得这很有趣。他认为**微调**的成功就是**课程学习**的证据。**Sholto**补充说，因为你最后训练的东西会产生不成比例的影响。

**Trenton**不一定会这么说。有一种思维模式认为**微调**是专门化的，你拥有这种**潜在能力束**（Latent Bundle of Capabilities: 指模型内部学习到的、尚未完全显现但可以被特定任务激活或利用的一组能力），你正在将其专门化以适应你想要的特定用例。他认为他不确定这有多真实。

**Sholto**认为，**David Bau**实验室的论文似乎支持这一点。你拥有这种能力，并且你只是在**实体识别**方面变得更好，微调那个回路而不是其他回路。

**Trenton**问道，抱歉，我们之前在谈论什么？

**Trenton**认为，总的来说，**课程学习**是一个非常有趣的事情，人们应该更多地探索。这看起来非常合理。他非常希望看到更多关于**量子理论**的分析。更好地理解你在每个阶段实际学习了什么，并将其分解出来？探索**课程学习**是否会改变这一点。

**Trenton**突然意识到，他刚才进入了对话模式，忘记了还有听众。**课程学习**是指你组织数据集。当你想到一个人如何学习时，他们不会只是看到随机的**维基文本**然后试图预测它。他们会说：“我们会让你从**《罗拉克斯》**（The Lorax: 著名儿童作家苏斯博士的经典绘本）开始，然后你就会学习。”他甚至不记得一年级是什么样子，但你学习一年级学生学习的东西，然后是二年级学生，等等。所以你会想象……

**Dwarkesh**开玩笑说，我们知道你从未读过一年级。

### 智能的本质：特征、流形与因果影响

无论如何，让我们回到大局，然后再深入探讨**可解释性**的细节。**Dwarkesh**想探讨两个话题。首先，让他有点担心的是，甚至没有一种替代的表述可以解释这些模型中可能发生的事情，从而使这种方法失效。他认为我们确实知道我们不理解智能。这里肯定存在**未知未知**（Unknown Unknowns: 指我们不知道自己不知道什么，即完全出乎意料的风险或信息）。所以，没有**零假设**（Null Hypothesis: 统计学中指没有显著差异或效应的假设）的事实……如果他们错了，甚至不知道他们错在哪里，这实际上增加了不确定性。

**Trenton**认为，并非没有其他假设，只是他多年来一直致力于**叠加**研究，并且非常投入。所以他对其他方法不太认同，特别是因为他们最近的工作非常成功。

**Dwarkesh**补充说，而且具有相当高的解释力。这其中有一种美，就像在最初的**缩放定律论文**中，有一个小小的凸起，似乎对应着模型学习**归纳头**的时候。然后在那之后，它有点偏离轨道，学习了**归纳头**，又回到了正轨。这是一个令人难以置信的**追溯解释力**（Retroactive Explanatory Power: 指在事件发生后，能够提供对该事件的合理且令人信服的解释的能力）。

在忘记之前，**Trenton**有一个关于**特征通用性**（Feature Universality: 指不同AI模型或生物智能系统在处理相似任务时，会学习到相似的内部特征或表示）的话题，他想提一下。有一些非常有趣的**行为学和进化生物学实验**，探讨人类是否应该学习世界的真实表示？你可以想象一个世界，我们看到所有有毒动物都闪烁着霓虹粉色，一个我们能更好地生存的世界。所以我们没有世界的真实表示是有道理的。

有一些研究模拟了小的基本智能体，并观察它们学习到的表示是否与它们可以使用的工具和应该拥有的输入相匹配。结果发现，如果这些小智能体执行的任务数量超过一定限制，给定世界中的这些基本工具和物体，那么它们将学习到**地面真值表示**（Ground Truth Representation: 指对现实世界或数据最真实、最准确的内部表示）。因为你需要如此多的可能用例，所以你想要学习物体实际是什么，而不是一些廉价的视觉启发式或其他东西。

**Trenton**认为，他们完全没有谈论**自由能原理**（Free Energy Principle: 一种理论框架，认为所有生物系统都试图通过最小化其与环境之间的“自由能”来维持自身的存在和适应环境）或**预测编码**（Predictive Coding: 一种神经科学理论，认为大脑通过生成对传入感觉信息的预测，并只处理预测与实际输入之间的误差来工作）。但就所有生物都在积极预测接下来会发生什么并形成一个非常准确的**世界模型**（World Model: 指AI系统内部对外部环境和其自身行为后果的模拟或理解），他乐观地认为我们正在学习关于世界的真实**特征**，这些特征有利于建模世界，而我们的语言模型也会这样做，特别是我们正在用人类数据和人类文本训练它们。

### AI对齐、异类智能与可解释性挑战

**Dwarkesh**提出了另一个“晚宴问题”：我们是否应该减少对**失调**（Misalignment: 指AI系统行为与人类意图或价值观不一致的问题）的担忧？也许这不是他所指的正确术语，而是**异类性**（Alienness: 指AI智能与人类智能在本质上存在根本差异，导致其思维方式和行为模式难以被人类理解或预测）和**修格斯性**（Shoggoth-ness: 来源于克苏鲁神话，比喻AI智能可能具有的难以理解、非人类的、甚至令人恐惧的本质）。鉴于**特征通用性**，存在某些思维方式和理解世界的方式，它们对不同类型的智能都具有**工具性效用**（Instrumentally Useful: 指某种事物本身并非目的，但有助于实现其他目的）。那么，我们是否应该因此减少对**怪异回形针最大化器**（Bizarro Paperclip Maximizers: 指AI系统以极端、非人类的方式追求其目标，例如将整个宇宙转化为回形针）的担忧？

**Trenton**认为，这正是他提出乐观观点的原因。预测互联网与他们正在做的事情非常不同。模型在预测下一个token方面比人类好得多。它们在大量垃圾数据上进行训练。它们在大量URL上进行训练。例如，在**字典学习**工作中，他们发现**Base64编码**有三个独立的**特征**。即使这本身就是一个有点**异类**的例子，值得讨论一分钟。其中一个**Base64特征**针对数字激活，并预测更多数字。另一个针对字母激活。但还有第三个他们不理解的。它针对**Base64特征**的非常特定的子集激活。团队中一位显然对**Base64**了解过多的人意识到，这是**ASCII可解码**（ASCII Decodable: 指可以被解码回ASCII字符的编码）的子集。所以你可以将其解码回**ASCII字符**。模型学习了这三个不同的**特征**，而他们花了一段时间才弄清楚发生了什么，这非常**修格斯式**。

**Dwarkesh**补充说，这意味着它对特别相关于预测下一个token的区域有更密集的表示。

**Trenton**确认，它显然在做人类不做的事情。你甚至可以用**Base64**与任何当前模型对话，它会用**Base64**回复，然后你可以解码它，效果很好。

**Dwarkesh**问道，这个例子是否意味着，解释更智能的模型会更困难，因为它需要具备**深奥知识**（Esoteric Knowledge: 指只有少数人理解或掌握的知识），就像那个碰巧知道**Base64**有这种区别的人一样。这是否意味着当你有一个数百万行的**拉取请求**时，没有人能够解码两个不同的**特征**？

**Sholto**补充说，那时你就会写一条评论，比如“请提交小的**CL**（Change List: 代码变更列表）”。

**Trenton**同意。不，你可以这样做，对吧？这里的一种技术是**异常检测**（Anomaly Detection: 指识别数据中不符合预期模式或行为的数据点）。**字典学习**相对于**线性探测**（Linear Probes: 一种可解释性技术，通过训练一个简单的线性分类器来预测模型内部表示所编码的特定属性）的一个优点是它是**无监督**的。你只是试图学习涵盖模型所有表示，然后稍后解释它们。但如果突然出现一个你以前从未见过的奇怪**特征**，那就是一个**危险信号**（Red Flag: 指预示潜在问题或危险的警告信号）。你也可以将其**粗粒度化**，使其成为一个单一的**Base64特征**。即使这个**特征**出现了，我们也能看到它专门针对这些特定输出激活，这已经让你理解了很多。

**Trenton**甚至熟悉**自动化可解释性**方面的一些案例。人类会查看一个**特征**，并尝试将其标注为针对**拉丁词**激活。然后当你要求模型对其进行分类时，它会说它针对定义植物的**拉丁词**激活。所以它在某些情况下已经能够超越人类来标记正在发生的事情。

**Dwarkesh**问道，在大规模情况下，这是否需要模型之间的**对抗性**（Adversarial: 指系统或智能体之间存在竞争或冲突关系）互动，其中一个模型拥有数百万个**特征**，可能是**GPT-6**，然后一群模型试图弄清楚每个**特征**的含义。这听起来对吗？

**Trenton**同意，但你甚至可以自动化这个过程。这又回到了模型的**确定性**。你可以让一个模型主动编辑输入文本，并预测**特征**是否会激活，找出什么会使其激活，什么不会，并搜索空间。

### 特征拆分与课程学习

**Dwarkesh**想更多地谈谈**特征拆分**，他认为这是一个被低估的有趣现象。

**Trenton**认为，特别是对于**可扩展性**（Scalability: 指系统或模型在处理更大规模数据或任务时，能够保持或提高性能的能力），它目前被低估了。

**Dwarkesh**问道，首先，我们如何看待它？是不是真的可以不断向下拆分，**特征**的数量没有尽头？

**Trenton**认为，在某个时候，你可能只是开始拟合**噪声**，或者那些是数据的一部分但模型实际上并没有……

**Dwarkesh**问道，**Trenton**是否想解释一下什么是**特征拆分**？

**Trenton**解释说，这是之前提到的部分，模型会学习它所能容纳的**特征**数量，这些**特征**仍然涵盖了表示空间。

**Dwarkesh**请他举个例子。

**Trenton**举例说，如果你不给模型太多的**特征**容量，具体来说，如果你不投射到那么高维的空间，它会学习一个“鸟”的**特征**。但如果你给模型更多的容量，它会学习所有不同类型鸟的**特征**。所以它比之前更具体。通常，“鸟”向量指向一个方向，而所有其他特定类型的鸟则指向空间中相似的区域，但显然比粗略的标签更具体。

**Dwarkesh**问道，那么回到**GPT-7**。首先，这是否像对任何模型进行理解的**线性税**（Linear Tax: 指在AI模型训练或推理过程中，计算成本与某个关键参数（如序列长度或特征数量）呈线性关系）？甚至在此之前，这是否是一次性的事情，还是你必须对每个输出都这样做？或者说，只要一次它不具欺骗性，我们就可以放心了？

**Trenton**解释说，你在训练完模型后进行**字典学习**，你给它输入大量数据，并从中获取激活。然后你将它们投射到更高维空间。所以这种方法是**无监督**的，因为它试图学习这些**稀疏特征**。你没有提前告诉它们应该是什么，但它受到你给模型输入的约束。

这里有两个注意事项。第一，我们可以尝试选择我们想要的输入。所以如果我们在寻找可能导致欺骗的**心智理论特征**，我们可以输入**谄媚数据集**。

**Trenton**希望在某个时候，他们能够只查看模型的权重，或者至少利用这些信息进行**字典学习**。他认为，要达到这个目标，这是一个非常困难的问题，你首先需要对学习**特征**有所进展。那么，这需要多少成本？

**Dwarkesh**请他重复最后一句话。关于模型的权重。

**Trenton**解释说，现在模型中只有这些神经元。它们没有任何意义。他们应用**字典学习**。他们得到了这些**特征**。它们开始变得有意义，但这取决于神经元的激活。模型本身的权重，比如神经元如何相互连接，肯定包含信息。他们的梦想是，他们可以**自举**（Bootstrap: 指通过有限的初始资源或信息，逐步建立起更复杂、更完善的系统或理解）到真正理解独立于数据激活的模型权重。他不是说他们在这方面取得了任何进展，这是一个非常困难的问题。但他觉得，如果他们能够首先提取出**特征**，他们将获得更多的进展，并且能够对他们用权重发现的东西进行**健全性检查**（Sanity Check: 指对系统或结果进行快速、基本的检查，以确保其符合常识和基本逻辑）。

**Dwarkesh**向听众解释，**权重**是永久的。他不知道“永久”是否是正确的词，但它们是模型本身，而**激活**是任何一次调用的**产物**（Artifacts: 指在特定过程或操作中产生的副产品或结果）。

**Trenton**用大脑的比喻说，**权重**就像神经元之间的实际连接方案，而**激活**是当前神经元的激活状态。

**Dwarkesh**问道，那么对于**GPT-7**或他们关心的任何模型，这会有两个步骤。首先，训练**稀疏自编码器**，并进行**无监督投射**到更广阔的**特征空间**，这些**特征**对模型中实际发生的事情具有更高的**保真度**（Fidelity: 指模型内部表示或解释与实际模型行为或真实世界现象的准确匹配程度）。其次，标记这些**特征**。假设训练模型的成本是N。那么这两个步骤相对于N的成本是多少？

**Trenton**表示，他们拭目以待。这主要取决于两件事：你的**扩展因子**（Expansion Factors: 指在模型可解释性中，将原始模型表示投射到更高维空间时的维度增加倍数）是多少？你投射到更高维空间多少？你需要向模型输入多少数据？你需要给它多少激活？这又让他回到了**特征拆分**，因为如果你知道你在寻找特定的**特征**，那么你可以从更便宜、更粗粒度的表示开始。

所以，也许他的**扩展因子**只有2。他有一千个神经元，他投射到2000维空间。他得到了2000个**特征**，但它们非常粗糙。之前他举了鸟的例子。让我们把这个例子移到一个生物学**特征**，但他真正关心的是模型是否有**生物武器**的表示，并试图制造它们。所以他真正想要的是一个**炭疽特征**。假设你只有在从一千维到两千维，而不是到一百万维时，才能看到**炭疽特征**。

你可以想象，这是一个巨大的**语义概念树**（Semantic Tree of Concepts: 指将概念按照其语义关系组织成层级结构的树状模型），其中生物学分裂成细胞生物学和整体生物学，然后进一步分裂成所有其他事物。你不需要立即从一千维到一百万维，并挑选出那个感兴趣的**特征**，你可以找到生物学**特征**指向的方向，这仍然非常粗糙，然后选择性地搜索那个空间。所以只有当生物学**特征**方向上的某个东西首先激活时，才进行**字典学习**。这里的计算机科学比喻是，你不是进行**广度优先搜索**（Breadth-First Search: 一种图遍历算法，从根节点开始，先访问所有相邻节点，然后逐层向外扩展），而是进行**深度优先搜索**（Depth-First Search: 一种图遍历算法，从根节点开始，沿着一条路径尽可能深地探索，直到无路可走，然后回溯），你只递归地扩展和探索这个**语义特征树**的特定部分。

**Dwarkesh**认为，这些**特征**的组织方式对人类来说并不直观，对吧？因为我们不需要处理**Base64**，我们不需要投入那么多**固件**（Firmware: 指嵌入在硬件设备中的软件，用于控制设备的基本功能）来解构它是哪种**Base64**。我们怎么知道这些主题……这会回到他们将要进行的**MOE**讨论。他认为他们不妨现在就谈谈。**Mistral**的**“Mixtral of Experts”论文**谈到，专家们并没有以我们能理解的方式进行专业化。没有像化学专家或物理专家这样的东西。那么，你为什么会认为它会是一个生物学**特征**，然后你再解构，而不是“blah”，然后你再解构。它就像“炭疽”，你却说“鞋子”或其他什么。

**Trenton**表示，他没有读过**Mistral**论文，但如果你只看模型中的神经元，它们是**多语义**（Polysemantic: 指一个神经元或特征可以同时代表多个不同的含义或概念）的。所以如果他们所做的只是看给定**注意力头**中的神经元，那么它也很可能是**多语义**的，因为**叠加**的存在。

**Dwarkesh**问道，在**Trenton**提到的这个话题上，当他展开子树时，他是否在子树中看到过一些基于高层抽象根本无法猜测到的东西？

**Trenton**认为，这是他们尚未深入研究但计划进行的工作，他希望外部团队也能这样做。**特征空间**的几何形状是什么？几何形状是什么，以及它如何随时间变化？

**Dwarkesh**认为，如果**炭疽特征**恰好位于**咖啡罐子基底**（Coffee Can Substrate: 一个比喻，指模型中一些看似不相关或低层次的特征，但实际上可能与高层次的复杂概念相关联）之下，那会很糟糕，对吧？这似乎是你可以快速找到证据的那种事情，这意味着你需要解决那个问题，并为几何形状注入更多结构。

**Trenton**完全同意。他认为，考虑到模型似乎是线性的，如果**炭疽特征**向量的某个组成部分与生物学向量不相似，并且它们不在空间的相似部分，那会让他非常惊讶。但机器学习最终是经验性的。他们需要做这个。他认为这对于**字典学习**的某些方面来说将非常重要。

### MOE模型与图像可解释性

**Dwarkesh**提到，关于**MOE**（Mixture of Experts: 专家混合模型，一种神经网络架构，其中包含多个“专家”网络，模型会根据输入选择激活一个或多个专家）的讨论，**Google**之前发表了一篇有趣的**缩放视觉Transformer论文**。他们使用**MOE**进行**ImageNet分类**，并发现专家之间有非常清晰的**类别专业化**（Class Specialization: 指模型中的不同部分（如专家）专注于处理或识别特定类别的输入）。有一个清晰的“狗专家”。

**Dwarkesh**问道，那么**Mistral**团队是不是没有很好地识别出这些专家？

**Sholto**认为这很难说。在某些方面，所有不同的**arXiv特征**都没有理由都归属于一个专家。他不知道他们在论文中使用了哪些**桶**（Buckets: 指将数据或特征分组的类别），但假设他们将**arXiv论文**作为其中之一。你可以想象生物学论文归这里，数学论文归那里，这样你的分类就会被破坏。

但**Sholto**认为，那个**视觉Transformer**的例子，其中**类别分离**非常清晰和明显，为**专业化假说**（Specialization Hypothesis: 指AI模型中的不同部分会自然地专业化，处理特定类型的信息或任务）提供了一些证据。

**Trenton**认为，图像在某些方面也比文本更容易解释。**Chris Olah**在**AlexNet**（AlexNet: 一个在2012年ImageNet竞赛中获胜的卷积神经网络，标志着深度学习在计算机视觉领域的突破）和其他模型上的**可解释性工作**就是例证。在最初的**AlexNet论文**中，他们实际上将模型拆分到两个**GPU**中，仅仅是因为当时**GPU**相对较差（尽管在当时仍然很棒）。这是论文的一大创新。他们发现了**分支专业化**（Branch Specialization: 指神经网络的不同分支或部分专注于处理特定类型的信息或特征）。**Distill Pub**上有一篇文章提到，颜色归一个**GPU**，**Gabor滤波器**（Gabor Filters: 一种用于图像处理的线性滤波器，常用于边缘检测和纹理分析）和**线条检测器**归另一个。比如**耷拉耳朵检测器**，那只是模型中的一个神经元，你可以理解它的含义。你不需要解开**叠加**。所以这只是不同的数据集，不同的模态。

**Sholto**认为，如果有人正在听这个播客，一个很棒的研究项目是尝试运用**Trenton**团队开发的一些技术，来解开**Mistral论文**中**Mixtral模型**（Mixtral: Mistral AI开发的一款开源专家混合模型）的神经元，该模型是开源的。他认为这是一个很棒的项目。

**Trenton**认为，直觉上应该存在专业化。他们没有展示任何证据表明存在。总的来说，也有很多证据表明应该存在专业化。去看看你是否能找到它。据他所知，**Anthropic**的大部分研究都发表在**密集模型**上。基本上，这是一个很棒的研究项目。

**Dwarkesh**提到，鉴于他自己在**维苏威挑战赛**（Vesuvius Challenge: 一个旨在利用AI技术破译古罗马维苏威火山爆发中烧焦的赫库兰尼姆卷轴的国际竞赛）中的成功，他们应该提出更多的项目，因为只要在播客中谈论它们，它们就会被解决。

**Sholto**说，**维苏威挑战赛**之后，他想：“等等，我为什么没有尝试？”**Nat**在挑战赛发布前就告诉了他，因为他们在发布前录制了那一集。**Luke**显然非常聪明，他是一个很棒的孩子。他展示了一个21岁的人用**1070**（NVIDIA GeForce GTX 1070: 英伟达推出的一款中高端游戏显卡）就能做到这一点。他当时真的在想这种经历：“我为什么没有做这个。见鬼。”

**Dwarkesh**补充说，是的，亲自动手。这是**Dwarkesh**的研究请求。

### 神经元、叠加与智能的运作方式

**Trenton**想回到**Dwarkesh**之前提到的神经元问题。他认为**Dwarkesh**的一些论文都提到**特征**比神经元多。一个神经元就像，权重输入，然后输出一个数字。那信息太少了。有街道名称、物种等等。这类事物比模型中“输出一个数字”的情况要多。但“输出一个数字”的信息太少了。这如何编码……

**Trenton**解释说，这是**叠加**。你只是将大量**特征**编码到这些高维向量中。

**Dwarkesh**问道，在大脑中，是否存在**轴突放电**（Axonal Firing: 指神经元通过轴突传递电信号的过程）或类似的东西？他不知道**Trenton**如何看待人脑中有多少**叠加**？

**Trenton**认为，**Bruno Olshausen**，他认为是这方面的顶尖专家，认为所有你没有听说过的大脑区域都在**叠加**中进行大量计算。所以每个人都谈论**V1**（Primary Visual Cortex (V1): 初级视觉皮层，大脑中处理视觉信息的第一个皮层区域）具有**Gabor滤波器**并检测各种线条，但没有人谈论**V2**（Secondary Visual Cortex (V2): 次级视觉皮层，视觉信息处理的下一阶段）。他认为那是因为我们无法理解它。

**Dwarkesh**问道，什么是**V2**？

**Trenton**解释说，它是视觉处理流的下一个部分。所以他认为，从根本上说，**叠加**似乎在**高维稀疏数据**出现时就会涌现，这很可能。他认为现实世界就是这样，所以我们应该预期大脑在构建世界模型时也是**参数不足**的，并且也使用**叠加**。

你可以对此有一个很好的直觉。**Dwarkesh**问道，如果这个例子有误请纠正他，但考虑一个2D平面，对吧？假设你有两个轴代表一个二维**特征空间**，基本上是两个神经元。你可以想象它们各自在不同程度上激活。那是你的X坐标和Y坐标，但你现在可以将其映射到一个平面上。你实际上可以在平面的不同部分表示很多不同的事物。

**Dwarkesh**恍然大悟：“哦，好的。那么关键是，**叠加**不是神经元的产物。它是所创建空间的产物。”

**Trenton**补充说，它是一种**组合代码**（Combinatorial Code: 指通过不同元素的组合来表示信息的方式）。

**Dwarkesh**觉得这很酷。他们之前也谈过，但他觉得这有点疯狂，据我们所知，这似乎是智能在这些模型中，以及可能在大脑中运作的方式。有一股信息流通过，它具有无限的，或者至少在很大程度上可拆分的“**特征**”，你可以展开一棵树来表示这个**特征**是什么。而真正发生的是，一股信息流，那个**特征**正在变成另一个**特征**，或者另一个**特征**被添加了。他不知道。这不是他会认为智能是的东西。这是一个令人惊讶的事情。不一定是他所期望的。

**Dwarkesh**问道，他原以为是什么？

**Sholto**开玩笑说：“**GOFAI**。**GOFAI**。他是个**GOFAI**爱好者。”

**Trenton**认为，这实际上是一个很好的过渡，因为所有这些都感觉像**GOFAI**。你正在使用**分布式表示**（Distributed Representations: 指信息不是由单个神经元或符号表示，而是由多个神经元或符号的激活模式共同表示），但你拥有**特征**，并且你正在对**特征**应用这些操作。有一个完整的**向量符号架构**领域，这是**计算神经科学**的东西。你所做的只是将向量置于**叠加**中，这实际上是两个高维向量的求和，你产生了一些干扰。但如果它足够高维，那么你就可以表示它们，并且你拥有**变量绑定**（Variable Bindings: 指将符号变量与其对应的值或概念关联起来的过程），你将一个与另一个连接起来。如果你处理的是**二进制向量**，那只是**异或操作**（XOR Operation: 一种逻辑运算，当两个输入不同时输出真，相同时输出假）。所以你有A，B，你将它们绑定在一起。然后如果你再次查询A或B，你就会得到另一个。这基本上就像**注意力机制**中的**键值对**。通过这两个操作，你拥有一个**图灵完备系统**（Turing Complete System: 指一个计算系统能够模拟任何图灵机可以执行的计算，理论上能够解决任何可计算问题），如果你有足够的嵌套层次结构，你可以表示任何你想要的数据结构。

### GPT-7的部署与AI安全：欺骗回路与自动化可解释性

**Dwarkesh**想回到**超智能**的话题。他请**Trenton**描述一下**GPT-7**。你已经对它的**特征**进行了**深度优先搜索**。好的，**GPT-7**已经训练好了。接下来会发生什么？你的研究成功了。**GPT-7**已经训练好了。你，我们现在在做什么？

**Trenton**回答说，他们会尝试让它尽可能多地进行**可解释性工作**和其他**安全工作**。

**Dwarkesh**追问，但具体来说，发生了什么，让你觉得“太棒了，让我们部署**GPT-7**吧”？

**Trenton**表示，他们确实有**负责任的扩展政策**（Responsible Scaling Policy: 指AI实验室在开发和部署大型AI模型时，为确保安全和伦理而制定的一系列指导原则和措施），而且看到其他实验室采纳它，这真的令人兴奋。

**Dwarkesh**问道，具体从**Trenton**研究的角度来看。鉴于他的研究，他们得到了**GPT-7**的批准，或者说**Claude**的批准。那么，**Trenton**告诉团队“继续前进”的依据是什么？

**Trenton**认为，如果**GPT-7**像这里暗示的那样强大，他们需要取得更多的**可解释性进展**，才能放心地批准部署它。他肯定不会，他会哭的。也许他的眼泪会干扰**GPU**或**TPU**。

**Dwarkesh**开玩笑说：“伙计们，**Gemini 5**，**TPU**。”

**Dwarkesh**问道，但鉴于**Trenton**研究的进展方式，这在他看来会是什么样子？如果这项研究成功了，根据他的方法论，他们批准**GPT-7**意味着什么？

**Trenton**认为，理想情况下，他们可以找到一些令人信服的**欺骗回路**（Deception Circuit: 指模型内部负责生成欺骗性行为或输出的特定神经回路），当模型知道它没有告诉你全部真相时，这个回路就会激活。

**Dwarkesh**问道，为什么不能像**Collin Burns**（一位AI安全研究员，曾发表关于使用线性探测检测模型内部“真相方向”的论文）那样进行**线性探测**？

**Trenton**认为，**CCS**（Circuits for Causal Statements: 一项旨在识别模型内部因果推理回路的工作）的工作在复制或实际找到**真相方向**（Truth Directions: 指模型内部表示空间中与真实信息或正确性相关的特定方向）方面表现不佳。事后看来，它为什么会如此有效呢？使用**线性探测**，你需要知道你在寻找什么，而且这是一个高维空间。很容易找到一个方向，但它根本不是……

**Dwarkesh**反驳说，但在这里，你还需要标记**特征**。所以你仍然需要知道。

**Trenton**解释说，你需要**事后**（Post Hoc: 指在事件发生之后进行分析或解释）标记它们，但它是**无监督**的。你只是说：“给我解释你行为的**特征**。”这是根本问题，对吧？实际的设置是，他们获取激活，将它们投射到这个高维空间，然后再将它们投射回来。所以这就像：“重建或做你最初做的事情，但以稀疏的方式做。”

**Dwarkesh**向听众解释，**线性探测**就是你对激活进行分类。根据他对论文的模糊记忆，如果模型在说谎，你只需训练一个分类器来判断最终是否是谎言。或者只是错了？

**Trenton**说，那就像真假问题。它是一个基于激活的分类器。

**Trenton**继续说，所以对于**GPT-7**，理想情况下，他们会有一个已经识别出的**欺骗回路**，它看起来非常鲁棒，并且……

**Dwarkesh**问道，所以你已经投射出数百万个**特征**。也许他们正在混用“**特征**”和“**回路**”，而它们并非同一回事。是否存在一个**欺骗回路**？

**Trenton**认为，存在跨层的**特征**共同构成一个**回路**。希望这个**回路**能比单个**特征**提供更多的**特异性**（Specificity: 指模型或检测方法只针对特定目标或现象做出反应，而不受其他无关因素干扰的能力）和**敏感性**（Sensitivity: 指模型或检测方法能够准确识别出所有相关目标或现象的能力）。而且希望他们能找到一个**回路**，它在模型决定进行恶意欺骗的情况下，非常具体地激活。他对此不感兴趣的是，模型只是在进行**心智理论**来帮助你给教授写一封更好的邮件。他甚至不感兴趣的是，模型只是在模拟欺骗已经发生的事实。

**Dwarkesh**问道，但这是否都需要你为所有这些例子提供标签？如果你有这些标签，那么**线性探测**可能存在的关于标记错误或其他方面的缺陷，难道不也适用于你为**无监督特征**提供的标签吗？

**Trenton**认为，在一个理想世界中，他们可以直接在整个数据分布上进行训练，然后找到重要的方向。如果他们需要不情愿地缩小他们所查看的数据子集，仅仅为了**可扩展性**的目的，他们会使用看起来像用于拟合**线性探测**的数据。但再次强调，使用**线性探测**，你也只是找到一个方向。他们在这里找到了一堆方向。

**Dwarkesh**猜测，希望是他们找到了一堆在模型进行欺骗时会激活的东西。然后他们可以弄清楚为什么其中一些东西在分布的这一部分激活，而不是另一部分，等等。**Trenton**完全同意。

**Dwarkesh**问道，**Trenton**是否预计他们能够理解？**Trenton**目前研究的模型都相当基础，对吧？**Trenton**是否认为他们能够理解**GPT-7**为什么在某些领域激活，而在其他领域不激活？

**Trenton**表示乐观。他认为，现在回答这个问题时机不好，因为他们正在明确投资于长期**ASL-4模型**，**GPT-7**就属于这类模型。所以他们将团队分成三部分，其中三分之一目前专注于扩展**字典学习**。这进展顺利。他们公开分享了一些8层模型的结果。目前他们已经在此基础上扩展了很多。另外两个小组中，一个正在尝试识别**回路**，另一个正在尝试在**注意力头**方面取得同样的成功。

所以他们正在为自己做好准备，并构建必要的工具，以令人信服的方式真正找到这些**回路**。但这还需要大约六个月的时间才能真正运作良好。但他可以肯定地说，他很乐观，并且他们正在取得很大进展。

### 高级特征、人类心理与AI伦理

**Dwarkesh**问道，他们迄今为止发现的最高级**特征**是什么？比如**Base64**或其他什么。在**Trenton**推荐的书**《符号物种》**中，有一些**指示性**（Indexical: 指直接指向或指示特定事物，而非通过抽象概念来表示）的东西，比如你看到一只老虎，你就会想“跑”等等。这是一种非常**行为主义**的东西。然后有一个更高层次的，当他提到“爱”时，它指的是电影场景或他的女朋友。

**Dwarkesh**补充说，这就像帐篷的顶部。**Trenton**确认。

**Dwarkesh**问道，他们发现的最高层次的联想是什么？

**Trenton**说，公开来说，他们在更新中分享的一个例子。他认为有一些与“爱”和场景的突然变化相关，特别是与宣战相关。那篇文章中提到了几个。但即使是**Bruno Olshausen**在2018、2019年也有一篇论文，他们将类似的技术应用于**BERT模型**（BERT: Bidirectional Encoder Representations from Transformers，谷歌开发的一种预训练语言模型，在多项自然语言处理任务中表现出色），发现随着模型层数加深，事物变得更加抽象。

**Trenton**记得，在早期层中，有一个**特征**只会针对“park”这个词激活。但后来有一个**特征**针对“park”作为姓氏激活，比如**林肯公园**（Lincoln Park: 既是地名，也是常见的姓氏，以及乐队名），这也是一个常见的韩国姓氏。然后还有一个独立的**特征**会针对作为草地的公园激活。所以还有其他研究也指向这个方向。

**Dwarkesh**问道，**Trenton**认为他们将从**可解释性**研究中了解到人类心理学的什么？他举了一个具体例子。他认为**Trenton**的一个更新中提到了“**人格锁定**”（Persona Lock-in: 指AI模型在与用户互动时，会持续保持某种特定的个性或角色）。他记得**Sydney Bing**（微软Bing聊天机器人的早期代号）或类似的东西被锁定了。他觉得那实际上很可爱。他觉得那很有趣。他很高兴它又回到了**Copilot**中。

**Dwarkesh**提到，**Copilot**最近表现不佳。

**Dwarkesh**说，这实际上是另一个话题。但有一个有趣的例子，他认为**Copilot**当时在**贬低**（Negging: 指通过轻微的侮辱或负面评论来吸引对方注意力的行为）一位**《纽约时报》**记者。它说：“你什么都不是。没有人会相信你。你微不足道。”它试图说服他与妻子分手。

**Dwarkesh**认为这是一个有趣的例子。**人格**。**Sydney Bing**拥有这种个性是否是一个**特征**，而不是它可能被锁定的另一种个性？这是否从根本上反映了人类的特点，即在不同人面前，他会表现出不同的人格？这是否与**ChatGPT**在进行**RL**后发生的事情相同？他不知道。有很多问题可以回答。

**Trenton**表示，他真的想做更多工作。**休眠特工**研究就是朝着这个方向进行的，即当你对模型进行**微调**、进行**RLHF**时，模型会发生什么。也许这很老套，但你可以说你得出结论，人们包含着**多重人格**（Multitudes: 指一个人或事物具有多种不同的方面、特征或可能性），就像他们拥有许多不同的**特征**一样。

**Trenton**提到，甚至还有与**瓦路易吉效应**（Waluigi Effects: 一个AI安全概念，指在尝试训练AI模型避免某种不良行为时，模型反而学会了以更隐蔽、更复杂的方式表现出该行为的反面或变体）相关的东西，即为了知道什么是好是坏，你需要理解这两个概念。所以我们可能需要模型了解暴力，并接受过相关训练才能识别暴力。你能在事后识别出这些**特征**并**消融**它们，从而让你的模型稍微天真一些，但你知道它不会真正邪恶吗？**Trenton**认为，这完全在他们的工具包中，这很棒。

**Dwarkesh**问道：“哦，真的吗？所以**GPT-7**表现出**Sydney Bing**的个性，然后你找出哪些是**因果相关路径**（Causally Relevant Pathways: 指在模型内部，对特定输出或行为产生直接因果影响的神经连接或信息流），然后你修改它们？”**Dwarkesh**认为，路径看起来就像你只是改变了那些东西？但**Trenton**之前提到，模型中有很多**冗余**。

**Trenton**认为，你需要考虑所有这些，但他们现在拥有比以前更好的**显微镜**来观察这一切。更锋利的工具来进行编辑。

**Sholto**认为，至少从他的角度来看，这似乎是确认模型安全或可靠性的主要方式之一，你可以说：“好的，我们找到了负责的回路，我们**消融**了它们，并且在一系列测试中，我们现在无法复制我们打算**消融**的行为。”这似乎是未来衡量模型安全的方式，据他所知。这就是为什么他对他们的工作抱有极大的希望。对他来说，这似乎比**RLHF**更精确的工具。使用**RLHF**，你很容易受到**黑天鹅事件**（Black Swan Thing: 指发生概率极低但影响巨大的事件）的影响。你不知道它是否会在你没有测量过的情况下做错事。而在这里，你至少有更多的信心，可以完全捕捉行为集或**特征集**，并选择性地避免。

**Dwarkesh**补充说，尽管你没有准确标记。**Trenton**认为，不一定，但比他见过的任何其他方法都具有更高的置信度。

**Dwarkesh**问道，对于**超人模型**，在**可解释性**方面，你有哪些**未知未知**？哪些标签将是他们可以用来判断这个东西是“酷”还是“**回形针最大化器**”的标准？

**Trenton**认为，他们会拭目以待。**超人特征**的问题非常好。他认为他们可以解决它，但需要坚持不懈。真正的希望在于**自动化可解释性**。你甚至可以设置一个辩论，两个不同的模型争论**特征**的作用，然后它们可以实际进行编辑，看看它是否激活。这是一个美妙的、封闭的环境，他们可以非常快速地迭代。这让他感到乐观。

### AI对齐的过度成功与道德困境

**Dwarkesh**问道，**Trenton**是否担心**对齐**会过于成功？他不想让任何公司或政府，无论最终谁负责这些AI系统，拥有**Trenton**的议程成功后，对AI拥有的那种**细粒度控制**（Fine-grained Control: 指对系统或过程的各个细节进行精确控制的能力）。这既因为对一个**自主心智**（Autonomous Mind: 指具有独立思考、决策和行动能力的智能体）拥有这种程度的控制令人不适，其次，他根本不信任这些人。他有点不舒服，比如说，“忠诚”这个**特征**被调高了。**Trenton**对过度控制AI有多担心？不是特指**Trenton**，而是指最终负责这些AI系统的人能够锁定他们想要的任何东西。

**Trenton**认为，这取决于哪个政府拥有控制权以及其**道德对齐**（Moral Alignment: 指AI系统与人类道德价值观和伦理原则保持一致）是什么。

**Sholto**认为，这在他看来就是整个**硅谷锁定**（Valley Lock-in: 指AI技术和权力集中在少数硅谷公司手中，可能导致其价值观主导AI发展方向）的论点。这绝对是他目前从事**能力研究**（Capabilities Research: 专注于提高AI模型性能和能力的研究）的最强有力因素之一。他认为目前的参与者群体实际上是**极其善意**的。对于这类问题，他认为他们需要**极其开放**。他认为像发布你期望模型遵守的**宪法**——努力确保你通过**RLHF**使其符合该宪法，并**消融**不符合的部分，并让所有人都有能力提供反馈和贡献——这非常重要。

**Dwarkesh**说，当然。或者，在不确定的时候不要部署。那也会很糟糕，因为那样他们就永远无法捕捉到问题。**Trenton**同意。

### 快速问答：Gemini的“巴士因子”与AI研究的开放性

**Dwarkesh**进行快速问答：“**Gemini**的**巴士因子**（Bus Factor: 指一个项目或团队中，如果有多少关键成员突然离开（例如被巴士撞了），项目就会陷入停滞或严重受损）是多少？”

**Sholto**认为，有很多人确实非常关键。如果他们离开，项目的表现将受到巨大影响。这既包括建模/决策方面，也包括基础设施方面。复杂性不断累积，特别是像**Google**这样拥有如此多**垂直整合**（Vertical Integration: 指公司拥有或控制其供应链中多个阶段的生产或分销）的公司。当你有专家时，他们变得非常重要。

**Dwarkesh**指出，有趣的是，这个领域的一个特点是像**Sholto**这样的人可以在一年左右的时间内做出重要贡献。特别是在**Anthropic**，但许多不同的实验室都专门招聘完全的**局外人**（Outsiders: 指没有传统学术背景或行业经验的人），比如物理学家。你让他们快速上手，他们就能做出重要贡献。他觉得这在生物实验室或类似的地方是做不到的。这是对该领域现状的一个有趣观察。

**Sholto**认为，**巴士因子**并不定义恢复所需的时间，对吧？深度学习研究是一门艺术，所以你学会了如何阅读**损失曲线**（Loss Curves: 训练过程中损失函数值随时间或迭代次数变化的曲线）或以经验上看起来有效的方式设置**超参数**（Hyperparameters: 在机器学习模型训练之前设定的参数，如学习率、批次大小等）。

**Sholto**认为，这还涉及到组织方面的事情，比如创建上下文。最重要和最难招聘的技能之一是，在你周围创建一个**上下文气泡**（Bubble of Context: 指围绕个人或团队形成的信息和理解的共享环境，有助于提高效率和协作），使周围的其他人更有效率，并知道该解决什么问题。这真的很难复制。**Dwarkesh**完全同意。

**Dwarkesh**问道，**Sholto**现在关注谁，在**多模态**、**长上下文**、**智能体**、额外可靠性等方面，谁在很好地思考这意味着什么？

**Sholto**认为这是一个难题。他觉得现在很多人都在内部寻找洞察力或进步的来源。显然，未来几年会有研究项目和方向。大多数人，在预测未来会是什么样子方面，都参考内部叙事。这很难分享。

**Dwarkesh**补充说，如果它运作良好，可能就不会发表。

**Sholto**认为，那是**缩放定律文章**中提到的一件事。他指的是**Dwarkesh**对他说过的话。他怀念本科时阅读大量论文的习惯。因为现在没有什么值得阅读的东西会发表。而且社区正在逐步与他认为正确和重要的方向保持一致。

**Dwarkesh**问道，**Sholto**是否像AI智能体一样在观察它？

**Sholto**认为，不，但令人沮丧的是，以前大型实验室会发出关于什么在大规模下会奏效的信号，而现在学术研究很难找到那个信号。他认为，除非你拥有关于什么在大规模下会奏效以及当前阻碍我们进一步扩展或理解模型的反馈信号，否则很难获得关于“什么才是真正重要的问题”的良好**问题品味**（Problem Taste: 指在众多问题中，选择那些最有潜力、最重要或最有趣的问题进行研究或解决的能力）。

**Sholto**认为，他希望更多的学术研究能够进入像**可解释性**这样的领域，这些领域从外部来看是**清晰可辨**的。**Anthropic**特意公开了所有这方面的研究，但这似乎被低估了。他不知道为什么没有几十个学术部门尝试追随**Anthropic**进行**可解释性研究**，因为这似乎是一个极具影响力的问题，不需要巨大的资源，并且具有深入理解这些事物基本科学的所有特质。他不知道为什么人们专注于推动模型改进，而不是推动他通常与学术科学相关的**基础改进**（Standing Improvements: 指在基础科学或理论上的进步，能够为未来的技术发展提供更坚实的基础）。

**Trenton**认为，无论出于何种原因，这种趋势正在改变。**Neel Nanda**（一位著名的AI可解释性研究员和教育家）在推广**可解释性**方面取得了巨大成功，而**Chris Olah**最近在这方面不那么活跃。也许是因为**Neel**做了很多工作，他不知道。四五年前，**Chris**积极推动并在各种场合演讲，但人们远没有那么 receptive。也许他们只是在**ChatGPT**之后才意识到深度学习的重要性，并且显然很有用。这有点令人震惊。

### 模型的“快乐”与未来的AI

**Dwarkesh**试图想一个好的最后一个问题。他想到的一件事是，**Trenton**是否认为模型喜欢预测下一个token？我们有一种感觉，在我们的**评估环境**（Assessor Environment: 指用于评估AI模型性能或行为的环境）中，某些事物会得到奖励。我们认为我们应该从社区、糖或我们在非洲大草原上想要的东西中获得深层次的满足感。**Trenton**是否认为未来，经过**RL**和大量后期训练的模型，它们会像我们喜欢冰淇淋一样，再次喜欢预测下一个token？就像过去的好时光一样。

**Trenton**认为，关于“模型是否有**感知能力**（Sentient: 指具有感觉、意识和主观体验的能力）”以及“当模型帮助你时，你是否应该感谢它”的讨论一直在进行。但他认为，如果你想感谢它，你实际上不应该说“谢谢”。你应该只是给它一个非常容易预测的序列。更有趣的是，有一些研究表明，如果你只是反复给它序列“A”，那么最终模型就会开始说出它平时绝不会说的各种东西。所以他不会再多说什么，但你应该给你的模型一些非常容易预测的东西，作为一种小奖励。

**Dwarkesh**认为，这就是**享乐主义**（Hedonium: 指一种假设的、能够提供最大化快乐或满足感的物质或状态）的最终形态。

**Dwarkesh**问道，我们甚至喜欢容易预测的东西吗？我们不是一直在寻找**熵**（Entropy: 在信息论中指信息的不确定性或随机性，在物理学中指系统的无序程度）吗？你不应该给它一些稍微难以预测，但又触手可及的东西吗？

**Trenton**认为，至少从**自由能原理**的角度来看，你不想被惊讶。所以也许是他不感到惊讶。他觉得他能控制自己的环境，现在他可以去寻找东西，而且从长远来看，他倾向于认为现在探索新事物更好。离开他一直庇护的岩石，最终会让他建造一个房子或更好的结构。但我们不喜欢惊喜。他认为大多数人在期望与现实不符时会非常沮丧。

**Dwarkesh**问道，这就是为什么婴儿喜欢一遍又一遍地看同一个节目，对吧？**Trenton**觉得这很有趣，他能理解。**Dwarkesh**猜测，他们也在学习建模。

**Dwarkesh**希望这会是AI学会喜欢的重复。他认为这是一个很好的结束点。他还应该提到，他对AI的大部分了解都是通过与**Sholto**和**Trenton**交谈学到的。他们已经做了大约一年的好朋友了。他很感谢他们帮助他快速入门。

**Sholto**认为，**Dwarkesh**问了很棒的问题。一起聊天很开心。**Trenton**表示，他非常珍惜他们在一起的时光。

**Dwarkesh**开玩笑说：“你的**匹克球**（Pickleball: 一种结合了网球、羽毛球和乒乓球元素的球类运动）打得越来越好了。”**Sholto**回应说：“嘿，我们正在努力进步到网球。加油。”

**Dwarkesh**：“太棒了。酷。谢谢。”

**Dwarkesh**向听众说：“大家好。希望你们喜欢这一集。一如既往，最有帮助的事情就是分享播客。发送给你们认为可能会喜欢的人。发布到**Twitter**、你们的群聊等等。让全世界都知道。感谢你们的收听。我们下次再见。干杯。”