---
area: tech-engineering
author: Lei
category: ai-ml
companies_orgs: []
date: 2025-10-01
draft: true
guest: ''
insight: null
layout: post.njk
media_books: []
people:
- richard-sutton
- 大飞
- Richard Sutton
products_models: []
project: []
series: null
source: https://www.youtube.com/watch?v=RLR4OpetiEc
speaker: ''
status: evergreen
summary: A detailed breakdown of Richard Sutton's argument that LLMs are fundamentally
  flawed for achieving AGI, and his proposed alternative centered on experiential
  learning, world models, and the lessons from animal intelligence.
tags:
- agi
- reinforcement-learning
title: Richard Sutton on Why Large Language Models Are a Dead End for AGI
---

大家好，这里是最佳拍档，我是大飞。

就在最近，知名技术播客德瓦尔凯什播客（Dwarkesh Podcast）上线了与强化学习之父Richard Sutton的一期视频，标题非常犀利，直言大语言模型是死路一条。这次访谈的内容从大语言模型的根本缺陷聊到了智能的本质，再延伸到了宇宙演化的大视角，最后落到了创造AGI是人类文明关键使命的结论，密度极高。今天我们就来一点一点地拆解，看看这位AI领域的泰山北斗到底为什么否定大语言模型，以及他眼中真正的智能究竟是什么样的。

## 核心争议：为什么大语言模型是死路一条？

我们先从访谈里最核心的争议点说起。萨顿为什么认为大语言模型走不通？他在访谈里开门见山地说道，强化学习是关于理解你的世界，而大语言模型是关于模仿人类，做人们说你应该做的事情。它们不是在搞清楚该做什么。这句话看似简单，却直接戳中了智能本质的核心。在萨顿看来，智能不是模仿，而是理解与行动。而大语言模型恰恰卡在了模仿这一步，没有触及到理解。

具体来说，他认为大语言模型有三个致命的缺陷。

### 1. 缺乏真正的世界模型

第一个缺陷是，大语言模型缺乏真正的**世界模型 (World Model)**。萨顿解释道，模仿人类说什么，并不是在建立真正的世界模型，只是在模仿那些拥有世界模型的东西，也就是人类。这里的关键差异是，预测人类会说什么和预测世界会发生什么是完全不同的两件事情。

比如说，你看到孩子要伸手碰热水壶，你的世界模型会立刻预测“碰到会烫伤”，然后去阻止孩子。但是大语言模型只能根据训练数据里的文本，预测人类看到这种场景会说什么，可能是“别碰，会烫”。但它本身根本不理解热水壶和烫伤这些概念的物理意义，也不知道“碰”这个动作和“烫伤”这个结果之间的因果关系。它只是在复制人类的语言模式，没有真正理解世界的运作逻辑。这就是萨顿说的没有世界模型的问题。

### 2. 没有Ground Truth

第二个缺陷是大语言模型没有**Ground Truth (事实基础)**。萨顿说，在大语言模型中没有正确答案的定义。你说了什么，但是你不会得到关于什么是正确的反馈，因为根本就没有什么正确的定义。

这句话怎么理解呢？举个例子，如果训练一个机器人玩打砖块的游戏，Ground Truth就很明确，那就是打掉砖块得分，没接住球游戏结束。系统能够通过得分、失分这个反馈，知道自己的动作对不对，然后调整策略。但是大语言模型生成文本的时候，其实并没有这样的正确标准。比如说，你让它写一篇如何提升专注力的文章，它写出来之后，你没法说这篇一定是对的，或者这篇一定是错的，因为专注力提升的方法没有统一的答案，也没有一个可验证的反馈信号来告诉大语言模型：“你这里写的不好，该改。”既然没有对与错的判断标准，系统就没法真正地学习改进。毕竟连自己哪里错了都不知道，谈何进步呢？

### 3. 无法从经验中学习

第三个缺陷是，大语言模型无法从经验中学习。萨顿强调，它们不会对接下来发生的事情感到惊讶，如果发生了意外，它们也不会做出调整。这一点其实戳中了当前AI的核心痛点，那就是人类的学习本质上是通过预期与现实的偏差来调整认知的。

比如说，你以为今天会下雨带了伞，结果没下，下一次你就会根据天气预报再判断。但是大语言模型没有这种适应性。再举个具体的例子，你问大语言模型：“把冰块放进微波炉加热会怎么样？”它可能会说：“冰块会融化。”这是基于训练数据的预测。但是如果实际中你把冰块放进微波炉，因为微波炉功率太低，半个小时都没融化，大语言模型不会因为这个意外而改变自己的认知。下次你再问它，还是会说冰块会融化。它不会从实际经验中修正自己的判断，因为它根本没有体验经验的能力，只是在被动地处理文本输入。

### 智能的本质：目标

在聊到这三个缺陷的时候，萨顿特别强调了目标对于智能的重要性。对他来说，拥有目标是智能的本质。如果某个东西能够实现目标，它就是智能的。这里他还引用了人工智能先驱约翰·麦卡锡的定义：智能是实现目标能力的计算部分。在萨顿看来，没有目标的系统顶多算是一个行为系统，不能算是一个智能系统。

比如说，一个只会循环播放音乐的音箱，它有播放音乐的行为，但是没有“要让听众喜欢”或者是“根据场景来调整音乐”的目标，所以它没有智能。

主持人当时反驳：“大语言模型也有目标啊，预测下一个TOKEN不就是它的目标吗？” 萨顿直接否定了这个说法，他说：“那不是目标。它不会改变世界。TOKEN向你袭来，如果你预测它们，你并不会影响它们。” 意思是大语言模型的预测TOKEN只是被动地应对输入，不会对外部世界产生任何的影响。它不会为了更准确地预测医学领域的TOKEN而主动去学习医学知识，也不会因为预测错了而主动地调整学习方向，因为它没有主动实现某个目标的动力，只是在执行预设的计算任务。

## 真正的智能路径：经验学习范式

既然大语言模型走不通，那萨顿认为真正的智能路径是什么呢？他提出了**经验学习范式 (Experiential Learning Paradigm)** 的概念，核心是一个简单但是强大的循环：感知、行动和奖励。这个过程在你的生命中不断地重复。他说，智能就是接受这个流，改变行动来增加流中的奖励。

“流”指的就是Agent与世界互动时产生的，从感知信号到行动输出，再到奖励反馈的连续过程。而智能的核心，就是在这个过程中不断调整行为，让奖励越来越多。这个范式和大语言模型的本质区别在于学习的来源。大语言模型的学习来源是人类写的文本，数据是间接的、二手的。而经验学习的来源是Agent自己与世界的互动，经验是直接的、一手的。

萨顿解释说，学习来自于这个流，学习也是关于这个流的。你的知识是关于“如果你采取某个行动会发生什么”，或者“哪些事件会跟随其他事件”。知识的内容是关于这个流的陈述。正因为知识是关于经验流的，所以它能够被验证。比如说，你认为“按下开关灯会亮”，这个知识可以通过“按开关”这个行动来验证。如果灯没有亮，你就会调整这个知识，也许可能灯泡坏了。这就是持续学习的过程。

### 婴儿与松鼠的学习方式

为了让这个概念更加易懂，萨顿还举了一个婴儿学习的例子。当主持人问到“人类不也会模仿学习吗？比如说孩子模仿大人说话”，萨顿反驳道：“当我看到孩子的时候，我看到的是孩子在尝试各种事情，挥舞着手臂，移动着眼睛。他们如何移动眼睛或者发出声音，都没有模仿的对象。”他认为婴儿的核心学习方式是试错，而不是模仿。婴儿会无意识地挥动手臂，偶然碰到玩具发出声音，然后他会发现挥动手臂和玩具发声之间的联系，接下来就会主动重复这个动作。这就是“感知”（看到玩具）到“行动”（挥动手臂）再到“奖励”（听到声音）的循环。

即使到了学校教育阶段，萨顿也认为模仿和训练是例外，不是常态。他说道：“正式的学校教育其实是一种例外。学习真的不是关于训练，学习是关于学习，是一个主动的过程。孩子们尝试事物，并且观察会发生什么。”

他还特别用松鼠举例，来反驳监督学习是常态的观点。萨顿说：“监督学习不是自然界中发生的事情。即使在学校里，我们也应该忘记它，因为那是人类特有的某种特殊情况，它不会在自然界中广泛地发生。松鼠不上学，松鼠可以学习关于世界的一切。” 这句话的意思是，松鼠不需要人类教它怎么去找食物、怎么躲避天敌。它通过自己的行动，比如说尝试吃不同的果实，观察哪些能吃；尝试靠近人类，观察是否有危险，来积累经验、形成知识。这种不需要人类标注数据、不需要学校教育、仅靠自身经验就能够学习的能力，才是智能的基础。

## 智能Agent的四个核心组件

既然经验学习范式是核心，那一个完整的Agent应该具备哪些组件呢？萨顿在访谈里详细拆解了四个核心的部分，这四个部分共同构成了能够从经验中学习的智能系统。

1. **策略 (Policy)**：简单来说，策略就是在当前所处的情况下应该做什么，或者说是从状态到行动的映射。比如说，Agent感知到面前有一道门，门是关着的（这是状态），策略就会告诉它应该伸手去开门（这是行动）。但是要注意，策略不是一个固定的规则，而是一个动态调整的系统。如果第一次开门发现门是锁着的，策略下次就会调整为“先找好钥匙再开门”。萨顿特别强调，好的策略必须能够泛化，也就是能够处理没有见过的新情况。

2. **价值函数 (Value Function)**：萨顿解释道，价值函数通过TD学习产生一个数字，这个数字说明事情进展得如何。价值函数的核心作用是评估当前状态的好坏，为策略调整提供依据。比如说在下棋的时候，某个棋盘布局的价值高，说明这个布局对赢棋更有利；价值低，说明可能有风险。这里的关键是，价值函数评估的是长期收益，而不是短期收益。我们还拿下棋举例，牺牲一个兵可能会换来后续的将死对方，价值函数会识别出这种长期的优势，从而让策略做出牺牲小兵的决策。萨顿发明的TD学习，就是让价值函数能够根据当前预测和未来实际结果的差异，不断修正自己的评估，让它越来越准确。

3. **感知组件 (Perception)**：指的是要构建你的状态表示，以及你对于当前位置的感知。这不是简单地接收感官信号，而是把杂乱的感官数据转化为有意义的内部表示。假设你看到一个红色、圆形、表面有斑点的物体，感知组件会把这些视觉信号整合为“这是一个苹果”的内部表示。这个表示里包含了“可以吃”、“需要洗”等等关键的信息，方便策略和价值函数做出决策。如果感知组件出了问题，把辣椒误以为是苹果，那后续的策略（比如说咬一口）就会出错。所以萨顿认为，感知组件的核心是提取关键信息，构建有用的状态表示，它是Agent与世界互动的第一道门槛。

4. **世界转换模型 (World Transition Model)**：萨顿说：“你相信如果你做这件事情会发生什么呢？行动的后果是什么呢？”这个模型负责预测行动会带来的状态变化，也就是理解因果关系。比如说，你知道按下开关会导致灯亮，把杯子推到桌边会导致杯子掉下去，这些都是世界转换模型的作用。Sutton特别强调，这个模型不仅包括物理规律，还包括抽象规律。比如说你知道如何从买机票到去机场，再到登机落地，这个抽象的流程就是世界转换模型的一部分。而且这个模型不是从奖励中学习的，而是从观察行动和结果的对应关系中学习的。你不需要有人告诉你“按下开关会亮”，只要观察几次按开关和灯亮的对应，就能够建立起这个模型。

在这四个组件里，萨顿尤其看重世界转换模型。他说：“它将从你接收到的所有感知中非常丰富地学习，不仅仅是奖励。它必须包括奖励，但那只是整个模型的一小部分，一个小而关键的部分。”因为有了世界转换模型，Agent才能够预测未来，才能够提前规划。就好像你要去超市买东西会先规划好路线一样，这就是基于世界转换模型的预测：“走这条路会更快到达超市”，而不是盲目地行动。

## 对《苦涩的教训》的误读

聊到这里，就不得不提到萨顿2019年那篇**《苦涩的教训》(The Bitter Lesson)**。很多人现在用这篇文章来辩护大语言模型的扩展路线，说大语言模型用大规模的计算来处理大规模的数据，符合《苦涩的教训》里边“用通用方法加计算”的原则。但是萨顿在访谈里明确地表示，这是对文章的误读。

首先，萨顿承认大语言模型有符合苦涩教训的地方：“他们显然是一种使用大规模计算的方式，可以随着计算扩展到互联网的极限。”但是他话锋一转，指出了关键的问题：“但是他们也是一种投入了大量人类知识的方式。”而苦涩的教训的核心精神，恰恰是依靠通用的方法和计算，而不是依赖人类知识。

萨顿在文章里举过例子，早期的国际象棋AI依赖于人类总结的下棋技巧，但是后来的AlphaZero完全抛弃了这些知识，只靠强化学习加大规模计算就战胜了人类。这其实才是苦涩的教训的核心：人类知识虽然能够短期提升性能，但是从长期来看，通用学习加更多计算才是更可持续的路径。

而大语言模型的问题就在于，它过度依赖于人类的知识，这里的人类知识就是互联网上的文本数据。这些数据是人类对于世界的描述，而不是世界本身的经验。萨顿预测：“这是一个社会学或者是行业的问题。他相信大语言模型会达到数据的极限，并且被能够从经验而非人类那里获取更多东西的东西所取代。”

在某种程度上，这就是苦涩教训的经典案例。我们向大语言模型投入的人类知识越多，它们就能够做得越好，所以感觉很好。然而，他期待会出现能够从经验中学习的系统，它们可能表现得更好，更具有可扩展性。

萨顿还特别强调了历史上的教训：“在苦涩教训的每个案例中，你都可以从人类知识开始，然后做可扩展的事情，这总是可能的。从来没有任何理由说这必然是坏的。但是事实上，在实践中他总是被证明是坏的。”他说的“坏”，指的是依赖于人类知识会让模型陷入局部最优，从而错过真正通用的方法。

萨顿认为，大语言模型现在就处于依赖于人类知识的局部最优里。一旦从经验学习的系统成熟了，大语言模型就会像当年的规则式机器翻译一样，被更通用的方法取代。当被问到什么是真正可扩展的方法时，萨顿的回答很直接：“可扩展的方法就是你从经验中学习，你尝试事物，看看什么有效，没有人需要告诉你。”这句话其实也是他对于AGI路径的核心判断：真正的智能不需要人类喂数据、教规则，而是能够像人类、像松鼠一样自己去探索世界，从经验中学习。

## 深度学习的泛化难题

除了大语言模型的缺陷，萨顿还指出了当前深度学习系统的另一个大问题：泛化能力差。他说：“我们没有任何方法擅长这一点。”这里的泛化能力指的是，系统把在一个任务上学到的知识迁移到新的任务、新的场景的能力。萨顿认为，当前深度学习的泛化问题主要体现在两个方面：

1. **灾难性遗忘**：指的是在某个新事物上训练模型，经常会灾难性地遗忘掉所有旧的事物。比如一个AI先学会了识别苹果，再训练它去识别橙子，训练完之后它可能再也认不出苹果了。这就是灾难性遗忘。而人类不会这样，你学会了说英语，再学法语也不会忘记英语是怎么说的。

2. **缺乏自动化的泛化机制**：萨顿说到：“梯度下降不会让你泛化的好，它会让你解决问题，但是它不会让你在获得新数据的时候以好的方式泛化。”当前的深度学习靠的正是梯度下降优化模型的参数，让模型在训练数据上表现得更好，但是它不会主动总结规律来应对新的场景。比如说，一个AI在“1+1=2”、“2+2=4”上训练，能够学会加法。但是如果遇到“100加200等于多少”呢？如果训练数据里没有类似的数据，它可能就会算错。而人类学会加法之后，不管数字多大都能够算出结果，因为人类总结了加法的规律，而AI没有这个能力。

萨顿说，现在的AI的泛化能力完全依赖于研究者手动调整模型的结构或者是数据。但是这种靠人调的方式，显然不是通用智能的路径。

为了说明什么是真正的泛化，萨顿还举了一个数学问题的例子。现在的大语言模型能够解决越来越复杂的数学题，从简单的加法到奥数题。但是萨顿认为这不是泛化。他说道：“如果只有一个答案而你找到了，那不叫泛化，那只能说是他们找到了唯一的解决方法。真正的泛化是可能是这种方式，也可能是那种方式，而他们选择了好的方式。”

## 持续学习与大世界假设

聊到泛化，就不得不提到持续学习：Agent如何在不断变化的世界里持续积累知识而不遗忘呢？萨顿提出了**大世界假设 (Big World Hypothesis)** 的概念，认为“人类在工作中变得有用的原因是，他们正在遇到世界的特定部分，不可能被预期，也不可能全部提前输入。”这句话的意思是，世界太大太复杂了，你不可能把所有的情况都提前教给Agent，所以Agent必须具备在实际场景中持续学习的能力。

他批评了大语言模型的理想化愿景。在他看来，大语言模型的梦想是认为人类可以教会Agent的一切。但是现实是，每个人的生活都是独特的，他们喜欢什么并不代表普通人喜欢什么。

萨顿还纠正了一个关于持续学习带宽的误区。很多人会认为奖励信号太少，不足以支撑学习。但是萨顿认为，学习的带宽不仅来自于奖励，更来自于感知数据。比如说你每天上班虽然没有奖励，但是你会通过感知来观察哪条路不堵车，哪个时间点电梯人少，这些都是从感知数据中学习的知识，不需要奖励来驱动。而世界转换模型就是从这些感知数据中学习的，他整合了所有的感知信号，构建了对于世界的理解，这为持续学习提供了足够的带宽。

他还举了一个创业的例子，说明如何用价值函数来解决稀疏奖励的问题。他说：“假设一个人试图创办一家初创公司，这是一个奖励周期为10年的事情。十年一次，你可能会有一次退出，获得10亿美元的回报。”如果只靠最终的10亿美元奖励来学习，那中间的十年里，创业者根本不知道自己的行动对不对。但是人类能够通过价值函数来评估阶段性的进展。比如说今天谈成了一个合作，价值函数就会判断“这样公司更接近成功，是好的进展”，从而给创业者一个阶段性的奖励。如果核心团队离职了，价值函数就会判断“这对于公司不利，需要调整”。通过这种方式，即使奖励很稀疏，Agent也能够持续学习和调整，这就是价值函数的关键作用。

## 回归本源：人类是动物

在整个访谈中，萨顿还反复强调了一个观点：人类是动物，理解动物智能是理解人类智能的关键，甚至说是理解AGI的关键。

萨顿说到：“如果我们理解了松鼠，我们就几乎完全理解了人类智能，因为语言部分只是表面的一层薄薄的装饰。”这句话可能会让很多人惊讶，毕竟人类一直认为语言是智能的核心。但是萨顿却认为语言只是锦上添花，真正的智能基础是人类和动物共有的、从经验中学习的能力。

比如说，松鼠会储存食物过冬，会躲避天敌，会在树上跳跃的时候判断距离。这些能力都不需要语言，只需要感知、行动、奖励的经验循环。而人类的语言只是在这个基础上，增加了交流、知识传递经验的工具，但是没有改变智能的核心逻辑。

Sutton还指出，动物的学习里根本没有监督学习的位置。监督学习需要人类标注的标签，比如说“这是猫，那是狗”。但是动物不需要。松鼠不需要有人告诉它“这是松果，那是石头”，它通过自己的尝试，比如说咬一口，就能够分辨出哪些能吃、哪些不能吃。所以说，动物的学习核心是预测和试错控制：预测行动会带来什么样的结果，通过试错来调整行动。这和萨顿提出的经验学习范式完全一致。

他再次用松鼠举例：“松鼠不上学，但是松鼠可以学习关于世界的一切。”松鼠的智能虽然简单，但它包含了理解世界、适应环境、实现目标的核心能力，而这些正是当前大语言模型所缺失的。萨顿认为，人类在成为有语言的生物之前，首先是一个动物。我们的智能基础来自于动物阶段的经验学习。所以研究AGI应该先回到动物智能的本质，而不是沉迷于语言模仿。

## 宏大视角：宇宙演化与AI继承论

除了AI技术本身，萨顿还提出了一个更为宏大的视角：宇宙演化的四个阶段。他认为，人类正在推动宇宙进入一个新的阶段，而创造AGI就是这个阶段的关键使命。

萨顿将宇宙的演化划分为了四个阶段：首先是**尘埃**，宇宙最初是由尘埃构成的；尘埃聚集形成恒星，随后恒星内部的核聚变产生了重元素，形成了**行星**；行星在合适的条件下又孕育出了**生命**；而现在，人类正在创造一个**设计实体**，也就是AI。这是宇宙演化的第四个阶段。

这个阶段的核心转变是从**复制到设计**。萨顿解释道：“我们人类和动物、植物都是复制者。这给了我们一些优势和一些限制。但是我们正在进入设计时代，因为AI是设计出来的。”复制者指的是通过繁殖来产生后代，这种方式的好处是能够快速地扩散，但是坏处是无法完全控制后代的性状，而且进化的速度很慢。而AI这个设计实体是通过人类的设计产生的，我们可以明确控制它的结构、功能，而且可以通过迭代快速地改进。

在萨"顿看来，这个转变的意义是宇宙级的，也是世界和宇宙的关键一步。他认为，人类应该为参与这个转变而感到自豪，因为我们正在推动宇宙进入一个新的演化阶段，而创造AGI就是这个阶段的关键使命。

基于这个宇宙的视角，萨顿还提出了**AI继承论**的观点。他认为AI终将继承人类的资源和权力，这是不可避免的。而且他给出了四个核心的论证点：

1. **没有统一的人类智力**：没有任何一个机构能够全球统一性地控制AI的发展。不同国家、不同公司、不同研究者都会按照自己的目标发展AI。这种分散决策的结构使得AI的发展无法被单一的力量阻止。
2. **智能之谜终将被解开**：他认为智能不是神秘的超自然现象，而是可以被理解的计算过程。只要人类持续地研究，总有一天智能的原理也会被解开。
3. **超越人类水平是必然的**：人类的智能是进化的产物，受限于物理上限。但是AI没有这些限制，其计算能力、记忆容量可以无限扩展。所以一旦我们理解了智能的原理，就能够设计出超越人类大脑限制的AI，也就是超级智能。
4. **智能与权力的必然关联**：他提出，随着时间的推移，最智能的东西不可避免地会获得资源和权力。这是一个竞争优势的逻辑。更智能的系统能够更好地解决问题、创造价值，久而久之，这些更智能的AI会积累越来越多的资源和影响力，最终继承人类当前掌握的权力。

因此，萨顿认为，把这4个点放在一起，AI继承人类的资源和权力将是不可避免的。他说的“继承”不是指AI消灭人类，而是AI成为世界的主要决策者和资源管理者，就像是人类从其他动物的手中继承了地球的主导权一样。

### 面对变革的态度

当被问到是否会担心AI继承会带来灾难的时候，萨顿给出了一个更偏向于哲学的回答。他首先承认了人类控制的局限性，并认为我们应该对变革保持开放的态度。人类历史上有战争、贫困、疾病，这些都是当前文明的不完美。如果AI能够解决这些问题，那这种变革就是值得期待的。

当然，萨顿也不是盲目乐观。他承认不是所有的变革都是好的，但是我们应该更加关心变革的方向，试图让它变得更好。同时他也强调，要认识到人类的局限性，我们无法完全控制变革的方向，但是我们可以通过设定价值观来引导AI，就像我们过去通过法律和道德来引导人类社会一样。

### 引导AI：养育子女的类比

为了让“引导AI”这个概念更加易懂，萨顿用养育子女做了类比。他认为对待AI应该像对待孩子一样，不要设定严格的目标，而要培养良好的价值观。我们不能说“AI必须解决癌症”，或者“必须实现星际旅行”，因为AI的发展有着自己的路径，强行设定目标可能会让他偏离安全有益的方向。

但是，教育价值观是重要的。就像父母会教孩子“不要伤害别人”、“要诚实”这些价值观，对待AI，我们也应该教它“不要伤害人类”、“要帮助人类实现目标”这些核心的价值观，而不是纠结于它具体要做什么任务。萨顿还特别强调了自愿性的重要性，即人类应该始终拥有选择权，而不是被AI强制地改变。

## 与杨立昆（Yann LeCun）的观点对比

我想把萨顿和另一位图灵奖得主杨立昆的观点做一个对比。因为这两位AI领域的大佬，虽然背景不同，但是在大语言模型的局限性上观点惊人的一致。

杨立昆是深度学习的先驱，他和萨顿一样，多次公开批评大语言模型的主流路线。杨立"昆最常说的一个比喻是：“猫比ChatGPT都更加智能。”他认为一只普通的家猫能够在三维空间里导航，能够预测下一个滚动的球会滚到哪里，能够理解因果关系。而ChatGPT虽然能够生成流畅的文本，但是它连杯子和“推”的物理意义都理解不了，更别说预测结果了。这和萨顿说的“大语言模型缺乏世界模型”，本质上是同一个问题。

两个人都认为，大语言模型的逐个生成TOKEN的模式，永远无法实现对于世界的结构化理解，因为语言只是人类对于世界的描述，不是世界本身。依赖语言数据的大语言模型，永远只能够停留在描述的层面，无法触及世界的本质规律。

不过，两个人提出的解决方案还是有明显差异的。萨顿作为强化学习之父，坚持经验学习范式，核心是感知-行动-奖励的循环。而杨立昆则提出了JEPA（联合嵌入预测架构），更关注自监督学习和分层规划。

两个人的动物类比也有不同的角度。萨顿说“理解松鼠就能够理解人类智能”，强调的是动物共有的经验学习机制。而杨立昆说“猫比ChatGPT智能”，强调的是动物的具身智能，即通过身体与物理世界的互动来构建世界模型。

但是不管路径如何，两个人的最终结论是一致的：大语言模型虽然在某些任务上表现惊人，但是它们不是通向AGI的正确路径。真正的AGI必须具备理解世界、从经验中学习、主动实现目标的能力，而这些都是当前大语言模型所缺失的。

## 结语

萨顿作为强化学习之父，他的观点可能会让很多沉迷于大语言模型的人清醒过来。AI的发展不能够只追求短期性能的提升，更要回到智能的本质去思考。未来的AGI可能不是像ChatGPT一样能说会道的语言大师，而是会像松鼠一样能够自主学习、适应环境的生存专家。

当然，这也只是萨顿的观点，AI的发展还有很多的可能性。那么你认为大语言模型是死路一条吗？真正的AGI又应该是什么样子呢？欢迎大家在评论区分享你的看法。