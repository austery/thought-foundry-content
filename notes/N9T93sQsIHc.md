---
area: tech-engineering
category: ai-ml
companies_orgs: []
date: '2025-09-15'
draft: true
guest: ''
insight: ''
layout: post.njk
media_books:
- best-partners-tv
products_models: []
project:
- ai-impact-analysis
series: ''
source: https://www.youtube.com/watch?v=N9T93sQsIHc
speaker: Best Partners TV
status: evergreen
summary: Datology创始人Ari Morcos强调，在AI发展中数据是最被低估的变量。面对模型增长放缓，他提出通过极致的数据策展，让模型更高效学习，突破性能瓶颈，并详细阐述了其系统性工程和实践经验。
tags:
- data
- llm
- synthetic-datum
title: AI的尽头是更好的数据：Datology创始人解析数据策展的重要性
---

### 引言：AI发展中的数据困境与突破

过去两年，整个AI行业因**大模型**（Large Models: 指参数量巨大、拥有数十亿甚至万亿参数的深度学习模型，通常需要海量数据和算力进行训练）而狂踩油门，参数量从百亿推到万亿，算力预算也从几千万美金飙升到几十亿美金。然而，近期新出的模型能力增长逐渐放缓，简单粗暴地堆砌算力和数据带来的边际效益不断递减。许多人开始怀疑**大模型**的**Scaling Laws**（模型扩展法则: 指随着模型规模、数据量和计算资源的增加，模型性能呈现可预测的提升规律）是否已经失效，甚至担忧我们是否真的撞上了数据墙。

**DatologyAI**的创始人阿里·莫科斯（Ari Morcos）对此给出的答案是：数据是AI研究中影响最大，但投入却最少的领域。在Latent Space的播客节目中，他反复强调的核心观点是“模型吃进去什么，就会成为什么”。他认为，与其无休止地堆砌算力、陷入规模化收益递减的陷阱，还不如回归本源，通过极致的**数据策展**（Data Curation: 对数据进行系统化的收集、评估、组织、维护和管理，以确保其质量、可用性和价值），让模型能吃得更好，学得更聪明。

### Ari Morcos的学术背景与数据洞察

要理解阿里为何押注于数据，需要回顾他的学术经历。阿里的博士背景并非计算机，而是神经科学。他的博士课题是训练小鼠“数数”，然后记录成千上万个神经元的活动，尝试解释计数行为背后的动力学，并以此来研究智能的生物学基础。这段经历让他养成了用**经验科学**（Empirical Science: 通过观察和实验来获取知识，并以此来理解和改进系统的科学方法）来思考问题的习惯，即先通过实验去理解系统，再利用这种理解去改进系统。

为了处理这类高维数据，他开始学习并深入机器学习领域。2011年之后，AlexNet、DQN等里程碑接连出现，让他下定决心转向AI。带着**经验科学**的思维，阿里想要为深度学习建立一套“可解释的科学框架”，让大家不仅知道什么好用，更要理解为什么好用。因此，他理想中的论文前半段应该是解释原理，后半段再根据原理来改进模型。

然而，现实很快给他泼了一盆冷水。他发现弄清楚为什么有效其实并不算难，真正难的是用这种理解去把系统做得更好。这让他意识到，很多看似与性能相关的可解释指标，一旦直接去优化，往往优化的只是相关性，而非因果性。这种挫败感在2020年到达了顶峰，他也意识到数据才是其中的决定性因素。

### 归纳偏置的“苦涩教训”

当时，阿里研究的是**归纳偏置**（Inductive Bias: 模型在从未见过的数据上进行预测时所做的隐含假设或倾向，它指导模型从有限数据中泛化），即如何通过改动模型的架构或目标，把先验知识注入模型，让模型在小数据场景下能够学得更好。这也是当年的主流方向。

其中，他的一项工作是把**ViT**（Vision Transformer: 一种将Transformer架构应用于计算机视觉任务的模型）权重等效初始化成**CNN**（Convolutional Neural Network: 卷积神经网络，一种专门处理图像、视频等网格状数据的深度学习模型），这样既能拥有**CNN**的**归纳偏置**，又能在训练中逐步忘掉这种偏置。结果却耐人寻味：他发现，在小数据场景下，比如小于50万张图的时候，软性**归纳偏置**很有用。这类方法后来在火山预测等数据稀缺的科学任务上被频繁引用。但是一旦数据量上来之后，这种优势就逐渐消失了，尤其当样本超过百万数量级的时候，精心设计的偏置甚至开始拖后腿。

相比之下，**Transformer**（Transformer: 一种基于自注意力机制的深度学习模型架构，在自然语言处理和计算机视觉等领域取得巨大成功）也自带了较少的**归纳偏置**，却在超大数据上表现亮眼。于是阿里开始反思，他研究了六年的偏置，在**大数据**时代似乎并不怎么关键。这正是**苦涩的教训**（The Bitter Lesson: 指出在机器学习中，长期来看，通用的、能更好利用算力与数据的简单方法，最终会胜过依赖人类专家知识的复杂技巧）在他身上的真实写照：能更好利用算力与数据的通用方法，最终会胜过依赖人类专家知识的特定技巧。

他在访谈中说道：“这对我来说是一个非常痛苦的时刻，我花了六年职业生涯研究**归纳偏置**，但是好几篇论文同时告诉我，我一直在做的事情，其实没那么重要。”冷静之后，他的眼前只剩下两条路：要么把GPU转得更快，要么去研究数据。对于一名非硬件工程师而言，答案自然是后者。另外，数据研究最吸引他的一点在于，科学上有趣的问题往往和实践中有用的问题会高度重合。例如，理解一个数据点为什么有用，几乎可以立刻指导数据集的改造，从而提升模型的性能。这种看上去的知行合一，让他选择把职业筹码压在了数据上。

### 数据研究的长期投入不足与时代背景变化

在进入这个研究领域之后，阿里发现了一个残酷的事实：相对于数据的影响力来说，数据在AI研究中其实是长期投入不足的。他也看到，这背后其实是科研文化、激励机制和历史惯性所共同作用的结果。

首先是文化偏见，数据工作往往被视作二等工作，既是脏活累活苦活，也被看做是所谓的管道工程，缺乏顶级科学家所追求的荣耀感。所以很多人觉得数据清洗是一种无聊的、重复性的劳动。但是其实不少一线研究者也会承认，要想做出好的结果，第一件事就是把数据搞明白。模型终究只是它所见过的数据的镜像，只是这种琐碎又关键的工作，在文化上并没有得到应有的尊重。

其次是研究激励的错位，长期以来机器学习的范式都是给定一个数据集，比如ImageNet或者Kaggle，然后去优化测试集的表现。在这种逻辑下，数据往往被当作一种常量，创新也自然集中在模型与算法两个方面。

最后则是时代背景的变化。2019年前，业内的主流是**监督学习**（Supervised Learning: 利用带有标签的数据进行训练，模型学习从输入到输出的映射），那是一个数据稀缺的时代。有标签的数据成本高，而且因为有人类的参与，质量至少有下限。但是，自从**自监督学习**（Self-supervised Learning: 模型通过数据本身生成监督信号进行学习，无需人工标注，从而利用大规模无标签数据）崛起之后，游戏规则被彻底改写了。无论是语言里的预测下一个词，还是视觉里的对比学习，模型都开始从无标注的数据里进行自我学习了。随之而来的，就是数据规模从百万级跃迁到万亿级，增长了百万倍。

这也导致AI的核心矛盾从数据稀缺开始转为数据过多，并带来了一系列后果。比如由于数据多到模型永远学不完，导致模型更容易欠拟合；抓取来的互联网数据充满了冗余、低质甚至有害的信息，导致数据的质量下限消失。还有就是，过去的**Scaling Laws**都是基于一个理想化的假设，那就是所有的数据都是**独立同分布**（Independent and Identically Distributed, I.I.D.: 统计学假设，指数据集中的每个样本都是独立地从同一个概率分布中抽取的），也就是假设所有数据点的价值都是相等的。但是这一点已经开始失效了。

于是，当我们来到数据过多的时代后，如何去除冗余数据，又该如何度量信息的增益，这些过去不显眼的问题就成了头号难题。显然，我们对于模型“垃圾进、垃圾出”（Garbage in, Garbage out）的朴素常识理解，与前面这些假设是不兼容的。因此，**数据策展**的重要性被推到了一个前所未有的高度。

### 数据策展的系统工程与关键概念

阿里提到，很多人会把数据工作等同于数据的筛选或者清洗，但在他看来，这些工作只是冰山一角。真正的**数据策展**其实是一个更系统的工程，它涉及到多个环节的步骤：

1.  **过滤（Filtering）**：识别并剔除低质量的、低信息增益的数据。
2.  **重均衡（Rebalancing）**：由于现实中的数据经常会呈现长尾形态，需要对它进行上/下采样，让模型学到完整的分布而不是只学到头部的模式。
3.  **序列化（Sequencing）**：喂给模型的先后次序很关键，这让**课程学习**（Curriculum Learning: 训练模型时，从简单到复杂逐步提供训练样本或任务，模拟人类学习过程）重焕生机。在一个永远欠拟合的时代，合理的编排顺序能够让模型的开发人员用更少算力达成同等的效果。
4.  **合成数据（Synthetic Data）**的使用：考虑如何用模型来生成高质量的合成样本，从而增强原始数据集。
5.  **批处理（Batching）**：如何组织和处理批次，同样会影响学习的速度。

此外，对于**数据策展**，阿里认为要重点理解冗余和自动化两个概念。

首先是**数据冗余**。在**数据策展**的几个环节中，过滤的核心挑战之一就在于如何处理冗余。完全去除冗余显然是错误的，因为它会损害模型的泛化能力；但是无限的冗余，同样也是一种灾难。阿里用了一个形象的比喻：大象和狗。大象的形态差异较少，主要分为亚洲象与非洲象，让模型学会大象的概念不需要海量的样本，太多重复也只会是浪费。但是狗则完全不同，狗有数百个品种，每个品种之间的体型、毛色、形态都差异巨大，要让模型真正理解狗这个概念，所需要的数据量和冗余度显然要远高于理解大象的概念。而一个优秀的**数据策展**系统，需要在无监督的条件下，自动发现成千上万个类似大象和狗的概念，评估各自的复杂度，再决定每个概念应该保留多少冗余。

这对于人类来说是一个很难胜任的任务。有人可能会说，请一些专家让他们挑出来不就好了吗？阿里却认为，这套复杂的系统必须是自动化的，甚至要刻意排除人的干预。他引用了斯坦福的DCLM（DataComp for Language Models）项目作为例子。在这个项目中，大约30位顶尖博士生用两年的时间搭建了一个自动筛选高质量网页文本的系统。最后做了个测试，让这些刚研究完筛选策略的专家去预测系统会保留还是剔除掉某条样本，测试的结果是这些专家的预测准确率和随机猜测根本没有什么区别。

为什么会这样呢？阿里的解释是，一个数据点的价值其实并不是由它本身决定的，而是由它与训练集中所有其他数据点的关系所决定的。举例来说，就算有一万篇《哈姆雷特》的剧情摘要，每篇的质量都很高，但是模型真的需要一万篇吗？人类无法在脑中装着整个数据集来进行这种全局的权衡判断，但是算法可以，机器可以。因此，**数据策展**必须是自动化的，不仅因为规模，更因为人类在对这类问题的判断上并不可靠。

### 合成数据：范式、争议与实践经验

除了过滤以外，在**数据策展**的技术环节里，**合成数据**（Synthetic Data: 通过算法或模型生成的人造数据，旨在模拟真实数据的特征和统计属性）也是眼下最热门的方向之一，但也充满了争议，尤其是关于**模型坍塌**（Model Collapse: 指在模型通过合成数据进行训练时，生成数据多样性逐渐丧失，模型最终只输出有限的、重复的内容，导致性能下降）的担忧。

为了解释这部分的原因，阿里将**合成数据**分为了两种截然不同的范式：

1.  **“从无到有”**：这种方法能让一个**大模型**凭空生成新的知识，但是这是危险的，因为它极容易导致**模型坍塌**。生成模型会倾向于过拟合数据分布的**众数**（Modes: 数据分布中出现频率最高的数值或区间），而欠拟合**长尾**（Tails: 数据分布中频率较低、位于两端的极端值）。如果用它生成的数据再进行训练，会加剧导致多样性的不断丧失，最终模型只会输出千篇一律的内容。
2.  **“转述或重写”**：这种方法更为安全。它的核心思想是知识来源于原始数据，而非生成模型。模型扮演的角色仅仅是将原始数据中的信息用一种更清晰、更结构化、或更符合下游任务的形式重新组织一遍。因此，对于一个模型来说，它只需要知道如何转述就行了，甚至不需要理解内容本身是什么。这意味着可以用一个相对较弱的模型去生成能教会一个更强模型的数据。这个观念打破了传统**知识蒸馏**（Knowledge Distillation: 将一个大型、复杂的“教师模型”的知识迁移到一个小型、简单的“学生模型”中，使学生模型在保持较小规模的同时，性能接近教师模型）中学生无法超越教师的天花板，因为知识的源头是高质量的原始数据，而非教师模型。

Datology AI近期的Beyond Web论文就系统总结了他们在**合成数据**上的七点体会：

1.  **合成数据**并不等同于**知识蒸馏**。简单摘要虽然也可以把单位token的信息密度提上去，做出类似用生成来驱动数据集的效果，但是精心设计的改写策略通常能够走得更远。
2.  要想真正“破墙”，得靠好的数据。如果只让模型续写网络上的文本，相当于重复的数据，收益很有限。真正能够突破质量瓶颈的，是那些可以填补原始分布空白的合成样本。
3.  高质量的种子很重要，但不是全部。以好的数据为源头，改写后的质量显著会更好，但是只靠好的源头还不够，还要有合适的策略组合。
4.  风格匹配虽然有用，但是天花板很快出现。互联网上的对话体不到5%，可对话是主要的交互方式。因此，适当提高对话比例确实有一定的收益，但是这个收益也会迅速饱和。
5.  数据的多样性决定了持续收益。单一的改写，比如把内容全部都转成问答形式，在训练的早期可能有效，但是很快会出现瓶颈；只有采用多样的策略，万亿token的长程训练才能够持续提升性能。
6.  改写模型本身的影响不大。即使用不同系列的模型，比如Llama、Mistral、OLMo来当改写器，产出的合成质量也相差不多；而且改写器自身的性能强弱与它产出数据的最终价值也并不是线性相关的。
7.  小模型其实也够用。改写器从1B升到3B时的收益明显，但是从3B升到8B则会趋近于饱和。也就是说，做高质量的**合成数据**不一定需要参数规模很大的模型，从而显著降低了门槛。

基于这些经验，BeyondWeb在8B模型实验中给出了亮眼的结果：用BeyondWeb数据来进行训练，速度比普通的网络数据快了7.7倍；甚至在BeyondWeb上训练的3B模型，表现能够超过在其他数据集上训练的8B模型。这无疑展示了**数据策展**的杠杆效应。

### Datology AI的价值主张：更快、更好、更小

回到自己的公司Datology，阿里用三个词来概括了DatologyAI对客户的价值：更快、更好、更小（Faster/Better/Smaller）。

首先，Datology的训练更快。这不只是节省了几百万美元的单次训练费用，更关键的是迭代速度的提升。原本需要10天的训练，如今可能一夜就能完成，这样也让实验的迭代次数可以随之呈指数级上升。

其次，Datology训练出来的模型性能更好。因为好的数据就如同算力的倍增器，在同样的预算下，用经过**数据策展**的数据，效果会更好。所以对于想用一千万的预算做出过去一亿预算水平的团队，这点最具吸引力。

最后，Datology训练出来的模型更小。对于走在AI落地前沿的企业而言，推理的成本是**TCO**（Total Cost of Ownership: 总拥有成本，指在产品或服务的整个生命周期中，除了购买价格之外，还包括运营、维护、升级、报废等所有相关成本）的大头。所以一个“参数减半但是效果不减”的**专用小模型**（Specialized Small Model: 针对特定任务或领域进行优化和训练的小型模型，通常参数量远小于通用大模型，但在其专业领域表现出色且成本更低）相比通用**大模型**来说会更具商业上的优势。举个例子，如果一年在推理上花5000万美元，但是部署的模型比所需要的大了一倍，那就等于白烧了2500万。而重新训练一个同等性能的**专用小模型**可能只需要两三百万，这笔账显然很好算。更何况，很多企业需要的不是一个能写诗、能聊天的通用**大模型**，而是一个一英寸宽、一英里深的专家模型，这样它才能以99.999%的可靠性，用尽可能低的成本去完成那一小撮的核心任务。

阿里更是指出，过去阻碍企业训练自己模型的有两大障碍，分别是训练的基础设施和数据。而如今，像MosaicML、Together AI等公司已经大大降低了训练的门槛。因此，Datology的使命就是推倒另一座大山，也就是数据的屏障。他还举了一个具体的案例，那就是他们与RC基础模型的合作。他们从25万亿token的原始池子起步，经过**数据策展**，筛选到7万亿token的高质量集合。结果不仅模型的性能变得更强了，达到同等水平的训练速度也得到了显著的提升。这说明**数据策展**的收益是可以叠加的，即便从当下最好的开源数据出发，依然能够进一步挖出可观的增量。

### 结语：数据策展时代的到来

从Datology公司的名字上就可以看出阿里他们的野心：Datology就等于Data加上Ology，相当于专注于**数据策展**这门学问上。而阿里的愿景，也正是把这门新的学科自动化、工具化，让曾经只在顶级实验室里口口相传的数据秘笈，变成任何想训练自有模型的团队都能够触手可及的基础设施。

我们不妨也换个角度来想想，当行业还在模型和算力上疯狂内卷的时候，真正能够改写游戏规则的，或许正是对数据的重新认识。无论如何，AI的尽头未必是更大的模型，但一定是更好的数据。一个属于**数据策展**的时代，也许正在到来。