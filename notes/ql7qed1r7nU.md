---
area: tech-engineering
category: ai-ml
companies_orgs: []
date: '2025-08-24'
draft: true
guest: ''
insight: ''
layout: post.njk
products_models: []
project:
- ai-impact-analysis
series: ''
source: https://www.youtube.com/watch?v=ql7qed1r7nU
speaker: Best Partners TV
status: evergreen
summary: 回顾吴恩达在谷歌大脑的开创性工作，探讨“规模决定性能”和“单一学习算法”两大基石，以及他对AI未来、教育和工作模式的深刻洞察。
tags:
- brain
- education
title: 吴恩达：从质疑到突破——谷歌大脑的开创性旅程与AI的未来展望
---

### 吴恩达访谈回顾：谷歌大脑的诞生与AI的未来

本期内容将回顾吴恩达教授在The Moonshot播客的专访，围绕其学术生涯起点、**Google Brain**（Google Brain: 谷歌内部的一个人工智能研究团队，专注于深度学习和神经网络）项目的创立与发展，以及**人工智能**（Artificial Intelligence, AI: 模拟人类智能的理论和技术）的未来趋势展开。访谈深入探讨了Google Brain赖以成功的两大颠覆性基石——“规模至上”与“单一学习算法”的提出与论证，以及这些观点在当时学术界主流范式下所遭遇的巨大阻力与争议。此外，吴恩达还谈及了Google Brain团队的关键合作、硬件选择的曲折之路、早期应用场景的开拓，以及他对AI未来、教育和工作的看法，展现了那段充满挑战与突破的岁月。

### 早期探索与核心理念的萌芽

故事从吴恩达的博士时期说起。当时，**强化学习**（Reinforcement Learning: 一种机器学习方法，让智能体通过与环境互动学习如何做出决策以最大化累积奖励）领域还相当冷门，但他却用一个小型**神经网络**（Neural Network: 一种模仿人脑神经元连接方式的计算模型，用于模式识别和复杂问题处理）成功控制了一架无人直升机保持悬停，这在当时是一项重大突破。正是这个实验，让他对神经网络有了更深刻的认识，也为他后来提出的“规模决定性能”和“单一学习算法”观点奠定了基础。

### “规模决定性能”：数据驱动的坚持

然而，在当时，这些观点可没少受到质疑。大约在2010年，当吴恩达在学术会议上与人们谈论扩展深度学习算法的必要性时，一些非常资深的学者建议他应该去发明新的算法，而不是构建更大的神经网络。就连他很尊敬的前辈**约书亚·本吉奥**（Yoshua Bengio: 加拿大计算机科学家，深度学习领域的先驱之一）也曾告诫他，这样做对他的职业生涯没有好处。

但吴恩达并没有被这些声音所动摇，因为他手里握着一件“秘密武器”——数据。他的学生亚当·科茨（Adam Coates）和洪拉克·李（Honglak Lee）绘制了一张图表，横轴是模型的规模，纵轴是模型的性能。他们测试了大量不同的模型，结果发现，所研究的每一种模型，其性能曲线都坚定地一路向右上角攀升。基于这些数据，吴恩达确信：构建的模型越大，它的性能就会越好。他知道，作为一名科学家，不能光靠民意调查来做研究，虽然听取他人意见固然重要，但最终必须有自己坚信不疑的假设，而他的假设正是由这些数据所塑造的。

### “单一学习算法”：通用智能的愿景

除了“规模决定性能”的假设以外，“单一学习算法”的提出也很有启发性。吴恩达当时深受**神经重塑**（Neural Plasticity: 大脑在经验影响下改变自身结构和功能的能力）实验的启发。这些实验表明，如果大脑的某个区域受损，其他的区域可以重新学习并接管受损区域的功能，比如用处理听觉的大脑区域去学习处理视觉。这让他开始思考，我们真的需要为视觉、听觉等不同的任务开发完全不同的算法吗？还是说，可能存在一种通用的学习算法，给它什么样的数据，无论是文本、图像还是音频，它都能够学会处理呢？

现在看来，“单一学习算法”这个提法可能过于简单，甚至在某种程度上是错误的。吴恩达也认为，当时过分强调了从神经科学中寻找直接灵感的必要性，来自神经科学的具体细节在大多数情况下其实并没有太大帮助。尽管如此，那个更高层面的想法——也就是人类大脑可以依赖一种通用算法来处理多种任务——依然极具启发性。它引导着吴恩达去思考，与其让一万名研究者去发明一千种不同的算法，不如集中一小部分人去发明一种足够强大的算法，然后用各种数据去训练它。事实证明，这条路走对了。不过，在当时，这个想法简直就是异端邪说。吴恩达记得有一次在国家科学基金会的研讨会上，他讨论了“单一学习算法”的假说，当时他还很年轻，在演讲中调侃了计算机视觉领域传统的手工工程方法，一位非常资深的计算机视觉研究者当场站起来对他大声斥责。作为一个年轻教授，那次经历确实有点打击人，但是多年以后，他可以笑着回顾这一切，因为结果证明他的方向是正确的。

### 谷歌X实验室的机遇与拉里·佩奇的支持

带着这些想法，吴恩达开始寻找能够将它付诸实践的地方。而当时谷歌旗下的创新实验室**X**（Google X Lab: 谷歌旗下的一个秘密研发机构，专注于“登月计划”式的突破性创新项目）成为了那个关键的平台。这背后，德国计算机科学家、前**Udacity**（Udacity: 一个提供在线教育课程的平台）的CEO塞巴斯蒂安·特龙（Sebastian Thrun）起到了重要的作用。当时在斯坦福，吴恩达和塞巴斯蒂安的办公室就隔着一堵墙。

吴恩达在斯坦福的学生已经通过实验证明，构建的神经网络越大，学习系统的性能就越好。他感觉自己手里握着一份“秘密”数据，其实说它秘密也不尽然，因为他到处宣讲，但是人们就是不相信他，这或许反而是件好事。塞巴斯蒂安听了吴恩达的想法之后对他说：“谷歌有的是计算机，你为什么不直接去向谷歌推介这个想法，让他们把谷歌海量的计算资源给你用，去构建一个前所未有的大型神经网络呢？”

于是，塞巴斯蒂安为吴恩达安排了一次向**拉里·佩奇**（Larry Page: Google的联合创始人之一）推介的机会。吴恩达记得自己用笔记本电脑精心准备了幻灯片，带齐了所有的材料，但是他们是在一家日式餐厅见的面，那种环境下实在不方便拿出笔记本电脑。所以最后，他只是和塞巴斯蒂安一起，跟拉里·佩奇口头聊了聊他的想法。幸运的是，拉里·佩奇当场就接受了他的观点，并且授权了他与塞巴斯蒂安以及X实验室的合作，推进这个后来成为谷歌Brain的项目。那顿晚餐，吴恩达至今记忆犹新，对他而言，那是一次决定成败的谈话。他至今仍然非常感谢拉里·佩奇愿意相信他那个在当时听起来非常疯狂的愿景。

### 杰夫·迪恩的加入与伙伴关系

进入X之后，谷歌Brain项目正式启动。而**杰夫·迪恩**（Jeff Dean: 谷歌人工智能领域顶尖专家，谷歌AI负责人）的加入，被吴恩达视为天大的幸运。在拉里·佩奇的指导下，当吴恩达和塞巴斯蒂安准备启动这个项目的时候，拉里·佩奇让吴恩达去和谷歌内部的许多人交流。吴恩达和杰夫·迪恩、格雷格·科拉多（Greg Corrado）、汤姆·迪恩（Tom Dean）、杰伊·亚格尼克（Jay Yagnik）等许多人都聊过。他向杰夫·迪恩展示了自己的想法：如果能用上更大规模的神经网络，一切都会变得更好。这个想法让杰夫·迪恩兴奋不已。随着项目的推进，所有参与者都明白，如果能让杰夫·迪恩更深入地参与进来，他将发挥不可估量的价值。幸运的是，杰夫·迪恩对他们的工作抱有极大的热情。

随着时间的推移，尤其是在他深度参与之后，他们自然而然地形成了分工：杰夫·迪恩成为了系统专家，他构建了谷歌大部分的基础设施，并且对规模化有着深刻的理解；而吴恩达则专注于机器学习的算法。这种伙伴关系被证明是无价的。吴恩达在机器学习方面的专业知识，加上杰夫·迪恩对计算机系统的了解，让他们能够有效地利用谷歌的基础设施，扩展机器学习算法，最终交付了一个真实而且有影响力的成果。

杰夫·迪恩为谷歌乃至世界带来的一项重要贡献，就是解决了一个极其困难的问题：如何在全世界所有潜在的搜索信息中找到用户想要的东西，并且在毫秒内返回结果。这意味着必须将问题分解，而这种“分解问题再重组结果”的模式，与他们在训练日益庞大的神经网络时所做的工作非常相似。杰夫·迪恩发明了后来大名鼎鼎的**MapReduce**（MapReduce: 一种编程模型，用于处理和生成大数据集的并行分布式算法）技术，它的核心就是将工作分解，分配到许多计算机上进行并行执行，然后再将结果汇总。这是他们早期训练模型的第一个版本，后来随着不断迭代，最终才催生了**TensorFlow**（TensorFlow: 一个由Google开发的开源机器学习框架）等技术。

### GPU/TPU的曲折之路与应用场景的开拓

然而，在谷歌内部，拥抱**GPU**（Graphics Processing Unit: 图形处理器，最初用于处理图像，后广泛用于并行计算和深度学习）的过程却花了更长的时间。吴恩达回忆，如果说谷歌Brain有什么遗憾，那就是他希望能够更早地拥抱GPU，或者说更早地启动**TPU**（Tensor Processing Unit: 谷歌为机器学习工作负载设计的专用集成电路）的决策。其实他们很早就看到了GPU的潜力，比如当时就有一两台GPU服务器，但是吴恩达至今仍然能够想起它被放在某人的办公桌下，周围缠绕着一堆电线的场景。通过那台计算机，他们意识到了GPU的巨大作用。但是从谷歌整体基础设施的角度来看，存在一个当时看来也很合理的顾虑，那就是当时谷歌的计算基础设施非常统一，任何人写的代码几乎都可以在任何地方无缝运行。然而，GPU是一种截然不同的硬件，这意味着程序员需要做额外的工作来适配它。他们当时还在考虑，购买大量的GPU是否对其他任务，比如YouTube的视频转码也有利，以及它们除了训练AI模型以外，是否还有其他的用途。由于这些顾虑，他们在谷歌内部推进GPU的步伐有所放缓，没有像吴恩达当时可能希望的那样积极。最终，吴恩达在斯坦福大学的团队率先用GPU进行了演示性工作。后来随着谷歌Brain团队更多地转向GPU，并且开始构建TPU，效果之好，可以说是有目共睹。

在谷歌Brain早期的时候，面对像翻译、语音转文本或者图像识别等众多的领域，他们又是如何挑选重点方向的呢？吴恩达刚加入X时，最早做的事情之一就是在谷歌内部开设了一门关于神经网络的课程。他记得当时和汤姆·迪恩以及格雷格·科拉多密切合作，这门课非常成功，有近百人参加。他们每周开会，分享吴恩达关于神经网络和规模化的“非主流”想法，并且讨论在谷歌大脑的工作。幸运的是，这帮助他们在谷歌内部结识了许多朋友，找到了许多盟友。

他们最早合作的团队之一是语音团队，原因有二。首先，他们认为规模化在改进语音识别方面有巨大的潜力。当时，语音搜索还不像现在这么成熟，但是与手机应用对话，用声音在谷歌上搜索这个想法确实非常激动人心。所以他们想要提高语音转录的准确性。另外，当时的语音团队已经开始研究神经网络了，他们觉得通过帮助语音团队实现规模化，可以显著提升谷歌的语音识别能力。所以，这在一定程度上是顺势而为，关键在于谁愿意与他们合作，以及他们认为可以和谁一起共同推动“规模化”的这个愿景。

除了语音识别以外，吴恩达还参与过谷歌的街景项目。当时，他们利用计算机视觉分析街景图像，读取门牌号码，以便更精确地在谷歌地图上定位房屋。事实证明，那个项目在当时的影响力甚至超过了语音识别。他还记得讨论过如何帮助广告业务，早期人们对于赋能网络搜索业务是持怀疑态度的，他当时费了九牛二虎之力才说服了网络搜索团队。幸运的是，广告团队对此要开放得多。此外，当时马里奥·凯罗斯（Mario Queiroz）的团队在YouTube上也加入了人工智能，在基于内容为视频打标签以及内容审核方面，做了许多非常出色的工作。由于吴恩达开设的那门有近百名谷歌员工参与的课程，许多不同的应用团队都表现出了浓厚的兴趣。有时也会有人想要加入他们，但是他们无法提供全职的岗位，他们就会说：“那我们先合作吧。”也正是这种方式促成了大量的合作。

### 谷歌Brain的“毕业”与转型

从吴恩达开始在X工作，到谷歌Brain从X毕业并且迁入谷歌的核心部门，大概不到两年的时间。对于这次“毕业”，吴恩达的感觉是复杂的。对他来说，X实验室过去是，现在依然是一个非常特别的地方。他记得在X的办公楼里工作，感觉妙不可言。比如，离他十英尺远的，就是当时的自动驾驶团队，也就是现在的**Waymo**（Waymo: 谷歌旗下的自动驾驶汽车技术公司）；旁边是研究热气球的团队；还有研究语言的团队。所有这些团队都在离他办公桌几步之遥的地方做着各种大胆、前沿、激动人心的探索。所以，尽管从X“毕业”被视为一次成长和迈向新阶段的标志，但是他认为，最终迁入谷歌核心部门，更贴近业务并且获得更多资源绝非坏事，他也没有任何的遗憾。当然，离开X那幢充满疯狂创意、每天都有新奇事物涌现的办公楼，确实有些伤感。

搬到谷歌后，情况发生了一些变化。他们变得更加专注于神经网络和规模化，减少了与形形色色的人闲聊的时间，也错过了体验更多早期原型的机会。可以说他们变得更加“企业化”了，但是这里绝非贬义，因为对于谷歌Brain团队而言，与更多的谷歌业务部门紧密联系显然是有很多好处的，你只需要步行一分钟，就能与那些正在构建重要应用的团队交流，并且寻求合作。

### 领导权交接与Transformer架构的崛起

后来，吴恩达逐渐将重心从谷歌Brain转移到了**Coursera**（Coursera: 一个大型开放式在线课程平台）的日常运营上。最初，他和联合创始人达芙妮（Daphne Koller）在斯坦福大学教授一门机器学习课程。由于谷歌Brain进展顺利，而且他相信可以将团队领导权交给出色的搭档杰夫·迪恩，他觉得自己已经准备好迎接新的挑战了。相比之下，Coursera当时仍然处于非常早期的阶段，更需要他的日常领导。他和杰夫·迪恩谈过之后，花了近一年的时间逐步将领导权移交给了他。幸运的是，这次交接也非常的顺利。

在离开X之后，**Transformer架构**（Transformer Architecture: 一种基于自注意力机制的深度学习模型架构，广泛应用于自然语言处理任务）才被正式发明和发表。吴恩达认为，Transformer论文的绝妙之处在于，它的作者们是在谷歌Brain强调“规模化”的传统中成长起来的。因此，Transformer网络架构的许多决策，都是为了设计一个能够在GPU上高效扩展的神经网络。比如，**注意力机制**（Attention Mechanism: 一种神经网络技术，允许模型在处理序列数据时动态地关注输入序列中最重要的部分），它是一种非常巧妙的方式，让神经网络能够决定要关注句子中的哪些部分。

在Transformer论文发表之前，像**循环神经网络RNN**（Recurrent Neural Network, RNN: 一种特殊的神经网络，善于处理序列数据，如时间序列或自然语言）这些主流的算法，它们在处理翻译任务的时候，比如将一句英文翻译成法文，会先读完整个英文句子，试图将其中的全部信息记在脑中，然后再生成法文翻译。这种方式效果虽然还可以，但是难度很大，毕竟记住一个长句子是相当困难的。而Transformer论文提出了一种创新的架构，它会保留完整的英文句子，然后在生成法语句子的时候，根据当前生成单词的位置，将“注意力”动态地聚焦到英文句子中最相关的部分。事实证明，这种能够同时审视整个英文输入和整个法文输出，并且在处理过程中动态决定关注点的机制，需要巨大的计算量。但是正因为它在GPU和TPU这样的并行硬件上具有极佳的扩展性，所以效果斐然，后来也成为了现代基础模型的基石。我们不再只是从英文翻译到法文，而是将用户的提示词“翻译”成他们所提出的问题的答案。而Transformer论文之所以如此出色并且获得巨大成功，很大程度上是因为作者们非常巧妙地设计了神经网络架构，确保每一步都是高度的并行化，能够在GPU上高效地运行。这为模型在海量数据上进行训练奠定了坚实的计算基础，也让它在性能上取得了卓越的突破。

### “猫咪视频”的里程碑与X文化的启示

谷歌Brain项目还有一个里程碑时刻被人们津津乐道，那就是“猫咪视频”的突破。他们构建了一个当时可能是世界上最大的神经网络，让它观看海量的无标签的YouTube视频，从中自主学习。吴恩达记得他的博士生（Quoc Le）有一天把他叫过去说：“嘿，吴恩达，快来看看我的电脑。”吴恩达走过去，看到了一张模糊的、黑白且略带幽灵感的猫脸图片。这是算法在观看了无数YouTube视频后自主“发现”的，因为YouTube上恰好有很多的猫咪视频。一个算法在没有任何人告知它“猫”为何物的情况下，仅仅通过观察海量的数据，就自主识别出了猫脸的概念。那真是一个了不起的突破时刻，震撼地展示了AI从原始数据中自主学习的巨大潜力。

回顾在X的经历，吴恩达认为还有很多宝贵的经验。比如在塞巴斯蒂安领导的早期X团队中，有一种非常宝贵且罕见的特质，那就是思想的交叉传播。他记得有一次Waymo团队的成员来找他，说道：“吴恩达，想不想坐坐我们的无人驾驶汽车？”他欣然前往，在山景城市中心体验了一辆早期的Waymo原型车。这种开放性、思想的自由交流以及勇于尝试新奇事物的意愿，是极其罕见和珍贵的。他非常喜欢当时团队里那种“我们正在做有意义的工作”的氛围，大家来这里不是为了做些无聊的事。他记得拉里·佩奇过去常常问大家：“如果你正在做的事取得了超乎想象的成功，会有人在乎吗？”言下之意是，去做那些能让你给出肯定回答的事情。这种感觉非常好，在X的任何角落，人们都抱着一个信念：永远不要做那些即使成功了也无足轻重的事情。

还有一个吴恩达个人非常看重的东西，那就是速度。创新之初，几乎可以肯定自己并不清楚方向，因此，快速执行、快速试错的能力是成功的关键。他发现，面试时几乎所有候选人都会说自己行动迅速，但是实际执行速度的差异是巨大的，能够达到10倍，甚至100倍。他见过有的领导者能在15分钟的谈话中做出决定，也见过另一些领导者面对同样情况会说：“很好，我们先花三个月研究一下，到时再碰头。”这种执行速度的差异令人震惊。创新的悖论在于，像谷歌这样的大公司，绝对不能让一个工程师随意尝试可能导致主搜索瘫痪的操作。但是在X，他们创造了一个安全的环境，可以在谷歌Brain上随心所欲地探索，即使没有权限，也绝对不会有意外搞垮谷歌搜索的风险。这种环境让他们能够快速行动、大胆尝试。执行速度与**沙盒式**（Sandbox: 在计算机安全中，指一种隔离的运行环境，用于测试不受信任的程序，防止其对系统造成损害）安全防护的结合，确保没人能够危及“母舰”的安全。这是一种极难实现的平衡，而吴恩达相信X做到了。主持人也认为，缩短学习的周期至关重要，我们不应该在乎花了多久才取得伟大的成就，或者发现自己走错了路，应该关心的是从提出假设到获得可评估结果之间的时间。如果这个周期是一个小时，而不是一个月，那么这两种情况简直是天壤之别。

### AI的未来展望：应用、教育与普惠

聊完了谷歌大脑的发展历程，我们再来看看吴恩达现在的工作，以及他对AI未来的看法。如今，他将大量时间投入在经营**AI Fund**（AI Fund: 吴恩达创立的风险投资工作室，专注于孵化人工智能领域的创业公司）上。这是一个风险投资工作室，他们平均每月孵化一家新的创业公司，其中也包含了当年从X的运作模式中学到的一些早期经验。同时，他通过deeplearning.ai和Coursera继续从事大量的人工智能教育工作。

他认为人工智能的前景极其广阔，像谷歌这样的公司在训练基础模型方面做得非常出色，最新版的**Gemini**（Gemini: 谷歌开发的一系列多模态人工智能模型）就充分展示了团队的卓越工作。他对未来将建立在这些基础模型之上的应用数量感到无比的兴奋。他渴望投身到这样一个环境，每天都有无数绝佳的应用场景，有着明确的市场需求，能够改善人们的生活，却从来没有人去着手构建。这让他备受鼓舞。

在AI Fund，从一个想法到成立一家公司，大约需要六个月的时候，其中一半的时间都会花在招聘CEO上。一旦CEO就位，他们会和AI Fund一起工作三个月。这三个月后，成功率大约是75%，另外25%的情况是他们或者CEO决定不再继续。基本上，只要CEO和他们共事满三个月，他们就会启动公司。

吴恩达认为，人工智能领域的一大变化是原型验证的成本已经急剧下降。如果你有一个想法，现在构建原型、吸引用户来验证或者推翻它的成本极低。即使想法被证伪，那也很好，可能只损失了几天时间和几千美元。这种转变正在真正的加速创新，尤其是在应用层面。这与基础模型层形成了鲜明对比，后者仍然需要动辄数十亿美元的预算和庞大的数据中心基础设施才能实现。在主持人看来，这就像电力与晶体管的区别，它们是20世纪末计算机行业和互联网基础设施的基石，都具有深远的赋能作用，但是必须在它们之上构建成千上万的东西才能真正实现它们的价值。同样，基础模型、机器学习以及如今向全世界开放的大模型，就像电力、晶体管一样，它带来了难以置信的可能性，但是必须用它来做点什么。

吴恩达也认同这一点。他说，如果回顾美国的电气化进程，就会发现建造发电厂是一项宏伟而伟大的事业，许多人投身其中并且取得了巨大的成功。但是如果你再看看消费电子行业，或是是利用电力制造出的产品，那个产业的规模要比发电行业大得多。他认为人工智能也将如此，构建人工智能模型本身将是一个巨大的产业，但是它的规模远不及我们利用AI构建海量应用所共同创造的价值。

除了对AI产业的展望，吴恩达对教育也同样充满热情。这源于他从小父母的教诲：重要的不是个人，而是如何帮助他人成功。后来在斯坦福大学，他记得教机器学习课程的时候，年复一年地走进同一间教室，讲授同样的课程，甚至说着同样的笑话。过了一段时间，他开始反思，就帮助学生成功而言，这真的是对他的时间最高效的利用吗？于是，在几年的时间里，他尝试录制视频并且免费发布到网上，任何人都可以观看。他还尝试了自动评分测验之类的工具，并且从**可汗学院**（Khan Academy: 一个非营利性教育组织，提供免费在线课程和练习）的经验中学到，应该制作更短的视频。事实上，在Coursera一炮而红之前，其实有过五个可能不为人知的版本，其中一些可能只有20个用户。但是正是这些尝试，让他学到了如何构建可扩展的在线教育平台的宝贵经验。当这项工作取得成功后，他觉得自己有机会将教育的触角延伸到极为庞大的受众，于是邀请了联合创始人，一同开启了Coursera的征程。

对于未来十年人工智能和机器学习领域可能会发生的变化，吴恩达也有着自己的见解。他热切期盼的一件事，就是让每个人都能学会编程，或者说掌握这种由人工智能辅助的新型的编程方式。因为他不仅在自己的职业生涯中写了大量的代码，也在个人生活中为孩子们编写应用程序。就在一周前，他还为女儿写了一个打印乘法口诀的小应用，他只花了不到一天的时间就构建出了一个新原型，可以在手机上调用并且进行互动。这个过去需要几周甚至几个月才能构建的原型，如今却在几小时内就能完成，而且在AI的辅助下，几乎不需要写太多代码。他还认为，社会对软件工程的需求是巨大的，许多人都希望能够编写更多的程序，但是传统方式的成本太高了。目前，美国50个州里有4个州要求高中毕业生必须接受一定的计算机教育，他希望有朝一日全美50个州都能将其纳入到毕业要求里。如果能让每个人都学会利用计算机去创造，而不仅仅是消费，他相信这将极大地赋能每一个人。所以，在他的眼中，未来最重要的技能之一，就是驾驭计算机、让它为我们所用的能力，因为计算机将变得空前的强大。归根结底，在一个我们需要用全新的方式来教导孩子编程的世界里，下一代将比我们这一代拥有更强大的力量。

而对于美国之外，甚至是一些发展中的经济体，吴恩达希望人工智能能够带来巨大的普惠效应。因为当今世界最昂贵的商品之一就是智慧。请一位经验丰富的专科医生诊疗，费用高昂；请一位资深的导师一对一辅导孩子，同样代价不菲。虽然他看不到让人类智慧变得廉价的途径，因为培养一个人才本来就需要巨大的投入，但是让AI变得廉价却存在一条清晰的路径。这意味着如今只有相对富裕的人才能雇佣某些专业人士为他们服务，但是在未来，每个人都可能拥有一个庞大、博学、智能的团队来帮助他处理各种事务。这将极大地提升无数人的生活水准。

最后，关于人工智能对劳动力的影响，吴恩达引用了一句名言，那就是AI不会取代人类，但是使用AI的人将取代不使用AI的人。这句话最初是他的朋友库尔特·兰洛茨（Kurt Langlotts）针对放射科医生所提出的。这就像在如今的知识经济中，如果一位求职者不懂得使用谷歌搜索，大家会觉得不可思议。他相信在未来，对于大多数的岗位来说，同样不会雇佣那些不懂得高效使用AI的人。不过话说回来，薪酬最终会与生产力挂钩。既然AI能大幅提升人们的生产力，他认为许多人的经济状况反而会因此改善，他们通过使用AI变得更加高效，从而能够赚取更多收入。

### 总结

以上就是吴恩达对他在谷歌Brain历程的回顾以及他对人工智能一些方面的看法。这不仅让我们看到了机器学习当年的路线之争，以及谷歌早期那令人激动的创新文化，也让我们看到了吴恩达投身于教育的热情和执着。相信对AI领域的很多人，也许不一定同意吴恩达的观点，但是或多或少都听过他的公开课。希望通过本期视频，让大家更多的了解吴恩达，了解人工智能的发展历程。