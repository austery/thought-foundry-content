---
title: A/B 实验：揭秘不为人知的核心价值与统计误区
summary: 本文深入探讨A/B实验的真正价值，强调其从“惊喜”中发现机会，并纠正常见的统计误解。文章详细介绍了实验体系的设计、核心统计知识，以及CUPED、贝叶斯方法和序贯检验等高级测试，旨在帮助读者构建更严谨、高效的实验流程。
area: "life-family"
category: culture
project: []
tags:
- ab-testing
- culture
- data
- product-management
companies_orgs: []
products_models: []
date: 2025-09-24
author: Lei
speaker: 课代表立正
draft: true
guest: null
insight: null
layout: post.njk
series: null
source: https://www.youtube.com/watch?v=9kh1YvzZYks
status: evergreen
---
大家好，今天就跟大家系统完整地讲解一下**A/B实验**（A/B Testing：一种包含两个变体A和B的随机对照实验），希望能帮助你直接达到A/B实验的最顶级认知。

我为什么有这样的自信呢？因为我曾在亚马逊和Meta工作过，很早就意识到了A/B实验的作用，并制作了许多相关视频。我现在所在的公司Statsig是市面上最领先的A/B实验软件或平台提供商，是一家SaaS公司，像OpenAI、Anthropic、Atlassian、Notion、Figma这样的公司都是我们的客户。我作为公司的Evangelist，工作内容不仅是与客户交流，还要和许多业界领袖进行学术和应用上的探讨。即使是像Lyft、DoorDash、Netflix这些非我们客户的公司，我个人也与他们有非常深度的交流。因此，我完全了解整个业界的水平以及我自己的水平。

接下来，我会分几个部分来讲解：
1.  **为什么要做实验**：实验的价值是什么？这一点其实和很多人的想法不同。
2.  **实验的目标**：围绕实验目标，介绍实验体系应该如何设置和打造。
3.  **核心统计知识**：讲解A/B实验中最基础、最重要的统计知识——**Hypothesis Testing**（假设检验：一种统计推断方法，用于判断样本数据是否足以支持某个特定的假设），以及为什么课本上讲的假设检验是错误的。它不一致地混合了Fisher的p-value框架和Neyman-Pearson的假设检验框架，产生了很多误解。可以说，你学到的几乎所有关于假设检验在A/B实验中的应用，因为这个不一致的基础，基本上都是错的。我们需要建立一个正确的基础。
4.  **高级测试方法**：在正确的基础上，我会介绍三种相对高级的测试方法：
    * **CUPED**（通过回归调整的方差缩减方法：一种利用实验前数据减少实验指标方差的技术），也就是回归调整。
    * **Bayesian**（贝叶斯方法：一种基于贝叶斯定理的统计推断方法）与**Frequentist**（频率学派：一种将概率解释为大量重复试验中事件发生频率的统计学派）的区别。
    * **Sequential Testing**（序贯检验：一种样本量不预先固定的统计检验方法）。
    * 对于其他方法，如Switchback（轮换实验）、Grid Search（网格搜索）、Geo Testing（地理测试）等，我会简要介绍其用途，但不会重点讲解。

### 为什么要做实验：价值源于“惊喜”

很多人有一个误解，认为我们做实验是为了看到实验结果是正向的，然后决定这个功能是否要上线。但我要强调的一点是，实验的价值来源于“惊喜”（The value of experimentation comes from surprises）。你做实验的目标，不应该是为了证实你原有的好想法，而是应该去挑战你的想法。

当你一个好的想法被实验验证了，其实实验本身没有提供任何价值，所有的价值都来自于那个好想法本身。但是，当你以为的一个好想法，被实验发现其实是个坏想法；或者一个你觉得没什么价值的想法，被实验发现是一个非常好的想法——这才是实验的真正价值。

Ron Kohavi在他的书中提到，Bing搜索做了一个非常微小的改动，结果带来了上亿美元的销售额提升。这正反两方面的例子都体现了实验的价值。他还在书和论文中讲过，在他任职的公司里，早期假设的成功率大约在3%到30%之间，平均下来基本上是20%。所以说，实验的价值来自于惊喜，而你在80%的情况下都会感到惊喜。

也就是说，当你在做实验前写下假设，比如“我认为这个功能会带来某个指标的正向变化”，实验结果在80%的情况下会告诉你这个假设不成立。这为什么重要呢？假设你的业务指标是这样增长的，如果你不知道哪些上线的功能是正向的、哪些是负向的，你就会上线很多你以为是正向但实际上是负向的功能。这样，你的指标增长就会时上时下，很难实现复利效应（compound）。如果你做好了实验，哪怕不因此多做什么，仅仅是把那些有负面影响的功能砍掉，你就可以获得一个更高的指数级增长。

这是实验最根本的意义：实验的价值源于惊喜。因此，我们应该想方设法地提高实验的覆盖率，希望每一个新功能都经过实验验证。很多临时做实验的公司，往往只对他们有信心的想法进行实验，这其实基本没有意义。

### 如何构建规模化的实验体系

#### 核心技术洞察：功能开关与实验一体化

有人可能会问，100%的功能都做实验可能吗？我告诉你是绝对可能的，像Meta以及我们Statsig的很多客户（包括OpenAI）就是这么做的。据我所知，未使用我们产品但同样做到这一点的公司还有Canva。

要实现这一点，核心的技术洞察在于，要把**Feature Gate**（功能开关：一种无需部署新代码即可远程开启或关闭产品功能的机制）和实验设计成同一个对象（Object）。在Meta，这套系统叫做Gatekeeper和Delta。**Feature Gate**本质上是一套条件规则，决定了用户能否看到某个功能。你可以设置各种条件，比如只对内部员工开放测试，或者在加拿大对10%的用户开放，在美国对50%的用户开放，也可以根据iOS或Android系统进行划分。所有用户都会获得包含新功能的代码，但功能开关作为一层配置，控制着用户是否能看到这个功能。

在美国公司里，这已经是开发者部署新功能的标准实践。上线一个功能基本都通过功能开关的方式进行分阶段发布（staged rollout），比如先对1%的用户开放，检查是否有bug，然后逐步扩大到10%、100%。去年CrowdStrike导致的全球机场系统蓝屏事故，就是因为他们没有使用功能开关，这也因此在网上受到了很多批评。

当你的底层系统将功能开关和实验设计成同一个对象时，只要在功能开关的基础上加入随机化（randomization），再接入你的统计引擎（stats engine）进行计算，就能做到100%的新功能都可以进行实验。工程师在构建和发布功能时，使用功能开关是默认流程，这样你就免费获得了对应的实验能力，无需任何额外设置。

#### 文化价值：赋予工程师自主权

这样的基础设施还带来了A/B实验一个非常重要的文化属性：赋予底层工程师非常强的自主权（agency），而不需要先达成共识（consensus）再去构建。

在没有实验文化的公司，通常的流程是大家先开会讨论，达成共识或制定规划后，工程师再去执行。这个建立共识的过程可能会花费好几周时间。而如果使用实验，你会发现发布功能的风险是可控的。绝大多数公司会选择“在开关后测试”（test behind a gate）或“在实验后测试”（test behind an experiment）的路径。我们先把功能的演示版本做出来，先在内部进行实验，然后开放给1%的用户，观察是否有严重的负面效果。如果没有，就扩大到10%，这时通常就有足够的用户量来判断它对关键指标的影响是好是坏。如果效果好，再逐步扩大范围，直到最终在50%的测试组和50%的对照组中验证效果，确认功能是好的之后，再全量上线给100%的用户。

这样做的好处是，当我们讨论一个功能是否应该上线时，我们是带着数据和已经开发好的功能来进行讨论的，而不是空谈。为什么80%的假设都是错的？我在采访MIT教授Kevin时，他提到他们做的实验90%也是错的。我问他为什么，他说因为我们现在做的产品，改进都非常困难。如果你的成功率是100%，那说明你做的东西根本不值得做。因为产品已经很成熟，生态系统很复杂，我们做的改进本身就很难，人脑无法在做事之前就预测到所有结果。

例如，我们的一个客户Rec Room，他们更新了UI，设计得非常新颖，结果他们的关键指标“创建的聊天线程数”却大幅下降。看到数据后，再回头看那个漂亮的UI，才发现原来“消息”按钮从一个非常显眼的位置被移到了角落里。虽然它还是一个独立的按钮，但用户找不到了。看到了数据再看这个解释，你会觉得非常合理；但如果不看数据，我相信没人能凭直觉发现这个问题。

这正是徐老师一直强调的**智识诚实**（intellectual honesty）。人的认知其实非常狭隘，我们认知世界的方式也很肤浅，人脑并不擅长应对复杂环境。实验的必要性就在于此，它带给我们智识诚失，通过事实告诉我们想法的不足之处，从而创造价值。同时，它也能催生一种更好的、工程师更有自主权的文化，让大家可以更快地做事，而不是把时间浪费在争论一些没人知道对错的事情上。

#### 数据质量：可信赖的指标体系

围绕着规模化实验的目标，实验体系应该如何设计？
第一，功能开关和实验应该是一个统一的对象。A/B实验是一个最容易开始但最难规模化的系统之一。起步时，任何一个数据科学家用一个Notebook甚至一个Excel都能做实验。但这种方式非常容易出错，而且不同人做的结果可能不一样，随机化分配也容易出错。

第二步通常是自动化数据处理和计算流程。这能让一个公司从一年做5个实验提升到50个，但还不够。因为每个实验仍然需要工程师去单独设置，并且随着时间推移，数据质量会下降。

因此，你需要一个可扩展的实验基础设施。当功能开关和实验结合后，工程师无需额外设置实验，实验成为默认开启的。这样你就可以从50个实验扩展到5千、5万甚至50万个，瓶颈不再是实验本身，而是你有多少新想法。

在这个基础上，数据质量变得至关重要。数据必须是可信的（trustworthy）。我曾在腾讯做过很多数据清理工作，发现即使一开始把数据治理得很好，时间一长又会变差。因为从数据日志（logging table）到最终的指标（metrics）之间，存在非常多复杂 convoluted 的数据流水线（pipeline），其中的代码难以维护，数据也非常碎片化。分析师的任期通常只有1-1.5年，人一走，没人知道他写的东西是什么，代码就没法改动。

我们公司提供的产品是一个**指标目录**（metrics catalog），它通过一个产品化的界面，用可视化的拖拉拽方式，让你在定义好日志表后，进行各种聚合、分组、过滤和设置时间窗口等操作。这能满足绝大多数公司从日志到指标的全部需求。所有的指标都在一个产品中集中定义，从而实现端到端可追溯（end-to-end traceable）。你可以清楚地看到任何一个指标（如收入）来自哪个表、计算逻辑是什么（计数、求和、去重计数等），保证了数据的可信度。

有了一个简单的实验定义和一个可信的数据基础，你就可以设法让实验覆盖率达到100%，从而更快地开发，并了解每个功能上线的真实效果。

### A/B实验的核心统计学知识

这时，很多数据科学家，尤其是初级或学院派的（我称之为“学究型数据科学家”），往往会起反作用。他们错误地认为自己的职业价值在于把事情做复杂，而不是做简单。但只有把事情做得更简单、更标准化，才能让实验更具可扩展性。你把实验搞复杂了，就要问自己，由此带来的价值是否值得牺牲实验的速度、沟通的效率以及业务方的接受度。

我看到很多数据科学家一看到实验，第一反应就是“我们要做非参数检验吗？”“要不要用强化学习或GenAI？”这都是没有认识到实验的真正价值，是典型的“拿着锤子找钉子”，为了建立自己的职业价值。我之前的视频讲过，“越难越有价值”（harder things are more valuable）是一个巨大的思维误区。我们应该认识到，“有价值的东西更有价值”（more valuable things are more valuable）。而最有价值的事情，就是让实验尽可能多地覆盖新功能开发，不断从“惊喜”中学习，用因果数据指导产品发展。

#### 纠正错误的假设检验认知

作为数据科学家，当实验做得更多、更快时，我们如何保持严谨，提高决策质量？首先，我们要对**假设检验**、**统计功效**（power）、**最小可检测差异**（minimum detectable effect, MDE）和**样本量**（sample size）有清晰的理解。

实验永远面临一个权衡：在样本量基本给定的情况下，实验跑得越长，就越能检测到更小的效果，结果也越准确。如果一个功能本身影响巨大，比如能带来10%的提升，可能跑几天就知道它是好的，直接上线即可。数据科学家真正需要面对的问题是，像Meta这样的公司，能否检测出0.1%-0.2%的效果；对于普通公司，能否检测出0.5%-2%的效果。

为了能在更短的时间内（比如一两周而不是五六周）检测出这些微小的效果，有几点很重要：
1.  **并行实验**（Concurrent Experiment）是可以的。你应该同时运行很多实验，交互效应（interaction effect）通常不是一个大问题。当一个用户同时处在实验A和实验B中，只要这两个实验是正交的，它们的实验结果仍然是准确的。学术界很看重这个问题，但经验数据显示，它在实际中很少发生，不值得我们为此而牺牲实验速度。
2.  **方差缩减**（Variance Reduction）：你想方设法减少实验的方差。

要理解方差缩减，就必须先理解假设检验。关于假设检验的详细内容，我会把我的英文视频链接放在这里，未来也会出中文视频。简单来说，假设检验揭示了样本量、最小可检测效果、零假设、备择假设、显著性水平（alpha）、第二类错误概率（beta）和统计功效之间的关系。在样本量和功能效果基本给定的情况下，数据科学家唯一能做的就是**减少方差**（reduce variance），也就是减少实验中的噪音。

#### 提升实验效率的高级方法

##### 方差缩减：CUPED
减少方差最好用的方法就是**回归调整**（regression adjustment），也就是**CUPED**。更高级一点的方法是多元回归调整（multivariate regression adjustment），我们公司称之为CURE。它不仅可以用实验前的数据来预测实验后的数据以减少方差，还可以利用用户的其他属性进行回归拟合来降低方差。

CUPED在很大程度上借鉴了双重差分（difference-in-differences）的思路，但它可以在每个用户级别上进行回归调整，因此能极大地降低方差。可以说，CUPED能帮你消除80%可以被消除的方差。如果你还没用CUPED，你应该用；如果你已经用了，其实能再优化的空间已经不大了，不必花太多精力去研究更花哨的技术。

##### 贝叶斯 vs. 频率学派
**Bayesian**（贝叶斯方法）和**Frequentist**（频率学派）是那些学究型数据科学家特别喜欢讨论的话题。贝叶斯方法的好处在于它提供了一个哲学上非常自洽的解释体系。但很重要的一点是，这两种方法只是对同样数据的不同解释（interpretation），它们并不会改变你的实验数据本身。

如果使用无信息先验（no prior）的贝叶斯方法，你会发现虽然解释不同，但最终的数据和决策规则与频率学派是完全一样的。真正的区别在于使用带有先验信息（with priors）的贝叶斯方法。

一种是**对点估计（point estimation）设置先验**。比如，我认为这个功能效果是2%，那我的计算就从2%开始，而不是0。这显然很危险，因为先验的定义非常主观，很容易被滥用。比如一个副总裁想上线某个功能，给它一个10%的先验，那无论收集多少数据，结果可能都很难降到负值。

另一种是**对置信区间（confidence interval）设置先验**。这在一定程度上是有意义的，因为频率学派的置信区间有一个问题：你得到的点估计几乎100%不是真实的处理效应。当你做很多实验时，这会导致“1+1<2”的问题，并且会让你上线很多本不该上线的功能，即错误发现率（false discovery rate）很高。但这个问题不只有贝叶斯方法能解决，频率学派加上一些简单的规则也能解决。

总而言之，贝叶斯和频率学派只是对同一数据的不同哲学解释，最终的决策规则和结果往往可以趋于一致，不值得花大量时间去争论。

##### 序贯检验：控制错误发现率
最后讲一下**Sequential Testing**（序贯检验）和控制错误发现率。我问大家一个问题：一个本该上线的好功能没有上线，和一个本不该上线的坏功能上线了，哪个危害更大？

通常是后者危害更大。因为做实验的成本相对较低，但一个功能一旦上线，它会影响更多用户，并且其代码会进入代码库，成为长期的维护成本。所以，上线一个坏功能的成本非常高。这就是我们为什么要控制**错误发现率**（False Discovery Rate, FDR）。

导致错误发现率高的一个常见原因就是**偷看数据**（peeking）。比如，你通过功效分析计算出实验需要跑两周，但在第一周时，你发现结果已经统计显著为正了，于是你提前结束实验并决定上线。这种行为长期来看一定会提高你的错误发现率。因为实验结果本身是波动的，你看到的可能只是一个随机的高点，如果继续观察，它很可能会回归到不显著的区间。但人总是有偏见的，看到正向结果就想上线。

**序贯检验**的作用就是通过消耗你的统计功效（spend your power），在一开始给你一个更宽的置信区间，让你更难得到统计显著的结果。随着实验的进行，这个置信区间会逐渐变窄，直到预设的实验结束时间点，它会和不做序贯检验的置信区间宽度一致。

这个方法可以系统性地降低错误发现率。背后的哲学是，你的调整应该越保守越好，你应该假设大家会无限次地偷看数据。在实际操作中，这是一个很好的决策规则：实验一般不应该提早上线，但可以提前终止。也就是说，当实验出现显著的负向结果时，通常意味着有bug或糟糕的用户体验，你应该立即终止实验。但对于正向结果，没必要急于求成，等到预定的时间再做上线决策就好。综合管理学和人性来看，使用一个保守的序贯检验来调整置信区间、惩罚偷看行为，是降低整个系统错误发现率的有效方法。

以上就是关于CUPED、贝叶斯和序贯检验的讲解。如果大家对Switchback、Grid Search等其他测试方法感兴趣，欢迎留言，我们有机会再聊。我知道今天讲的内容很深，但我相信对于从事实验工作的数据科学家、产品经理或工程师来说，这些内容都是非常有用的。

希望大家喜欢我身边的这盆篝火，我们下期再见。