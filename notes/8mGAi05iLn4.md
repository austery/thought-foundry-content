---
author: INDIGO 数字镜像
date: '2026-02-23'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=8mGAi05iLn4
speaker: INDIGO 数字镜像
tags:
  - agent-architecture
  - cognitive-limits
  - ai-safety
  - human-ai-collaboration
  - future-of-education
title: Agent 演进、AI 组织与智能边界：与 UBC 教授李霄霄的深度对话
summary: 本期节目邀请到 UBC **李霄霄教授**，深入探讨 AI **Agent** 的演进、单智能体与多智能体协作的效率问题，以及人类认知局限对 AI 设计的启发。对话涵盖了 **Agent** 的学术定义、市场应用、规划能力对比（如 **Claude** 和 **Gemini**），以及 AI 安全与可解释性研究。嘉宾还分享了 AI 部署的挑战、容错性考量，并对 AI 时代下教育理念的转变、**Critical Thinking** 的重要性、以及如何拓展人类与 AI 共同的智能边界提出了独到见解。节目最后对年轻人未来的职业选择和教育方向给出了建议。
insight: ''
draft: true
series: ''
category: ai-ml
area: tech-engineering
project: []
people: []
companies_orgs:
  - UBC
  - Vector Institute
  - Google
  - OpenAI
  - Anthropic
  - xAI
  - Tesla
products_models:
  - GPT
  - ChatGPT
  - Claude
  - Gemini
  - Codex
  - Grok
  - Optimus
media_books:
  - 《Artificial Intelligence A Modern Approach》
status: evergreen
---
### 开场与嘉宾介绍

**Indigo**: 欢迎回到 **INDIGO TALK**。这一期我们来线下录制节目，我们现在的环境在温哥华，一个非常好的视野。今天我邀请到了一位来自于 **UBC** 的教授，他是在人工智能实验室的**李霄霄教授**。教授来简单介绍一下自己吧。

**霄霄**: 大家好，首先非常感谢 **Indigo** 的邀请，有机会在这里跟大家分享一些我们对 AI 前沿的看法。简单做一下自我介绍吧。我是博士在**耶鲁大学**，做的是神经科学、神经影像和人工智能之间的交叉研究。2020年毕业之后，加入了**普林斯顿大学**，做了一年的博士后，研究的方向主要是机器学习系统、分布式运算，还有 AI 的安全隐私。2021年我加入**UBC**（英属哥伦比亚大学），我在这边 Lead 的是 **Trusted and Efficient AI (TEA) lab**。去年我就评上副教授，我现在既在 **UBC**，然后也在加拿大一个非常有名的 AI 实验室，叫做 **Vector Institute**，是 **Geoffrey Hinton** 他当时创办的，在那边做研究员。在我学术休假期间，我也在**谷歌**做访问的研究员。

**Indigo**: 那是一个标准的 AI Researcher，现在在加拿大这边，对吧？应该算起来做 AI 的研究已经12年了。从**耶鲁**到**普林斯顿**，我听到**普林斯顿**的介绍，应该有那个分布式的 Decentralize 这些 AI，还有 AI 的 Security 这一块。现在在 **UBC**，我们离 **UBC** 很近，开车就十来分钟。

**霄霄**: **UBC** 确实很好的校园，欢迎大家来参观。我认为是在北美非常现代和漂亮的一个大学。

**Indigo**: 我们今天就想聊的话题，因为正好是，我们叫霄霄教授，好吧，叫霄霄。我们待会称呼的时候就更加自然一点，你叫我 **Indigo**，我叫你霄霄。

**霄霄**: 没问题。

**Indigo**: 从你的研究方向开始，我知道你在之前两三周左右给我看了一下你们正在 Work On 的一个论文，关于 **Agent**，就是 **Multi Agent** 或者是 **单 Agent** 在工作效率的问题。不过我们不要把话题一下带的这么深，我们可以先从最开始的基础开始。你先跟我们聊一下，你之前在**普林斯顿**，包括来 **UBC** 这边的一个研究的主要工作内容是什么呢？

### AI 研究方向与实验室

**霄霄**: 我开始做 AI 研究的时候，当时是 Pre-**GPT** 的时代，那个时候还是基于一些卷积神经网络的任务。我们关注更多的是这些 AI 模型的一个效率和安全的问题。即使在 Pre-**GPT** 的时代，在图像分类任务也好，语言翻译任务也好，当时的 AI 模型其实已经有了比较高的准确性。但是考虑到 AI 部署到实际应用中时，它就会存在效率和安全性的问题。所以那些是我主要在**普林斯顿**博士后期间，还有最开始来到 **UBC** 当时的研究。但很快大家就发现，在2022年、2023年就到了 **GPT** 大模型的时代。虽然之前我们也关心到了 **Transformer**，但那之后我们的研究会有一个转向。

**Indigo**: 我很好奇，你现在在的实验室，**Trusted and Efficient AI (TEA) lab**，你们主要是研究什么内容？

**霄霄**: 我们还是不管底层的 AI 技术怎么改变，我们相信提高 AI 的可靠性还有它的效率是一个永恒的课题。所以我们还是说，在基于现在已经发展到的 AI 底层框架和技术上面，我们想进一步提升它的安全可靠性、可信性、可解释性，还有它的效率问题。

**Indigo**: 这个 lab 和 **Hinton** 教授成立的那个，嗯，它是属于两种机构吗？

**霄霄**: **Hinton** 教授成立的那个叫做 **Vector Institute**（向量研究所），它更相当于一个研究所，一个研究中心。那边会 Host 很多高校的教授去那边做 Faculty Member，我们有很多资源可以从那边获得。它相当于一个研究俱乐部，更像一个生态，让大家一起做 AI 的 Professor 和学生可以认识合作的一个平台。

### 加拿大在 AI 领域的贡献

**Indigo**: 我再问一下关于加拿大这边。其实这一轮 AI，大家总是把加拿大给低估了，这一轮 AI 爆发就来自于加拿大，包括从 **Hinton** 开始，对吧？包括我们从 **McGill 大学**那边的，现在比较火的叫做 **Continual Learning**，**Alberta 大学**的，是**强化学习 (RL)**。那边有另外一个大神，写那个**《The Bitter Lesson》**的 **Richard S. Sutton**，他也是和 **Gemini**（**Google Labs**）这边强力合作，他们在一块做研究，一块出论文，一块出各种东西。就是和**阿尔伯塔**、**多伦多大学**、**麦吉尔**，好像 **UBC** 的没什么。

**霄霄**: 我，因为我们看这些您刚说的大佬，都是功成名就很多年前了，都是已经获得图灵奖的了。可能我们 **UBC** 这边的话就比较年轻。我觉得随着时间的验证，希望我们这边也会涌现出一些大佬。

**Indigo**: 加油加油。因为这样，我刚才说这一轮，从研究上面，加拿大对整个这一次新一代的 AI 革命，或者叫神经网络、深度学习算法，其实都有很大的贡献，包括现在的**强化学习**。加拿大在不停地给美国的商业公司输送人才，加拿大负责做研究，去美国创业赚钱。

**霄霄**: 我们也很欢迎大家来加拿大这边来学 AI 的研究这些工作。

### Agent 的定义与规划能力

**Indigo**: 我把问题带回来，你刚才讲到你们研究室做的是**可信的 AI** 和**高效能的 AI**。现在有一个最火的话题，2025年叫做代码资源，AI 都能够写代码。所有实验室2025年也出现了 **Agent**，比如说像 **Manus** 这样的智能体。但是2026年一开年，**OpenClaw**（之前叫 **Clawdbot**）这样的开源产品，一下子让整个 **Agent** 好像就换了一个身份又出现了一样。我觉得这个力量很大。我们可以说一下，从你们的眼光里面看，从你研究方面来看，**Agent** 到底是什么？

**霄霄**: 这是一个非常好的话题，这也是我们实验室现在在研究的一个课题。对我们来说很欣喜地看到大家在讨论这个话题，并且应用 **Agent** 在他们的实际生活中。但是，刚刚您说的好像25年 **Agent** 变成一个特别火的话题，从学术的程度来讲，**Agent** 不是一个新的概念。它比较早的定义在，或者说比较 Official 的定义在90年代有一本 AI 教科书，叫做**《Artificial Intelligence A Modern Approach》**，是 **Peter Norvig**（他现在也是**谷歌** AI 的一个 Director）和 **Stuart Russell** 他们写的一本书。他们在里面定义了 **Agent** 是一个可以感知环境，并且根据环境能够自主决策，以最大化目标的这么一个系统。

**Indigo**: 就是95年那个时候的。听上去跟 **Richard Sutton** 讲的一样，他后来写的那个**阿尔伯塔计划**里面，**Alberta Plan**，讲到了感知环境，自我进化。

**霄霄**: 这是一个在大模型以前大家也会公认的一个定义。不过目前我们看到的市场上面对于 **Agent** 的定义会更加广泛一些，无论是说只是一些对话的链路，或者是**工作流 (Workflow)**，大家也把它贴上了 **Agent** 的标签。我们的话，其实不会特别在乎 **Agent** 具体的定义是什么，但是对于定义的理解会影响到我们对它的一个期待，或者是我们技术路线的选择。

**Indigo**: 了解。你现在来看一下，**Agent**，我给大家普及一下。我周围问过很多朋友，他们都在用 AI，他们到底用 AI 干嘛？就是跟 AI 聊聊天，问问题。实际上现在最强大的模型，你跟他聊天问问题，你用任何模型都没有什么差别了，哪怕开源的模型。但是你想深入地让他去帮你代表你去干一些事情，执行很多事情的时候，这个差别其实挺大的。在这个里面，我觉得是**规划能力**，他得有自主**规划能力**。你怎么看这两个区别？

**霄霄**: 这个本质区别还挺大的。我们看 **ChatGPT**，它提供了一些 **Chatbot** 之类的，它主要是一个问答的功能。可能有一个范式会介于这种 **Chatbot** 和 **Agent** 之间，叫做 **Workflow**。**Anthropic** 之前给了 **Workflow** 和 **Agent** 一个非常清晰的定义的区分。他认为 **Workflow** 是一个大模型和工具在一个既定的路线下面进行交互的系统。而 **Agent** 是一个大模型，它有自主地去做一个 **Planning** 和进化，并且使用工具，Even 去创造工具的系统。所以你刚讲了一点非常核心的区别，就是它能不能够根据用户设定的目标进行自主的**规划**。我觉得这个并不是所有的大模型现在都能够非常完美地做到这一点。

### Agent 模型对比与个人体验

**Indigo**: 你觉得最好的是哪些？你在用的时候？

**霄霄**: 我这个不好去评判吧。但是大家公认的，美国这边的御三家，包括国内的御三家之类的，这些都不错。而且你可以看到最新的这些模型，他们的 **Reasoning** 的能力上去了之后，他们这个 **Agent** 的**规划能力**也在提升。

**Indigo**: 反正我就直接说，我也不在乎什么，我就觉得我用的最多，**Planning** 里最好的是 **Anthropic Claude**。可能他写代码的深度不一定是最好，但是现在 **OpenAI** 的 **Code assist** 很好。但是他真的他的 **Planning** 的适应性，你用他的时候你的感知，因为这个 **Planning** 很重要，就是 AI 能够很好地理解你在说什么，而且他完全能够还原，抓住你的想法，然后做一个很合适的 Plan 出来，而且很有效地完成这个 Plan。而且他还会自我反省，当这个东西碰到问题的时候，他自己退回来，我要去改我的计划，我再修改我的计划，我自我审查，他有一点点**元认知能力**了，他知道我在干什么，他知道他自己在干什么。

**霄霄**: 你说这个我也有观察到。我觉得可能就是 **Anthropic**，他可能最开始就是想去做一个 **Agent** 的基础建设，所以从这个角度上来说的话，可能对用户的体感来说，这点是比较好的。

**Indigo**: 是的。其他的模型有 **Planning** 能力，但是 **Gemini** 挺快的，速度巨快。它也有一点 **Planning** 能力，但是我感觉做事情深入下去的时候，还是 **Anthropic** 好。但这样你就有不同，你有的东西需要有代理任务的时候，就可以交给 **Claude** 去做。如果我只是回答快速回答的时候，或者大家深度思考一下，**Gemini** 也可以，包括 **ChatGPT** 也是可以的。**ChatGPT** 现在最新更新的 **Codex**，5.2还是5.3吧，写代码能力大家都说挺好的，因为他在这方面强化了。国产的像 **MiniMax**、**千问**、**Kimi**，他们之间都有。御三家两边的其实都有这些能力，但是具体这个有效性来说，我觉得还是 **Claude** 在我的认知里面是比较好的。

**霄霄**: 所以我们会把 AI 的任务分成一种聊天对话式任务，还有一种让他去做事情的任务。我觉得可能不同的 AI 的 **Frontier lab**，他们的技术路线可能是有一定的差异性的。

### 人类认知局限与 AI

**Indigo**: 用人类的学习和认知去理解 AI，我们现在的 AI 是处于什么样的时刻？

**霄霄**: 这个我觉得没有办法很好地去定义 AI 在哪个时刻。我会从我的角度上觉得，我们对 AI 的认知过于局限，太少了。我们很多时候只是用人类的认知去类比 AI 的认知，让我们知道它是不是也跟人类的认知一样，有一定的局限。然后可以用人类解决自己认知局限的方法去提升 AI 的能力。我可以举个比较简单的例子，比如说局限性。之前也是一本很古早的书，讲的是人类社会学的书，讲的就是人类的认知是有限的，应该是那个理论叫做**认知有限理论**。对于所有自然界的复杂系统来说，人大脑所具备的能力是远远小于解决这些复杂问题它所需要的理性能力的。具体例子就是说，我们下国际象棋，我们可以完全有一些非常理性的解，你在下每一步的时候能算出这一步他最后的赢率是多少，使得你每一次都能选择最优的解，这是我们叫做一个理性的能力。但是人类的能力，大概你只能往前看3-5步，然后你去想，这是不是可能让我赢的一个方法。所以人类没有办法完全解决一些复杂任务，我们把这个叫做人类认知的一些局限之一。从其他一些局限，就我个人的感知，我之前给你发那篇文章，我们讲的一件事情就是，当人他有很多的技能的时候，他就会**认知崩溃**。这也是我们最近在看的一件事情，如果你给一个 **Agent** 特别多的技能，他可能也会有类似的这种**认知崩溃**的东西。

**Indigo**: 没关系，我就喜欢听这个。**认知崩溃**，我想听你再详细解释一下，为什么掌握太多技能之后会**认知崩溃**？

**霄霄**: 这可能从我们自己的体验去说吧，你同时能够做多少项任务？我觉得可能每个人大概就是同时能做三项任务，可能已经差不多了。或者说你同时能具备多少项技能？一个人不可能既是一个滑雪冠军，又是一个奥赛冠军，又会画画。你可以从周围人类的例子上面发现，一个人并没有办法有大量的技能。这在认知心理学里边，其实也是有这样的例子的。从最早期的时候，不好意思，又牵扯到一些学术上的东西，最早期就是有个叫 **Hick's Law** 的时候，他们是在做了一个实验，他给了很多的灯，把它编码成一个矩阵，让人去做实验。如果是那些灯亮了之后，他就需要按那个按钮，代表这个灯是亮的。他发现当你这个灯越来越多的时候，人去准确地按哪个灯亮的时候，这件事情就不那么准确了，或者他所需要的思考时间就会增强。另外的时间就发现，当你给一些，大脑有一个叫做**工作记忆**的容量，当你所做的事情超过你**工作记忆**容量的时候，你就会瞬间地崩塌，你的崩溃，所有的崩溃。这些东西都已经反映到，其实我们人类的认知它是有这种局限的。其实我们可以通过对人类认知的局限去反推 AI 会不会有类似的这些局限性。

**Indigo**: 我感觉不会啊，我加算力就好了，我不同的实体，实例就是1000个，或者成百上千个，同时他们自己做自己的事情的时候。

**霄霄**: 你讲的一点非常好，我们在讲类比的时候，可能是我们拿一个 AI **Agent** 和一个人去做类比，我们说可能一个 AI **Agent** 他可能有这样的这种认知局限。但你刚讲的就是组织，其实人类架构也是这样的，每个人的能力是有限的情况下，所以人类就会有这种组织、分层、抽象的能力。你完全可以用同样的解决人类认知局限的方法去解决 AI 它的认知局限，所以就会有了这样一种**多智能体**的架构。

**Indigo**: 我们回到前面来，你前面问到的那个问题是人类用人类的学习认知去理解 AI 嘛。人类会有认知局限，因为我们脑容量有限，大脑带宽有限，关键是我们注意力有限，**Attention** 是有限的，一次只能注意几个事情。但是 **Transformer** 是多头注意力，它同时注意所有的事情，然后它就变形展开了。但是它其实也能发现它有注意力局限，当你的 **Context** 很长很长，上下文特别大的时候，它就会 Lost，它就会丢失在上下文里面了。但是它还是这个能力比人类好多了，对不对？

**霄霄**: 对对对，但就是它也有它的局限，它上下文的，这也是它的一个局限之一。这个就相当于你内存永远是有物理限制的，上下文就是它的物理限制，我们得用各种方法。

**Indigo**: 因为这些限制，所以单个，但是我现在感觉单个 AI 的 **Efficiency**，这个效率，就是它计算效率和它使用技能的，比如说你现在用 **Claude**，我告诉你，我这有五个 **Skill** 可以用，它能够在它的一个 **Planning** 里面去把这些 **Skill** 排得很好，你给他再多 **Skill** 他都学得会，都不像人。我说是一个人和一个 AI **Agent**，那这个怎么看呢？

**霄霄**: Yes or No。你刚讲到一点，当你给 AI 五个 **Skill** 的时候，你会发现他能力比他没有 **Skill** 或者只有一两个 **Skill** 的时候要强。我们最近的研究也发现，你不是说能够无限地增强这个 AI 的能力。当你给到越来越多的 **Skill**，我们的研究发现，当你给到 **Skill** 大概是在20到30个的时候，他能达到一个最好的 **Performance**。再给多的时候，他可能他的 **Performance** 也会 **Saturate**，也会饱和。当然它的原因是什么，我们只能做一些有可能的猜想。但是我觉得 AI，它现在包括 **Agent**，它的运算底层里边有一些很复杂的现象，我们也没有办法解释。

**Indigo**: 现在确实是不可解释性的，现在都是在做可解释性研究。那么我再问回来，你们在做安全的时候，你们会尝试去做这些事情吗？可解释性的研究。

**霄霄**: 会吧。我们也会发现，当你有多智能体一起协作的时候，我们会发现，当你其中有一个智能体叛变了，他可能会让你整个组织的决策都会走向错误的极端。然后我们去发现为什么在哪一轮的沟通当中，这个叛变的智能体它带来了决策性的影响和改变。但是，我们所谓的解释性很多时候是从这个 **Agent** 他自己的输出去反推的。我们并没有说从真正的底层神经网络的架构，比如说 **Transformer**，他们的注意力，或者像大模型的一些算法，去理解为什么 AI 会涌现出这样的能力和这样的安全性的问题。

**Indigo**: 了解。这听上去还是未知的东西比较多。

**霄霄**: 我感觉是非常有趣，因为就是有这么多未知的东西，所以大家才可以有很多可以尝试的。

**Indigo**: 我只从个人的使用感觉和对于我们 AI，我们叫**硅基智能**的特点来看。因为它的能力的提升，你可以用来 **Scaling**，扩大，或者我一把硬件的能力加强，内存变大，GPU 变快，让它这个变得更好。但是物理限制是有的，就跟人脑有物理限制嘛。但是我们大脑其实更有效一点，在某些程度上，它是非常有效的，它非常有效率的，就是学习过程很有效率，比 AI 快多了。实际上我们叫做潜移默化的，叫做泛化式学习能力要比 AI 好。你看两个例子就懂了，AI 要大量训练才行。

**霄霄**: 这个我觉得 AI 也有可能赶上了。

**Indigo**: 我们回头后面再聊。我可能要更正一点，我们虽然有时候拿 AI 跟人类去类比，但是我并不觉得 AI 一定要非得跟人类去类比。因为本来**硅基生物**和**碳基生物**它们的底层原理就不一样。我们用的还是底层的神经的网络电信号。现在的 AI，比如说我们刚刚说的这大模型也好，**Agent** 也好，它核心还是现在 **Transformer** 的架构下面的一些生成网络。所以我觉得没有必要一定要给他们去做一个 Exactly One to One Mapping 吧。很多时候，我们的研究只是说想通过人的理解，人的认知，去发现 AI 是不是也有类似的认知局限，然后用去解决人类认知局限的一些方案，去解决 AI 类似认知局限的问题。

**Indigo**: 我记得以前你刚刚说的那个词比较好，就是**硅基智能**。我觉得**硅基智能**的逻辑，虽然说我们都用了神经网络，用了这种类似的结构，但是因为硬件架构完全不同，或者实现方法完全不同，会产生另外一种完全不同的智能形式。它可能单体的能量效率没有人类在，虽然我们叫生物智能，这么高能量效率。但是人不能这样子插上管子插电，但是**硅基**可以。**硅基**你可以不停地给它更多电能源，给它更多的算力，可以快速复制。所以说它可能通过暴力的方式，产生完全不同形式的智能出来，而且关键是它是可以复制的。

**霄霄**: 我同意，你讲的这点特别好。我们只想到我们人类进化到现在，我们是进化或自然选择所得到的一个结果。但其实 AI 有一点很可怕的是，它可以自己进化。现在有很多可以让 AI 自己，我们叫做 **Self-Evolving 算法**。但还有一点，它可以被设计。我们人类可以给它附加外界的一些能力，让它设计成一个非常 Powerful 的方法。但人类目前是没有办法被设计的。所以我觉得从这种角度上面，其实 AI 它的进展可能会比我们人类智能的发展，在某种程度上不一样的路线。

**Indigo**: 不一样的路线。我们必须得承认另外一个，在我们所认知的宇宙里，我们反正智慧生物也没找到，就这智慧生物都在这个地球上面。那么在我们所了解的这个星球上面，新的智慧物种已经诞生了，而且它进化非常快，而且是超过我们的速度的。所以我觉得这个，我们往**硅基**的融合这个，必须得接受这个现实。

**霄霄**: 这个听上去有点可怕，虽然听上去有点可怕，但是确实也得警示一下。

### 单智能体与多智能体协作效率

**Indigo**: 我们再聊一下关于你研究的话题。你给我看过那个论文前，前面我们都在聊 AI **Agent**。我刚才已经提到了，单个 **Agent** 和单个 AI 实体的运作可能会有一些 **Context** 的限制，各种东西。我们说人也有这种限制，但是 AI 其实它的计算量提升，它的技能掌握程度会更好一些。但是**多 Agent** 的时候，它就能够像人类的组织一样去协作了。我记得你们研究的这个是**多 Agent 协作**和**单 Agent 工作**，这是单个 **Agent** 使用各种技术，使用各种技能的时候。而你的结论好像**单 Agent** 这种更有效率一些，还是？你可以说一下你的研究方向和这个结论。

**霄霄**: 我们最近做了一个比较有趣的研究，就是去看如果你给一个 **Agent** 赋予技能的话，它能不能去取代**多 Agent** 的一些任务。直观上来讲，它可以提高效率。因为**多 Agent** 会有很多 **Communication Cost**，这可以跟人类来类比。你如果要跟其他人去协同合作一个东西的话，沟通就会有信息的损失。如果这件事情你可以在自己能力范围内干完，可能你是会一个高效的方法能把它做好的。在我测试的一些比较简单的任务里面，我确实发现了，这个**单智能体**如果你给它赋一些技能的话，它可以，效果会比**多智能体**一起协作要更快，能做好这些任务，甚至更好。

**Indigo**: 有没有更具体的一些数据，或者是呈现一些案例在这里面？

**霄霄**: 我测试的可能就是一些比较简单的数学题，或者有一些写代码的任务。还没有机会再测试到一些更加复杂的，比如说在这种计算机使用上面的一些更加复杂的任务。

**Indigo**: 那 **Claude** 不是刚刚 **Opus** 发布了4.6吗？它里面出了一个 **Agent Team**。它上次不是官方发了一个演示，说它启动了一个 **Agent Team**，它是为了做测试，压力测试。里面有16个 **Agents**，有的负责代码，有的负责写这个代码，有的负责写那个代码，有的负责代码审核，有的负责做 **Planning**，有的负责做这个测试。还有各种各样的角色，然后他们让这一个 **Agent Team** 一块完成了一个编译器，就是 C 的编译器完成了，然后这个编译器可以重新编译 **Linux** 内核。

**霄霄**: 我有看到这个。他们那个还是一个**多智能体**，或者有一些 **Sub-Agent**（子智能体）的设计方法。在他那个情景下面，这是合理的。因为本来他的任务比较复杂，**Agent** 嘛，现在还是基于大模型这样的一个基座的话，最后他还是会由他的上下文的容量受限。所以你讲到那个复杂任务的话，在我看来，它确实是一个比较适合用**多智能体**的范式去设计的一个任务。因为他有不同角色，我们叫 **Persona**，不同的角色去做这些，就像一个小团队一样。因为我一个写代码的人，很难去在我这代码里面再去发现我的错误。因为他换一个角度来思考这个问题，就是有人负责做，有人负责审核，就做的人和审核人一定要分开。

**Indigo**: 你刚才讲的这个也是一个，当我们一定需要两个独立的智能体去协作的时候，也需要**多智能体**这么一个框架，而不是**单智能体**。因为**单智能体**它的整个记忆空间是没有办法隔离的，就相当于一个人他既做裁判又做比赛选手，这样就不合理了。那你们研究里面，你们**多智能体协作**的这个用的这个背景的场景是什么？就是为什么它没有单个 **Agent** 的带技能会更好？

**霄霄**: 我们学术上面还是严谨一点。我们去关注的是某一类的**多智能体**的任务，它是特定的那种可以串行去计算的。就比如说，当你一个智能体做完一个任务之后，它的输出会接到下一个智能体，并且他们的记忆是可以被共享的。这就跟你讲的那个可能是不一样的，因为你讲的那个就相当于，其实你有可能不同的智能体搭配需要多任务并行去计算，并且你还需要他们在某种程度上面进行隔离。所以我们的研究是没有考虑到这种情况的，我们更加考虑到是串行的任务流。

**Indigo**: 那举个例子，串行任务流。就是大家就相当于是我这边有123，就是我们两个是智能体，那边还有几个智能体，然后我们共享同一个任务清单，这个 Plan，然后我们把这些任务拆成小块，然后我们按顺序执行。这种情况下，突然就是搬砖，我们一块搬是更快，还是一个人搬更快？

**霄霄**: 你这么理解非常对。就比如说我们两个非要做个菜，我说我是一个切菜能手，你是一个煮菜的能手，然后我就负责切菜，你就负责煮。然后这样子的方法会比我，我们就发现，其实如果一个人他完全可以切菜和做饭的话，他可能会比我们这样子串联的两个智能体要更加高效和做得更好一点。

**Indigo**: 就是你在很多情况下，在真实世界里面，有的时候一个人做事情会比几个人做事情更快。因为他一个人在他的 **Context** 也是连续的，他会很快速地把他自己的这些任务给优化掉，然后组织成一个调用，使用他自己的不同的技能。如果我把这个技能分配到不同的智能体上去，因为他们需要沟通，还浪费了时间。

**霄霄**: 这是一个主要的，沟通他所浪费的成本，比如说 **Token** 的成本，或者是信息的损失，这是主要的两个缺陷。还有一个就是，当你构造**多智能体**的时候，你不可控性就会更多了。因为你在考虑到这么一个串联的环节里面，只要有一个智能体它的错误是会产生的，它的错误会累积到所有它下游的智能体上面去。

**Indigo**: 我了解。那确实。我自己按照我的使用体验，我在用 **Claude** 的时候，特别是 **Claude Code** 的时候，我下达任务的时候，比如说我想去做一些编码转码，把东西分片，然后他自己就决定这个时候他要使用 **Sub Agent** 的。他把这五片让5个 **Agent** 同时做，就相当于他有个调度员。我觉得这个事情能够把它分包出去，我就分包给五个人就快了。如果说很多情况下，他还是一步一步做的，但是由他的这个组的 **Agent** 来决定什么时候要启动 **Sub Agent**，什么时候要拿回来自己做，他都有这个智能了。

**霄霄**: 你现在用 **Claude Code** 的 **Cowork** 的话，你会发现你给他一个任务的时候，他会自己写一个 **Skill**，或者写好几个 **Skill**，然后这些都是他自己去执行的。当然就像你说的，在需要把它分包给 **Sub Agent** 的时候，他也会把它并行的给分配下去。这是感觉挺聪明的，就像一个特别好的一个人类调度指挥在指挥大家工作一样。

### AI Agent 的调度能力与未来组织

**Indigo**: 这个里面正好涉及到了一个我们现在社会工作效率管理的问题。我们本身在这个世界上面是**多人类协作**的，这也会涉及到一个组织效率的问题。有时候组织人很多的时候，效率更低。

**霄霄**: 我非常同意。

**Indigo**: 但是人太少的时候不够用。这个边界和什么样的任务适合一个人串行来做，什么样的任务适合快速分包出去来做？我在用 **Claude** 的时候，我看到 AI 好像调度得非常好。

**霄霄**: 因为它里面就会涉及到一个**任务路由**的概念。它会根据你任务的复杂性，它去决定这件事情到底是自己单干，还是说它要把它分到给一些 **Sub Agent**，或者是多个 **Agent** 共同协作。

**Indigo**: 那么这个时候就得得出一个结论来了。我们为什么总觉得人类组织是一个特别冗余的，特别低效的组织？但是我们人，为什么人类这个社群，我们一块工作，因为有这么多公司出现的嘛。我们通过这种低效的合作，然后因为确实产生了比单个个体要牛X得多的事情出来。比如说你干工程，建大型的这种，建高铁，建水坝，建火箭，造火箭这些东西。其实这种大型的 **Build** 这种事情的时候，就是必须得**多人协作**。那这个里面就是因为有的人，每个人的专业能力不一样嘛，有的人负责生产，有的人负责检查。那么事实上在大多数的组织里面，效率是低下的，都通过这种路由和沟通，让这个摩擦掉了。那么我觉得组织会这样变化。如果说我们 AI **Agent**，AI 的模型能力在今年和明年再进一步提高，它有非常好的这种 **Agent** 和 **Sub Agent**，或者是像你们现在设计的这种一个 **Agent** 加上 **Skill**，它自己会平衡。比如说我现在用这3个工，我现在要干的这个事情，我加3个 **Skill**，我自己干就好了，我不需要其他 **Agent** 参与进来，他就自己决定了。当他发现这个任务可以分片分包出去，你们干得更快，他就决定分包出去了。所以这个 AI **Agent** 的调度能力比一个熟练的人类做得还好。

**霄霄**: 我觉得他是有这样的能力的。

**Indigo**: 那么我接下来就是 **Elon Musk**，他不是讲了个新故事，人家问他 **xAI** 做了 **Grok**，到时候怎么和他 **Tesla** 的机器人怎么结合。他回答我觉得非常好，这确实是考虑过了。而且未来就是这样子的，所有的机器人它的智能，它通过 **FSD** 都是在边缘完成的。在边缘是个相当于能力有限的智能，但是他做这些事情很好。但是 **Grok** 的能力足够强大，它能够在云端来进行复杂的调度和计算。然后它就可以非常好地去调度那个 **Optimus** 的那个 **Swarm**，他的机器人的 **Swarm**，然后在工厂里面快速干活。就是他居然有个大脑来指挥他，马上你去干这个，然后怎么分配他。其实这个效率就把现在为什么现在说一个工厂的生产效率，你们这个工厂的 **Workflow** 的组织和这个指挥人，这个指挥家很重要的，不然就乱七八糟里面就效率极低。那么我觉得 AI 以后扮演的角色会比人更好。

**霄霄**: 我觉得确实，特别是对于这种任务的复杂程度的估计，以及这么一个高效的分配机制来说的话，我觉得很多时候人在进行估算的时候，有比较多主观或者是非量化的估算。但其实 AI 它本来就是一个非常量化的机器，它确实能在某些方面上面可能比人做得出色。

**Indigo**: 所以说我觉得这个社会，如果说我们刚才前面讲的**硅基**和**碳基**怎么融合，我觉得单体的生物机制融合还是很难，因为技术还不够。但是 AI **Agent** 作为这个出色的调度员和这个出色的我们的智力补充者，融入到我们的公司或者社会的组织结构里面去，会更快一点。

### AI 组织架构与协同模式的思考

**霄霄**: 你讲得非常乐观。我可能泼一个冷水，你可以反驳我。包括我们现在看一些这种 **Agent** 的设计，它还是过度地模仿人类的一些组织架构或分层的一些范式。但是我觉得可能 AI 未必需要像人类这样去进行角色的分类或者组织。包括很多时候我们在讲人，我们也没有现在强制地让 AI 的思考过程（我们叫 **Chain of Thought**）强制地跟人类的思考过程去 Align。他思考方案路径可能完全不一样。所以就是我虽然觉得他让 **Agent** 现在去模仿人类组织架构是一个方式，但是我也期待他可能会有一种更加自身的，一种自己去学习他们组织架构的方式，然后人类的组织架构可以从他们的组织架构里边去反过来去学习。

**Indigo**: 我理解你的意思。因为人本身这个人能力，就是因为人有各种问题嘛，人你要休息，有情绪，还有什么自尊心，有 **EGO**，有各种问题，都有在一个组织里面，所以说它造成很多摩擦和障碍。所以人类社会公司里面会设计什么高层、中层、执行层不同的分层。但是对 AI 来说不需要了，有可能确实不需要了。他只要一个调度员，或者说都不需要调度员，或者说他是形成一种默契之间的调度。就跟打个比方说，我觉得一个蚁群，就是 **Swarm Learning** 的那种。**Swarm Learning** 就像鸟群，它没有 Leader。但是这一群鸟在一块飞的时候，动向它们去了干嘛，它能够形成非常有效的组织。它单个个体很傻的，蚂蚁单个个体很傻，但是它也没有指挥。但是在蚂蚁里面，会有一点点简单的分工，有人负责搬食物，有人负责送女王喂食，有人负责 Guard。就是这种简单的职能上的分工，然后他们一块行动，它能够表现出非常智能的东西。所以我个人会对这种**多智能体**它的协作的未来方向，我可能会比较脑洞大开，我觉得它不局限于人类的这种形式。

**霄霄**: 而且我觉得从出发点上来说，我们现在给 **Agent** 设计的这种协作方式，很多时候还是在默认可能 **Agent** 跟人类有类似的认知局限。这我前面讲的，想 Clarify 一个问题，我说我们虽然尝试做这样类比，但是我未必觉得这个是真实存在的。也有可能 AI 它足够强大，所以人类的一些组织架构的设计对它来说就是冗余的。就像你刚刚讲的那点，我很认同。

**Indigo**: 你刚才反驳也很好，我觉得确实是。因为为什么呢？我在想呢，我最近不是在写一本书吗？我书里有一章就是关于 **Agent** 的组织和未来的组织。其实刚刚给我很大的启发，我可能之前的惯性还是说 AI 怎么进入人世界的组织，然后去改造这个组织。你就还不如，我们就完全做一个 AI 原生的机构，或者说叫做我们叫做 Vendor，一个服务提供商，它里边的组织怎么样，他们自己来就好了，自己来决定，自己来演化。回头我写这一章的时候，我们可以再详细了解一下，我们可以再做一个这一块的访谈，我就把它写到书里面去。

**霄霄**: 挺好挺好。这给我很多启发在这里。

**Indigo**: 因为你现在看这种**多智能体**的话，它还是效率问题的一个问题。前段时间你说你玩这个 **OpenClaw**，你没有注意到你在叫它**多智能体**的时候，它对那个 **Token** 的消耗量大啊，很大，对吧？包括可能要用那个 **Claude Code** 或者 **Claude Cowork**，它的消耗量都很大的。我觉得这个可能也是一个没有解决，我们想在我们研究中解决的一个问题，就怎么让这种**多智能体**它的效率会提得更高。但是它应该是任务下下去了，它就干活嘛。在里面，你们有什么办法呢？

### AI 部署与容错性

**霄霄**: 这个尝试引到我们的科研上面。我觉得有两个东西我们现在做，我觉得比较有趣，就跟大家分享一下。一件事情，比较直接的，如果作为用户，我比较介意我现在让**多智能体**执行的任务它消耗很多 **Token** 的话，我们可能想要问的问题就是，如果我给定你这么多的 **Budget**，我比如说我给你十美元，你能不能把我的任务做成？我可能不需要一个100%满意的答案，你那答案可能需要我两千美金。我现在只给你10美金，你能不能做到90%？

**Indigo**: 这样限制他是吧？这样去限制他。

**霄霄**: 我们现在有个工作就是做这种带有这种限制性，**Budget 限制性**的一个 **Agent** 的优化。

**Indigo**: 那你是告诉他你的 **Token** 消耗有限，然后自己在有限的情况下他自己去想了，还是你们会有一些？

**霄霄**: 他自己去想。就比如说他有件事情，他有多个任务，他每个任务都想做到极致的满意之后再进行下一个任务的时候，我会告诉他，你这个任务已经可能达到了你这个八九十分的，你得放弃一下你对完美的追求，或者不要再去摄取更多的一个可能的答案，你得转过来再把下面的事情给做完了。

**Indigo**: 那这个挺好的。还有一种情况会出现，因为模型 **Model** 的智能不一样，有的 **Model** 聪明，有的 **Model** 傻一点。或者有的是开源的，在本地因为就**端侧**运行的会。你刚说安全嘛，很多东西我也是希望在**端侧**的。比如说**苹果**做得很好，**Apple** 很多东西都起了个早，它的那个 M 芯片里面有各种计算单元嘛，它把这个本地的语音处理、照片识别、那些扫描都用本地的 **NPU** 做了，就是本地计算。它完全没有通过云端，本地闲暇时间就把这个事情都算完了。要存本地很安全，不需要传到网上去，把我的照片都给别人看到然后再给你做一个识别。那么当这样的一个 **Agent** 的网络，或者说这样的一个 **Agent Swarm**，有的 **Agent** 很聪明，有的 **Agent** 就是他只能干这一件事情的。那这个东西是不是也会存在这种组织可塑性？也算是你们研究话题里面的一部分，或者说是你调用一些比较傻的模型，然后让他去干这些事情，然后你有没有这种一些优化？

**霄霄**: 你说这个完全是有可能的。因为如果你的一些所谓的比较傻的模型，或者说我们看的就现在比较小的模型，**小模型**你让他去做些专业化的事情的话，他完全是可能在他能力范围内去能够做得到的。所以当你在调度的时候，我觉得这可能是未来这种**端侧**的 **IoT** 方向上面**多智能体协作**的一个方向吧。你只需要有个超级无敌的大脑，你知道把哪些对应的任务分给到**端侧**，只能把那件任务做得比较好的一个模型去做，那就可以。我觉得可能是一个比较 Straightforward 的一个路径。但我刚听你讲的另外一点，让我想到的是，因为我们也做那种安全性的东西嘛。你可能现在发现包括那个 **OpenClaw**，大家虽然说在一个 **Mac mini** 上面可以部署，但是你还是用的是云端的 API。

**Indigo**: 我觉得聪明一点吧。而且不用耗本地的算力，因为很耗电的。

**霄霄**: 这其实就牵扯到另外一个问题，就部署的问题。我们也会发现，如果你是这种**多智能体**、**多 Agent** 的话，它的部署会比**单智能体**的部署需要更大的资源 **Memory**。你就相当于你同时需要多个智能体，就要多个大模型需要。所以有一个方向，我们也现在看的话，就是说假设，如果说我们人类都是从一些**多人合作**、**多智能体合作**的方法去设计一些任务，我们先把这些任务做好了。但是在我们的部署阶段，我能不能把这么一个**多智能体协作**的任务，把它能够编译到**单智能体**能做的上面。这是两个方向，一方面你在设计的时候，你可以按照**多智能体**去设计，去做到尽善尽美。但是你在实际部署在资源有限的情况下，我们可以考虑到把它再压缩，再编译到**单智能体**。

**Indigo**: **单智能体**。那就更省资源一点吧，但是任务完成会打折吗？

**霄霄**: 会打折吧。

**Indigo**: 那什么任务能够接受打折呢？

**霄霄**: 比如说一些简单，或者说容错率比较高的任务，写邮件，或者是分析一些文档。

### AI 失败与人类审判能力

**Indigo**: 我们正好聊一个话题，关于 AI。既然你们在做安全，你们怎么看 AI 的失败？怎么能够看待容错？因为确实现在我觉得有个范式变化，我们之前的程序都是100%正确的。因为你是一个数字，1+1=2，这样出来的，永远都得到的是一个确定性的答案。所以有了上一代时代软件，但是它就有局限嘛，你得了确定性答案，它就没法像人类一样去做泛化的这种智能型的推理，很多模糊性的任务就做不好。那么现在我们进入到 AI 可以做模糊性任务了，可以编排任务了。那么那就有另外一个问题，就是我们到底是成功率要多少？然后失败什么样的能接受？什么样的任务能接受100%失败？我们才能够再看看你们研究的东西。你可以跟我们分享一下。

**霄霄**: 我可能就避免讲特别多学术的东西，但是从比较宏观地讲，确实在不同任务上面，我们对容错率的感知是不一样。我举个例子，如果是在医疗上面，或者是在法律上面，我们对 AI 的容错率是更低一点的，我们希望它可能是尽可能接近100%。但做一些可能它犯错代价不太大的任务上面，我们可能更加包容。那什么样子？就像说写邮件，我可能写几个 Template 也不会特别要紧。或者我生成一个视频，只是做娱乐的目的，也无所谓。我从我技术层面角度上面可能不讲特别多，因为我们的观众可能就是有特别 Diverse 的 Background。我从我自己也是做教育的，我会觉得目前一点就是，人类一定要有对 AI 给你的结果的一个**审判的能力**。之前也有一些研究表明，当你 AI 的能力越强的时候，他发现人类犯错的概率反而越大了，是他越来越相信 AI 的结果，而不认真去审查他的结果。我觉得这种 **Critical Thinking** 是人类一个非常重要的能力，也是现在学生他们需要去培训的一个很重要的能力。

**Indigo**: 那挺难。因为你想想看那个专家，如果说我是某一个领域的专家，比如说你是一个非常有经验的医生，就做大脑手术或者说肺部手术的，你可能很快地识别出来一个 AI 出的结果里面的问题，因为你是专家，你已经足够专业，而且你还有很多直觉在里面。这直觉是非常奇妙的一种东西。但是很多人他做不到啊，他没有 Sense，他不在这个行业里面，就是普通人。

**霄霄**: 这个太对了。我觉得其实绝大多数人都是普通人。所以我们在讲到 AI 对教育的影响也好，它还是，我觉得还是目前来说，让大家意识到 AI 有可能失败，这是一件非常重要的事情。

**Indigo**: 这个教育很重要。因为确实之前我在写这本书里面就讲了，很多人就完全相信 AI，他完全相信 AI，说什么他都觉得是对的。然后他这个就没有自我了，自己什么都不会了。但是这个里面会有一个问题，我觉得会有问题，就是说，要让普通人识别出一个专业 AI 里面的错误，我觉得这个是不现实的。你不可能让任何一个人都去接受专家训练。那么那是不是意味着未来要做这种专业级别的 AI 的这种出这种非常高可靠结果的时候，旁边必须得有非常专业的人类和他一块来做这个事情来出结论？就是如果我需要极高可靠，比如像医疗，我知道你们也在研究医疗的，是不是得必须得这样？还是说完全可以放心？或者说针对医生的专业人群的训练会有一些变化？

**霄霄**: 我觉得是不一定一定要有一个非常专业的人类去 Verify 每一个 AI 的结果。我们在对 AI 的输出的时候，可能有些方式可以让大家更加警惕到 AI 他可能犯了错误。我举个例子，现在你跟 **Chatbot** 去聊天的话，他给你输出的这些句子就是白底黑字。我们现在有一个面向这种医疗行业的 AI 研究，我们的输出会高亮一些 AI 有可能会答错的一些点，然后需要我们的用户就是医生，他们去 Verify 这个答案可能是对的。另外呢，我们可以把对应的医学术语给它链接到一些文献，还有一些以往的 **Clinical Guideline**，让他们去查。只有当他们 Confirm 了之后，不一定他们自己有这个能力，但我们会让他强制让他关注到这样高亮的词语，并且链接到外部的文献，让他们核实后，之后他们才能进一步进到下一步的分析里面。

**Indigo**: 相当于你会把那种会出错的地方能找出来。我觉得是一个设计，AI 输出设计的一个问题。

**霄霄**: 输出设计。那就是说什么是有哪个地方，比如说因为 AI 本身是概率嘛，你告诉你这个答案，就是它的概率是多少，然后给它一个结果出来，或者说可信度，你的 **Confidence Score** 有多少。如果它的可信度非常低，我们在0到1之间，它可信度只有一个0.2左右，它也给你输出了对应那个词，它可能会被高亮出来，需要人类进一步地进行一个审查。

**Indigo**: 那确实是个好办法。在医疗里面，或者其实很多在科研内容里面，或者说专业领域里面都需要。但对于我觉得对于普通人来说，好像确实 AI 搜索结果比他们自己能想到的好多了。

**霄霄**: 所以我觉得还是需要一个 **Critical Thinking** 的训练吧。我觉得现在确实如果大家过分依赖 AI 的答案的话，会让大脑产生一定的惰性。这是很严重的一个问题。

### AI 对教育与认知的挑战

**Indigo**: 我记得前两周，前一周，也是 **Anthropic** 做的研究还不错，他们也出了个报告，就是说你经常在使用 **Claude Code** 之后，你的哪些编程能力会退化。其实所有人都退化了。

**霄霄**: 会退化。

**Indigo**: 这个会退化。我经常就讲这个东西，可能就有点偷懒。就好像你想锻炼你的肌肉，你去练举重。但是后来你说现在有个高科技，它可以是外骨骼，然后我接上它之后举重很轻松。那么但是你执行这整套之后，其实你并没有锻炼到你的肌肉。最终我觉得以后会成为一种训练吧。你刚才说外骨骼这个比方很好，如果我经常在套外骨骼，那我的肌肉一定会萎缩的。那所以说我们接下来就两个选择，我除了用外骨骼之外，我得戒断。我现在说我自己用 **FSD** 我都不会开车了，所以每周7天，我有一天我自己开。

**霄霄**: 对啊，这是一个办法。对对对。

**Indigo**: 那就是我天天戴外骨骼搬东西，那有一天我自己搬，我要锻炼一下。或者说我要去健身房。当你在做工作的时候，你可以用外骨骼增强你自己，但是你还得保持你自己身体的健康，你保持肌肉的活力，你还是得去健身房。为什么以前古代的时候为什么大家都挺壮的，那劳力活都得自己搬嘛。你现在又有车又有东西，什么东西都有，都自动化了，那所有现代人都退化了。而现在大量的工作都做在这个 **Office** 里面，我觉得很快做 **Office** 人员变少了，因为你没有 **Office** 可以做了是吧。

**霄霄**: 讲到这我又想到有人会觉得之后可能不需要大学，不需要教育。因为 AI 已经能做这么多事情，你也可以能从 **GPT** 里边学很多的这个东西嘛。但是我觉得可能以后教育的理念，或者大学的本质，它并不是说在给你一些知识，而是说帮助人类去锻炼他的一些思维。这些东西并不是你可以直接能够从现在的 AI 里面所获得的。所以这个这很矛盾，我们新的教育。

**Indigo**: 正好顺着这个东西说，我们其实聊一个比较，我觉得一直是我在思考的问题，包括我在写书的时候，我也考虑过这个问题。我们叫做这可能是一种**认知坍塌**，或者叫做一种你认知就是萎缩了。慢慢就会，因为你当用大量的外界东西帮你的时候，你如何维护自己还有一些基本的认知？你想想看，我现在用 AI 会非常方便，我所有的知识我都不用记了。但是还有一个问题就是说，我坐在这个电脑前面，我面对 AI，我不知道问什么。

**霄霄**: 就是不知道问什么，那就是因为你没有目标。

**Indigo**: 那我如何才能有目标呢？很多人人生下来不是，但所有人/动物生下来我们都有，我们活着嘛，活着就是我们目标。但是对人这不一样，他有一些意义更高的目标，有人想事业有成，有人想赚钱，或者有人想追求艺术创作。就是在这个目标，在你很小的时候，你得让自己有这样的目标感。不然为什么我觉得很多人没有能动性，就是因为他没有很好很强的目标感驱动。而且我们之前大量的教育并没有教你要有这么强的目标感。

**霄霄**: 过去的教育最大的问题，就是说我把这个知识教会给你了，你会用它就行了。

**Indigo**: 这个相当于从上上个200多年前开始的教育。因为那个时候大量的人，世界上很多工作，或者说打仗这样的事情，我需要有技能的人来做这个事情。那么这个开学校了，我训练你怎么听口令，我训练你怎么走正步，我训练你怎么去用这个器械去战斗，怎么去训练怎么合作。然后后面随着这个知识更多的时候，我需要战争更复杂，然后经济更复杂，我需要学更多的东西。一切都是因为我要用这个技能我才去学。那么很清楚，我受教育目标我就学技能，学 **Skill**。那现在 **Skill** 我拿就好了，就像这个礼物一样，我还需要学吗？我不用学了。

### AI 拓展人类边界与未来经济

**霄霄**: 我觉得可能也是现在 AI 发展的两种方向。我们可以看到 AI 一方面有很多 AI 是提升人类的效率，它其实做的是人类能做的事情，但是能够以更快或者一种更好的方法把它做好。但是我觉得 AI 可能也潜在地有另外一种能力，就是**扩展人类的边界**。这件事情可能是要人和 AI 一起探索的，或者是由人的能动性主观去探索的。我举个比较简单的例子，我们在思考这一轮 AI 所谓的工业革命对人类的改变，相比于之前蒸汽时代一些改变，我们那个时候有很多很伟大的发明，包括飞机。飞机它其实不是提升人类效率，而是**拓展人类边界**的一件事情。但是我们现在看，其实 AI 它还没有朝着那个方向去发展。所以我觉得人类的创造力和寻找一些东西的意义，能够跟 AI 一起协同地去做这些伟大的事情。

**Indigo**: 了解。你说两个方面我都很认同。第一个是帮我们提升效率，帮我提效，这是显而易见的，现在大家都要做这些事情。但提升效率的同时，会让我们陷入到一个就是说你的认知会坍塌，因为你经常不这样做这个事情，你基本的认知结构就没有了。那这个事情呢，所以说这个回到了一个很核心的问题，我应该学习，应该学什么？我应该平时，就像以后真的像现在我老了体力劳动不用做了，大家要保持肌肉，我只能去健身房。那以后很多东西不用学了，我要保持这个我脑子里面有对世界有基本的，那我大脑这个世界有基本的理解和认知，那我只能去哪？去学校去健身房去锻炼。就是可能我就会有很多这种俱乐部，叫做什么**认知提升班**，或者**认知形成**。就是他可能就是让你做一些基本的，让你大脑形成这样的一个意识，或者形成这样一个结构，不至于忘掉对世界的这样理解。学校就是教你这样一套**通识教育**。你确实不用学怎么去做计算的，但是你的**通识**，大脑对这个世界理解的完整性，道德是什么，然后逻辑是什么，这种教育其实更重要了。因为之前可能我们都在学技能嘛。之前就是我们现在认为学校里面那些很无用的专业，可能会变得更有用了。

**霄霄**: 这个确实，还是那个报告里面我最近看也有趣一点，说 AI 可能最先取代的比较多了，比如说像法律之类的这些我们所谓的白领工作。取代比较少的就是比如说零售、销售、建筑这些行业。

**Indigo**: 这体力活吧。但是我说了，还有一种认知活就是，就是相当于是我能很好地就通识型的认知，认知后我感觉我你很 **Smart**，很有智慧。然后你可以把各种行业知识给像哲学，或者学一些理论的物理和数学这种计算的思维方式很重要。

**霄霄**: 我同意。我觉得这种跨领域的资源整合和创造力类似的东西的话，目前还是人类的一项优势，也是需要我们去保持的一种思维方式。

**Indigo**: 因为这个就是说我想的逻辑，在 AI 能够辅助我们完成很多技能型工作的时候，我们需要有这个能力，以至于我们在没有 AI 的时候，我们快速恢复我们的知识系统。

**霄霄**: 我觉得还有一点就是说，包括现在有一些简单任务的话，我也不会通过 AI 去做。我觉得很多东西是我浅层意识里面，它虽然也是一个我们叫做 **System Two**，它是经过一个思考去完成了任务，但是可能有些东西映射在我们人类大脑里面的东西，会让我一个更快的反应，而我不需要去等待。比如说现在的大模型它去反应好几秒才给我一个答案。

**Indigo**: 那就是直觉训练。一些直觉。这是第一个，我觉得可能教育会这样变化，教的东西不一样了。就是我们之前，我说最前面，我们之前所有教育都是技能教育。可能有的大学里面，有的大学在北美大学风格不一样，那有的学校就挺适合教这种东西的，培养这个人的创造力。但是有的学校很工科，就是需要训练工程师，有的就是训练科学家的，像**普林斯顿**。然后第二个，就是我觉得更有意义更大的事情，就是**拓展边界**。这个时候你刚才给的非常好的一个，我们认为现在这一轮 AI 革命带来了更有意义的事情。如果说不**拓展边界**，那我们的经济就永远是这个饼就这么大，你效率再高，你也只是我效率高的把效率低的吃掉了，我饼变不大。

**霄霄**: 是的。

**Indigo**: 那如果边界变大了之后，那我们就要做更大的饼了，更大的饼，那我经济还会增长。如果饼不大，经济增长不了，就把饼闹大。那这个边界就是刚刚讲的科学艺术，科学边界，艺术边界和这些研究的边界。总体上来说，就是一种创造力的一些边界。它可能需要很多的这种领域的融会贯通。

**霄霄**: 那确实是的。

**Indigo**: 这个就跟之前说，其实有个例子挺好的，挺形象的。我们为什么一个变革发生的时候，总是让这个我们在这空间上移动的效率变高。比如说蒸汽机之后，那叫蒸汽，蒸汽车，就快嘛。然后后面还有铁路的，然后电力时代就有了那个铁轨，就是高铁。然后后来还有引擎，有了之后就有飞机嘛。我们永远都是因为这个移动速度变快了之后，让这个世界突然一下这个结构变，就是物理的结构，移动物理世界的结构发生了变化之后，就会有让这个空间就是饼就变大了。你就新的职业和新的这个，他就是因为让那个我们人类在这个空间里面移动的效率变得极高了。那么我们这一次除了 AI 是让我们头脑的，就是以前**乔布斯**说那个电脑就是个人大脑自行车。其实现在 AI 才是叫智力的这个叫做加速器。那么有了 AI 能够让我们智力边界变高。其实还有一点就是说，我如果说在物理层面上来看呢，就是两方面，一方面是机器人可以让我们的效率变得更高，因为它可以帮我们干很多事情。然后另外一摊呢，我们人移动是这样移的，往上移的，就是你可以往外，你可以把地球想象成之前的你只是在表面，然后马上你可以在外空间上面，在近地轨道上面，可以干很多事情。就**升维**了。

**霄霄**: 就是直接升了一维。

**Indigo**: 所以说我觉得未来把饼做大的几个地方，就是 AI 能够把 AI 饼做大。然后这个空间运转能力，我们能够把饼做大。然后 AI 还能够驱动这个机器人把生产的效率提高了，饼做大。所以说我觉得在这个下面来看，这个教育和边界的意义就全变了。

**霄霄**: 我觉得这个它也是会随着 AI 的发展，它会去改变的这么一些行业。

### 大学教育与职业选择建议

**Indigo**: 那么最后吧，我们来总结一下。你做教育，你是教育工作者，总结一下，你从你的这个怎么来看这个大学的教育，在现在这样的一下面的一些，或者说给年轻人选大学，或者选未来求职一些建议。

**霄霄**: 这是一个很现实的问题。我觉得吧，大学的教育目前来说还是非常有意义的。它还是在锻炼两种思维能力，我们之前也谈到过，一个是 **Critical Thinking**，就是你对问题和答案的一个审视能力。另外一个是逻辑思考的能力。所以说我觉得就目前来说，我还是觉得教育它是一个有意义有价值的行业。但是就对于我们的学生来说的话，我们会发现之前或者这几年吧，或者更早一些，会有越来越多的学生选择 AI 的专业。我觉得这可能要保持一定的审慎的态度。就像 **Anthropic** 最近出的那个报告来说的话，我们会发现有一些行业可能会更早地被 AI 取代，有些会晚一些。

**Indigo**: 程序员。

**霄霄**: 所以就是可能在选择这个专业方向上面，大家可能会不要统一往一些可能高密度又同样有高风险被取代的一些行业去挤兑。但是我们也要抱有乐观的心态，很有可能有一些新的专业会在 AI 热潮之后去诞生。

**Indigo**: 什么专业 For Example？

**霄霄**: 就像你说的，怎么让人和 AI 更好地去协作，这可能说不定是商学院管理学里边需要一个新的课题。当然也可能做 AI plus Whatever，他们都可能形成一个新的学科。讲到这个，可能就是说人和 AI 之间目前看到一个趋势，就是需要他们的协同能力。所以我觉得还是要保持，要学会跟 AI 去协作。这也是我们在大学教育里边一个非常重要的课题。

**Indigo**: 这个还挺难的。但是我刚才我要总结一下。我觉得我们之前所有的教育都是在教技能，那当然也会启发思考，但是从来没有教你怎么去找意义和定自己的目标。这个我觉得是非常的，因为很多人其实打工人嘛，反正你上班就是为了老板给你下目标的，你回家活着就是目标。这实际上如果说这样的牛马工作越来越少，然后能够给你下目标的老板都不找你了，那么你自己得形成自己的目标。而且我觉得当这个就是繁重的那些，就我们叫做比较耗散的那些工作，就是我们叫**牛马工作**，就是重复性的工作，需要硬技能的工作，重复性的工作越来越少的时候，你人很多时间就被释放出来了。那么当然，这一个释放出来的比较现实的问题，很多人可能适应不了这个世界变化。就跟从农业社会往工业时代变，从工业时代往信息时代变，很多人就淘汰掉了。他就当一辈子农民吗？他就不学任何技能。很多人就会被淘汰掉，这是自然法则。所以说大家很多人都不想自己或者是自己的下一代当变成被淘汰掉的人。那么我觉得这个里面，你从小就开始寻找目标和意义感，最重要的就是。

**霄霄**: 我同意，家庭要给他这样的一个环境，你就折腾就好了。你哪怕你自己去学。

**Indigo**: 其实很多我觉得欧洲这种比较过得富裕，富裕生活过得很久的地方，其实这些都是这样子。你看他们好多人其实我就喜欢艺术，特别 **Artist** 的嘛。比如说我就喜欢创作这些东西，他所有的东西他通过他的创作，而且现在这些创作很容易形成网络。我喜欢在这个里面去做一些 **Creative** 的事情，我能够形成一个 **Network**，大家一块 Enjoy 这个事情，大家都得到了满足。而且 AI 最后还能帮你强化创作，做得好。其实我觉得任何人接下来这是关于做 **Creative** 的事情，把 **Creative** 边界，把创意的边界打开之后我们会进入到一个全新的文艺文化，**新的文艺复兴**的时代。所以这个时候我们很多普通体力工作劳动力都会减少，大家都在这个人去做新的创作，形成新的这种子网络。我觉得会是，而且这个人也会产生新的经济嘛，我并不清楚他是什么样的经济。我觉得未来可能10年左右，会极大的可能性往这个方向发展。

**霄霄**: **Indigo** 你讲得特别好，我觉得意义这个东西很重要，而且你的眼光也放得特别长远。我就再讲一点比较近期的吧，短期来看，不得不承认一点，就是说人类得在满足温饱之后才能寻找他们的意义。所以可能就有人会问一个问题，就是说做怎么样的工作不被 AI 取代。其实这个问题不仅是 AI 有了之后会思考这个问题，包括可能大模型之前我就有学生加入我们组里面，他们会问，就是说我们学生培养的目标是什么。其实不管是不是 AI，我们都希望做一个不可能被轻易取代的人。或者说我们的工作对于 AI 来说，是一个在超出它的分布的一个工作。

**Indigo**: 超出它的分布。

**霄霄**: 我举个例子，为什么你刚刚讲到 AI 可能会很快地取代程序员。是因为我们有大量的代码在 **GitHub** 上面，或者是公司的内部，你可以去训练这个 AI 模型，它就掌握了编程的一项技能。但是如果你的工作它是一个比较 **Unique** 的工作，然后 AI 它并没有办法在你所做的这个工作上面获得大量的数据进行学习的话，那你被 AI 取代的可能性就会更低一点。

**Indigo**: 是的。大概率你可能会变成一个给 AI 搞数据的人。只要自己不参与进去，只要不给他喂大量的数据，这也是自己形成一个护城河的一个方法。这为什么之前 **OpenAI** 会招了很多医生去给他们标注，或者用 **Biology** 去标注他们那些医学的数据。另外一点我想补充的就是说，我们今天可能聊得过于乐观，觉得 AI 能够做很多事情，能够取代了人类的一些工作。但是因为我是做计算机的嘛，在软件工程里边，有个非常著名的**九十九十法则**。就是说我们可能90%的代码花了90%的时间去完成，但剩下的10%的代码也需要花90%的时间去完成。我觉得目前我们看到 AI 的发展普遍的话可能会过于乐观，但有可能我们只在中途，剩下10%的难度会超乎我们的想象。

**Indigo**: 就是99.9%后面那个0.1%还会花掉90%的时间。我觉得是这样的。你可以从自动驾驶的发展规律也可以验证到这么一个法则。

**霄霄**: 挺好的，我觉得这样才能平衡，不然我们真的没事做了。

**Indigo**: 人永远做最后最尖端的这个事情。那我这个未来不好确定。我听上去我们每次聊一次节目之后，未来又好又不好，都是这种结论。

**霄霄**: 这种模糊的状态就是个客观的状态。

**Indigo**: 那好吧，我们正好我们是在春节前，我准备在春节前把这一期放了。那我们就当农历新年前面录制的最后一期送给大家。祝大家春节快乐，也祝大家马年快乐。

**霄霄**: 好，拜拜。