---
author: TechButMakeItReal
date: '2025-11-13'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=mL24RKiBeU0
speaker: TechButMakeItReal
tags:
  - tech-bubbles
  - scaling-laws
  - psychological-biases
  - market-speculation
  - deceptive-marketing
title: 科技泡沫是如何诞生的：从互联网到人工智能
summary: 本文深入探讨了科技泡沫的形成机制，以当前的人工智能泡沫为例，并与互联网泡沫和铁路狂热等历史事件进行对比。文章分析了泡沫背后的技术局限性（如AI缩放定律的失效、数据稀缺）和一系列心理偏差（如线性思维、错失恐惧症、过度自信和外推偏差）。通过剖析特斯拉全自动驾驶和Meta元宇宙的营销策略，揭示了语言和市场预期如何助长泡沫，强调了理解这些模式对投资者和消费者至关重要。
insight: ''
draft: true
series: ''
category: finance
area: market-analysis
project:
  - ai-impact-analysis
  - market-cycles
  - historical-insights
people:
  - Mark Zuckerberg
  - Elon Musk
  - Andre Shleifer
  - Cassie Kozyrkov
companies_orgs:
  - Meta
  - General Motors
  - Cruise
  - Waymo
  - Google
  - Tesla
  - OpenAI
  - Nvidia
  - Goldman Sachs
products_models:
  - Full Self-Driving
  - Autopilot
  - ChatGPT
  - GPT-3
  - GPT-4
  - Tesla Model S
  - Gamma AI
media_books: []
status: evergreen
---
### 科技泡沫的本质与重复的错误

人工智能泡沫已经在萎缩，并且有迹象表明，有时最强烈的迹象并非来自股市。
它体现在“全自动驾驶”（Full Self-Driving）与“受监督的全自动驾驶”（Full Self-Driving Supervised）这两个词语的差异中。
你能分辨出其中的区别吗？
它们几乎是相同的短语，但却代表着完全不同的产品。
这一个词语揭示了科技泡沫如何诞生以及我们为何总是相信它们的一切。
问题是，如果这些迹象一直都在，我们为何还会上当？
投资者又是如何上当的？
答案根本不在于技术本身。
而是因为人类就是如此。
现在关于人工智能泡沫的讨论甚嚣尘上，无论你是否相信它会破裂。
但一直困扰我的问题是：为什么？
为什么会形成泡沫？
我们经历过互联网泡沫。
我们经历过2008年金融危机。
我们为何会重蹈覆辙？
今天我们不是要预测或猜测泡沫何时或是否会破裂。
而是要理解科技泡沫最初是如何产生的。
让我们深入探讨。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The AI bubble is already deflating, and there are signs, and sometimes the most powerful one is not in the stock market.
It's in the words "full self-driving" versus "full self-driving supervised."
Can you tell the difference?
It's almost the same phrase but entirely different products.
That single word tells you everything you need to know about how tech bubbles are born and why we always believe them.
And the question is, if the signs were always there, why did we fall for it?
How did investors fall for it?
And the answer isn't about tech at all.
It's because that's what people do.
There is so much noise about the AI bubble right now, whether you believe it or not, whether it'll burst.
But the question that kept bothering me is why?
Why do they form?
We've seen the dot-com bubble.
We've seen 2008.
Why are we repeating the same mistakes?
Today isn't about predicting or speculating when or whether it'll burst.
It's about understanding why tech bubbles are born in the first place.
Let's dive in.</p>
</details>

### 语言的转变与元宇宙的兴衰

你可能已经注意到，行业是如何从**自主智能体**（Autonomous Agents: 能够独立感知环境、做出决策并执行任务的AI系统）、**元宇宙**（Metaverse: 一个虚拟的、沉浸式的数字世界，用户可以在其中互动）转向**副驾驶**（Co-pilots: 辅助用户完成任务的AI工具）、虚拟助手和AI伴侣的。
最近，人工智能的语言学确实发生了转变。
但请记住，就在两年前，Meta公司还曾积极推动元宇宙叙事，将其作为公司的旗舰方向。
还记得马克·扎克伯格戴着眼镜的那些视频吗？
那些视频无处不在。
著名的Facebook更名为Meta，并向其**现实实验室**（Reality Labs: Meta旗下专注于VR/AR硬件和软件研发的部门）、VR设备、虚拟形象和虚拟生态系统投入了数十亿美元。
这是该公司自成立以来，Facebook首次进行全面品牌重塑，而这一切都因为人工智能而发生。
然而，快进到今天，整个元宇宙叙事正逐渐成为过去式。
当前的主流趋势是**个人超级智能**（Personal Super Intelligence: 指高度个性化、能够超越人类智能水平的AI系统）。
你可以感受到**智能体AI**（Agentic AI: 强调AI系统自主行动和决策能力的趋势）主题的影响。
有趣的是，他们仍在投资和生产硬件。
你一定见过他们最近推出的支持AI的眼镜。
眼镜和许多AI硬件现在是他们的主打产品，但元宇宙的语言正被慢慢地扫到一边，因为超级智能取代了整个品牌定位。
而这种语言上的品牌重塑，如果可以这样说的话，在其他行业中也能看到。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">You might have noticed how the industry began shifting from autonomous agents, metaverse to co-pilots, virtual assistants, AI companion.
The linguistics of AI have really shifted lately.
But remember how just two years ago, Meta has aggressively pushed the narrative of metaverse as its flagship direction.
And remember the videos with Mark Zuckerberg wearing the glasses?
Those videos were everywhere.
The famous Facebook became Meta and they invested billions into reality labs, VR sets, avatars, virtual ecosystems.
This was the first time since the company's inception when Facebook did a full-scale rebranding, and all of it happened because of AI.
But fast forward to today and the whole meta narrative is slowly becoming a thing of the past.
The current vibe is personal super intelligence.
You can smell the effect of Agentic AI theme.
The interesting thing is that they still invest and produce hardware.
You must have seen the AI-enabled glasses that they launched recently.
The glasses and a lot of AI hardware is their thing now, but the metaverse language is being slowly swept under the rug because super intelligence took over the entire branding.
And this linguistic rebranding, if I may, can be spotted in other industries as well.</p>
</details>

### 自动驾驶的承诺与现实

例如，**机器人出租车**（Robo-taxi: 自动驾驶出租车服务）市场，特别是美国汽车工业巨头**通用汽车**（General Motors, GM）。
在通用汽车内部，他们有一个名为**Cruise**的部门。
Cruise的主要业务是自动驾驶汽车和机器人出租车服务。
三年前，其最初的定位是自动驾驶机器人出租车。
他们的承诺和叙事是在旧金山及其他主要城市推出完全无人驾驶的**L4级别**（Level 4: 高度自动化驾驶，在特定条件下无需人类干预）汽车。
Cruise的公关、投资者演示文稿和新闻稿都将其描述为一项突破，即广泛可用的无人驾驶按需机器人出租车服务。
但随后在两年前，即2023年10月，Cruise部门因多起事故而陷入困境。
甚至发生了一起出租车在碰撞后拖拽行人的事件。
正如你所料，这引发了巨大的反弹。
Cruise的运营许可证被暂停，公众安全担忧加剧，整个叙事立即转向“运营暂停、调查进行中、根本原因分析”。
你可能会问，那**Waymo**（Google旗下自动驾驶技术公司）是如何成功的呢？
Cruise和Waymo都曾争夺自动驾驶无人出租车业务的主导地位，但在品牌、公关以及最重要的是运营决策上存在巨大鸿沟。
Cruise曾积极宣传自己是完全无人驾驶L4级别自动驾驶机器人出租车的先驱。
以防你对这些自动驾驶等级不熟悉。
**L0级别**（Level 0: 无自动化）意味着你驾驶汽车，你负责一切。
**L1级别**（Level 1: 驾驶辅助）意味着汽车可以自动完成一项任务，例如巡航控制、车道保持或制动，但你仍然控制其他一切。
现在，**L2级别**（Level 2: 部分自动化）意味着汽车可以做两件事：转向和制动，或者转向和加速。
因此，它可以在理想条件下在高速公路上自动驾驶，但你必须保持警惕，并随时准备接管。
你负责监督，你是责任人。
**L3级别**（Level 3: 有条件自动化）意味着汽车可以在特定条件（如高速公路）下处理大多数驾驶任务，但如果出现问题或条件变化，它可能会要求你接管。
在这种情况下，你负部分责任，并处于待命状态。
**L4级别**（Level 4: 高度自动化）正是Cruise所承诺的，也是Waymo最终能够实现的。
汽车可以在没有人为干预的情况下自行驾驶。
正常操作不需要人类驾驶员，但系统有局限性。
例如，只能在良好天气下或在某些特定道路上。
L4级别意味着你可以在车里睡觉，无需注意路况。
最后是**L5级别**（Level 5: 完全自动化），汽车可以在任何条件下、任何地方自行驾驶。
永远不需要人类。
这是完全无人驾驶，目前尚不存在。
因此，Cruise的叙事侧重于快速商业化推广。
他们大胆宣称完全移除人类驾驶员，然后扩展到更广阔的地理区域。
其商业模式旨在通过打车应用实现纯移动运营，其整个商业模式和市场进入策略都是一次性的颠覆。
他们没有公开谈论分阶段推出。
另一方面，Waymo是Google的产品。
Waymo是一家技术原生公司的产物。
Waymo显然准备得更充分，了解技术发布的工作方式。
他们将其宣传为世界上第一个商业化的L4级别自动驾驶服务，但在亚利桑那州凤凰城进行了有计划的、安全至上的试点。
他们非常重视用户体验，并且信息传递不断强调持续的安全监督。
他们采用了分阶段推出策略，并将自动驾驶定位为渐进式创新，而非快速颠覆。
与通用汽车不同，Waymo凭借其自动驾驶声明以及清晰地沟通限制和暂停，得以维持运营。
最后，我最喜欢的一个例子。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">For example, robo taxi market and especially the giant of the American automotive industry, General Motors.
And within the GM, they have a division called Cruise.
Cruise's primary focus is autonomous vehicles and robo taxi services.
Three years ago, the original positioning was autonomous robo taxis.
The promise and the narrative was fully driverless level four cars in San Francisco and other major cities.
Cruis's PR investor decks press releases all described as a breakthrough in widely available uncrewed ondemand robo taxi service.
But then two years ago in October 2023, the Cruise division got in trouble because of the multiple incidents.
There was even one when a taxi was dragging a pedestrian after a collision.
And as you would imagine, this was followed by a major backlash.
Cruz's permits got suspended, public safety concerns, and the whole narrative immediately shifted to operation suspended, investigation ongoing, and root cause analysis.
You may ask, but how did Waymo pull it off?
Then Cruise and Waymo both fought for dominance in autonomous driverless taxi business, but there was a chasm in branding, PR, and most importantly, operational decisions.
Cruise aggressively marketed as a pioneer of fully driverless, Level four, autonomous robo taxi.
Just in case you're unfamiliar with this whole leveling thing.
Level zero is no automation.
You drive the car.
You drive everything.
You're responsible for everything.
Level one is driver assistance.
The car can do one thing automatically.
Cruise control, lane keeping, or braking, but you still control everything else.
Now, level two, partial automation.
The car can do two things.
Steer and brake or steer and accelerate.
So, it can drive itself on a highway in ideal conditions, but you must stay alert and be able to take over at any point.
You are supervising and you're the one responsible.
Level three, conditional automation.
When the car can handle most driving tasks in specific conditions like highways, but it can demand you take control if something goes wrong or conditions change.
In this case, you're partially responsible and you're on standby.
Level four, high automation.
This is what Cruise promised and what Waymo was able to achieve in the end.
The car can drive itself without human input.
A human driver is not needed for normal operation, but the system has limits.
For example, only in good weather or only on certain roads.
Level four means that you can sleep in your car and not pay attention.
And finally, level five, full automation.
The car can drive itself anywhere in all conditions.
No human needed ever.
This is complete driverless and this does not exist yet.
So for Cruise, the narrative was focused on rapid commercial rollout.
A lot of bold claims about removing a human driver entirely and then scaling up to wider geography.
The business model aimed for mobile only operations all done through a ride app and their whole business model and go to market strategy was all at once disruption.
There were no public talks about a phase roll out.
Waymo on the other hand is Google's product.
Waymo is a child of a tech-native company.
Waymo was clearly a lot more prepared knowing how tech launches work.
They market it as the world's first commercial also level four self-driving service but with a measured safety first pilot in Phoenix, Arizona.
They put a big emphasis on the user experience and the messaging kept reassuring about ongoing safety oversight.
They went with a phased roll out and framed autonomy as incremental innovation, not fast disruption.
And unlike GM, Waymo stayed afloat with its autonomous claims and their clearly communicating limitations and suspensions.
And lastly, my favorite example.</p>
</details>

### Gamma AI：一个不夸大其词的工具

我很少分享关于我自己的个人事实，但这里有一个。
如你所知，我做了很多研究。
我喜欢剖析数据并处理统计资料。
我甚至喜欢在凌晨两点阅读科学论文。
但是，当我需要整理我的发现并进行演示时，无论是在视频中还是在工作中发布产品，让所有这些数据变得可读、易懂且美观，对我来说都是一种折磨。
在视觉方面，我真的缺乏创造力。
作为一个从事产品工作的人，我不能仅仅不参加演示。
我必须进行全公司范围的更新。
我必须进行团队演示。
我必须进行功能发布和客户网络研讨会。
我需要的幻灯片不仅仅包含要点，它们需要有说服力。
如果我要向五种不同的受众进行演示，它们需要以五种不同的方式在五种不同的场合产生效果。
这就是为什么像**Gamma AI**这样的工具对我来说是救星。
我只需把我需要的所有东西，一股脑儿地扔进Gamma，然后告诉它该怎么做。
我专业地使用它，也和我的YouTube团队一起使用。
在YouTube方面，我们正在为一月份的大规模重新发布做准备。
幕后有太多事情需要完成。
Gamma就是我如何将所有数据、所有凌乱的笔记以及我脑海中的一切，转化为可以与团队分享的精美演示文稿的方式。
我知道我不会因为缺乏设计技能而受到评判。
我只需要专注于我擅长的事情，剩下的交给Gamma处理。
与我们在这个视频中讨论的产品不同，Gamma不会过度承诺。
它不声称用AI魔法取代你的整个工作流程。
它只做一件事，而且做得非常出色：接收你的内容并将其转化为可供演示的文稿。
如果你至少尝试过一次Gamma，你就会知道看到你的数据转化为真正美丽的东西是多么令人满意。
没有学习曲线。
没有失望。
只有在你需要时，就能得到精美的演示文稿。
非常感谢Gamma赞助了视频的这一部分。
你无法想象我的1月份准备工作变得多么轻松。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I rarely share personal facts about myself, but here's one.
As you know, I do a lot of research.
I love dissecting data and working with stats.
I actually enjoy reading scientific papers at 2:00 in the morning.
But when I need to compile my findings and present, whether it's on video or if I'm presenting a product release at work, making all of that data readable, digestible, and aesthetically pleasing, is a torture for me.
I am really not creative when it comes to visuals.
And as someone who works in product, I don't get to just opt out of presentations.
I have to do companywide updates.
I have to do team presentations.
I have to do feature releases and customer webinars.
And I need slides that don't just contain bullets.
They need to land.
And if I'm presenting to five different audiences, they need to land five different times in five different ways.
That is why a tool like Gamma AI is a lifesaver for me.
I just take everything I need.
I dump it into Gamma and I tell it what to do.
I do it professionally and I do it with my YouTube team.
On the YouTube side, we're preparing for a massive relaunch in January.
There is so much that needs to be done behind the scenes.
And Gamma is how I turn all of my data, all of my messy notes, everything that's on my mind into beautiful presentations that I can share with my team.
And I know I'm not going to be judged for the lack of my design skills.
I just need to focus on what I'm good at.
And Gamma handles the rest.
Unlike the products that we're talking about in this video, Gamma doesn't overpromise.
It's not claiming to replace your entire workflow with AI magic.
It's doing one thing exceptionally well.
Taking your content and making a presentation ready.
If you've tried Gamma at least once, you know how satisfying it feels to see your data transform into something actually beautiful.
There's no learning curve.
There's no disappointment.
Just polish presentations exactly when you need them.
Huge thanks to Gamma for sponsoring this portion of the video.
You have no idea how much easier my January prep has become.</p>
</details>

### 特斯拉FSD的误导性营销

特斯拉将其品牌从“全自动驾驶”重塑为“受监督的全自动驾驶”，如果你仔细思考，**自动驾驶**（Autonomous: 指车辆在特定或所有条件下无需人类干预即可自行驾驶）与**受监督的自动驾驶**（Supervised Autonomous: 指车辆具备自动驾驶功能，但仍需人类驾驶员持续监控并随时准备接管）之间存在天壤之别。
它们听起来相似，但用户体验却完全不同，因为受监督的自动驾驶本质上承认该产品是L2级别驾驶辅助，而非L5或L4级别的自动驾驶。
因为L4或L5级别意味着你可以在驾驶座上睡着，更不用说在后座了。
特斯拉的这种虚假宣传在美国、中国和澳大利亚引发了多起关于欺骗性营销的诉讼。
这种营销的问题在于，当你声称“全自动驾驶”时，你暗示的是L4或L5级别的自动驾驶，这意味着车辆在任何条件下都不需要人类输入或监控。
媒体和营销都喜欢“无人驾驶”这个词。
而作为普通消费者，当你听到“无人驾驶”时，你会想到什么？
你会想到汽车可以自己驾驶。
但“受监督的全自动驾驶”在现实中意味着系统完全需要人类持续监督，驾驶员随时准备接管。
这是L2级别的自动驾驶，L2级别在功能上等同于巡航控制。
这种差异不是语义上的，而是类别上的。
L2级别意味着你始终负责。
L4或L5级别意味着你可以在座位上睡觉。
这不仅仅是功能上的缩减。
它是一种根本不同的产品。
如果我可以说，特斯拉的案例是整个AI、自动驾驶汽车或相关科技领域更大模式的“煤矿里的金丝雀”（Canary in the Coal Mine: 指预警危险的早期信号）。
今年八月，特斯拉因一起**非正常死亡案件**（Wrongful Death Case: 指因他人的过失、鲁莽或故意行为导致死亡的法律诉讼）被起诉，并被判支付近2.43亿美元。
那是一起发生在六年前的事故，当时一名驾驶员驾驶一辆2019年款**特斯拉Model S**（Tesla Model S: 特斯拉公司生产的一款纯电动豪华轿车），并启用了**自动辅助驾驶**（Autopilot: 特斯拉的驾驶辅助系统，提供车道保持、自适应巡航等功能）。
汽车驶近一个有多个停车标志和闪烁红灯的T形路口。
驾驶员将视线从道路上移开，驾驶员和特斯拉都没有对路口发生的情况做出反应，汽车径直穿过了路口。
现在听听判决的细分。
这很重要。
责任是这样划分的：67%的过错归咎于驾驶员，33%归咎于特斯拉。
归咎于特斯拉的33%意味着系统本身被认为是造成伤害的一个重要原因，而不仅仅是被驾驶员误用的被动工具。
这对整个汽车行业发出了一个信号。
现在他们必须谨慎对待自己的营销。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Tesla's rebranding from full self-driving to full self-driving supervised, which if you really think about it, there is a world of a difference between autonomous and supervised autonomous.
They sound similar, but they're completely different user experiences because supervised autonomous essentially admits that the product is level two driver assistance, not level five or level four autonomy.
Because level four or level five implies that you can fall asleep in a driver's seat, let alone in the back seat.
This false advertising on Tesla's part triggered a number of lawsuits in the US, China, and Australia over deceptive marketing.
The problem with this marketing is that when you say full self-driving, you imply level four or level five autonomy, meaning that the vehicle requires no human input, no monitoring under any conditions.
The media and marketing loves the word "driverless."
And what do you hear as an average consumer when you hear "driverless?"
You hear the car can drive itself.
But "full self-driving supervised" in reality implies that the system fully requires constant human supervision with the driver ready to take control at any point.
This is level two autonomy and level two is functionally equivalent to cruise control.
The difference is not semantic, it's categorical.
Level two means you're always responsible.
Level four or five means you can sleep in your seat.
This is not just scaling back.
It's a fundamentally different product.
Tesla's case is a canary in the coal mine, if I may, for a much larger pattern across AI, across autonomous vehicles or adjacent tech sectors.
In August this year, there was a lawsuit against Tesla in regards to a wrongful death case with Tesla ordered to pay almost $243 million.
There was a crash.
It happened six years ago when a driver was operating a 2019 Tesla Model S with autopilot engaged.
The car approached a T intersection with multiple stop signs and flashing red lights.
The driver took his eyes off the road and neither the driver nor Tesla reacted to what was happening in the intersection and the car drove straight through it.
Now listen to the verdict breakdown.
This is important.
The liability was split like this.
67% of fault assigned to the driver.
33% assigned to Tesla.
That 33% assigned to Tesla means that the system itself was found to be a meaningful cause of harm, not just a passive tool that was misused by a driver.
And this was the signal to the entire automotive industry.
And now they have to be careful with their marketing.</p>
</details>

### 科技泡沫的深层原因：技术局限与心理偏差

如果你正在听这段内容并思考，好吧，Meta曾经对整个元宇宙充满热情，但现在他们正在逐渐转向。
特斯拉曾表示到2020年我们不再需要出租车司机，但不得不收回此言。
但现在每个人都在谈论泡沫。
这个泡沫是如何形成的？
如果**ChatGPT**（ChatGPT: 由OpenAI开发的人工智能聊天机器人，以其强大的自然语言处理能力而闻名）爆发之前就有迹象，那它为何还会发生？
所以我过去一周都在研究泡沫为何形成，我们为何不吸取教训，以及为何即使我们知道泡沫即将到来，我们仍然不断购买AI股票。
答案在于这是**人类本性**（Human Nature: 指人类固有的行为、思维和情感模式）。
而这个泡沫之所以形成，是技术不准确性、经济压力和大量心理偏差共同作用的完美风暴。
让我来向你证明。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And if you're listening to this and thinking, okay, so Meta was excited about the whole metaverse, but now they're kind of shifting away from that.
Tesla said we wouldn't need taxi drivers by 2020 and had to take it back.
But now everybody's talking about the bubble.
How did this bubble form?
If there were signs prior to the explosion of ChatGPT, why did it happen?
So I spent the last week studying why bubbles form, why we don't learn, and why even though we know that the bubble is coming, we keep buying AI stocks.
And the answer to that is that it's human nature.
And the reason this bubble has formed is a perfect storm of technical inaccuracies, economic pressures, and a mountain of psychological biases.
Let me prove it to you.</p>
</details>

### AI缩放定律的失效

直到最近，所有人工智能的进步都依赖于**缩放定律**（Scaling Laws: 指在机器学习模型中，增加计算资源、数据量和模型规模能可预测地提升性能的经验法则）的概念。
缩放定律本质上是一种假设，即增加计算能力（我指的是计算机处理数据的速度和数量，即输入AI的文本、图像和其他信息的量）和模型规模（即AI拥有的参数数量）可以提高性能的可预测性。
可以将其想象成机器上旋钮的数量。
因此，缩放定律假设增加这三者都能提高性能的可预测性。
其核心思想是“越多越好”。
数据越多越好。
模型越大越好。
计算能力越强越好。
你可能会问，这是谁提出的？
这种因果关系从何而来，它真的存在吗？
答案有些令人惊讶，因为实际上没有人真正提出过。
30年来，研究人员和数学家只是不断观察到缩放是有效的。
更大的模型意味着更大的成果。
更多的数据意味着更少的错误。
这纯粹是经验性的模式识别，而非物理定律。
所以他们一直在这样做，但没有人真正深入探究它为何有效，以及当可供缩放的东西用尽时会发生什么。
而他们确实用尽了可供缩放的东西。
他们用尽了高质量数据，因为互联网上几乎所有高质量、多样化的人类文本都已被收集并用于训练大型AI模型。
他们开始触及计算硬件的物理极限。
还记得**摩尔定律**（Moore's Law: 指集成电路上可容纳的晶体管数量大约每两年翻一番的经验法则）吗？
本质上，它观察到芯片的计算能力每隔几年就会翻倍。
而这条定律正在急剧放缓，因为这些芯片正接近原子尺寸的极限。
它们现在正触及天花板。
缩放定律多年来一直有效，但现在每提高1%的性能都需要10倍甚至100倍的资源，这使得额外的收益变得极其昂贵。
但有一个更深层次的问题，使得数学假设的不准确性在某种程度上变得无关紧要。
那就是数据本身，数据的质量和稀缺性。
互联网大约包含500万亿个文本**标记**（Tokens: 在自然语言处理中，指文本被分割成的最小有意义的单位，可以是单词、子词或字符）。
但其中很多都是垃圾数据。
最重要的是，由于硅芯片接近物理极限，摩尔定律正在放缓。
自2012年以来，AI计算能力每三个月翻一番，但半导体制造产能已全部预订到2026年。
当你听到95%的AI项目失败时，根本原因几乎总是糟糕的数据。
许多**大语言模型**（LLMs: Large Language Models，基于深度学习，能够理解、生成和处理人类语言）所依赖的基础——缩放定律的概念——正面临**收益递减**（Diminishing Returns: 指在生产过程中，当某种投入要素增加时，其边际产出逐渐减少的现象）。
要获得第一个单位的改进需要一个单位的数据，但下一个单位需要10个，再下一个需要100个。
但每个人都期待着一场革命，而这场革命并没有发生，因为它在技术上不可能发生。
因此，这是技术层面，现在让我们转向心理层面。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The entire AI progress until recently has relied on the notion of scaling laws.
Scaling laws are essentially principles that assume that increasing computing power, and by computing power I'm referring to how fast and how much a computer can process data, meaning the amount of text, images, and other information that are fed into AI, and model size, which is how many parameters AI has.
Think of it as like the number of knobs that you can have in a machine.
So scaling laws assume that increasing all three improves performance predictability.
The underlying idea is "the more the better."
The more data the better.
The bigger the model the better.
The more computing power the better.
You may ask who came up with that.
Where does this causal relationship come from and does it even exist?
And the answer is somewhat surprising because nobody really did.
For 30 years researchers and mathematicians just kept observing that scaling worked.
Bigger models meant bigger results.
More data meant fewer mistakes.
It was pure empirical pattern recognition, not a law of physics.
So they kept doing it, but nobody was really digging into why it worked and what happens when you run out of things to scale.
And they did run out of things to scale.
They ran out of high-quality data because nearly all high-quality diverse human text on the internet has already been collected and used to train large AI models.
They started reaching physical limits of computing hardware.
Remember the Moore's law?
Essentially, the observation that chip power doubles every couple of years.
And that law is slowing down dramatically because those chips simply approach the limit of atomic size.
And they're now hitting the ceiling.
Scaling laws worked for years, but now each 1% improvement requires 10 or even 100 times more resources, which makes additional gains outrageously expensive.
But there is a deeper problem that makes the inaccuracy of mathematical assumptions somewhat irrelevant.
And it's the data itself, the data quality and the data scarcity.
The internet contains roughly 500 trillion tokens of text data.
But a lot of it is garbage.
And on top of it, the Moore's law is slowing because the silicon chips approach physical limits.
AI compute has been doubling every three months since 2012, but semiconductor manufacturing capacity is fully booked through 2026.
When you hear 95% AI project failure, the root cause is almost always poor data.
The foundation that a lot of LLMs are based on, the notion of scaling laws, has diminishing returns.
To get to the first unit of improvement requires one unit of data, but the next one requires 10 and then 100.
But everybody expecting revolution but the revolution isn't happening because it cannot happen technically.
So this was the technical layer and now let's move on to the psychological layer.</p>
</details>

### 心理偏差如何助长泡沫

人类容易受到一系列偏差的影响。
但谈到泡沫，这正是我们**线性思维**（Linear Thinking: 指倾向于将事物发展视为直线式、等速前进的思维模式）遇到**指数世界**（World of Exponents: 指事物以非线性、加速方式增长的现象）的时刻。
这种偏差是双向的。
投资者低估了人工智能的进步速度。
例如，他们错过了**英伟达**（Nvidia: 一家全球领先的图形处理器和人工智能计算公司）数据中心收入从6亿美元增长到410亿美元的指数级增长。
他们将**Transformer模型**（Transformer Models: 一种基于自注意力机制的深度学习模型，在自然语言处理领域取得了巨大成功）视为渐进式改进，而实际上它们是**不连续的飞跃**（Discontinuous Leaps: 指技术或发展上的突然、非线性的巨大进步），结果导致那些迟入市场的投资者产生了巨大的**错失恐惧症**（FOMO: Fear Of Missing Out，指害怕错过投资机会或社会事件的焦虑情绪），并急于追赶。
反之，当公众高估我们离**人工通用智能**（AGI: Artificial General Intelligence，指具备与人类同等或超越人类智能水平的人工智能系统）这一神奇实体有多近时，他们会线性地推断近期发生的快速进展，并完全忽视大语言模型在技术上已达到性能巅峰的事实。
人工智能研究人员完全承认这一点。
多个人工智能科学家团队告诉媒体，缩放定律正在失效，**OpenAI**（OpenAI: 一家致力于开发和推广友好型人工智能的美国人工智能研究实验室）的联合创始人明确表示，通过扩大预训练规模所获得的结果已经**平台化**（Plateaued: 指增长停滞，达到一个稳定水平）。
从**GPT-3**（GPT-3: OpenAI开发的大型语言模型，是ChatGPT的前身）到**GPT-4**（GPT-4: OpenAI开发的更先进的大型语言模型）的快速改进，并不意味着它将在几年内线性外推到AGI。
它不是线性函数的原因是，缩放定律是**对数**（Logarithmic: 指增长速度逐渐减缓的曲线，而非直线）的，而不是线性的。
每一次改进都需要10到100倍的资源。
这种“如果人工智能在两年内进步如此之大，想象一下十年后会怎样”的思维框架，完全忽视了物理极限、数据质量和**S曲线**（S-curves: 描述技术或产品生命周期中采用率或性能增长模式的曲线，初期缓慢，中期加速，后期趋缓）。
ChatGPT在2022年11月的病毒式传播时刻，成为科技意识中最常见的例子。
**Cassie Kozyrkov**（Google首席决策科学家）称之为一场用户体验革命，因为那时人工智能成为每个房间的讨论主题。
两个月内达到1亿用户。
你能想象吗？
这是有史以来最快的消费应用普及速度。
每一次对话、每一个新闻周期、每一次财报电话会议都提及ChatGPT。
ChatGPT的爆炸式普及成为所有人工智能的**心智模型**（Mental Model: 指人们对事物运作方式的一种内在认知框架），投资者因此推断：如果ChatGPT增长如此之快，所有人工智能都将快速增长。
这再次完全忽视了ChatGPT是一个面向消费者的应用，其**投资回报率**（ROI: Return On Investment，投资回报率）尚未得到证实。
而要真正成为主导并改变世界、成为一场革命，它必须在企业和**B2B**（Business-to-Business: 企业对企业）领域得到采用，而这些领域遵循完全不同的采用曲线。
在心理层面，发生的情况是人们看到了“几乎有效”的产品。
特斯拉的“受监督的全自动驾驶”并非L5级别，但人们在心理上将其快进到“完全有效”。
他们所做的是忽视了从95%到100%的差距往往比前95%要困难得多，呈指数级增长。
阻止“几乎有效”变为“完全有效”的最大限制是对人类监督的需求。
这打破了整个**单位经济学**（Unit Economics: 指分析单个产品或服务所产生的收入和成本，以评估其盈利能力）方程。
我想更深入地探讨一下特斯拉全自动驾驶的承诺，因为围绕它的语言品牌宣传助长了泡沫的形成。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Humans are prone to a series of biases.
But when it comes to bubbles, this is exactly the moment when our linear minds meet the world of exponents.
And this bias cuts both ways.
Investors underestimated how fast AI could progress.
For example, they missed Nvidia's exponential data center revenue growth from 600 million to $41 billion.
They dismissed transformer models as incremental improvements when they were discontinuous leaps, and as a result, massive FOMO among those who enter the market late and rush to catch up.
And the opposite way when the public overestimates how close we are to this magic entity of AGI because they extrapolate rapid progress that happened very recently linearly and completely ignore the fact that LLMs have technically reached their peak performance.
AI researchers fully admit it.
Multiple groups of AI scientists told the press that scaling laws are breaking down, and OpenAI co-founder clearly said that the results from scaling up pre-training have plateaued.
The fact that there was a rapid improvement from GPT-3 to GPT-4 does not mean that it will linearly extrapolate to AGI within years.
And the reason it's not a linear function is because scaling laws are logarithmic, not linear.
And each improvement requires 10 to 100 more resources.
This mental framing that if AI progressed so much in two years, imagine where it'll be in 10 completely dismisses physical limits, data quality, and S-curves.
ChatGPT's viral moment in November 2022 became the most available example in tech consciousness.
Cassie Kozyrkov called it a UX revolution because that's when AI became the subject of discussion in every room.
A 100 million users in two months.
Can you imagine that?
The fastest consumer app adoption ever.
Every conversation, every news cycle, earnings calls reference ChatGPT.
And ChatGPT's explosive adoption became the mental model for all AI and investors extrapolated.
If ChatGPT grew this fast, all AI will grow fast.
Which again completely ignores the fact that ChatGPT is a consumer-facing app with unproven ROI.
And for something to really become dominant and change the world and become a revolution, it has to be adopted in enterprise and B2B which follows completely different adoption curves.
And again on the psychological level, what happens is that people see the products that almost work.
Tesla's full self-driving supervised, which isn't level five supervised, and mentally fast forward to works.
And what they do is they ignore that the gap between 95% and 100% is often exponentially harder than the first 95%.
The single biggest constraint preventing almost works to works is the need for human supervision.
And this breaks the entire unit economics equation.
I want to drill a little bit more into Tesla's full cell driving promise because the linguistic branding around it contributed to the fact that we have a bubble.</p>
</details>

### 马斯克的承诺与FSD的定价

埃隆·马斯克在2016年预测：全自动驾驶将在两年内解决。
这成为了自动驾驶汽车概念的**心智锚点**（Mental Anchor: 指人们在决策时，倾向于依赖最初获得的信息作为参考点）。
直到今天，特斯拉的投资者仍然将马斯克的承诺作为基准。
从2016年10月到2022年9月，全自动驾驶的价格从3000美元上涨到15000美元。
在短短6年内，一款从未完全交付的软件价格上涨了400%。
市场愿意相信**FSD**（Full Self-Driving: 特斯拉的全自动驾驶系统）将在合理的时间框架内实现完全自动驾驶。
每一次价格上涨都反映了马斯克的承诺、产品预览、测试版发布和路测。
但同样，作为一名在软件产品管理和开发领域建立职业生涯的人，它并没有在实际环境中得到充分测试。
为特斯拉辩护，在科技行业中，只要能端到端地运行，演示一个80%就绪的产品是正常的。
如果你的产品能够以其基本形式进行演示，并且能够端到端地解决一个实际问题，那么80%对于演示来说就足够了，因为**上市时间**（Time to Market: 指产品从概念到投放市场所需的时间）至关重要。
你展示80%的功能，如果它有效，你就继续修补剩下的20%，并为发布做准备。
但FSD是硬件（汽车）内部的一段软件。
你不能交付一辆80%的汽车。
你可以展示80%但人们支付的是100%的钱。
消费者支付了5倍的溢价，因为他们相信如果早点购买，以后会价值数十万美元。
但这里的关键词是“相信”。
人们愿意为FSD支付的价格与他们相信完全自动驾驶即将实现的程度成正比。
同样的事情也发生在OpenAI身上。
他们800亿美元的估值锚定了所有AI初创公司的预期，因为投资者以OpenAI为基准来判断新的AI公司。
如果OpenAI做到了，其他人也会做到。
这再次完全忽视了OpenAI拥有独特的优势。
OpenAI不是一家典型的AI公司。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Elon Musk's 2016 prediction: full self-driving will be solved within 2 years.
That became the mental anchor for the idea of autonomous vehicles.
Tesla's investors still till this day reference Musk's promises as the baseline.
From October 2016 to September 2022, the price for full cell driving rose from $3,000 to $15,000.
That's a 400% increase in just 6 years for software that was never delivered.
The market wanted to believe that the FSD would achieve full autonomy within a reasonable time frame.
Each price hike reflected Musk's promises and product previews, beta releases, street testing.
But again, it wasn't tested in the wild as somebody who has built a career in software product management and development.
In Tesla's defense, it is normal for a tech industry to demo a product that is 80% ready as long as you can make it work end to end.
If you can demo your product in its basic form and it will solve a real problem end to end, 80% is sufficient for a demo because time to market is everything.
You show the 80% and if it works, you continue patching up the remaining 20 and you prepare for the release.
But FSD is a piece of software inside hardware, a car.
You can't deliver 80% of the car.
You can show 80% but people are paying for 100.
Consumers paid 5x premium because they believed that if I buy an early it'll be worth hundred thousands of dollars later.
But the operative word here is "believed."
The price people were willing to pay for FSD was directly proportional to their belief that full autonomy was imminent.
The same thing happened with OpenAI.
Their $80 billion valuation anchored expectations for all AI startups because investors judge new AI companies against OpenAI baseline.
If OpenAI did it, everyone else will do it.
Which again completely ignores that OpenAI has unique advantages.
OpenAI is not a typical AI company.</p>
</details>

### 历史的教训：英国铁路狂热

为了更好地理解这一点，我想给你一个过去这种现象如何显现的例子。
我想让它更具创意一些。
我不想使用互联网泡沫或2008年金融危机的类比，而是想回顾19世纪30年代英国的**铁路狂热**（Railway Mania: 19世纪30-40年代英国铁路建设和投资的投机狂潮）。
在19世纪30年代，第一批客运铁路取得了出人意料的成功，投资者获得了巨大的实际回报。
然后投资者开始**外推**（Extrapolate: 指根据已知数据或趋势推断未知情况）。
如果这些第一批线路让我们致富，那么更多的铁路就意味着更多的财富。
议会批准了数百条线路。
股市上涨，新公司成倍增加，资金涌入。
许多计划考虑不周或完全可疑，但每个人都在做，所以为什么要错过呢？
但到了1846年，33%承诺修建的铁路从未建成。
资金收紧、丑闻、失败，所有这些都开始堆积，然后市场崩溃。
铁路股价暴跌。
许多投资者损失惨重。
项目被取消或被更强大或真正的公司吸收。
尽管遭受了损失，英国在随后的几个世纪里拥有了世界上最好的铁路网络，但这都是在狂热和大量损失之后才实现的。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">To put this in perspective, I want to give you an example of how this manifested in the past.
And I wanted to make it a little bit more creative.
And instead of using the dot-com or 2008 analogy, I want to recall the railway mania in the United Kingdom during 1830s.
In 1830s, the first passenger railways succeeded beyond expectation and investors saw huge real returns.
Then the investors extrapolate.
If these first lines made us rich, more rails, more wealth.
Parliament says yes, do it.
Approves hundreds of lines.
Stock market goes up, new companies multiply, money floods in.
Many schemes are poorly considered or outright dubious, but everyone is doing it, so why miss out?
But by 1846, 33% of promised railways are never built.
Tightening money, scandals, failures, all start to pile up and then they crash.
The railway share prices collapse.
Lots of investors lose fortunes.
Projects are canceled or absorbed by stronger or real companies.
Despite the losses, the UK is left with the world's best rail network for the next centuries, but only after the mania and plenty of losses.</p>
</details>

### 社交媒体与集体错觉

回到我们当前的泡沫，谈到**回音室效应**（Echo Chambers: 指人们在社交媒体或其他信息环境中，只接触到与自己观点相似的信息，从而强化原有信念的现象），社交媒体对此毫无帮助。
X（原Twitter）、LinkedIn、YouTube、Reddit，都根据用户兴趣过滤内容，内容创作市场也随之迎合需求。
结果我们看到成百上千的人在大喊“AI将改变一切”，而他们可能前一天才刚谷歌搜索过什么是AI。
我的意思是，甚至这些人都开始谈论AI了。
而这种无数的偏差不断产生**网络效应**（Network Effects: 指产品或服务的价值随着用户数量的增加而增加的现象），并创造出这种自我强化的错觉。
你有一个正反馈循环。
采纳会带来更多的采纳。
你有了**社会认同**（Social Proof: 指人们在不确定时，会观察他人的行为来决定自己的行为），因为AI采用者的数量象征着价值，无论那些真正懂行的人如何大声疾呼，因为，是的，当TikTok上有大量20秒的短视频时，谁会去听AI科学家长达三小时的播客呢？
最重要的是，还有**错失恐惧症**（FOMO: Fear Of Missing Out，指害怕错过投资机会或社会事件的焦虑情绪）。
结果，这场AI采纳竞赛渗透到多层次的人群和公司中。
每家**财富500强**（Fortune 500: 《财富》杂志评选的全球最大500家公司）公司都在2024年宣布了AI计划，并非因为它们验证了投资回报率，而是因为竞争对手都在这样做，董事会要求“我们的AI战略是什么？”
无论是否适合，“AI赋能”都成了营销中的强制性要求。
小型初创公司甚至在使用非常基础的自动化时，也开始在他们的**路演演示文稿**（Pitch Decks: 用于向投资者介绍公司、产品和商业计划的演示文稿）中加入AI。
这并非因为人们不明白自己在做什么。
他们不是傻瓜，而是因为行业有此要求。
数千家公司将聊天机器人重新命名为智能或**智能代理**（Smart Agents: 能够执行特定任务并与环境互动的智能系统）。
结果是，只有一百多家公司在做真正的人工智能，而数千家公司只是AI的“跟风者”，并伴随着巨大的FOMO。
当基本的批判性思维被抛诸脑后，因为你感到压力，不得不随波逐流，而且FOMO如此强烈，以至于它在机构层面运行，因为害怕别人赢得市场的恐惧从未如此之高。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And back to our bubble, when it comes to echo chambers, social media didn't help.
X, LinkedIn, YouTube, Reddit, all filtered content to match user interests and the content creation market followed to deliver to the demand.
And we ended up seeing hundreds or thousands of people screaming AI will transform everything who googled what AI was the day before.
I mean even these guys are talking about AI.
And this myriad of biases keeps creating network effects and creates this self-reinforcing delusion.
You've got a positive feedback loop.
Adoption begets more adoption.
You've got social proof because the number of AI adopters signals value regardless of what these guys who actually understand what they're talking about screaming because yeah, why listen to a three-hour podcast by an AI scientist when there is a plethora of 20-second clips on TikTok.
And to top it off, the fear of being left behind or FOMO.
And as a result, this AI adoption race seeps through multiple levels and levels of people and companies.
Every Fortune 500 company announced AI initiatives in 2024, not because they had validated ROI, but because competitors were doing it, the boards demanded what's our AI strategy.
Regardless of the fit, AI powered became mandatory in marketing.
Smaller startups started adding AI to their pitch decks even when using very basic automation.
And it's not because people don't understand what they're doing.
They're not idiots, but because the industry demands it.
Thousands of companies rebranded chat bots as intelligent or smart agents.
And the result is 100 something companies doing genuine AI and thousands of AI lookalikes with massive FOMO.
When basic critical thinking goes out the window because you feel pressured to simply follow the crowd and the FOMO is so strong that it runs on institutional scale because the fear of somebody else winning the market has never been higher.</p>
</details>

### 机构投资者的外推偏差

你可能会问，那投资者呢？
如果这一切都如此明显，他们为什么还要向那些尚未证明投资回报率的项目投入如此多的资金？
这就是**外推偏差**（Extrapolation Bias: 一种认知偏差，指人们倾向于将最近的趋势线性地推断到未来）导致老练投资者、机构投资者和首席财务官（CFO）受害的现象。
这是行为金融学中最反直觉的发现之一。
矛盾之处在于，专业人士和业余人士一样都会进行外推。
有一位诺贝尔奖级别的经济学家，名叫**安德烈·施莱弗**（Andre Shleifer），他发表了一篇关于专业投资者和首席财务官在预测未来回报时如何从过去的表现进行外推的优秀著作。
他的主要发现是，过去的表现几乎完美地预测了未来的预测。
在他进行的一项实验中，相关性高达0.78，在该实验中，他调查了专业投资者对未来12个月股票的预测，这些预测与过去12个月的回报高度相关。
在他的研究中，同样的问题也影响着首席财务官，因为他们对自己公司股票的预测几乎总是过去业绩的反映，而非基于基本面或客观分析。
关于这个主题有多项研究证实，个人投资者过度外推收益，并且外推偏差存在于多个层面。
但为什么，他们为什么会上当呢？
研究证明，机构投资者确实比散户投资者表现出较少的极端行为偏差，但他们仍然参与**追逐动量交易**（Momentum Trading: 一种交易策略，指买入上涨的资产并卖出下跌的资产，期望趋势继续）。
他们有情绪波动和**过度自信偏差**（Overconfidence Bias: 一种认知偏差，指人们高估自己的能力、知识或判断力）。
在我发现的数据中，74%的基金经理认为自己在投资方面高于平均水平。
过度自信的投资者相信他们可以通过研究和积极交易来超越市场。
但例如在2023年，在过去10年中，只有25%的**主动管理型共同基金**（Actively Managed Mutual Funds: 由基金经理积极选择投资组合，旨在跑赢市场基准的共同基金）跑赢了市场。
行为偏差存在于每个人身上，甚至包括专业的资金经理，因为专业身份并不能消除这种偏差。
这份被多方引用的著名**高盛报告**（Goldman Sachs Report: 高盛集团发布的市场分析或研究报告）显示，尽管有所有证据，人工智能主题仍可能持续多年，因为泡沫需要很长时间才能破裂。
经验丰富的投资者知道这一点，但仍无法抗拒参与，资金不断涌入，尽管他们知道我们正处于炒作阶段。
如果我们用互联网泡沫做类比，我们可能正处于1997-98年的等效阶段。
我们知道这是一个泡沫，但我们仍然认为在它破裂之前还有时间。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And you may ask, but what about investors?
If this is so obvious, why are they pouring so much capital into something that hasn't proven the ROI?
And this is the phenomenon of sophisticated investors, institutional investors and CFOs falling victim to extrapolation bias.
This is one of the most counterintuitive findings in behavioral finance.
The paradox is that professionals extrapolate just like amateurs.
There is a Nobel Prize caliber economist whose name is Andre Shleifer who published a great piece of work on how professional investors and CFOs extrapolate from their past returns when forecasting future returns.
His key finding is that past performance nearly perfectly predicts future forecasts.
There is a 78 correlation in the experiment that he ran and in that experiment he surveyed professional investors about their predictions for the next 12 month stock and the predictions were highly correlated with the last 12 months returns.
In his findings, the same problem affects CFOs because their forecasts for their own company stocks were almost always a reflection of their past performance, not fundamental or objective analysis.
There are multiple studies done on this topic that confirm that individual investors over extrapolate earnings and that the extrapolation bias exists on so many levels.
But why why do they fall for it?
Research proves that institutional investors do exhibit less extreme behavioral biases than retail investors, but they still engage in hurting momentum trading.
They have mood swings and overconfidence bias.
In the data that I found, 74% of fund managers think that they're above average at investing.
Overconfident investors believe that they can outplay the market through research and active trading.
But in 2023, for example, only 25% of actively managed mutual funds outperformed the market over the previous 10 years.
Behavioral biases are present among everybody and even professional money managers because the bias is not eliminated by professional status.
This well-known Goldman Sachs report referenced in multiple sources shows that despite all evidence, AI theme can run for years because bubbles take a long time to burst.
And experienced investors know this but can't resist participating and money keeps flotting in despite knowing that we are in the hype phase.
If we use a dot-com analogy, we're probably in the 97-98 equivalent.
We know it's a bubble, but we still think we have time before it bursts.</p>
</details>

### 泡沫的循环与人性

每一个科技泡沫在发生时都感觉独一无二。
但如果你把视角拉远，这种模式就会变得令人痛苦地熟悉。
一项新技术出现，语言和措辞将其夸大。
资金将其放大，而信念则完成了其余部分。
然后现实赶上，措辞开始改变。
“自动驾驶”变成了“受监督的自动驾驶”。
“元宇宙”变成了“超级智能”。
“智能体”变成了“助手”。
然后市场随之调整。
我们称之为**市场修正**（Market Correction: 指资产价格从近期高点下跌10%或更多的现象），但实际上，这是人性在重新调整自身。
我们真诚地希望这能让你一窥表面之下正在发生的事情，并提醒你，我们以前见过这种情况。
一如既往，感谢你的收听。
我们下次再见。
再见。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Every tug bubble feels unique while it's happening.
But if you zoom out far out, the pattern is painfully familiar.
A new technology arrives, the language, the linguistics inflate it.
Money amplifies it and belief does the rest.
Then reality catches up and the words start to change.
Autonomous becomes supervised.
Metaverse becomes super intelligence.
Agents become assistants.
And then the markets follow.
We call it a correction, but in reality it is human nature resettling itself.
We really hope that this gave you a glimpse into what's happening under the surface and a reminder that we have seen this before.
As always, thank you for listening.
We'll see you next time.
Bye.</p>
</details>