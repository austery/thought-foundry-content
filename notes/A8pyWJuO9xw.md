---
area: tech-engineering
category: ai-ml
companies_orgs: []
date: '2025-09-13'
draft: true
guest: ''
insight: ''
layout: post.njk
media_books:
- best-partners-tv
products_models: []
project:
- ai-impact-analysis
series: ''
source: https://www.youtube.com/watch?v=A8pyWJuO9xw
speaker: Best Partners TV
status: evergreen
summary: 斯坦福大学CS231N课程由李飞飞主讲，串联5.4亿年视觉进化史，探讨视觉如何成为AI智能的基石，回顾计算机视觉发展历程，并展望深度学习、AI伦理与未来应用。
tags:
- health
- technology
title: 斯坦福CS231N：李飞飞解读5.4亿年视觉进化史与AI智能基石
---

### 斯坦福CS231N课程概述与核心观点

斯坦福大学2025年春季的**CS231N**（Convolutional Neural Networks for Visual Recognition: 针对视觉识别的卷积神经网络）课程已开启首次授课，主讲人是全球AI领域的顶尖学者、计算机视觉的奠基人之一**李飞飞**（Fei-Fei Li）。这堂课程不仅串联了5.4亿年的视觉进化史，更回答了深度学习革命中的一个核心逻辑：为什么“看懂世界”是AI真正走向智能的第一步？

李飞飞教授在课上从**寒武纪大爆发**（Cambrian Explosion: 地球生命史上动物物种快速多样化的时期）的三叶虫讲起，聊到达芬奇的暗箱实验，再到2012年的AlexNet，最后探讨生成式AI如何让机器“学会创造”。整个过程既有科学史的温度，也有技术细节的深度，为听众提供了对AI视觉领域全新的认知。

李飞飞教授在课程一开始就明确了一个核心观点：AI早已经不是计算机科学的“独角戏”，而是一个高度跨学科的领域。我们常说的计算机视觉看似是AI的一个分支，实则和**自然语言处理**（NLP: Natural Language Processing）、语音识别、机器人技术深度绑定；更底层地，它还依赖数学、神经科学、心理学、物理学甚至生物学。在这张跨学科的“拼图”里，视觉的地位尤为特殊，它不只是智能的“一部分”，更是智能的“基石”。李飞飞教授指出，解开视觉智能的奥秘，就是系统性地解开整个智能的奥秘。

### 视觉的起源：寒武纪大爆发与智能进化

为了理解视觉为何是智能的基石，我们需要从视觉的起源讲起，这就要回到5.4亿年前的寒武纪大爆发。在生物课上我们可能学过，寒武纪大爆发是地球生命史上的一个“奇迹”，在大约1000万年的时间里，地球上突然出现了大量复杂的动物物种。而其中的一个关键驱动力，就是“眼睛”的出现。它指的不是我们现在看到的复杂晶状体眼睛，而是最原始的“感光细胞”。比如当时的**三叶虫**（Trilobite: 一种已灭绝的海洋节肢动物）就演化出了能够捕捉光线的简单结构。在这之前，生命体只能被动地进行新陈代谢，随着环境漂流；但有了感光细胞，它们第一次能够“主动感知”环境——哪里有光线？哪里可能有食物？哪里可能有天敌？这种“主动适应”的能力直接推动了神经系统的进化，也为后续智能的发展埋下了种子。

李飞飞教授特别强调，这长达5.4亿年的视觉进化史，本质上就是一部智能的进化史。而人类作为“视觉动物”，对视觉的依赖更是到了极致。我们大脑皮层中超过一半的细胞都在参与视觉处理。假如我们现在看着屏幕上的文字，大脑正在实时处理光线信号、识别文字形状、理解语义，整个过程快到我们根本意识不到，但背后是极其精密的视觉系统在运作。也正是因为人类视觉如此强大，让科学家们意识到，如果能让机器拥有类似的视觉能力，就能向真正的智能迈一大步。

### 机器“看见”的历史探索：从暗箱到神经科学

既然视觉如此重要，人类自然早早就开始探索“让机器看见”的方法。李飞飞教授在课上回顾了这段历史，其起点比我们想象的更早。早在古希腊和古代中国，思想家们就发现了“针孔成像”的原理，通过一个小孔，能将外界的景物投射成倒立的影像。而到了文艺复兴时期，**达芬奇**（Leonardo da Vinci: 意大利文艺复兴时期的艺术家、科学家）对这个原理做了更深入的研究，他设计的“**暗箱**（camera obscura: 一种利用小孔成像原理的绘画辅助工具）”，可以说是现代相机的雏形。

暗箱的核心是一个密闭的箱子，一侧有小孔，另一侧能承接外界投射的影像。当时的画家会用它来辅助绘画。但这里有个关键的问题：无论是暗箱、还是后来的相机，甚至是我们的人眼，本质上都只是“信息采集工具”。它们能捕捉光线形成影像，但是没法“理解”影像的内容。就像相机能拍下一只猫，但它不知道这是“猫”，不知道猫的习性，更不知道这只猫在做什么。真正的“看见”，核心其实是“理解”，而这正是计算机视觉要解决的核心难题。

那么，人类是如何从“造工具”走向“懂视觉”的呢？这里必须提到神经科学的突破。20世纪50年代，两位科学家**休贝尔**（David Hubel: 加拿大神经生理学家）和**威塞尔**（Torsten Wiesel: 瑞典神经生理学家）做了一项改变整个领域的实验。他们用电极监测麻醉状态下猫的初级视觉皮层，发现了两个关键规律：

第一，视觉皮层中的神经元都有自己的“**感受野**（receptive field: 神经元对其刺激有反应的特定区域）”。简单来说，每个神经元只对特定空间区域里的光线信号有反应。比如有的神经元只“关注”画面左上角的一小块区域，而且只对特定方向的边缘，比如水平边缘、垂直边缘有反应。这意味着，大脑处理视觉信息是从“拆解简单特征”开始的。

第二，视觉通路是“分层”的”。信号会从处理简单特征的神经元传递给更高级的神经元。比如，它会先识别边缘，再组合成角点，然后是物体的轮廓，最后形成完整的物体识别。这种“从简单到复杂”的分层处理逻辑，后来直接启发了神经网络算法的设计。1981年，休贝尔和威塞尔因为这项研究获得了诺贝尔生理学或医学奖。他们的发现，相当于为计算机视觉搭建了“模仿人类视觉”的蓝图。

### 计算机视觉的学科萌芽与早期挑战

有了神经科学的理论基础，计算机视觉作为一门独立学科开始萌芽。李飞飞教授在课上提到了两个关键的“起点”。

第一个是1963年，**拉里·罗伯茨**（Larry Roberts: 美国计算机科学家）完成了全球第一篇专注于“形状识别”的博士论文。他在论文里试图解决一个核心问题：如何让机器像人类一样，凭直觉理解物体的表面、边角和特征。比如看到一个立方体，机器能识别出它是“三维的立方体”，而不只是平面上的几个正方形。这篇论文标志着计算机视觉正式成为一个研究领域。

第二个是1966年，**麻省理工学院**（MIT: Massachusetts Institute of Technology）的一位教授组织了一个夏季项目，目标很“大胆”，那就是让几位优秀的本科生在一个夏天里解决计算机视觉的问题。现在回头看，这个目标显然太乐观了，计算机视觉的复杂度远超当时的想象。那个夏天自然没有实现目标，但是这个项目的意义在于，它让学界第一次意识到，让机器看见比想象中难得多，需要长期的系统性研究。

到了20世纪70年代，另一位关键人物**大卫·马尔**（David Marr: 英国神经科学家）写出了一本开创性的著作，为视觉处理搭建了第一个“系统性框架”。他提出，视觉处理应该分三个阶段：首先是“**原始草图**（primal sketch: 提取图像中的边缘、纹理等基础特征）”，也就是提取图像中的边缘、纹理等基础特征；然后是“**2.5D草图**（two and a half D sketch: 将前景和背景分开的中间表示）”，把前景和背景分开，比如区分出“球在地面上”，而不是两者混在一起；最后是“**3D表示**（3D Representation: 构建物体完整的三维结构）”，构建出物体完整的三维结构。

但是马尔的框架也暴露了一个核心难题：从2D图像恢复3D信息，本质上是一个“**不适定问题**（ill-posed problem: 缺乏足够信息来确定唯一解的问题）”。简单说，三维世界的景物投射到二维平面上时，会丢失大量的信息，比如一个圆形，从正面看是圆，从侧面看是椭圆。机器怎么知道这个椭圆其实是“圆形的投影”呢？自然界其实早就解决了这个问题，大多数动物都有两只或多只眼睛，通过“三角测量”的方法来获取3D信息。人类也有这种能力，但是精度有限，我们能大概判断物体的远近，却没法像激光雷达那样精确地测量距离。

除了技术难题，计算机视觉还面临一个“本质区别”，那就是视觉和语言不同。李飞飞教授在课上特别强调了这一点：语言是人类纯粹创造出来的东西，并非是自然之物，你没法指着某个东西说“这就是语言”，因为语言是大脑生成的、一维的、有顺序的符号；但是视觉根植于物理世界，遵循物理定律，比如光的反射、物体的运动规律等等。这种区别直接影响了AI算法的设计，正如语言模型可以通过学习文字的序列规律来建模，但是视觉模型必须理解物理世界的规律，难度要大得多。

### AI寒冬下的潜流与认知科学的启示

到了20世纪80年代末到90年代，AI领域进入了“寒冬”。由于之前的研究承诺没能兑现，投资和热情大幅降温，计算机视觉也受到波及。但是李飞飞教授特别指出，寒冬之下，暗流仍在涌动，计算机视觉、NLP、机器人学的研究没有停止，而且认知科学和神经科学的发展也为后续的突破埋下了伏笔。

比如，心理学家**欧文·比德曼**（Irving Biederman: 美国心理学家）做过一个有趣的实验，给受试者看两张自行车的图片，一张背景正常，一张背景被打乱。结果发现受试者识别“正常背景的自行车”更快。这说明，人类的视觉不是孤立识别物体，而是会结合“全局场景”来理解局部。这个发现后来影响了计算机视觉的“场景理解”研究。

还有认知神经科学家**西蒙·索普**（Simon Thorpe: 法国认知神经科学家）的实验，他让受试者观看一个快速播放的视频，并要求识别其中是否有人。结果发现，人类大脑能在看到图像后的150毫秒内就能完成初步的分类。这个速度远超当时的计算机算法，也让科学家意识到，人类视觉的“高效”背后，一定有着更为精妙的处理机制。

### 21世纪的转折点：ImageNet与深度学习革命

进入21世纪，计算机视觉迎来了两个关键转折点：一是数据的爆发，二是深度学习的成熟。而这两个转折点，都和李飞飞教授的工作密切相关。

首先是数据问题。早期的计算机视觉研究，最大的瓶颈之一就是“数据不够”。当时的数据集通常只有几千张图片，而机器学习算法需要海量的数据才能“学会泛化”。简单来说就是，数据太少，模型只能靠死记硬背训练数据，遇到没见过的图像就会出错。李飞飞教授和她的学生们意识到了这个问题，于是启动了**ImageNet**（大规模视觉识别挑战赛: ILSVRC）项目。他们从互联网上的十亿张图片中筛选、清理，最终构建了一个包含1500万张图片的数据库，覆盖22000个物体类别。这个类别数量和人类在童年时期学习识别的物体数量大致相当。为了推动研究，他们还开源了数据集，并且创办了ImageNet大规模视觉识别挑战赛，也就是ILSVRC。这个比赛会从ImageNet中选出1000个类别、100万张图片作为竞赛数据，邀请全球研究者比拼算法精度。

2011年，第一届ILSVRC的最佳算法错误率接近30%，这个成绩远不如人类不到3%的错误率。但是到了2012年，一切都变了。**杰弗里·辛顿**（Geoffrey Hinton: 英国计算机科学家，深度学习的先驱之一）和他的学生带着一个叫“**AlexNet**（一种深度卷积神经网络）”的卷积神经网络参赛，直接把错误率几乎降到一半。这个结果震惊了整个领域，也正式拉开了深度学习革命的序幕。

### AlexNet成功的核心原因与深度学习的驱动力

为什么AlexNet能取得这么大的突破呢？李飞飞教授在课上拆解了两个核心原因：

第一是“**反向传播**（backpropagation: 神经网络训练中用于更新模型参数的算法）”算法的成熟。早在1986年，**大卫·鲁梅尔哈特**（David Rumelhart: 美国认知心理学家）、杰弗里·辛顿等人就提出了反向传播。它的核心逻辑是先让模型根据输入数据输出一个结果，然后计算这个结果和“正确答案”的误差，再通过微积分的链式法则，把误差“反向传递”到模型的每一个参数，从而调整参数让误差变小。这个算法解决了“神经网络如何高效学习”的问题，让模型不再需要手工调整参数。

第二就是“海量数据”的支撑。AlexNet的架构其实和32年前的神经认知机没有本质区别，但是它用了ImageNet的100万张图片来训练。数据量的爆炸，让这个“高容量模型”终于能够学会泛化。李飞飞教授强调道，当时很多研究者只关注算法架构，却忽视了数据的重要性，而深度学习的成功证明，数据才是驱动高容量模型发展的核心。

### AI的伦理、向善应用与人类视觉的边界

在讲完技术突破之后，李飞飞教授把话题拉回到了人的身上。AI不只是一项技术，更是关乎人类社会的事业。

在AI伦理方面，李飞飞教授指出，AI算法，尤其是大模型，会“继承”人类社会的偏见。因为训练数据来自人类活动，而人类社会本身就存在偏见。最典型的例子就是人脸识别算法，有些模型在识别白人男性时的准确率很高，但是在识别黑人女性时错误率会大幅上升。这种偏见如果应用在贷款审批、求职筛选等领域，会对特定群体造成不公平的影响。因此，“以人为本的AI”不是一句口号，而是必须解决的现实问题。

其次是AI的“向善应用”。李飞飞教授自己的研究就聚焦在医疗健康领域，比如用计算机视觉来分析医学影像，帮助医生更早发现疾病；以及开发针对老年人和病患的护理机器人，通过视觉识别他们的动作和状态，提供及时的帮助。这些应用让我们看到，AI技术可以真正改善人类的生活。

同时，李飞飞教授也坦诚地指出了AI的“边界”。尽管计算机视觉取得了巨大突破，但人类视觉的“细腻与情感维度”，仍然是机器难以企及的。比如，我们能从孩子的眼神中看到“好奇”，从老人的笑容中感受到“温暖”，能理解一张幽默图片的“笑点”。这些包含情感、文化、经验的感知，对机器来说依然是巨大的挑战。

### CS231N课程结构与学习目标

在李飞飞教授讲完核心内容后，课程的联合讲师**埃桑·阿德利**（Ehsan Adeli: 斯坦福大学教授）教授详细介绍了CS231N的课程结构。这门课程主要分为四大主题：

1.  **深度学习基础**：从最基础的“**线性分类器**（Linear Classifier: 一种通过线性函数对数据进行分类的模型）”讲起，比如如何用一条直线，或者说超平面，来区分“猫”和“狗”的图像特征；然后讲解“**过拟合**（Overfitting: 模型在训练数据上表现良好，但在未见过数据上表现差的现象）”和“**欠拟合**（Underfitting: 模型未能充分学习训练数据中的模式）”的问题，以及如何用“**正则化**（Regularization: 减少过拟合的技术）”等技术来解决；最后深入神经网络的原理，包括**卷积层**（Convolutional Layer: 神经网络中用于特征提取的层）、**池化层**（Pooling Layer: 神经网络中用于降采样和特征聚合的层）、**全连接层**（Fully Connected Layer: 神经网络中每个神经元都与前一层所有神经元连接的层）等核心组件，以及如何训练和调试模型。
2.  **感知与理解视觉世界**：这部分聚焦计算机视觉的核心任务，比如**目标检测**（Object Detection: 识别图像中物体的位置和类别）、**语义分割**（Semantic Segmentation: 将图像中每个像素分类到特定类别）、视频分析等。讲解每种任务的解决思路和经典模型，比如用于目标检测的**YOLO**（You Only Look Once: 一种实时目标检测系统），用于语义分割的**U-Net**（一种用于生物医学图像分割的卷积网络）等等。同时会介绍“**注意力机制**（Attention Mechanism: 允许模型聚焦于输入数据特定部分的机制）”等技术，帮助理解模型“如何做出决策”，比如模型在识别猫的时候，为什么会“关注”猫的头部而不是背景。
3.  **大规模分布式训练**：这部分是2025年课程的新增内容，包括如何训练“大视觉模型”，比如**数据并行**（Data Parallelism: 在多个设备上并行处理数据）**模型并行**（Model Parallelism: 将模型分割并在多个设备上运行）技术，以及如何解决大模型训练时的算力和内存瓶颈。
4.  **生成式和交互式视觉智能**：从**自监督学习**（Self-supervised Learning: 模型通过数据本身生成监督信号进行学习）到**生成模型**（Generative Model: 能够生成新数据的模型），再到**视觉语言模型**（Vision-Language Model: 结合视觉和语言理解的模型）和**三维视觉**（3D Vision: 处理三维场景和物体的方法）的工作原理。

课程的核心学习目标有三个：一是学会将实际的计算机视觉问题“形式化”为明确的任务，比如把“自动驾驶识别障碍物”转化为“目标检测任务”；二是掌握开发和训练视觉模型的技能，包括数据处理、模型构建、训练调优等等；三是了解领域的前沿动态和未来方向，比如生成式AI、3D视觉的最新进展。阿德利教授还预告了下一堂课的内容——图像分类与线性分类器，这将是学习计算机视觉的“第一步”，也是最基础的一步。

### 结语

回顾整个过程，从5.4亿年前的感光细胞，到今天能生成图像的AI模型，我们看到的不仅是技术的进步，更是人类对“智能本质”的不断探索。