---
area: tech-work
category: ai-ml
companies_orgs:
- OpenAI
- Google
date: '2024-05-15'
draft: true
guest: ''
insight: ''
layout: post.njk
media_books:
- 《Model Spec》
people:
- John Schulman
products_models:
- ChatGPT
- GPT-4
- GPT-3.5
- GPT-3
- GPT-2
- LaMDA
- Meena
project:
- ai-impact-analysis
series: ''
source: https://www.youtube.com/watch?v=Wo95ob_s_NI
speaker: Dwarkesh Patel
status: evergreen
summary: 本次访谈中，OpenAI 联合创始人 **John Schulman** 深入探讨了AI模型的预训练与后训练、大型语言模型未来的能力（如复杂编程项目），以及泛化和样本效率的重要性。他分享了
  **ChatGPT** 的诞生历程、**RLHF** 在模型对齐中的关键作用，并讨论了先进AI带来的社会影响，包括开发者间的协调、保持“人类在环”的挑战，以及对AI发展速度和人机协作未来的展望。
tags:
- agi
- model
- technology
title: OpenAI 联合创始人 John Schulman 谈 AI 推理、RLHF 与 2027 年 AGI 计划
---

今天，我非常荣幸能与 **John Schulman** 对话，他是 **OpenAI** 的联合创始人之一，领导着后训练团队，并主导了 **ChatGPT** 的创建。他还撰写了许多人工智能和强化学习领域（包括 **PPO** 在内）最重要且被广泛引用的论文。**John**，很高兴与你交流，感谢你参加播客。

感谢邀请我参加播客，我是你们的忠实粉丝。

谢谢。我的第一个问题是，我们区分了预训练和后训练。让我们超越损失函数和训练机制的实际操作，从概念层面退一步来看，预训练究竟创造了什么？后训练又在此基础上做了什么？

### 预训练与后训练的本质

在预训练中，模型基本上是模仿互联网或网络上的所有内容，包括网站和代码等。因此，你得到一个可以生成看起来像互联网上随机网页内容的模型。模型还被训练以最大化似然性，它必须为所有事物赋予一个概率。其目标基本上是根据之前的 **token**（token: 文本中的最小单位，如单词或单词的一部分）来预测下一个 **token**。由于模型必须为其赋予概率——我们训练它以最大化对数概率——它最终会变得非常校准。它不仅可以生成网络上的所有内容，还可以为所有事物分配概率。

基础模型可以有效地扮演所有这些不同的角色或生成所有不同类型的内容。当我们进行后训练时，我们通常会针对更窄的行为范围，希望模型表现得像一个聊天助手。这是一种更具体的角色，它试图提供帮助，而不是模仿一个人。它回答你的问题或执行你的任务。我们正在优化一个不同的目标，这个目标更多地是关于生成人类喜欢并觉得有用的输出，而不是仅仅模仿网络上的原始内容。

### AI模型的未来能力展望

也许我应该退一步问这个问题。现在我们拥有的这些模型已经相当擅长充当聊天机器人。撇开这些过程目前的工作方式不谈，到今年年底发布的模型将能够做些什么？如果我们把所有事情都向前推进五年，你认为进展会是怎样的？

未来五年内，模型将会有相当大的进步。

具体是哪方面？

即使在一两年内，我们也会发现模型可以完成比现在复杂得多的任务。例如，你可以想象模型能够执行一个完整的编程项目，而不仅仅是给你一个关于如何编写函数的建议。你可以想象模型接受高级别的编程指令，然后自主地编写任何文件，进行测试，并查看输出。它甚至可能会在此基础上进行一些迭代。所以，任务会变得复杂得多。

### 实现长期连贯性的关键

从根本上说，实现这一突破的关键在于模型能够足够长时间地保持连贯性，以编写多个代码文件？从现在到那时，发生了什么变化？

我想说，这将来自于训练模型完成这类更困难任务的某种组合。目前大部分训练数据更像是每次只执行一个步骤。我预计我们将投入更多精力训练模型来执行这些更长的项目。这适用于任何类型的训练，比如进行 **强化学习**（RL: Reinforcement Learning: 一种机器学习范式，通过智能体与环境的交互来学习最优行为策略），以学习如何完成这些任务。无论是监督最终输出还是监督每个步骤，任何旨在执行这些长期项目的训练都将使模型变得更好。

由于整个领域都相当新，我认为在这种训练中有很多“低垂的果实”。这是一点。我还预计，随着模型变得更好，它们将更擅长从错误中恢复或处理边缘情况。当出现问题时，它们会知道如何恢复。模型将更具样本效率。你不需要收集大量数据来教它们如何回到正轨。只需少量数据或它们从其他能力中获得的泛化能力，就能让它们回到正轨。当前模型可能会卡住并迷失方向。

### 泛化能力与错误恢复

我想具体了解泛化能力如何帮助模型回到正轨。你能详细说明一下吗？我不确定这两个概念是如何关联的。

是的，它们并非直接关联。你通常会拥有少量涵盖所有情况的数据。如果你收集一个多样化的数据集，其中会包含各种信息。如果模型具有出色的泛化能力——即使只是从几个恢复到正轨的例子中学习，或者在预训练数据中就有几个模型恢复到正轨的例子——模型也能够将它所见过的其他情况泛化到当前情境。如果模型能力较弱，你可能需要足够多的数据才能让它做几乎任何事情。但你可能需要在一个特定领域或技能上投入大量精力。而对于更强大的模型，它可能无需任何训练数据或努力就能做出正确的行为。

### 长期任务的计算成本与能力飞跃

目前，这些模型可以连贯地工作五分钟。我们希望它们能够完成人类需要一小时、一周，甚至一个月才能完成的任务。要达到这些里程碑，是否每个都需要十倍的计算量，类似于当前预训练的缩放定律？还是说，这将是一个更精简的过程，只需达到更高的样本效率，就能直接完成数年才能完成的任务？

从宏观层面来看，我同意完成更长周期的任务将需要更高的模型智能。它们的训练成本也会更高。我不确定是否会有一个非常清晰的缩放定律，除非你以非常谨慎的方式设置或设计实验。可能会出现一些**相变**（Phase Transition: 系统从一种状态突然转变为另一种状态的现象），一旦达到某个水平，模型就能处理更长的任务。例如，当人们进行不同时间尺度的规划时，我不确定他们是否使用了完全不同的机制。我们思考一个月后、一年后或一百年后，可能都使用了相同的思维机制。我们实际上并没有进行某种强化学习，需要担心覆盖该时间尺度的折扣因子等等。

通过语言，你可以描述所有这些不同的时间尺度，然后你可以进行规划。当下，你可以努力实现你的目标，无论是一个月后还是十年后。我不确定这是否是相变，但我可能会期待模型也能如此，即某些能力可以在多个尺度上发挥作用。

### AGI的可能性与剩余挑战

如果我理解错了请纠正。你似乎在暗示，目前我们的模型在**每个 token**（token: 文本中的最小单位，如单词或单词的一部分）的基础上都相当智能。它们在每个 token 的基础上可能和最聪明的人类一样智能。阻止它们发挥最大作用的原因是，五分钟后，它们可能无法以连贯且符合你项目整体目标的方式继续编写代码。如果一旦开始这种**长周期强化学习**（Long-horizon RL: 旨在训练AI模型在长时间跨度内规划和执行复杂任务的强化学习方法）训练机制，就能立即解锁模型长时间保持连贯的能力，我们是否应该预测一旦该机制解锁，模型就能达到人类水平？如果不是，那么在我们能够规划一年并执行如此长时间的项目之后，还剩下什么？

我们一旦进入那种状态，会看到什么，以及进展会有多快，目前还不完全清楚。这仍然是不确定的。我不会期望通过任何这样的训练就能立即解决所有问题。模型还会存在其他各种缺陷，导致它们卡住或做出比人类更糟糕的决策。我并不认为这一个小的改进就能解锁所有能力。但**在执行长周期任务能力上的一些改进可能会大有裨益**。

你认为这是否合理？是否很可能存在其他瓶颈？我也很好奇这些瓶颈的性质可能是什么。它拥有所有这些预训练的表征。现在，由于长周期强化学习，它可以在很长一段时间内连贯地工作。还剩下什么？

也许人类专家在不同任务中带来了其他经验，比如品味或更好地处理歧义。如果我们想做研究，我可以想象这些因素会发挥作用。显然，模型的能力会受到一些世俗限制，比如它能否使用用户界面（UI）、与物理世界互动或访问某些事物。因此，可能会有很多世俗的障碍，这些障碍可能不会持续太久，但最初会减缓进展。

### AI专用界面与泛化能力

我们来谈谈为这些 **AI** 设计的网站。一旦它们在更多**多模态数据**（Multimodal Data: 包含文本、图像、音频、视频等多种数据类型的数据集）上进行训练，它们会与我们为人类设计的网站有何不同？需要什么样的用户界面（UI）？它将如何弥补它们的优缺点？这与我们目前为人类设计的用户界面会有何不同？

这是一个有趣的问题。我预计，一旦视觉能力有所提升，模型将能够通过视觉直接使用为人类设计的网站。因此，短期内没有必要改变它们。另一方面，有些网站将极大地受益于AI的使用。我们可能希望将这些网站设计成对AI而言更好的用户体验（UX）。我不太确定那具体意味着什么。假设我们的模型在文本模式下仍然比从图像中读取文本更好，那么你可能希望为模型提供良好的基于文本的表示。你还需要一个很好的指示，说明所有可交互的事物是什么。但我不会期望整个网络被彻底重新设计，到处都充满API。我们可以让模型使用与人类相同的用户界面。

我想，这正是语言模型带给我们的重要启示，对吧？它们可以在与人类相似的**能力范围**（Affordances: 指一个物体或环境的特性，暗示了它可以如何被使用或与之互动）内行动。我想回到你之前提出的观点，即这个过程可以更具样本效率，因为它可以通过预训练中学习到的经验，泛化到如何在不同场景中摆脱困境。你见过这种泛化和迁移的最有力证据是什么？未来模型能力的关键问题似乎在于泛化程度有多大。有没有什么让你觉得非常有说服力的例子？你是否见过模型学习到一些你意想不到的泛化能力？

### 后训练中的泛化与迁移实例

在后训练中，确实出现了一些有趣的泛化实例。一个众所周知的现象是，如果你用英语数据进行所有微调，模型会自动在其他语言中表现良好。因此，如果你用英语数据训练助手，它在西班牙语中也会做出合理的行为。有时你可能会在回复是英语还是西班牙语方面得到错误的行为。通常你会得到正确的行为，这意味着它会用西班牙语回答西班牙语查询。这是一个有趣的泛化实例，模型只是抓住了正确、有用的角色，然后自动在不同语言中做出了正确的事情。

我们已经看到了多模态数据的一些版本，如果你只进行文本微调，你也会在图像方面获得合理的行为。在 **ChatGPT** 早期，我们试图修复模型理解自身局限性的一些问题。早期版本的模型会认为它可以给你发送电子邮件或叫 **Uber** 等等。模型会试图扮演助手，它会说“哦，是的，我当然发送了那封电子邮件。”显然它没有。所以我们开始收集一些数据来解决这些问题。我们发现少量数据就奏效了，即使你把它和其他所有东西混在一起。我不记得具体有多少例子，但大约是30个例子。我们有相当少量的例子展示了这种普遍行为，解释了模型不具备这种能力。这很好地泛化到了我们没有训练过的各种能力。

### AGI的实现路径与潜在瓶颈

我仍然想回到这个问题，因为我不太确定我是否理解了。假设你有一个模型，它被训练成可以在更长时间内保持连贯。撇开可能存在或不存在的其他瓶颈不谈，明年你是否可能拥有达到人类水平的模型？我设想一个你可以像同事一样与之互动，并且它和人类同事一样优秀。你可以告诉它们去做事情，它们就能完成。你认为这种可能的能力图景有什么问题？

很难确切地说缺陷会是什么。当你今天与模型交流时，除了长期连贯性之外，它们还有各种弱点。它们也难以真正深入思考问题或注意你提出的要求。我不会期望仅仅稍微提高连贯性就能达到**通用人工智能**（AGI: Artificial General Intelligence: 具备与人类同等或超越人类智能水平的人工智能系统）。我想我无法确切阐明哪些主要弱点会阻止它们成为一个功能完善的同事。

那看起来，你应该为很快出现 **AGI** 的可能性做准备。

我认为这是合理的。

那么，如果没有其他瓶颈，计划是什么？明年左右，你就会拥有 **AGI**。计划是什么？

如果 **AGI** 比预期来得早得多，我们肯定会非常谨慎。我们可能需要稍微放慢训练和部署的速度，直到我们相当确定能够安全地处理它。我们需要对它将做什么以及能做什么有一个很好的掌握。如果它比预期来得早得多，我们必须非常小心。我们目前的理解在很多方面仍然是初步的。

谨慎意味着什么？你大概已经很谨慎了，对吧？你在部署前会进行这些评估。

也许这意味着不训练更智能的版本，或者在训练时非常小心。你可以确保它被适当地沙盒化等等。也许这意味着不进行大规模部署，或者在部署规模上要非常谨慎。

### AGI时代下的协调与安全部署

我们来设想一个场景。明年 **AGI** 出现。你没有训练一个更智能的系统，但你以一种相对谨慎的方式部署它。假设这种发展并非 **OpenAI** 独有。**AGI** 只是比我们预期的容易得多，所以它发生了。你稍作等待才部署。现在其他公司也拥有类似的能力水平。接下来会发生什么？在你等待部署期间，你在等待什么？在这种情况下，每家公司都在做什么？

博弈论有点难以思考。首先，我不认为这会在明年发生，但进行这次对话仍然很有用。它可能是两三年后，而不是明年。

两三年也很快了。

确实很快。你可能需要某种协调。每个人都需要就部署或进一步训练的合理限制达成一致，这才能奏效。否则，你就会陷入竞争动态，每个人都试图保持领先，这可能会危及安全。你可能需要那些进行这种训练的大型实体之间进行某种协调。

你们会协调暂停部署，直到什么？直到你们弄清楚模型中发生了什么？

我们可以暂停进一步的训练。我们可以暂停部署。我们可以避免某些可能更危险的训练类型。我们将制定一些合理的规则，规定每个人应该如何限制这些事情。

限制到什么程度？在某个时刻，这种智能所蕴含的潜在能量终将被释放。假设两年后我们拥有了 **AGI**。现在每个人都惊慌失措。AI 公司已经暂停了。我们计划等待的会是什么？

我对此没有一个好的答案。如果我们能那样协调，那将是一个相当好的场景。构建这些模型需要大量的资本投入，而且有很多复杂的环节。这不像每个人都能在家里重新创建这些东西。考虑到能够训练最大模型的实体数量相对较少，协调似乎是可能的。我不确定如何长期维持这种平衡，但我认为如果我们达到那个阶段，我们的处境会不错。

我们会吗？我仍然好奇，因为我不确定接下来会发生什么。从根本上说，好处是你把它推送到服务器，现在我们有了一堆智能体，或者它们可以把自己推送到服务器。现在我们已经协调好了所有人，但我不确定我们在这个世界中接下来要做什么。为什么这能为我们带来一个好的结果？

如果我们能让所有人都合理地协调起来，并且我们觉得能够很好地解决对齐相关的技术问题，那么我们就可以部署。我们将能够部署真正智能的 **AI**，它们可以作为人类意志的延伸，同时也能防止它们被灾难性地滥用。那将是极好的。我们可以安全地部署这些系统，它将带来巨大的繁荣和更快速的科学进步。这就是一个好的场景会是什么样子。

### 渐进式部署与安全保障

这说得通。我很好奇未来几年会发生什么。在最好的情况下，所有这些参与者都同意暂停，直到我们弄清楚我们正在构建的系统是**对齐**（Alignment: 指确保AI系统能够理解并遵循人类的意图、价值观和目标，从而避免产生意外或有害行为）的，它们本身不会试图发动政变，也不会让其他人这样做。那样的证据会是什么样子？

如果我们能够部署那些比之前版本更智能的系统，并且它们是逐步递增地更安全的，那将是更好的。我希望事情的发展方式不是每个人都必须协调、锁定一切，然后安全地发布东西。那会导致巨大的潜在能量积聚。我宁愿看到一个场景，我们都在不断发布比之前版本稍好的东西。我们这样做时，要确保我们有信心，每次迭代在安全性和对齐性方面的改进都与能力提升相对应。如果情况开始变得有点可怕，那么我们就能放慢速度。这就是我所希望的。

如果出现更不连续的跳跃，那么问题是“你如何知道你拥有的东西是否可以安全发布”。我无法给出通用答案。然而，为了使其更可接受，你可能需要进行大量的测试模拟部署，进行各种**红队演练**（Red Teaming: 一种安全测试方法，通过模拟攻击者的思维和策略来发现系统漏洞和弱点）。你希望以一种比你计划在现实世界中执行的操作更有可能失败的方式进行。你还需要一个非常好的监控系统，以便如果部署的系统出现问题，你可以立即检测到。也许你有一个系统在监视部署的 **AI**，观察它们在做什么，并寻找麻烦的迹象。

你需要某种**深度防御**（Defense in Depth: 一种安全策略，通过在多个层面和阶段部署不同的安全控制措施，以提供多重保护）。你希望结合“模型本身表现良好，在所有方面都具有无可挑剔的道德信心”和“我非常有信心它能极好地抵御任何严重的滥用”。你还需要在其之上进行非常好的监控，以便检测任何意想不到的麻烦。

### 长期强化学习中的风险评估

在进行**长周期强化学习**（Long-horizon RL: 旨在训练AI模型在长时间跨度内规划和执行复杂任务的强化学习方法）时，或者最终开始进行时，你关注的是什么？在广泛部署这些系统之前，你如何注意到这种不连续的跳跃？

你需要在训练过程中运行大量的评估（evals）。

具体是什么？在明知可能发生的情况下，进行长周期强化学习训练是否合理？或者这只是一个非常低的可能性？你是如何看待这个问题的？

如果你看到很多潜在的可怕能力，你就会在这种训练中非常小心。我想说，这不是我们现在需要害怕的事情，因为目前很难让模型做任何连贯的事情。如果它们开始变得非常好，我们就会认真对待这些问题。我们会进行大量的评估来测试它们的**不当行为**（Misbehavior: 指AI系统未能按照预期或指令行事，可能包括产生有害、不准确或不符合用户意图的输出），主要是为了模型的对齐。我们会检查它们是否会反抗我们。你可能还会寻找能力上的不连续跳跃。你需要对模型的能力进行大量评估。你还需要确保你训练的内容没有任何理由让模型反抗你。这似乎不是最难做的事情。我们用 **RLHF**（Reinforcement Learning from Human Feedback: 一种强化学习方法，通过人类反馈来训练AI模型，使其行为更符合人类偏好）训练它们的方式，即使模型非常智能，也感觉非常安全。模型只是试图产生一个令人类满意的消息。除了它产生的文本是否被批准之外，它不关心世界上任何其他事情。

显然，如果你正在做一些模型必须执行涉及工具的长时间动作序列的事情，那么它可能会有一些动机去做很多在产生最终结果的过程中对人类来说没有意义的古怪事情。然而，它不一定有动机去做除了产生高质量输出之外的任何事情。

关于**工具性收敛**（Instrumental Convergence: 指智能体为了达成其主要目标，可能会发展出一些共同的次要目标，例如获取资源、自我保护等，即使这些次要目标与主要目标本身无关）有一些旧观点，即模型想要接管世界，以便最终产生一些很棒的代码。如果你让它编写一个 **Flask** 应用程序，它会说“哦，是的，首先我需要接管世界。”在某种程度上，很难想象为什么对于像编写应用程序这样明确定义的任务，你会首先想要接管世界。当然，如果你分配一个像“赚钱”这样的任务，那么这可能会导致一些邪恶的行为作为工具性目标。

### RLHF与人类心理的类比

在我们回到那个话题之前，让我们退一步谈谈今天的 **RLHF** 系统和所有相关的事情。我确实想跟进那个观点，因为它很有趣。

以今天的 **RLHF** 及其对这些模型的影响方式，你将如何从人类心理学的角度来描述它？它是一种驱动力？一个目标？一种冲动？从心理学上讲，它是什么样的事物？它正在以何种方式被改变？不仅仅是聊天机器人的角色，而是“不要那样说话，要这样说话”或者“不要输出那种内容”。

这可能与人类的驱动力或目标有一些类比。你试图转向某个特定的状态集，而不是其他状态。我认为我们对驱动力或目标的理解还包含其他元素，比如实现目标时获得的满足感。这些事情更多地与学习算法有关，而不是模型在运行时（当你只有一个固定模型时）所做的事情。可能有一些类比，尽管我不知道它们有多接近。在某种程度上，模型确实以某种有意义的方式拥有驱动力和目标。在 **RLHF** 的情况下，你试图最大化通过奖励模型衡量的人类认可度，模型只是试图产生人们会喜欢并判断为正确的东西。

### 提高推理能力的两种思路

我听说过两种关于利用**内部独白**（Internal Monologue: 指模型在生成最终输出之前，在内部进行多步骤思考、规划或推理的过程）来提高推理能力的观点。至少在公开场合，我看到了两种观点，我很好奇你认为哪一种更有前景。一种是模型从一系列潜在的思维链条中学习其输出，并学会遵循导致正确答案的思维链条。然后它在部署前接受训练。另一种是你在部署时使用大量计算进行推理。这种方法涉及模型在部署时与自己对话。你认为当模型在推理方面变得非常出色时，哪种方式更接近其训练方式？是因为它只是进行了大量的推理计算？还是因为它被训练得擅长于此？

你可以将推理定义为需要某种测试时计算或某种推导的任务。根据定义，推理将是需要测试时计算和逐步计算的任务。另一方面，我也期望从训练实践中获得很多。现在，模型有两种学习方式。一种是在训练中，无论是预训练还是后训练。训练中大部分计算都花在预训练上，浏览数万亿个 **token**，筛选数万亿个 **token** 的信息。如果人类受到这种对待，他们只会完全困惑。这并不是一种非常高效的学习方式。

另一种方式是**上下文学习**（In-context Learning: 指大型语言模型仅通过输入提示中的示例，无需更新模型参数即可学习新任务的能力）。当然，这种方式更具样本效率，但每次实例都会被销毁。我很好奇你是否认为在这两者之间存在一种路径，即它不会在每次实例中被销毁，但也不会像仅仅看到数万亿个 **token** 那样随意。它更具目的性和主动性。

### 中期记忆与主动学习

你是指模型拥有某种**中期记忆**（Medium-term Memory: 指介于短期上下文记忆和长期预训练知识之间的一种记忆形式，能够跨越较长的交互或任务周期）吗？太多内容无法放入上下文，但规模比预训练小得多？

可能是记忆。我没有上下文。当然，当我准备这次对话时，我会思考我应该理解什么，仔细阅读，也许在阅读时思考。我不确定它在模型方面自然对应什么。那会是什么样子？

我明白了。所以这不仅仅是记忆，还在某种程度上专注于某个特定任务，或者在一个特定项目上投入大量精力。

我甚至不确定这是否是专业化。更多的是“我不理解这部分，所以让我更深入地研究。我已经理解了这部分。”我想这可能是针对你现有知识库的专业化。

我明白了。所以这不仅仅是关于在大量相关来源上进行训练，并在某个特殊领域进行微调。它还包括通过自己的推理来推理和发展一些知识，并利用某种**内省**（Introspection: 指模型审视自身内部状态、知识或推理过程的能力）或**自知**（Self-knowledge: 指模型对其自身能力、局限性以及所掌握知识的理解）来弄清楚它需要学习什么？

是的。

这确实感觉是当今系统所缺失的东西。人们还没有真正努力探索大规模训练（你生成一个单一的快照模型，它应该像部署模型一样做所有事情）与上下文学习之间的中间地带。部分原因是我们的上下文长度增加了很多，所以没有动力去做。如果你能达到十万甚至一百万的上下文长度，那实际上已经很多了。在很多情况下，这并不是瓶颈。

我同意你可能还需要通过某种微调来补充。你从微调和上下文学习中获得的能力可能在某种程度上是互补的。我期望我们能够构建出能够进行**在线学习**（Online Learning: 指模型在部署后，能够持续地从新的数据或交互中学习和更新自身知识或行为）并拥有某些认知技能的系统，比如内省自己的知识并寻找填补空白的新知识。

### 学习与记忆的融合

这一切都是同时发生的吗？它是否只是一种新的训练机制，所有这些事情可以同时发生，无论是长周期训练还是这种类型的训练？它们是独立的还是不独立的？模型是否足够智能，既能内省又能作用于更长的周期，从而在长周期任务中获得足够的奖励？

如果你正在做某种长周期任务，你会在执行任务时学习，对吧？完成需要大量步骤的任务的唯一方法是拥有在任务期间更新的学习和记忆。短期记忆和长期记忆之间存在一个连续体。我预计，当我们开始更多地关注长周期任务时，对这种能力的需求将变得清晰。在某种程度上，将大量内容放入上下文将带你走得很远，因为我们现在拥有非常长的上下文。你可能还需要像微调这样的东西。至于内省和主动学习的能力，这可能会自动从模型了解自身知识的能力中产生。模型确实对它们所知道的东西有一些校准。这就是为什么模型不会那么严重地**幻觉**（Hallucination: 指AI模型生成虚假、不准确或与事实不符的信息）。它们对自己的局限性有一些了解。这种能力可以用于主动学习。

有所有这些复杂的 **RL** 程序，其中许多都是你开创的。当模型本身足够智能，可以充当自己的环境并以更在线、更稳定的方式进行交互时，其中有多少仍然是相关的？进步的路径会比过去 **RL** 所需的解决方案更直接吗？

我认为**策略梯度算法**（Policy Gradient Algorithms: 一类强化学习算法，通过直接优化策略函数来学习智能体的行为策略）并不是样本效率最高的算法。所以，如果你想学得非常快，那可能不是你在测试时想做的事情。但谁知道呢？也许没那么糟。我认为动物的运动学习可能有点像策略梯度算法。例如，假设你正在学习投篮。那可能需要数千次尝试才能变得更准确。下面可能有一些策略梯度算法。如果你有一个模型试图完成一个项目或某种任务，那不会是最快的学习方式。我们更希望依赖**上下文学习**（In-context Learning: 指大型语言模型仅通过输入提示中的示例，无需更新模型参数即可学习新任务的能力），在那里你有效地拥有一个学习算法。它学会了如何探索。它学会了如何穷尽所有可能性，而不是一遍又一遍地做同样的事情并犯同样的错误。我们将能够做看起来更像**学习搜索算法**（Learned Search Algorithms: 指通过机器学习方法训练出来的搜索算法，能够更有效地探索问题空间，找到最优解）的事情。那将是用于特定任务的那种东西。

### ChatGPT的诞生与发展历程

很有趣。我想退一步问问你自己的经历，至少在 **OpenAI** 的经历。你领导了 **ChatGPT** 的创建。你是在什么时候意识到这些 **大型语言模型**（LLMs: Large Language Models: 指拥有大量参数、在海量文本数据上训练的深度学习模型，能够理解、生成和处理人类语言）是正确的方向？你是什么时候意识到聊天机器人或某种指导它们的方式会很有用？请带我回顾一下从这成为你主要关注点到整个过程的演变。

在 **ChatGPT** 之前，**OpenAI** 拥有这些**指令遵循模型**（Instruction Following Models: 指能够理解并执行人类给定指令的AI模型）。当时的想法是，我们有基础模型，人们可以用复杂的方式提示它们。但它们也很难提示。它们基本上是自动补全，所以你必须设置一个非常好的提示，并附带一些例子。

**OpenAI** 的人们正在努力将基础模型变得更容易提示。所以如果你只是写一个问题，它就会回答问题，而不是给你更多问题或其他东西。所以我们有这些指令遵循模型，它们就像基础模型，但更容易使用。这些是最初部署在 **API** 中的模型。或者在 **GPT-3** 之后，它们是下一代模型。

与此同时，肯定有很多人在思考聊天。**Google** 有一些论文，比如 **LaMDA**，更早的还有 **Meena**。他们有这些聊天机器人。它更像是一个真正专门用于聊天任务的基础模型。它非常擅长聊天。从论文中的例子来看，它更多地用于有趣的应用程序，模型会扮演某个角色并假装是那个角色。它并不是那么实用，无法帮助我重构代码。

所以肯定有人在思考聊天。我之前曾在一个名为 **WebGPT** 的聊天项目上工作，它更多地是关于在网络浏览和检索的帮助下进行问答。当你进行问答时，它确实希望以聊天的形式进行。你总是想问后续问题，或者有时模型应该问一个澄清问题，因为问题是模糊的。

在完成第一个版本后，很明显下一个版本应该是对话式的。所以我们开始开发对话式聊天助手。这是在 **GPT-3.5** 的基础上构建的，该模型于2022年初完成训练。那个模型在语言和代码方面都相当出色。我们很快意识到它在代码帮助方面确实非常出色。这是我们兴奋的事情之一。

我们为此工作了大部分时间。我们还将浏览作为另一个功能，尽管后来我们不再强调它，因为模型的内部知识非常好。浏览并不是它最有趣的地方。我们把它提供给亲朋好友使用了一段时间，并考虑进行公开发布。

实际上，**GPT-4** 在那年八月完成了训练。**OpenAI** 的旗舰 **RL** 工作是指令遵循工作，因为这些模型正在投入生产。**GPT-4** 的首次微调使用了整个堆栈。这些模型非常出色，每个人在看到指令微调的 **GPT-4** 后都非常兴奋。

它们非常出色。它们偶尔会给出令人惊叹的输出，但模型显然也相当不可靠。它有时会大量**幻觉**（Hallucination: 指AI模型生成虚假、不准确或与事实不符的信息）。它有时会给出相当不合逻辑的输出。所以它显然还没有完全准备好投入使用，但它显然非常好。之后，人们暂时忘记了聊天，这个替代分支。我们进一步推进它，最终将所有数据集，即指令数据和聊天数据混合在一起，试图得到一个两全其美的模型。聊天模型显然更容易使用。它在模型了解自身局限性方面自动具有更明智的行为。这实际上是我在开发它时感到兴奋的事情之一。我意识到许多人认为语言模型的缺陷，比如公然的幻觉，虽然不能完全修复，但可以通过相当直接的方法取得很大进展。

关于聊天的另一件事是，当我们有这些指令模型时，“以一种友好或有帮助的方式完成这段文本”是一个定义不清的任务。这个任务对模型和负责数据标注的人类来说都令人困惑。而对于聊天，人们对一个有帮助的机器人应该是什么样子有一种直观的感觉。因此，人们更容易理解模型应该做什么。结果，模型具有更连贯的个性，并且更容易稳健地获得相当明智的行为。

### ChatGPT的复现难度与AI发展速度

很有趣。任何人都可以使用你们公开可用的微调 **API** 来创建 **ChatGPT** 吗？

不完全是。我不记得当时哪些模型可用于微调。假设当时有 **GPT-3.5** 可用于微调，你可能可以制作出相当接近的东西。我不认为你只需一次纯粹由人类编写的数据的微调迭代就能做到。你需要进行多次迭代。

如果你不打算进行我们所做的 **强化学习**（RL: Reinforcement Learning: 一种机器学习范式，通过智能体与环境的交互来学习最优行为策略），你可能需要某种迭代式的**监督微调**（Supervised Fine-tuning: 一种机器学习方法，通过在特定任务的标注数据集上进一步训练预训练模型，以使其适应特定任务），其中人类编辑模型生成的输出。如果你在人类生成的数据上进行训练，即使质量非常高，模型也很难完美地拟合这些数据，因为它可能只是模型能够输出的东西。你需要做一些迭代性的工作，这更像 **RL**。如果你做了这些，你可能会非常接近，但这并非易事。我们还发布了另一个用 **RL** 训练的指令遵循模型，比 **ChatGPT** 稍早。如果你给它加上一个聊天包装器，你也会得到相当接近的结果，但那个模型在优势上有一些差异。那个模型擅长写作和诗歌，但在了解自身局限性、事实准确性等方面则没那么好。

回溯到 **GPT-3.5** 之前，我记得你曾说过你对 **GPT-2** 印象深刻。与你2019年的预期相比，AI 的发展速度是更快还是更慢？

自 **GPT-2** 以来，发展速度比我预期的要快。我当时相当认同规模化和预训练是个好主意。但当 **GPT-2** 完成时，我并没有完全相信它会彻底改变一切。直到 **GPT-3** 之后，我才改变了我和团队的工作重心。在那之后，我们聚在一起说：“哦，是的，让我们看看我们能用这种语言模型做些什么。”但在 **GPT-2** 之后，我还不完全确定。

### 计算资源分配与后训练的重要性

假设我们之前讨论的 **强化学习**（RL: Reinforcement Learning: 一种机器学习范式，通过智能体与环境的交互来学习最优行为策略）在这些更智能的模型上开始更好地工作。未来，用于预训练与后训练的计算资源比例是否会显著改变，从而更有利于后训练？

对此有一些论点。目前这是一个相当不平衡的比例。你可以争辩说，模型生成的输出质量高于网络上的大多数内容。因此，模型自己思考比仅仅模仿网络上的内容更有意义。所以我认为这有一个**第一性原理**（First Principles: 指从最基本、最核心的假设出发进行思考和推理的方法）的论证。我们通过后训练获得了许多收益。所以我预计我们会继续推动这种方法论，并可能增加我们投入的计算量。

当前 **GPT-4** 的 **Elo** 评分比最初发布的版本高出约一百分。这是否都是因为你所说的，即后训练带来的这些改进？

是的，大部分是后训练的功劳。改进有许多不同的独立维度。我们考虑数据质量、数据数量。还有就是部署和收集新数据的整个过程进行更多次迭代。还有改变你收集的标注类型。有很多因素累积起来，但它们共同给你带来了相当好的有效计算量增加。

这是一个巨大的增长。后训练有如此大的改进空间，这真的很有趣。是什么造就了一个擅长这种 **RL** 研究的人？我听说它非常棘手。你有什么样的直觉，能让你找到这些方法来处理数据和设置这些环境？

### 优秀RL研究员的特质

我现在在堆栈的不同部分积累了相当多的经验，从我研究生时期就开始研究的 **RL** 算法，到数据收集、标注过程，以及玩转语言模型。我想说我对这些事情都有涉猎，而那些擅长这类研究的人对整个堆栈都有一定的了解，并且对它的不同部分充满好奇。你既要**实事求是**（Empirical: 基于观察和实验的），让实验更新你的观点，又要**从第一性原理出发**（First Principles: 指从最基本、最核心的假设出发进行思考和推理的方法）思考。假设学习有效，那么理想的数据收集类型会是什么？就是这类事情。

### AI能力瓶颈与泛化能力探讨

因为自 **GPT-4** 以来似乎没有模型显著更好，所以有一种假设认为我们可能正在达到某种平台期。这些模型实际上并没有那么好的泛化能力，你将会遇到一个数据壁垒，超越这个壁垒，通过记忆大量预训练数据所解锁的能力将无法帮助你获得比 **GPT-4** 智能得多的模型。你认为这个假设是错误的吗？我们已经讨论了一些泛化的例子，比如西班牙语到英语。我想到一个例子是代码到语言推理的迁移。如果你在大量代码上进行训练，它在语言推理方面会变得更好吗？情况确实如此吗？你是否看到不同模态之间存在积极的迁移？如果你在大量视频和图像上进行训练，它会从合成数据中变得更智能吗？还是说，解锁的能力与你放入训练语料库中的确切标签和数据非常局部相关？

我将尝试回应所有这些问题。首先，我们是否即将触及**数据壁垒**（Data Wall: 指在AI模型训练中，由于高质量训练数据量的限制，模型性能提升遇到瓶颈的现象）？我不会从 **GPT-4** 发布以来的时间中得出太多结论，因为训练这些模型并为训练新一代模型做所有准备确实需要一段时间。我不会从这个事实中得出太多结论。有限的数据确实带来了一些挑战，但我不会期望我们立即触及数据壁垒。然而，我预计随着我们越来越接近它，预训练的性质会发生一些变化。

关于不同类型预训练数据的泛化，我想说，对这类问题进行科学研究相当困难，因为你无法创建那么多预训练模型。也许你无法训练一个 **GPT-4** 大小的模型，并在这个规模上进行**消融研究**（Ablation Studies: 指通过移除或修改模型中的特定组件或特征，来评估其对模型整体性能影响的实验方法）。也许你可以训练大量 **GPT-2** 大小的模型，甚至 **GPT-3** 大小的模型，使用不同的数据混合，看看你会得到什么。我不知道有任何关于涉及代码数据和推理性能等方面的消融研究的公开结果。我非常想知道这些结果。

我好奇一件事。其中之一是模型越大越智能。在 **GPT-2** 级别模型上进行的消融研究，如果表明没有太多迁移，这是否能为 **GPT-4** 级别模型在类似领域中的迁移水平提供证据？

是的，你可能无法得出这样的结论：如果迁移在 **GPT-2** 规模上失败，那么它在更高规模上也会失败。可能是对于更大的模型，你学习到更好的共享表示，而较小的模型则过于依赖记忆。更大的模型可以学习如何进行正确的计算。我预计这在一定程度上是正确的。

### 模型规模与样本效率的奥秘

这可能有一个非常简单的答案。你用相同的数据量训练更大的模型，它们变得更智能。或者为了获得相同的智能水平，你只需要用更少的数据训练它们。为什么会这样？它有更多的参数，看到更少的东西，现在它同样智能。为什么会这样？

我认为没有人能很好地解释参数数量与**缩放定律**（Scaling Law: 指在机器学习中，模型性能与计算资源（如模型大小、数据量、训练时间）之间存在的幂律关系）的关系。我甚至不知道最好的心智模型是什么。显然，如果你有一个更大的模型，你就有更大的容量。所以你最终应该能够获得更低的损失。

为什么更大的模型更具样本效率？我可以给你一个粗略的解释。你可以说模型是执行计算的不同电路的集合。你可以想象它正在并行进行计算，输出是它们的加权组合。如果你有更多的宽度……实际上，宽度与深度有些相似，因为对于残差网络，深度可以在更新残差流方面做一些与宽度类似的事情。

你正在并行学习所有这些不同的计算，而且在更大的模型中，你拥有更多这样的计算。所以你更有可能其中一个很幸运，最终猜对了很多，并被赋予更高的权重。有一些算法就是这样工作的，比如**混合模型**（Mixture Models: 一种统计模型，假设数据来自多个不同的子群或组件，每个子群都有自己的概率分布）或**乘法权重更新算法**（Multiplicative Weight Update Algorithms: 一类在线学习算法，通过乘法更新权重来逐步调整决策策略），在那里你拥有——我不想说**专家混合**（Mixture of Experts: 一种神经网络架构，其中包含多个“专家”网络，一个“门控”网络负责为每个输入选择或组合专家的输出），因为它意味着不同的东西——基本上是专家们的加权组合，带有一些学习到的门控。我刚才说得有点不准确，但你可以想象类似的东西。仅仅拥有一个更大的模型，就给了你更多机会获得正确的函数。当然，这不仅仅是你正在进行线性组合的完全不相交的函数。它更像一个库，你可能会以某种方式将函数链在一起。存在一些**可组合性**（Composability: 指系统或组件可以被组合起来形成更复杂系统或功能的能力）。所以我想说，一个更大的模型拥有一个更大的不同计算库，包括许多处于休眠状态且只在部分时间使用的东西，但它有更多的空间来寻找有用的电路。

### 未来几年的AI发展图景

抛开当前的研究问题，我想了解你对未来几年发展模式的设想。在对话开始时，我们谈到了进展非常快的情况，但现在让我们来看看**模式场景**（Modal Scenario: 指最可能发生或最常见的情况）。你会在某个时候解锁**长周期强化学习**（Long-horizon RL: 旨在训练AI模型在长时间跨度内规划和执行复杂任务的强化学习方法），但正如你所说，可能存在其他瓶颈。会发生什么？这些模型会有多好？它们将如何部署？它们将包含哪些其他模态，以及这些模态将在哪个阶段被解锁？我想了解你对未来几年更广阔的图景。

我预计新的模态会随着时间或很快被添加。我预计通过预训练和后训练的结合，能力会普遍持续提升，这将开启新的用例。

目前，AI 在经济中仍不占很大比重。它能帮助的职业比例相当小。我预计随着时间的推移，这个比例会更高，这不仅来自于模型的改进，也来自于人们弄清楚如何将它们整合到不同的流程中。因此，即使我们只是将模型冻结在当前状态，你仍然会看到它们的使用方式有很大的增长。我预计 **AI** 将被更广泛地用于更具技术复杂性的任务。我之前举了编程的例子，做更长的项目，但也帮助进行各种研究。我希望我们能以各种方式利用 **AI** 加速科学发展，因为模型有可能理解给定领域的所有文献，并能够筛选大量数据。这比一个人有耐心做的事情要多。我希望**形式因素**（Form Factor: 指产品或系统的物理设计、尺寸、形状和布局，以及其与用户交互的方式）是人们仍然主导这一切，并且你拥有可以指导和指向许多对你有用的不同问题的有帮助的助手。每个人都会拥有所有这些 **AI** 帮助他们做更多事情并完成更多工作。

### AI运行企业与人类监督的挑战

显然，在某个时候，它们将在任何它们想做的事情上都比所有人做得更好。那个过程会是怎样的？现在，它们显然只是在帮助你。在某个时候，它们将能够为你做事情，甚至可能为你运营整个公司。这会是一个平稳的过程吗？到那时，我们是否希望系统与用户充分对齐，以便他们可以指望公司按照他们期望的方式运营？

我们可能不希望立即让 **AI** 运营整个公司。我们可能希望由人类来监督这些重要决策并掌舵，即使模型本身已经足够优秀，能够成功运营一家企业。在某种程度上，这可能存在选择。我认为人们仍然会有不同的兴趣和想法，想让他们的 **AI** 追求什么样的有趣目标。**AI** 本身不一定有任何内在欲望，除非我们将其植入系统中。因此，即使 **AI** 变得极其强大，我仍然希望人类是 **AI** 最终行为的驱动者。

我怀疑经济均衡与此相去甚远，在一个公司中，你拥有**阿姆达尔定律**（Amdahl's Law: 一个计算机科学定律，指出系统整体性能的提升受限于其最慢或最不并行化部分的性能）的等效物。过程中最慢的部分将成为你的瓶颈。即使 **AI** 使公司中所有非人类的部分效率提高了10倍，公司仍然会被那个步骤所限制。如果一家公司决定在所有真正需要人类监督的事情上都保留人类参与，那么它们就会被其他公司超越。如果一个国家决定走这条路，其他国家就会击败它。我怀疑这是否是一个可持续的让人类参与其中的计划。

如果我们想让人类参与其中，这似乎是合理的，但事实证明，有任何人类参与的公司会被没有人类参与的公司超越，那么你显然需要某种法规，禁止在运营整个公司时没有人类参与。

但任何国家都有那么多公司，更不用说全世界了。我不知道是否最好对公司进行监管，并说你必须在重要流程中保持人类参与，但随后你必须定义什么是重要流程。你必须监控每一家公司，而且你还必须获得每个拥有公司的国家的合作。如果这是一个问题，它是否应该在模型部署之前就解决，以便如果你确实决定建立一家公司并依赖这些模型，它基本上会按照你想要的方式行事，并且你不需要人类参与其中？这个问题有意义吗？我只是想知道，在这种情况下，我们如何才能真正监控每一家公司，以确保人类参与其中？如果中国不决定这样做，会发生什么？

你必须让每个国家都同意这种监管制度，或者你需要所有的模型基础设施或模型提供商都同意这种要求。这肯定不是微不足道的。这展望得有点远，所以在看到任何类似情况之前，很难想象这个世界。例如，我们真的有信心 **AI** 运营的公司在各方面都更好吗？我们认为它们在大多数时候都更好，但偶尔会因为 **AI** 在某些方面仍然不够样本效率而出现故障吗？考虑当它们必须处理非常古怪的情况时。**AI** 运营的公司实际上可能具有更高的**尾部风险**（Tail Risk: 指发生概率极低但一旦发生就会造成巨大损失的风险），因为它们更有可能以大的方式出现故障。可能存在一些实际问题，这些问题将决定事情如何发展。也许如果你只是要求人们对各种责任负责，这也会稍微改变激励机制。

假设事实证明 **AI** 更擅长运营一切，而且它们也完全仁慈。假设我们已经完全解决了对齐问题，而且它们比人类更擅长对人类负责。那么也许让 **AI** 运营公司是可以的。但这还很遥远。我们更有可能处于这样一种情况：它们在短期内看起来更好，但仍然存在一些严重问题。实际上是实际考虑因素促使你更多地让人类参与其中，至少在近期内是这样。

### RLHF与多方利益的平衡

所以这是我们今天必须用 **RLHF** 处理的问题。你必须整合许多不同人类的偏好。对于未来更强大的系统，这可能会更加突出。但是当你提到我们希望这些最终将完全取代人类成为公司一部分的 **AI** 系统能够**对齐**（Alignment: 指确保AI系统能够理解并遵循人类的意图、价值观和目标，从而避免产生意外或有害行为）时，这意味着什么？它是否意味着它们基本上会做用户希望它们做的事情？它是否意味着它们必须产生某种我们作为 **OpenAI** 的利益相关者所乐见的全球结果？具体来说，那会意味着什么？

如果模型被用于这些高风险的用例，那么我们将不得不以与现在截然不同的方式思考 **RLHF**。我们还没有完全准备好，或者当前的方法可能还不够充分。我们需要在所涉不同利益相关者的需求之间做出妥协。我们正在发布一份名为《**Model Spec**》的文档。它阐述了我们希望模型在 **API** 和 **ChatGPT** 中的行为方式。我们试图讨论这个问题，即存在不同的利益相关者，有时他们的需求之间会存在冲突。在我们的案例中，我们将利益相关者视为：**最终用户**（坐在 **ChatGPT** 或其他应用程序前的人）、**开发者**（使用 **API** 为其他最终用户提供服务的开发者）、**平台**（**OpenAI**，我们不希望模型让我们面临法律风险）以及**全人类**（包括不属于用户或客户的人）。

显然，用户可能会要求模型做一些我们认为对其他人有害的事情。我们可能不得不拒绝。顺便说一句，这不一定是优先顺序。这些只是大约四类利益相关者。实际上，未来你可能还会加上模型本身。我们还没有到那一步。总之，我们有这些不同的利益相关者。有时他们有相互冲突的需求。我们必须就如何解决这些冲突做出一些决定。这并不总是显而易见的。我们必须权衡利弊，基本上粗略的启发式是，我们主要希望模型遵循你的指令，并对用户和开发者有所帮助。但当这侵犯到其他人的幸福或生活方式时，这就成了一个问题，我们必须阻止某些类型的使用。我们主要希望模型只是人类意志的延伸，并按照他们说的去做。我们不想过于家长式作风。我们希望保持中立，不将我们的意见强加于人。我们主要希望让人们用模型做他们想做的事情。

### ML研究的现状与未来方向

我之前有机会阅读了这份《**Model Spec**》。这是一个关于它如何很好地转化为模型自身行为的问题。我对其中权衡的合理性印象深刻。我相信实际的**边缘案例**（Edge Cases: 指在系统或程序运行中，发生在极端或不常见条件下的情况，这些情况往往难以预测或处理）被明确地阐述了，而不是那些显而易见的情况。在这种情况下，你们确实在追寻边缘案例。

我们希望它具有很强的可操作性，而不仅仅是一堆好听的原则。每个例子都告诉你一些关于非显而易见的情况，并对该情况进行推理。

我有几个关于研究现状的问题。众所周知，在社会科学中，事情很难复制。问题在于那里有多少科学是真实的，有多少是这些定制的、特制的实验。当你看到一篇普通的机器学习论文时，它感觉像是一篇非常扎实的文献，还是常常感觉像是社会科学中 **p 值操纵**（P-hacking: 指研究者通过选择性报告数据、调整分析方法等方式，使统计结果（p值）达到显著性水平，从而支持其预设假设的行为）的等价物？

每个人都对机器学习文献有所抱怨。总的来说，我认为这是一个相对健康的领域，尤其是与社会科学等其他领域相比。它很大程度上以实用性和让事物发挥作用为基础。如果你发表的东西不容易复制，人们就会忘记它。人们普遍接受，你通常不只是报告别人论文中的数字。你还会尝试重新实现他们的方法，并在相同训练数据集上与你的方法进行比较。如果你发表的方法很难实现或非常棘手，它们往往会被遗忘。

因此，人们实际上会努力开源他们的工作。也存在各种不利的激励。人们被激励去贬低他们正在比较的基线方法。还存在其他一些轻微的**病态**（Pathologies: 指系统或领域中存在的异常、不健康或功能失调的现象），比如试图让你的方法在数学上看起来很复杂。但总的来说，我觉得这个领域正在取得进展。我希望看到更多的科学研究，努力理解事物，而不是仅仅在基准上进行**爬山**（Hill Climbing: 一种优化算法，通过迭代地向目标函数值更高的方向移动来寻找局部最优解），并试图提出新方法。最近这方面的工作已经不少了。我们可以做得更多。我认为这对学术界来说是一件好事。

稍微跑题一下，我非常期待看到更多关于使用基础模型进行模拟社会科学的研究。这些模型对整个世界都有一个概率模型，你可以设置一个模拟问卷或对话，并查看任何事物是如何关联的。你可以想象任何你可能想到的特质，并查看它们可能与其他特质如何关联。如果人们能够通过以不同方式提示基础模型并查看哪些是相关的，来复制社会科学中一些更著名的结果，比如**道德基础**（Moral Foundations: 指人类道德判断和推理所依赖的普遍心理机制或直觉），那将是非常酷的。

那是斯坦福大学的什么实验？**阿施从众实验**（Asch Conformity Test: 一项社会心理学实验，研究个体在群体压力下改变自己判断或行为的现象）？如果它也能在语言模型中复制，那会很有趣。这非常有趣。我想问问大型实验室中其他研究的情况。其中有多少是作为实际的计算倍增器，增加或减少你获得某个结果所需的计算量，而有多少只是让学习更稳定和构建基础设施？

我试图提出的更广泛的问题是，自 **GPT-4** 以来，你是否觉得用相同的计算量，你可以训练一个更好的模型？或者你是否觉得你已经确保了 **GPT-5** 的学习可以更好、更具可扩展性，但现在我们不能用 **GPT-3.5** 的预算训练 **GPT-4** 了？

### 效率提升与RLHF的个性化挑战

效率提升方面肯定一直在进步。每当你有一个一维的性能指标时，你会发现不同的改进可以相互替代。你可能会发现后训练和预训练都能改善指标。它们在改善哪些指标方面会有略微不同的侧重。但归根结底，如果你只有一个数字，它们在某种程度上都会相互替代。对于像人类评估这样的东西，人类喜欢什么，我们在预训练和后训练两方面都取得了很大进展。

几个关于 **RLHF** 的快速问答。显然，**RLHF** 对使这些模型有用很重要。所以“**脑叶切除**”（Lobotomized: 指通过手术切除部分脑组织以改变行为，这里比喻AI模型在RLHF后可能失去某些原始能力或个性）的描述可能不准确。然而，有一种感觉，所有这些模型一旦以聊天机器人的形式出现，都有非常相似的说话方式。它们真的想“深入探讨”事物。它们想把事情变成要点。它们常常显得正式而乏味。有人抱怨它们不够有创意。就像我们之前谈到的，直到最近它们才能写押韵诗歌，而不能写不押韵诗歌。这是目前 **RLHF** 发生方式的特定结果吗？如果是，是因为评估者是谁吗？是因为损失函数是什么吗？为什么所有聊天机器人都是这种样子？

我想说，在具体的训练过程中，有相当大的变异空间。我们正在积极努力改进这一点，让写作更生动有趣。我们已经取得了一些进展，比如改善了 **ChatGPT** 的个性。它更有趣，当你试图和它闲聊时也更好，等等。它不那么像机器人了。有些习惯是如何产生的，比如“深入探讨”这个词，这是一个有趣的问题。我最近确实发现自己也使用了这个词。我不知道是不是受模型影响的。实际上，可能还存在一些有趣的效应，即语言模型和提供者之间发生了无意的**蒸馏**（Distillation: 在机器学习中，指将一个大型复杂模型的知识转移到一个小型简单模型的过程）。如果你雇人去做标注任务，他们可能只是将其输入模型。他们可能会打开他们最喜欢的聊天机器人，输入内容，让模型完成任务，然后复制粘贴回来。这可能解释了一些趋同现象。

我们看到的一些现象只是人们喜欢的东西。人们确实喜欢要点。他们喜欢结构化的回复。人们确实常常喜欢从模型中获得的大量信息。所以，这在多大程度上仅仅是后训练过程特定选择和设计的一个怪癖，又在多大程度上是人们真正内在需求，目前尚不完全清楚。

它似乎比某些人想要的更冗长。也许只是因为在标注阶段，评估者会更喜欢更冗长的答案。我不知道这是否是固有的，因为它是如何预训练的，并且停止序列不常出现，它真的只想继续下去。

标注中可能存在一些导致冗长的偏见。我们倾向于一次训练一条消息，而不是完整的交互。如果你只看到一条消息，那么一个只有澄清问题，或者一个简短回复并邀请后续的问题，会显得不如一个涵盖所有可能性的消息完整。

还有一个问题是，人们的偏好是否会根据模型输出流的速度而改变。显然，如果你坐在那里等待 **token** 出来，你会更喜欢它直奔主题。但如果它立即给你一堆文本，也许你并不在意是否有大量样板文件，或者是否有大量你需要略读的东西。你宁愿所有东西都在那里。

### 奖励模型与价值观对齐

**奖励模型**（Reward Model: 在强化学习中，根据人类反馈评估模型输出质量的模型，用于指导AI训练）是一个非常有趣的产物，因为它最接近我们所拥有的对人类需求和偏好的聚合。我正在思考更智能的模型。一个希望是，你可以给它一个我们想要的非琐碎且非显而易见的事物列表，比如《**联合国人权宣言**》。

另一方面，我想我听你提出过一个观点，即我们的许多偏好和价值观都非常微妙，因此它们可能最适合通过**成对偏好**（Pairwise Preferences: 指在两个选项之间进行选择，以表达个体偏好的一种方式）来表示。当你想到 **GPT-6** 或 **GPT-7** 级别的模型时，我们是会给它更多的书面指令，还是仍然会采用这种潜意识的偏好方式？

这是一个好问题。这些偏好模型确实学习了许多关于人们偏好的微妙之处，这些是很难在说明手册中明确阐述的。显然，你可以编写一本包含大量比较示例的说明手册。这正是《**Model Spec**》所做的。它有很多例子和一些解释。目前尚不清楚描述偏好的最佳格式是什么。

我猜测，无论你从一个捕捉模糊偏好的大数据集中能得到什么，你都可以将其提炼成一个更短的文档，主要捕捉其核心思想。更大的模型确实会自动学习许多关于人们可能觉得有用和有帮助的概念。它们会拥有一些复杂的道德理论，可以依附于这些理论。当然，仍然有很多空间可以依附于不同的风格或不同的道德观。因此，如果我们要编写一份文档，如果我们要对齐这些模型，我们所做的就是依附于一种特定的风格，一种特定的道德观。你仍然需要一份相当长的文档来准确捕捉你想要的东西。

### 后训练的护城河效应与标注者画像

更好的后训练能形成多大的**护城河**（Moat: 在商业领域，指企业为抵御竞争对手而建立的竞争优势）？公司目前通过模型规模等来区分自己。对于那些已经弄清楚你之前提到的所有关于数据处理的复杂性的人来说，这会是一个很大的护城河吗？

这确实构成了一道护城河，因为它是一个非常复杂的操作，需要大量熟练的人员来完成。需要大量的**默会知识**（Tacit Knowledge: 难以用语言或文字表达，通过经验和实践获得的知识）和组织知识。通过后训练，要创建一个真正拥有人们所关心的所有功能的模型，是相当复杂的。它需要相当复杂的努力和大量的研发积累。这使得它在某种程度上成为一道护城河。立即启动它并非易事。看起来，那些投入最认真预训练工作的公司，也在投入最认真的后训练工作。

在某种程度上，复制或启动更多此类工作是可能的。也有一种力量会削弱这种护城河效应。你可以**蒸馏**（Distill: 在机器学习中，指将一个大型复杂模型的知识转移到一个小型简单模型的过程）模型，或者你可以使用别人的模型并克隆其输出。你可以使用别人的模型作为判断者进行比较。那些大公司可能不会这样做，因为它违反了服务条款政策。这也会损害他们的自尊。但我预计一些小公司会这样做以起步。这在很大程度上能让你迎头赶上。

我想这有助于清除护城河。中位数标注者是什么样的？他们来自哪里？他们的政治倾向如何？他们的知识水平如何？

情况差异很大。我们确实为不同类型的任务或项目雇佣了具有不同技能的标注者。一个不错的心理模型是看看 **Upwork** 等平台上的人。看看那些从事远程工作的零工。这是一个相当国际化的群体。美国有相当多的人。我们为不同类型的标注雇佣了不同的人群，比如我们更侧重于写作还是 STEM 任务。从事 STEM 任务的人更有可能在印度或其他中低收入国家。从事更多英语写作和作文的人倾向于以美国为基地。有时我们需要为我们的某些活动雇佣不同的专家。其中一些人非常有才华，我们甚至发现他们至少和我们这些研究人员一样擅长这些任务，而且他们比我们细心得多。我想说我们现在拥有的人员非常熟练和尽责。

### 泛化能力与未来AI助手

关于**平台期叙事**（Plateau Narrative: 指AI模型性能增长达到瓶颈，难以再取得显著突破的观点），我听说的一点是，这些模型在特定事物上帮助你的许多能力都与**监督微调数据集**（Supervised Fine-tuning Dataset: 用于对预训练模型进行微调的标注数据集，包含输入和对应的期望输出）中非常匹配的标签有关。这是真的吗？它能正确地教我如何使用 **FFmpeg** 吗？是不是有人看到输入，看到你需要添加什么标志，然后有人类弄清楚并匹配到它？你需要雇佣所有这些在不同领域具有专业知识的标注者吗？如果是这样，那么随着时间的推移，让这些模型变得越来越智能，似乎会是一个更大的苦差事。

你并不完全需要那样。你可以从泛化中获得很多。基础模型已经接受了大量文档、代码以及 **shell 脚本**（Shell Scripts: 一种计算机程序，通过命令行解释器执行一系列命令）等的训练。它已经看过了所有 **FFmpeg** 的手册页、大量的 **Bash 脚本**（Bash Scripts: 一种在Unix-like操作系统中运行的脚本语言，用于自动化命令行任务）等等。即使只是给基础模型一个好的**少样本提示**（Few-shot Prompt: 一种提示工程技术，通过在提示中提供少量示例来指导大型语言模型完成特定任务），你也可以让它回答这类查询。仅仅训练一个用于帮助性的偏好模型，即使你没有在任何 **STEM** 领域训练它，它也会在某种程度上泛化到 **STEM**。所以，你不仅不需要如何使用 **FFmpeg** 的例子，你甚至可能不需要任何编程相关的东西就能在编程领域获得一些合理的行为。

也许是最后一个问题。我们以不同的方式触及了这个问题，但让我们把它整合起来。你说你正在训练更多**多模态数据**（Multimodal Data: 包含文本、图像、音频、视频等多种数据类型的数据集）。想必，这些东西能理解屏幕是什么样子，并能以更连贯的方式与它们互动。此外，你还将进行**长周期强化学习**（Long-horizon RL: 旨在训练AI模型在长时间跨度内规划和执行复杂任务的强化学习方法），所以它们将能够作为系统中的**智能体**（Agents: 在AI领域，指能够感知环境、进行决策并采取行动以实现目标的自主实体），以更集成的方式成为你工作流程的一部分。你期望那会是什么样子？从那里开始的下一步是什么？假设到今年年底或明年，你有一个可以与你在屏幕上协作的助手。这似乎是一个合理的预期吗？从那里开始，它会走向何方？

我当然期望事情会朝着那个方向发展。目前尚不清楚最佳的**形式因素**（Form Factor: 指产品或系统的物理设计、尺寸、形状和布局，以及其与用户交互的方式）会是什么。它可能像你电脑上的 **Clippy** 一样帮助你，或者更像云端的一个有帮助的同事。我们会看看哪种形式因素效果最好。我预计人们会尝试所有这些。

我期望一个有帮助的助手或有帮助的同事的心理模型变得更加真实。它将是你可以分享更多日常工作的东西。你不再只是给它一次性查询，而是有一个你正在做的完整项目，它了解你迄今为止在该项目上所做的一切。它甚至可以主动提出建议。也许你可以告诉它记住问我这个，以及我是否取得了任何进展。**主动性**（Proactivity: 指AI系统能够主动识别需求、预测问题并采取行动，而不是被动地等待指令）是目前缺失的一点。我希望我们能摆脱一次性查询，像搜索引擎一样使用模型，更多地转向与模型协作完成一个完整的项目。它了解我所做的一切。它会主动建议我尝试的事情，或者它会在后台进行工作。

这真的很有趣。这是最后一个问题。你认为你的工作何时会被取代？

哦，它会取代我的工作？也许五年。

很快。很有趣。**John**，这真的非常有趣。非常感谢你抽出时间。这似乎是 **AI** 过程中一个非常重要但人们了解不多的部分。深入探讨并听取你的想法非常有趣。

感谢邀请我参加播客。很高兴谈论所有这些事情。