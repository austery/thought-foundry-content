---
area: "society-thinking"
category: technology
companies_orgs:
- 加州大学伯克利分校
- Microsoft
- Nvidia
date: '2025-11-08'
draft: true
guest: ''
insight: ''
layout: post.njk
media_books:
- 《星球大战》
- 《玩具总动员》
- 《指环王》
people:
- Sam Altman
- Sergey Levine
- 黄仁勋
products_models:
- ChatGPT
- Optimus
- Unreal Engine
- LLM
- VLA
project: []
series: ''
source: https://www.youtube.com/watch?v=zR9tvIE_P-w
speaker: 硅谷101
status: evergreen
summary: Sora2的出现凸显了AI“有脑无身”与机器人“有身无脑”的矛盾。3D数字人被视为连接虚拟与现实、实现具身智能的关键路径。本文探讨了3D数字人的发展简史、2D与3D技术路线的差异，以及魔珐科技“星云”平台如何通过文生多模态3D大模型和云-端拆分架构，试图破解高质量、低延时、低成本的“不可能三角”。文章还分析了3D数字人行业面临的成本、数据稀缺和泛化性挑战，并展望了其在屏幕交互、泛娱乐及机器人领域的广阔应用前景。
tags:
- ai-robotic
- human
- intelligence
- llm
title: 给AI一个“身体”：3D数字人如何驱动具身智能的未来？
---
### AI发展：有脑无身与有身无脑的矛盾

最近，AI视频领域的热潮被**Sora2**（一个能够生成逼真短片的视频生成模型）点燃。它刚发布时，以假乱真的画质、自然的语态和生动的微表情令人非常震惊。在10月5号举办的硅谷101线下科技大会上，我们甚至用了一段Sora2生成的Sam Altman的AI视频。

然而，Sora2虽然代表了视频生成质量的飞跃，但它依然有局限性。因为它本质上依然是文生2D视频，能够生成影像，但无法实时交互。它属于被观看的内容，而非实时可交流的伙伴。因此，Sora2距离一些商业应用仍有一定距离，因为它无法满足不能有瑕疵、实时互动和低成本规模化这三大核心要求。

Sora2的火爆，其实反而凸显了当下AI发展的核心矛盾：我们正处于一个分裂的时代。一方面，以ChatGPT为代表的**大模型**（Large Language Model，简称LLM：一种基于海量数据训练的深度学习模型，能理解、生成人类语言）让AI拥有了前所未有的大脑，它们能推理、写作、编码，但是它们是无形的。另一方面，以波士顿动力、Optimus、Figure为代表的人形机器人拥有了强大的身体，它们能跑、能跳、能搬运，但它们还缺乏大脑，还不能达到与人真正交互的地步。也就是说，虚拟世界的大模型“有脑无身”，而现实世界的机器人“有身无脑”。这两条路径都只完成了未来的一半。

那么，在未来承担人机交互的载体究竟会是什么？如今，一种可能的融合方式，以及一种通往“**具身智能**”（Embodied Intelligence：指人工智能系统拥有物理身体，能够感知、理解并与物理世界进行交互的能力）的路径，就是**3D数字人**（3D Digital Human：通过计算机图形技术创建的、具有三维形态和交互能力的虚拟形象）。

你可能会觉得奇怪，3D数字人的应用场景不应该是游戏、AI陪伴、主播，还有直播带货这些场景中吗？其实远不止如此。因为要让AI真正地活过来，它不仅要能理解，更要能以人类的方式去表达，用表情传递情绪，用声音与语气沟通，用动作和形象建立存在感。更重要的是，它被视为目前“具身智能驱动层”的一个重要科技突破方向，让生成式AI的进化从2D到3D，再到现实的物理世界交互。

### 数字人的发展简史

首先，我们来聊聊数字人的发展简史。数字人的概念并非诞生于AI时代，它的演进可以说是一部跨越半个世纪的**计算机图形学**（Computer Graphics，简称CG：利用计算机生成、处理和显示图像的技术）、电影工业与互联网文化交织的历史。

数字人的最初形态孕育于电影工业，也是由CG技术驱动的。从乔治·卢卡斯在《星球大战》中构建的虚拟场景，到《玩具总动员》作为第一部全CG电影带来的革命，再到电影《指环王》中“咕噜”这一角色的诞生，数字人技术完成了从0到1的突破。“咕噜”的出现是一个里程碑，它首次将演员的表演通过动作捕捉技术，完整映射到一个全CG的虚拟角色上。这一阶段的数字人是技术与艺术的结晶，其特点也显而易见：制作周期以年为单位，完全非实时，是绝对的**专业内容生产**（Professional Generated Content，简称PGC：由专业机构或个人创作和发布的内容）。他们是荧幕上的奇观，而非大众可及的工具。

后来，随着互联网的普及，数字人开始走出昂贵的电影片场。2007年，日本基于语音合成引擎技术推出的初音未来，开启了虚拟歌姬的浪潮。她没有实体，却能举办万人演唱会，一个强大的**虚拟IP**（Virtual Intellectual Property：指在数字世界中拥有独立形象、人设和故事的虚拟角色或品牌）就此诞生。此后，从AYAYI到柳夜熙，国内超写实虚拟偶像相继引爆社交媒体。这一阶段，数字人的核心价值从技术奇观转向了IP价值。他们拥有了人设、故事和粉丝群体，但其交互能力依然薄弱，高度依赖背后的“中の人”（也就是真人演员）或预先制作的精美内容。他们是完美的数字木偶，却不是独立的数字生命。

真正的质变发生在AI技术，尤其是大语言模型突飞猛进之后。从微软小冰尝试用AI赋予聊天机器人情感，到AI虚拟主播开始7*24小时播报新闻，AI的注入使其从“能看能聊”进化到“会思考”。一个数字人不再只是被动执行脚本，而是能够理解你的问题，并且生成有逻辑、有知识的回答。数字人的“智能”拼图就被补上了。

### 2D与3D数字人技术路线的差异

在AI的加持之下，数字人行业逐渐分化为两条主流的技术路线：2D和3D。

第一种是2D的技术路径。无论是视频生成类模型，还是2D数字人平台，都属于这种类型。比如说像Sora，它就是一个视频版的Stable Diffusion，它能够生成视觉上逼真的短片，但并不能实时驱动角色进行表达或者交互。或者以Heygen等平台为代表的数字人平台，它是用真人视频录制加语音驱动口型，核心机制是通过AI模型根据输入的语音来合成视频中人物的二维唇形变化，使嘴部动作与语音对齐。简单来说，它主要是在预先录制好的真人视频上进行嘴型替换，让视频里的人看起来像是在说指定的话。这种方式的优势在于人物形象真实（因为是基于真人录制的），制作相对简单，但局限性也非常明显。

第二种就是3D的技术路径。这种方向本质是由语言驱动身体，实时生成声音、3D动作、表情和手势在内的多模态表达信号。它属于**文生多模态3D动作大模型**（Language Action Model，简称LAM：一种能够理解语言指令并生成多模态3D动作、表情和声音的大模型）。这区别于传统的LLM（Language Model）、**VLM**（Vision-Language Model：视觉语言模型）和**VLA**（Vision-Language-Action Model：视觉语言动作模型）。这种方式的技术壁垒更高，但是优势在于它是真正的“具身表达”能力。

总结一下，2D数字人的核心是语音驱动嘴型的视频合成技术，让AI“会说话”；而3D数字人核心是语言驱动身体的实时生成技术，让AI“会表达”。随着AI的进一步发展，我们对交互真实感的要求在逐步提高，具备实时交互和丰富表现力的3D路线也越来越成为重要的发展方向。

### 魔珐科技的破局之路与“不可能三角”

要真正实现媲美真人的交互效果，3D技术方向还有一些技术挑战，需要一个强大的技术班底来支撑。我们最近采访了魔珐科技创始人柴金祥教授。柴教授可以说是坚定的“3D数字人派”，他是美国卡内基·梅隆大学的机器人学博士，师从计算机图形学界泰斗、美国工程院院士Jessica Hodgins。卡内基·梅隆大学的机器人研究所是全球AI与机器人研究的圣地。柴教授当年的博士论文研究的就是如何让虚拟人具备自然的动作生成与控制能力，实现实时互动。他所在的团队也是世界上最早开始用AI做动画的团队之一。

一个有趣的现象是，柴教授的许多学术同僚，比如说加州大学伯克利分校副教授、Physical Intelligence（PI）的联合创始人Sergey Levine，都是从AI动画领域转向了机器人研究。这并非巧合，而是因为他们很早就意识到，虚拟世界的AI动画与物理世界的机器人控制，它在底层逻辑上是同源的。柴教授提到，以前图形学或动画领域与NLP（自然语言处理）或语音识别是两个方向，但今天AI出现后，同一波人可以同时做大语言模型、动画大模型、驱动和“造人”这一块。他发现，当AI出现后，这些东西只是数据的不一样，串起来后会发现所有东西都可以做，只要有数据。

在20多年的学术积累之后，柴金祥教授选择3D+AI这条在当时看似不可能，但也颇具前瞻性的方向，并在2018年创立了魔珐科技。如今，想要生成一个高质量的3D数字人，正如柴教授所概括的，它通常包括5个核心的环节：建模（创建人的形象）、绑定（让其动起来，有肌肉环节）、动画（驱动，让其表演，有表情动作手势）、解算（处理头发、衣服等物理效果，也叫特效）和渲染（最终输出视频）。这五个模块——建模、绑定、动画、解算、渲染——传统上都需要大量的专业人工和时间投入，是3D内容生产成本高昂的根源。而业界的新目标，正是要用AI去重塑这个流程。

在多年的技术积累之后，魔珐科技最近推出了“星云”平台。它的定位也很清晰，是全球首个面向开发者的具身智能3D数字人开放平台。“星云”试图解决一个重要的问题：当AI拥有了GPT这样聪明的大脑之后，该如何赋予它一个可交流、可互动、可信赖的身体呢？

要给AI补上这个身体，其实没有那么容易。在传统技术路径下，一直有一个“**不可能三角**”（Impossible Triangle：指在特定领域中，三个理想目标往往无法同时实现，只能选择其中两个或牺牲一个）的说法，导致惊艳的Demo很多，但是真正能够大规模商用的产品极少。我们在之前讲灵巧手的机器人系列视频中提到了机器人的“不可能三角”，而在3D数字人上，也存在着“不可能三角”，就是高质量、低延时、低成本这三种往往是互相掣肘的。

如果要想同时做到高质量加低延时，那比如说电影级别的那种实时动作捕捉，就需要顶尖的图形工作站和高端的**GPU**（Graphics Processing Unit：图形处理器，负责处理图像和视频输出）。这会导致成本爆炸，不可规模化。要想做到高并发加成本低，就又必须要去牺牲质量，往往这个效果粗糙，口型漂移，动作比较机械，有点像早期不成熟的AI数字人。但又想做到高质量加高并发，就又会导致延时严重，失去实时交互，只能用于离线内容的生成。

那么有什么样的破局方法呢？柴金祥教授认为，魔珐科技的“星云”平台在试图创建两个护城河，分别是“文生多模态3D大模型”与“创新性的云-端拆分架构”，用这样的方式来破局。

第一个重要点就是采取了3D这个路径。Sora2虽然不是做驱动的，它是做视频的，它采取的是2D。当然2D也有2D数字人的路径。3D的好处是它能够精准控制人的表情、动作，包括手势。Sora虽然能看到有表情动作，但它是一些prompt，你无法控制我的手要怎么动，我的脸要怎么动，我的身体要怎么动。第二个点就是因为它是3D，3D还有一个好处，就是当你做大模型的时候，因为3D是结构化的数据。比如说去描述一个人的表情、动作、手势的话，其实对现在3D的动画，不管多复杂，比好莱坞的3D角色还是3A级游戏的角色，参数一般来说到1000个左右就可以描述这个人所有的表演。但你现实世界去描述它的时候，它是个图像，那可能是1000*1000的图像，就是100万个像素。你去描述它的时候，它的空间很大。图像空间、视频空间要模型小，相对小了以后，它就会导致它的整个延时会低，实时性会高，成本也会相对比较低一些。所以这种结构化优势使得3D模型在训练效率和参数量上，相比2D视频模型更具潜力，训练所需的绝对数据量也远小于后者。

当然，与互联网上泛滥的文本和2D视频不同，高质量结构化的3D数据是极度稀缺的。从2018年起，柴教授的团队就通过服务游戏、影视等B端客户和自建内容，积累了超过1000小时的高质量3D动画数据。这些数据包含了细腻的表情、手势、动作和语音的精确匹配。基于这些数据，魔珐团队得以实现从语义理解到声音、表情、手势动作的全链路一体化生成。当大模型告诉数字人“该说什么”时，“星云”则告诉它“该怎么说”，并且实时生成匹配的语音口型、表情和动作，赋予其表现力。

我们再来说说第二道护城河。传统的数字人，无论是本地渲染还是云端串流视频，都极度依赖高端的GPU。所有3D你经常去跑，你一定要用渲染引擎、游戏引擎，最好的就是Unreal。Unreal其实你要做，你是跑不掉图形卡、显卡。一路并发的时候，一路基本上叫一张卡。一张图形卡基本上如果你是用民用卡的话，也要2万块钱左右。如果是1000个人，基本上成本就很难弄了，都要2000万的并发成本。所以这个事情其实是做3D的人所有人都逃不出去的。

AI时代，高昂的硬件成本是规模化的最大掣肘。而“星云”的架构则试图颠覆这一路径，它采用了一种创新性的云-端拆分模式。在云端，也可以说是“大脑端”，它只负责最核心的AI计算，也就是根据文本生成轻量的“语音参数+动作参数”，而不是庞大的视频流。而在端侧，也就是“本地侧”，在接收到这些参数之后，通过魔珐的“AI渲染/解算”技术，在本地设备上实时将参数“画”成最终的3D画面。柴教授提到，解算跟渲染以前是游戏引擎干的事情，现在他们用AI来做这个渲染和解算，使得他们不需要显卡。而且他们做得非常轻，轻到在端上非常低的、不是很强大的芯片都能跑。这种将对高端GPU的依赖转移到对轻量级AI解算能力的做法，不仅极大降低了硬件成本，还避免了云端渲染带来的高昂视频流的带宽成本。这是实现“低成本”与“高并发”的关键所在。以前如果说要显卡去计算，那主要的成本是在那张显卡上。所以这个成本基本上今天终端，比如像电视机上要放一个具身智能数字人，可以对话什么，他可以回答你问题，那这个成本基本上是非常非常低的，低到所有的今天大的电视机厂家都会觉得都可以接受。

归根结底，“星云”不是一个内容制作工具，而是让大模型有身体的底层基础设施。它的目标是赋能所有开发者，其商业模式也是对标大模型，按API的调用量来收费。

### 3D数字人行业的困境与挑战

尽管有前沿的创新尝试试图打破“不可能三角”，但是3D数字人行业要实现真正的大规模普及，依然面临三大核心挑战。

首先是造人的初始成本。英伟达CEO黄仁勋在发布会上曾经展示自己的数字人，其制作成本被业内估计高达10万美元。虽然“星云”平台极大地降低了驱动和渲染的成本，但是初始高精度建模和绑定的成本依然是阻碍中小企业和个人开发者入局的第一道门槛。要想实现ToC场景的普及，也就是人人都有数字分身的愿景，建模成本依然需要大幅度下降。

其次是数据的稀缺性。高质量3D动画数据的获取难度和成本极高，这是整个行业AI化的核心瓶颈。目前行业普遍处于AI公司缺数据、内容公司缺AI能力这样的尴尬境地。没有海量、多样化的3D数据作为“燃料”，AI的表现力就无从谈起。我们的嘉宾柴教授对此的策略就是用高质量的3D数据去打基础，同时在探索融合海量的2D视频数据。他认为最终的数据是3D加2D的数据，他们现在在做新的模型，有大量的2D数据，这些2D数据可能被标注过，有部分比如说人脸，大量的人脸，比如视频里面看到人脸的数据，有各种表情，那可以弥补3D数据里面的多样性。但人类的数据它没有结构化，因为它是2D的。怎么把3D跟2D结合起来，最后去做这个混合的模型，他们后面也会有新的大模型会去发布做这个事情。因为到最后，真正要部署下去的时候，这个模型真正要在终端用的时候，如果真的要走商业化，它一定是轻量级的。

第三是**泛化性**（Generalization：指AI模型在未见过的数据或新场景中表现出良好性能的能力）的问题。这是3D数字人乃至整个具身智能领域最核心的挑战。拿机器人来举例，比如说波士顿动力的机器人爬楼梯，挑战的地方就在于，如果你给他不同的楼梯高度，他不一定都能爬得很好。这就是为什么从前波士顿动力在宣传Demo里面经常放的是同一个楼梯。这就是泛化性的难题，能不能做到已有数据之外的事情。就拿爬楼梯举例，每个楼梯的高度、有多少层楼梯、不同楼梯本身的摩擦力、摩擦系数是多少，这些都是一些要泛化的参数。而在今天，恐怕还没有机器人公司可以说任何一个楼梯都能够爬得特别稳的。这种从特定场景Demo到全场景产品的鸿沟，也同样存在于3D数字人领域。

3D数字人同样面临着泛化性的难题，这里边的难点有三个：
1.  **交互的泛化**：当下的数字人大多只能对应标准的问答，所谓的**SOP**（Standard Operating Procedure：标准操作程序）。但是用户真实的提问是发散的，是充满了上下文切换和潜在意图的。数字人是否能在非标的对话当中依然保持有逻辑、有情感的回应？
2.  **动作的泛化**：能否根据指令生成在各种复杂环境下的合理动作？例如，在拥挤的街道行走和在空旷的草地奔跑，这样的形态当中，肢体动作和神态是截然不同的。AI是否能够理解这种差异，并且生成非重复、符合情景的动作呢？
3.  **情感的泛化**：这是最难的一环。数字人能否根据对话的内容和情绪，生成真正自然而非模板化的微表情？是发自内心的微笑，还是礼貌性的假笑？是专注的倾听，还是敷衍的点头？

总的来说，“泛化性”是衡量一个AI系统是否真正智能的关键，也是3D数字人从“数字木偶”走向“数字生命”必须要跨越的障碍。对此，魔珐的策略是区分“通用能力”和“垂直应用”。对于通用能力，大概可以分为四个方面：表达能力（沟通时的表情动作手势）、基本技能（走路跑步开门坐下等日常技能）、干活技能（抓东西开车搬东西等）和特殊技能（跳各种舞）。如果要去训练一个通用的大模型，所需数据量就会太大。这个过程中，他要分两部分：一部分是训练基座本身的大模型，另一部分是针对智能助手等“最佳实践”场景。例如，银行客服、AI陪练、AI面试官等岗位，不需要多样性，因为对于一个面试官来说，一定有一个表达方式最好。但如果是表演，多样性就要上去，因为需要个性化。魔珐在做的，就是先在银行客服、销售陪练、AI面试官等最佳实践的场景落地，这是一种“小泛化”，先验证价值，然后再去逐步构建通用的基座模型，提升“大泛化”能力。

### 3D数字人的未来图景与具身智能的交集

在未来，如果泛化能力得以加强，3D数字人所描绘的未来图景会是什么样的呢？

第一个大应用场景，就是让每一块屏幕“活”起来。这可能是3D数字人最短、最快、也最容易实现的路径。人机交互将从“遵循机器的逻辑”（我们需要在一个系统里点击）重新回归到“机器理解人的逻辑”（也就是自然对话中）。从政务大厅的办事窗口、银行的自助终端、酒店的前台、商场的导购屏，到博物馆的展陈屏幕，再到家中的电视、学习机，屏幕将不再是“被动显示窗口”，而是“主动交流的服务入口”。

柴教授认为，今天去看商业化，最有价值或最容易打的，肯定是说有身体跟没身体是不一样的，这是最大区别。例如，商用大屏，如**BI**（Business Intelligence：商业智能，指用数据分析来辅助商业决策）的问数屏、展厅屏幕、政务中心屏幕、酒店屏幕、银行屏幕或商场屏幕，以前只是个展示品。但你加了这个AI以后，它就变成个人了，就像把一个白领放在里面，这个价值非常非常大。以前是被动展示类品，现在你可以给他互动，他可以给你回答问题咨询，给你呈现各种信息。所以有没有“身体”，差别是特别特别大的。而且在线下的过程中，用户与他沟通的时候，不喜欢对着空气讲话。比如我语音交互，他给我的东西就是一个文本，或者用文本去打字，这肯定是不行的。

此外，未来在各种企业网站、在线课堂或者各类APP，迎接你的可能就不再是复杂的**UI**（User Interface：用户界面）和菜单，而是数字人向导。它将替代冰冷的“冷操作”，用自然对话完成所有服务。这一点在企业培训场景尤其突出。例如，面试是一个很重要的事情，如果用语音或文本去面试，肯定是不高效的。销售要去陪练，他要模拟下这个应用场景，如果只是文本去模拟，没有什么感觉。但如果是模拟的是一个客户，客户就是个医生，医生就在屏幕里面，我跟他对话去模拟，这种即时性跟文本差别是特别特别大的。

此外，在泛娱乐场景里，大量虚拟IP都可以升级。过去的虚拟IP是静态的贴纸、玩偶、手办，未来它们可以被“星云”这样的平台驱动，成为可以实时互动、唱歌、表演的数字偶像。在游戏行业里，大量的**NPC**（Non-Player Character：非玩家角色）也可以升级，这恐怕是游戏行业里面的终极梦想了。未来的游戏NPC将不再是只会重复“欢迎来到村庄”这样脚本角色，而是拥有大模型大脑、具身智能驱动层的“虚拟生命体”。他们会有自己的情绪、记忆和反应，能与玩家共同冒险，创造出独一无二的动态剧情。

3D数字人的另外一大重要应用，是与物理世界的“具身智能”人形机器人产生交集。我们在前面提过柴教授的一个观点，他认为动画与机器人理论同源，动画是在虚拟世界，和机器人代表的物理世界，在底层逻辑上是相通的，都关乎于3D世界的感知、规划和控制。3D数字人技术将从三个层面极大地加速机器人的进化。

第一，是在**运动学**（Kinematics：研究物体运动的几何性质，不考虑引起运动的力）层面。柴教授团队在做的控制动作这个事情，也就是运动学轨迹，AI生成。他认为3D方法在虚拟世界做，一定会100%用到现实世界，因为他们的数据比现实世界多。他们希望在人形机器人整个的上下游产业链里面去做，因为这个东西跟3D世界是一模一样，做了同一个事情。所以其实3D数字人模型生成的“怎么动”的数据，可以直接作为机器人**模仿学习**（Imitation Learning：通过观察专家行为来学习任务的机器学习方法）的输入，指导机器人的运动规划。

第二，是在**动力学**（Dynamics：研究物体运动与引起运动的力之间关系的力学分支）层面。有了运动学轨迹之后，就可以在虚拟世界当中通过物理仿真，生成让机器人实际动起来所需要的动力学控制，也就是如何去施加力。

第三，就是所谓的**Sim-to-Real**（Simulation to Reality：将虚拟仿真环境中训练的模型或策略迁移到真实世界中应用的技术）。在虚拟世界中，可以低成本、大规模地生成机器人训练所需要的、在现实中难以采集的数据。例如，各种极端情况下的摔倒与恢复，来弥补现实世界中数据采集的不足，加速机器人“小脑”（也就是运动控制）和“大脑”（也就是决策规划）的进化。

因此，3D数字人不仅是AI的“脸面”，更是其在物理世界行动前的“模拟器”和“训练场”。数字人技术的发展将与人形机器人的发展同频共振，帮助机器人从“能干活的蓝领”进化为“会交流的白领”，来共同推动具身智能时代的到来。

AI多模态能力在2025年出现了突飞猛进的进展，而其中3D数字人的进化，更像是一场正在发生的“人机交互”革命。这个赛道的探索者们希望通过底层的技术创新，试图破解行业长久以来的“不可能三角”。他们的核心目标是将3D数字人从影视和3A游戏中的昂贵“奢侈品”，变为人人可用、无处不在的“必需品”和“基础设施”。柴教授认为，AI本身大模型在文本大模型上的突破，带给了做人形机器人很大的信心，对于做3D数字人这个动作的驱动来说，也带来了非常大的技术基础突破。

尽管现在行业还面临着成本、数据和泛化性的挑战，但我们也能够看到具身智能和3D数字人产业的技术同源性，而后者的发展反而能在很大程度上反哺机器人的发展道路。就像柴教授说的，ChatGPT给了AI一个大脑，而3D数字人想给AI一个身体。展望未来，我们与AI的交互或许会由一个有温度、有形态、有情感的数字生命来拉开序幕。这样的数字生命会从3D赋能到具身智能，真正地与我们的物理世界产生交互，让智能和AI更快地来到我们身边。