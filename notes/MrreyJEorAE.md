---
author: Best Partners TV
date: '2025-10-30'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=MrreyJEorAE
speaker: Best Partners TV
tags:
  - ai-infrastructure
  - gpu-scalability
  - network-architecture
  - data-center-challenges
  - corporate-culture
title: 英伟达爆发式增长的深层逻辑：CTO Michael Kagan揭秘Mellanox收购与AI算力挑战
summary: 英伟达首席技术官Michael Kagan在专访中深入阐述了公司市值飙升的背后原因。他指出，除了GPU算力与AI浪潮，2019年对Mellanox的收购是关键转折点，解决了GPU算力横向扩展的致命短板。文章详细探讨了英伟达的双赢文化、AI集群对网络技术的极致要求、BlueField DPU的战略意义，以及大规模数据中心在能源、散热和故障容错方面的挑战，并展望了AI对科学进步的深远影响。
insight: ''
draft: true
series: ''
category: technology
area: tech-insights
project:
  - ai-impact-analysis
  - systems-thinking
people:
  - Michael Kagan
  - Jensen Huang
  - Steve Jobs
companies_orgs:
  - NVIDIA
  - Mellanox
  - Intel
  - Sequoia Capital
products_models:
  - GPU
  - i860
  - Pentium MMX
  - i860XP
  - i486
  - CUDA
  - NVLink
  - GPT-4
  - InfiniBand
  - BlueField DPU
  - Spectrum-X
  - ChatGPT
  - x86 architecture
  - 地球二号气候模拟器
media_books: []
status: evergreen
---
### 英伟达的崛起：被忽视的关键事件

在过去的六年里，英伟达的市值实现了惊人的增长，从2019年的约1000亿美元一路飙升至如今的4.5万亿美元，成为全球市值最高的公司之一。许多人都在探究，英伟达的爆发式增长究竟依靠什么？是其**GPU**（Graphics Processing Unit: 图形处理器，专门用于处理图像渲染和并行计算的芯片）芯片的算力优势，还是**AI浪潮**（AI Wave: 人工智能技术快速发展和广泛应用的趋势）带来的风口红利？

事实上，有一个关键事件常被人们忽略，那就是2019年3月，英伟达以70亿美元收购了以色列公司**迈络思**（Mellanox: 一家以色列高性能网络技术公司，后被英伟达收购）。2025年10月28日，在**红杉资本**（Sequoia Capital: 硅谷知名的风险投资公司）的欧洲100活动上，英伟达现任**首席技术官**（CTO: Chief Technology Officer，负责公司技术战略和研发的最高级别技术主管）迈克尔·卡根（Michael Kagan）接受了专访。他正是迈络思的联合创始人，也是当年推动两家公司整合的核心人物。在这次专访中，他阐述了英伟达的增长逻辑、AI算力的底层挑战以及未来技术的发展方向。本文将回顾这场专访，探讨为何说没有迈络思，就没有今天的英伟达。

### 迈克尔·卡根：从芯片设计到AI架构定义者

首先，我们来介绍一下迈克尔·卡根。他是一名拥有40年行业经验的老兵。早在1983年，他就加入了**英特尔**（Intel），参与了**i860**（Intel i860: 英特尔公司早期推出的一款RISC微处理器）、**奔腾MMX**（Pentium MMX: 英特尔奔腾处理器的一个版本，增加了MMX指令集以提升多媒体处理能力）等经典微处理器的架构开发。其中，他主导设计的**i860XP**（Intel i860XP: i860处理器的增强版本）芯片，还实现了首次在初代硅片上成功启动Linux系统的里程碑，而且计算性能远超当时的**i486**（Intel i486: 英特尔公司生产的第四代x86架构微处理器）芯片。

2000年左右，他联合创立了迈络思，专注于**高性能网络技术**（High-Performance Networking Technology: 旨在提供高带宽、低延迟和高可靠性的网络连接技术）。2019年迈络思被英伟达收购后，他在2020年5月正式出任英伟达CTO。一直以来，卡根都强调一个观点：没有软件支持的芯片只是昂贵的硅片。这也成了他推动英伟达从芯片制造商向AI基础设施架构定义者转型的核心思路。

### 英伟达的“双赢”文化与CUDA生态

卡根在一开场就提到了英伟达的企业文化——双赢。他表示，英伟达从不追求在存量市场里抢蛋糕，而是希望和客户一起把蛋糕做大。换句话说，英伟达的成功不是建立在对手的失败上，而是建立在客户的成功上。

举例来说，英伟达早期凭借GPU切入游戏市场，后来发现GPU的并行计算能力特别适合AI任务，于是推出了**CUDA生态**（CUDA Ecosystem: 英伟达开发的并行计算平台和编程模型，允许开发者利用GPU进行通用计算）。这个生态不只是销售芯片，更是给客户提供从硬件到软件的完整解决方案。例如，科研机构利用CUDA训练AI模型，企业利用CUDA部署推理服务。客户的AI项目成功了，自然会持续采购英伟达的产品，形成正向循环。这种“客户成功即自身成功”的逻辑，其实就是英伟达能在AI时代站稳脚跟的底层文化支撑。

### Mellanox收购：解决GPU算力扩展的致命短板

迈络思的收购，正是将这种双赢逻辑推向更大规模的关键一步。卡根在专访中反复强调，收购迈络思不是简单地补全产品线，而是解决了英伟达的一个致命短板——GPU算力的扩展问题。

在收购之前，英伟达的GPU扩展主要依靠**NVLink技术**（NVLink Technology: 英伟达开发的一种高速互连技术，用于连接多个GPU，实现高带宽数据传输）。但这种技术只能实现**纵向扩展**（Scale-up: 通过增加单个系统内的资源，如CPU、内存、GPU，来提升性能），也就是在单个节点内整合多块GPU。例如，一台服务器里放置8块GPU，通过NVLink连接，让它们像一块超大GPU一样工作。然而，如果客户需要训练千亿、万亿参数的AI模型，单节点的GPU数量根本不够用。例如，训练**GPT-4**（Generative Pre-trained Transformer 4: 开放AI公司开发的大型语言模型）这样的模型，可能需要上万块GPU协同工作，这就需要**横向扩展**（Scale-out: 通过增加更多独立的系统（节点）并将其连接起来，形成集群来提升整体性能）。而横向扩展的核心，就是网络技术。如果节点之间的网络速度慢、延迟高，哪怕每个节点的GPU性能再强，整个集群的效率也会被拉垮。

迈络思的技术正好解决了这个问题。卡根举例说，收购迈络思之后，英伟达可以通过迈络思的网络技术，将36台双GPU计算机连接起来，让它们在软件层面呈现为一个单一的GPU。这相当于将原本分散的小算力单元整合成了一个超大算力单元。而且，这种整合不是硬凑，而是通过软硬件协同优化。例如，迈络思的**InfiniBand网络技术**（InfiniBand Network Technology: 一种高性能、低延迟的计算机网络通信标准，主要用于高性能计算和数据中心）能够让不同节点的GPU之间直接交换数据，不用经过**CPU**（Central Processing Unit: 中央处理器，计算机的核心计算单元）中转，延迟大大降低；再配合英伟达的软件调度，整个集群的算力可以无缝扩展。客户无需修改太多代码，就能将原本在单节点上运行的任务迁移到上千节点的集群上。

### AI时代对算力需求的挑战与网络的重要性

为什么这种扩展能力在AI时代如此重要？因为AI的性能需求增长实在太快了。卡根在专访中给出了一组惊人的数据：AI模型的规模和复杂性每三个月就会翻一番。这意味着，为了满足客户需求，英伟达每年需要提供10到16倍的性能提升。而大家熟悉的**摩尔定律**（Moore's Law: 由戈登·摩尔提出，指集成电路上可容纳的晶体管数量大约每两年翻一番，性能也随之提升）所说的芯片性能每两年翻一番的速度，其实早已跟不上AI的需求。如果只依靠升级单块GPU的性能，例如增加晶体管数量、提升制程工艺，根本不可能满足每年10倍的增长。因此，必须依靠架构创新，也就是通过网络将海量GPU连接起来，用集群算力对抗AI的性能需求。而迈络思的网络技术，正是实现这种架构创新的核心工具。

说到这里，我们需要再深入探讨网络在AI集群中的作用。许多人认为网络只是传输数据的管道，只要带宽足够大就行。但卡根指出，AI集群的网络需要满足三个条件：**高带宽**（High Bandwidth: 网络在单位时间内能传输的最大数据量）、**极低延迟**（Ultra-low Latency: 数据从发送端到接收端所需的时间极短），以及**极小的延迟抖动**（Jitter: 网络数据包传输延迟的变化或波动）。

前两个条件容易理解：带宽大才能快速传输大量数据，延迟低才能减少计算单元的等待时间。但延迟抖动为何重要？卡根举了一个反例：如果一个网络的平均延迟是1毫秒，但有时是0.5毫秒，有时是2毫秒，这种波动就会导致集群中的GPU步调不一致。有的GPU算完了数据，等待其他GPU传输结果，有的GPU还在等待数据。整个计算过程就会断断续续。这种情况下，哪怕将任务拆分成1000份，分给1000块GPU，最后实际效率可能还不如分给10块GPU。而迈络思的网络技术恰恰在低抖动上做了极致优化。卡根表示，英伟达的网络不追求峰值性能的噱头，而是追求稳定的性能。无论多少个节点同时通信，延迟的波动范围都非常小。这样一来，客户才能将任务拆分成更多细小的部分，分给更多GPU并行处理，真正发挥集群的规模优势。例如，训练一个千亿参数的模型，用普通网络可能需要10天，用英伟达的网络可能只需要3天，效率提升非常明显。正因为如此，卡根才说，本质上，网络决定了集群的性能。

### BlueField DPU：数据中心的操作系统计算平台

除了连接GPU的计算网络，迈络思还有一项关键技术——**BlueField DPU**（Data Processing Unit: 数据处理单元，一种新型处理器，用于卸载和加速数据中心的基础设施任务）。卡根在专访中将其称为**数据中心的操作系统计算平台**（Data Center Operating System Compute Platform: 指DPU作为数据中心基础设施管理和计算的平台）。

我们可以这样理解：传统的数据中心里，服务器的CPU既要运行应用程序（例如AI推理），又要处理基础设施任务（例如网络的流量调度、存储管理等）。这就像一个人既要干核心工作，又要干后勤工作，效率肯定不高，而且还不安全。而BlueField DPU的作用，就是将后勤工作从CPU中剥离出来，让CPU专注于核心工作。它可以单独处理网络流量、存储管理、安全防护等基础设施任务，相当于给数据中心配备了一个专职后勤团队。

这样一来，就有了两个好处：第一，CPU的资源不再被浪费，应用程序的运行效率更高；第二，基础设施和应用程序在硬件层面被隔离，被攻击的风险大大降低。即使DPU层面出现问题，也不会影响到CPU上的应用数据。这种隔离思路，其实是未来大型数据中心安全和效率的关键。

### 大规模GPU集群的挑战：组件失效与网络设计

当然，构建大规模GPU集群不只是解决扩展和网络问题这么简单，还有许多现实挑战。卡根在专访中提到了两个核心难题：组件失效和网络设计。

首先是组件失效。大家可能都知道，电子设备的可靠性通常用几个“九”来形容。五个九，也就是99.999%，意味着每年的故障时间只有大约5分钟。但这是针对单个组件的。如果一个集群有100万个组件，例如GPU、服务器、网络设备，那么所有组件同时正常工作的概率几乎为零。卡根说，在大规模集群里，故障不是会不会发生的问题，而是一定会发生的问题。所以，必须从软硬件层面进行容错设计。例如，软件层面要有自动故障检测功能，一旦发现某个GPU失效了，能立刻把它的任务分配给其他GPU；硬件层面要有冗余设计，例如关键的网络设备有备份，某个设备坏了，备份能立刻顶上。这些设计看起来不起眼，但却是大规模集群能稳定运行的基础。

再来说网络设计。卡根强调，AI集群的网络和普通数据中心的网络完全是两个概念。普通数据中心的网络主要服务于松散耦合的微服务，例如一个电商平台，用户登录、商品搜索、下单支付是不同的微服务，它们之间的通信比较零散，对延迟的要求不高。而AI集群的网络需要服务于紧密耦合的单应用，例如一个AI训练任务要在10万台机器上同时运行，数据需要在这些机器之间频繁交换，任何一个节点的网络拥堵都会影响整个任务的进度。所以，AI集群的网络需要更精细化的控制，例如，能够根据任务的优先级调度流量，给AI训练任务分配更多带宽；以及能够实时监控网络状态，一旦发现拥堵，立刻调整数据传输路径。这种定制化的网络设计是普通数据中心网络无法满足的。

### 训练与推理：AI算力需求的新格局

聊完了集群的技术挑战，卡根和主持人又聊到了AI领域另一个热门话题——训练和推理的差异。许多人会觉得AI的算力需求主要来自训练，毕竟训练一个大模型需要上万块GPU运行好几天。但是卡根在专访中纠正了这个观点：推理的算力需求可能比训练还大。

为什么这么说呢？这就涉及到训练和推理之间的区别。训练是让AI学会东西的过程，例如给模型喂大量图片，让它学会识别猫和狗。这个过程包含两个阶段：前向传播和反向传播，最后还要把多个节点的参数合并。而推理是让AI用学会的东西解决问题的过程，例如把一张新图片输入模型，让它判断是不是猫。

早期的AI推理，主要是感知类的任务，例如图像识别、语音识别，这些都是一次性的，也就是输入一个数据，得到一个结果，计算量不大。但是生成式AI出现之后，推理的性质就变了。例如**ChatGPT**，你输入一个提示词，它生成回答的时候，不是一次性算完，而是逐个**token**（Token: 在自然语言处理中，文本被分割成的最小有意义单元，可以是单词、字符或子词）生成。每生成一个字或者一个词，都要把之前的内容重新输入模型，重新计算一次。这种递归式的推理，计算量自然就上去了。

而且，推理还有两个不同的阶段：**预填充**（Pre-fill: 在生成式AI推理中，模型处理用户输入的提示词阶段，通常计算密集型）和**解码**（Decode: 在生成式AI推理中，模型逐个生成输出token的阶段，通常内存密集型）。预填充阶段是处理提示词，模型需要先理解这个提示的含义，把相关的上下文加载到内存里，这个阶段需要大量计算，属于计算密集型；解码阶段是生成回答，逐个token输出，这个阶段主要是把内存里的上下文数据读出来处理，属于内存密集型。针对这两个阶段的特点，英伟达还专门推出了不同的**SKU**（Stock Keeping Unit: 库存单位，指代不同型号或配置的产品），有的SKU优化了计算性能，适合预填充；有的SKU优化了内存带宽，适合解码。而且，这些SKU都保持了相同的编程接口，客户可以根据自己的负载灵活切换，不用修改代码。这就是可编程性带来的优势。

更重要的是，一个模型通常只会训练一次，但是会被推理无数次。例如ChatGPT，训练一次可能需要几万块GPU运行几周，但是训练完成后，全球上亿用户每天都在使用它进行推理。哪怕每个用户每天只生成100字的回答，总的计算量也远远超过了训练时的计算量。卡根甚至开玩笑说，他妻子现在跟ChatGPT聊天的时间都比跟他聊得还多。这虽然是玩笑，但也反映了推理需求的广泛性。所以，未来AI算力的竞争，不只是训练的竞争，更是推理的竞争。

### 跨数据中心挑战与能源散热的极限

除了单个数据中心的挑战，卡根还提到了跨数据中心的问题。现在许多大型企业和科研机构会将AI任务分散到多个数据中心，例如一个数据中心在北美，一个在欧洲，这样既能避免单点故障，又能靠近用户降低延迟。但是跨数据中心有一个天然的限制，那就是光速。光在光纤里的传播速度大约是每秒20万公里，跨大陆的数据中心之间，延迟差异可能达到几十毫秒。如果两个数据中心之间的网络用大缓冲区来缓解延迟差异，又会导致延迟抖动，因为缓冲区里的数据多了，传输时间就会不稳定，影响AI任务的效率。

为了解决这个问题，英伟达推出了**Spectrum-X交换机**。这种交换机的核心特点是提供**遥测数据**（Telemetry Data: 从远程或难以访问的来源自动收集和传输的数据），它能够实时监控网络的拥堵情况、延迟变化，然后把这些数据反馈给终端节点。终端节点再根据这些数据自动调整通信模式。例如，如果发现和某个远程数据中心的延迟很高，就会减少数据传输的频率，避免缓冲区堆积；如果发现本地数据中心的网络很通畅，就会增加传输频率。通过这种动态调整，就能在跨数据中心的场景下，既保证低延迟，又避免抖动。

当然，数据中心的规模也不是越大越好，它还受到两个关键因素的限制：能源和散热。卡根说，现在大型AI数据中心的功耗已经达到了GW级，差不多能满足100万户家庭的用电需求。未来如果要建造十GW级的数据中心，能源供给就是一个大问题。而且，功耗越高，散热压力就越大。传统的**风冷技术**（Air Cooling Technology: 使用空气作为冷却介质来带走电子设备热量的技术）已经无法满足高密度GPU集群的散热需求，因为GPU工作时会产生大量热量，如果散热不及时，芯片会降频，性能会下降。所以，英伟达现在已经全面转向**液冷技术**（Liquid Cooling Technology: 使用液体作为冷却介质来带走电子设备热量的技术）。液冷的散热效率要比风冷高很多，风冷只能带走芯片表面的热量，而液冷可以直接接触芯片，把热量快速带走。利用液冷技术，一个机架的功率可以从传统风冷的20千瓦提升到100千瓦以上，计算密度大大增加。卡根预测，未来的数据中心会越来越依赖于液冷技术，甚至可能出现全液冷的数据中心，整个机房都用液体来散热，才能做到既高效又节能。

### 英伟达与英特尔的合作及文化整合

在专访的后半部分，卡根还谈到了英伟达与英特尔的合作。许多人认为英伟达和英特尔是竞争对手，毕竟英特尔生产CPU，英伟达生产GPU，都在争夺数据中心的市场。但是卡根不这么认为，他说，将**加速计算**（Accelerated Computing: 利用专用硬件，如GPU，来加速特定计算任务，而非通用CPU）和**通用计算**（General-Purpose Computing: 使用CPU执行各种通用任务和应用程序）融合起来，能为两家公司开辟新的市场。具体来说，英特尔的**x86架构**（x86 Architecture: 英特尔开发的一系列指令集架构，广泛应用于个人电脑和服务器）在通用计算领域有很强的优势，例如运行操作系统、处理事务性数据。而英伟达的GPU在加速计算领域有优势。两者结合，就能服务于那些既需要通用计算又需要加速计算的场景。例如，制造业的AI质检系统，一方面需要英特尔CPU来处理生产线上的实时数据，另一方面需要英伟达GPU运行AI模型检测产品是否有缺陷。这种互补合作，不是争夺存量市场，而是开拓新市场，正好符合英伟达的双赢文化。

说到文化，卡根还分享了迈络思并入英伟达后的文化整合过程。他说，迈络思和英伟达的文化相似又互补。两家公司都重视创新，都以客户为中心。但是迈络思更专注于网络技术的细节，英伟达更擅长构建生态和平台。为了确保整合成功，卡根当时的首要任务是让迈络思的员工感到安心。毕竟，一家以色列的本土公司突然变成全球巨头的一部分，员工很容易有被冷落的感觉。**黄仁勋**（Jensen Huang: 英伟达的联合创始人、总裁兼首席执行官）当时也明确表示，网络技术是英伟达未来的核心，迈络思的团队是关键。最终，整合的效果超出了预期，迈络思的员工留任率达到了85%-90%，英伟达在以色列的员工总数甚至增长了两倍多，还计划建造新的园区。卡根说，这可能是科技史上最成功的合并之一。

### AI的长远价值与未来展望

在专访的最后，卡根还聊了聊AI的长远价值。AI不只是会改变商业，还能够推动科学的进步。他以**地球二号气候模拟器**（Earth-2 Climate Simulator: 英伟达与合作伙伴共同开发的气候模拟平台，利用AI技术进行高精度气候预测）为例，这个模拟器利用AI技术，能够精准模拟今天的人类活动对50年后全球气候的影响。这相当于把历史变成了一门实验科学，科学家可以调整不同的变量，观察未来的变化，而不是只能依靠过去的数据推测。

此外，AI还能帮助人类发现新的物理定律。卡根说，人类发现物理定律的过程本质上是观察现象、归纳规律，而AI在处理海量数据、归纳规律方面有天然优势。例如，在**粒子物理**（Particle Physics: 研究构成物质和辐射的基本粒子及其相互作用的物理学分支）领域，AI可以分析粒子碰撞产生的海量数据，找出人类尚未发现的规律，甚至可能推翻现有的理论。

至于大家关心的AI的增长速度会持续多久，卡根提出了一个自己的观点。他认为英伟达正在把产品发布周期从两年缩短到一年，而且每一年都实现指数级的性能提升，关注点也在从单芯片性能转向整个系统的性能。虽然这种增长无法预测具体能持续多久，但是他可以肯定的是，AI会像电力一样，彻底改变世界。卡根还做了一个很形象的比喻：**史蒂夫·乔布斯**（Steve Jobs: 苹果公司的联合创始人之一）曾经把电脑比作**思想的自行车**（Bicycle for the Mind: 史蒂夫·乔布斯对电脑的比喻，意指电脑能增强人类的思维效率），因为它能帮助人类更高效地思考，而AI就是**思想的宇宙飞船**（Spaceship for the Mind: 对AI的比喻，意指AI能极大拓展人类思维的边界和能力），它能让人类的思考突破时间和空间的限制，涉足那些之前想都不敢想的领域。

### 总结

总而言之，在英伟达CTO迈克尔·卡根看来，英伟达的成功不只是依靠GPU芯片的硬实力，更是依靠双赢的文化、迈络思带来的网络技术突破，以及对AI算力需求的深刻理解。未来，AI的竞争一定会越来越激烈，但是无论如何，基础设施的重要性都只会越来越高。毕竟，没有强大的算力和网络，再先进的AI算法也无法落地。