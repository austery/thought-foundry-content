---
author: a16z
date: '2025-12-22'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=ULszsXDyjMY
speaker: a16z
tags:
  - llm
  - ai-agents
  - voice-ai
title: 'AI Agents in 2026: 3 Predictions For Whatâ€™s To Come'
summary: This article discusses three key predictions for AI agents in 2026, focusing on the evolution of AI user interfaces, designing for agents rather than humans, and the rise of AI voice agents.
insight: ''
draft: true
series: ''
category: ai-ml
area: tech-work
project:
  - ai-impact-analysis
people:
  - Mark Andrew
  - Stephanie Zayn
  - Olivia Moore
companies_orgs:
  - a16z
products_models:
  - GPT-4
media_books: []
status: evergreen
---
### AI Agents in 2026: Three Key Predictions

#### Prediction 1: The Death of the Prompt Box

The primary user interface for AI applications is expected to shift beyond the traditional prompt box. **Mark Andrew**, a partner on the AI apps investing team, predicts that the next wave of AI apps will require significantly less prompting. These apps will observe user behavior and proactively intervene with suggested actions for review.

The market opportunity for this shift is vast, moving from a $300-400 billion software spend to a $13 trillion labor spend in the US alone. This change aims to make AI applications more efficient and user-friendly, potentially transforming how businesses operate.

#### Prediction 2: Designing for Agents, Not Humans

**Stephanie Zayn** emphasizes the importance of **designing for agents rather than humans**. As AI agents become more prevalent, the way we create content and design applications will need to change. Traditional optimization for human consumption will be less relevant, and **machine legibility** will become a key focus.

This shift will impact various industries, from content creation to software development. The optimization for agents will prioritize **insight** and **relevance**, potentially leading to high volumes of hyper-personalized content.

#### Prediction 3: The Rise of AI Voice Agents

**Olivia Moore** predicts that **AI voice agents** will become increasingly prominent in 2026. These agents are already being deployed in various sectors, including healthcare, banking, and recruitment.

In healthcare, voice agents are being used for tasks such as scheduling, reminders, and even sensitive calls like post-surgery follow-ups. In recruitment, voice AI enables candidates to interview instantly at their convenience.

The growth of voice AI is expected to continue, with potential applications in government services and consumer health and wellness. This technology has the potential to improve efficiency and reduce costs across industries.

<details>
<summary>Original English</summary>

Welcome to Big Ideas for 2026. We'll hear from Mark Andrew on the evolution of AI user interfaces and how the way we interact with intelligent systems is fundamentally changing. Stephanie Zayn discusses what it means to design for agents rather than humans, a shift that's reshaping product development. And Olivia Moore will share her thoughts on the rise of AI voice agents and their growing role in our daily lives. These aren't just predictions. They're insights from the people working directly with the founders and companies building tomorrow's future.

I'm Mark Andrew, a partner on our AI apps investing team. My big idea for 2026 is the death of the prompt box as the primary user interface for AI applications. The next wave of apps will require way less prompting. They'll observe what you're doing and intervene proactively with actions for you to review. The opportunity we're attacking used to be the 300 to 400 billion of software spend annually in the world. Now what we're excited about is the $13 trillion of labor spend that exists in the US alone. It's made the market opportunity or the TAM for software about 30 times bigger. If you start from there and then you think about okay if all of us want this software to be doing work for us ideally it's doing work with at least if not more competency than a human could right and so um I like to think about like well what do the best employees do? What do the best human employees do? and and I've recently been talking about this graphic that was floating around on Twitter. It's a pyramid of like the five types of employees and and the ones with the most agency and why they're the best. So, if you start at the bottom rung of the pyramid, it's like people who identify a problem and then come to you and ask for help and ask what to do. And that's like the lowest agency employee. But, uh if you go to the S tier, like the the most high agency employee you could possibly have, they identify a problem. They do research necessary to diagnose where the problem came from. They look into a number of possible solutions. They implement one of those solutions and then they keep you in the loop or they come to you at the very last minute and say like, do you approve of this solution I found? And that's what I think the future of AI apps will be. And I think that's what everyone wants and that's what we're all working toward. So I feel pretty confident that we're almost there. I think LLMs have continued to get better and faster and cheaper. And I think there's a world in which the user behavior will still necessitate a human in the loop at the very end to sort of approve things certainly in high stakes contexts. But I think the models are more than capable of getting to a point where it's suggesting something really smart on your behalf and you basically just have to click accept. As you guys know, I'm pretty obsessed with the notion of an AI native CRM. And I think this is like a perfect example of what these proactive applications could look like. So in today's universe, a salesperson might go open their CRM, explore all the open opportunities they have, look at their calendar for that day, and try to think about, okay, what are the actions I can take right now to have the greatest impact on my funnel and my ability to close deals with the CRM of tomorrow. Your AI agent or your AI CRM should be doing all these things on your behalf in perpetuity, identifying not only like the most obvious opportunities that are in your pipeline, but going through your emails from the last 2 years and harvesting, you know, this was once a warm lead and you kind of let it die, like maybe we should send them this email to to drum them back up into your process, right? So, I think there are so many ways in which drafting an email, harvesting your calendar, going through your old your old call notes, like the the opportunities are just endless. The ordinary user will still want that last mile approval almost 100% of the time. They will want the human part of the human in the loop to be the final decision maker. And that's great. I think that's like the natural way in which this will evolve. I can imagine a world in which the power user is basically taking a lot of extra effort to train whichever AI app it's using to have as much context about their behavior and how they perform their work as humanly possible. These will utilize larger context windows. These will utilize memory that's been baked into a lot of these LLMs and make it such that the power user can really trust the application to do 99.9% of the work or maybe even 100 and they'll pride themselves on the number of tasks that get done without a human needing to approve them.

Hi, my name is Stephan Deng and I'm an investing partner on the A16 ZR team. My big idea for 2026 is creating for agents, not for humans. Something I'm super excited about for 2026 is that people have to start changing the way they create. And this ranges from creating content to designing applications. People are starting to interface with systems like the web or their applications with agents as an intermediary. And what mattered for human consumption won't matter the same way for agent consumption. When I was in high school, I took journalism. And in journalism, we learn the importance of starting with the five W's and H in the lead paragraph for news articles and to start with a hook for features. Why? For human attention. Maybe a human would miss the deeply relevant, insightful statement buried on page 5, but an agent won't. For years, we've optimized for predictable human behavior. You want to be one of the first search results back from Google. You want to be one of the first items listed on Amazon. And this optimization is not just for the web but as we design software too. Apps were designed for human eyes and clicks. Designers optimized for good UI and intuitive flows. But as agent usage grows visual design becomes less central to overall comprehension. Before during incidents engineers would go into their graphana dashboards and try to piece together what was going on. Now AISR take in telemetry data. They'll analyze that data and they'll report back with hypotheses and insights directly into Slack for humans to read. Before sales teams would have to click through and navigate Salesforce or other CRM to gather information. Now agents will take that data and summarize insights for them. We're no longer designing for humans, but for agents. The new optimization isn't visual hierarchy, but machine legibility. and that will change the way we create and the tools that we use to do it. It is a question we don't know the answer to what agents are looking for, but all we know is that agents do a much better job at, you know, reading all of the text in an article versus maybe a human would just read, you know, the first couple paragraphs. There are a bunch of tools out there that different organizations use to just make sure that they show up when consumers are prompting chat GBT asking for the best corporate card or the best shoes to buy. And so there's like a bunch of what we call GEO tools out there in the market that people are using, but um everybody is asking the question what AI agents want to see. I love this question. um when humans may choose to exit the loop entirely. We're already seeing that happen in some cases. Our portfolio company Decagon is answering questions for a lot of their customers already autonomously. But for other cases, security operations or incident resolution, we typically see a little bit more human in the loop where the AI agent takes first stab at trying to figure out what the issue is, running the analysis and serving to the humans different potential situations. Those tend to be cases of higher liability, more complex analyses, uh, that we see humans staying in the loop and will probably stay in the loop for much longer until the models and the technology get to incredibly high accuracy. I don't know if agents will be watching Instagram reels. Um, it's really interesting, at least on the tech side. It is really important to optimize for that machine legibility piece. optimized for insight, optimized for relevance especially versus in you know in the past it was more about hooking people in capturing attention in flashy ways. What we're seeing already is case of high volume hyperpersonalized content. And maybe you don't create one extremely relevant article, extremely relevant and insightful article, but maybe you're creating extremely high volumes of lowquality content, but addressing different things that you may think an agent wants to see. almost like the equivalent of keywords in the era of agents where cost of creation of content kind of goes to zero and it's really easy to create high volumes of content. That's a potential risk around just high volumes of things to be able to try to capture Asian attention.

I'm Olivia Moore and I'm a partner on our AI applications investing team. My big idea for 2026 is that AI voice agents will start to take up space. In 2025, we saw voice agents break out from something that seemed more like science fiction into something that real enterprises are buying and deploying at scale. I'm excited to see voice agent platforms expand working across platforms and modalities to handle full tasks and bring us closer to the true AI employee vision. So, we've seen nearly every vertical have enterprise customers that are testing voice agents. if not deploying them at pretty significant scale. Healthcare is probably the biggest one here. We're seeing voice agents in nearly every part of the health care stack. Calls to insurers, pharmacies, suppliers, but also in perhaps more surprisingly patientfacing calls. It could be things like scheduling and reminders that are kind of table stakes, but also even more sensitive calls like postsurgery follow-up calls or even intake calls for psychiatry are being handled by voice AI. I think honestly a big driver here is just the high turnover and the difficulty in staffing in healthcare right now which makes voice agents that can perform with some reliability a pretty good solution. Another category that's like that is banking and financial services. You would think there's so much compliance and regulation that voice AI can't operate there yet. But it turns out this is an area where voice AI actually outperforms because humans are actually very good at violating compliance and regulations and voice AI can get it every time and importantly you can track how voice AI is performing over time. Lastly, I would say another area where voice has taken off is recruiting. This is everything from retail frontline jobs to entry-level engineering roles to even mid-level consulting roles. With Voice AI, you can create an experience for candidates where they can interview instantly at whatever time works for them and then they're sent through the rest of the human recruiting process. We've seen big improvements on accuracy and latency this year as the underlying models get better and better. Actually, in some cases, I've heard of voice agent companies slowing down their agent or introducing background noise to make it sound more like a human. When it comes to BPOS's and call centers, I think some of them are going to see a softer transition and others are going to maybe see a a harder cliff when it comes to the threat from from AI and specifically voice AI. It's kind of like how people say AI isn't going to take your job. A human using AI will. What we're seeing is a lot of end customers may still want to just buy the solution, not buy technology that they have to implement. So, they might still use a call center or BO in the kind of near to medium term, but they're probably going to use one that's going to offer a cheaper price or be able to do more volume because they're utilizing AI. Interestingly, there's a couple geographies where humans are still actually cheaper on a permanent basis than kind of best-in-class voice AI. And so, it'll be interesting to see as the models get better if costs come down there. And then call centers in those markets might face a little bit more of a threat than they do now. AI is actually remarkably good at multilingual conversations and heavy accents. Oftent times I'll be on a meeting and there'll be maybe a word or a phrase I don't catch. Um, and I'll check like my granola transcripts and it has it down perfectly. So I think that's a good example of what most ASR or speechtoext providers can do. Now there's a couple use cases that I'm hoping we see a lot more of next year. Anything government. So we were investors in prepared 911. If you can run 911 calls and and they were the non-emergency calls, but if you can run that with voice AI, you should be able to run DMV calls and anything else government related that right now is so frustrating as a consumer and so frustrating if you're the worker on the other end of the phone. I'm also really intrigued to see more in consumer voice AI. It's mostly been B2B so far just because it's so obvious to replace or supplement a human on the phone with much lower cost AI. One category in consumer voice that I'm excited about is around kind of health and wellness more broadly. We're already seeing voice companions take off in assisted living facilities and nursing homes, both as a companion for again the residents, but also they can kind of track different measures of wellness over time. We see voice AI as more of an industry than a market, which in our opinion means there's going to be winners across and at every layer of the stack. If you're interested in voice AI or or if you want to build in voice AI, I would recommend you check out the models. There's lots of amazing platforms like 11 Labs where you can test both creating your own voice and creating your own voice agent and you get a really good sense of what's possible and what's to come.
</details>