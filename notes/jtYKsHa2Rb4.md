---
author: Big Think
date: '2025-12-03'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=jtYKsHa2Rb4
speaker: Big Think
tags:
  - workplace-ai
  - human-ai-interaction
  - organizational-culture
  - performance-gap
  - user-adoption
title: AI在职场中的三大挑战：谷歌专家揭示的“选择性升级”、“自主偏好”与“自给自足螺旋”难题
summary: 谷歌组织与领导力发展专家Martin Gonzalez探讨了AI在职场应用中面临的三大挑战。他指出，AI可能导致“选择性升级”，拉大员工间绩效差距；“自主偏好”使得人们倾向于控制AI，即使牺牲效率也追求自主权；以及“自给自足螺旋”可能削弱团队协作与组织文化。文章呼吁领导者需深思熟虑，平衡AI的效率与人文需求，以避免重蹈社交媒体“孤单共处”的覆辙。
insight: ''
draft: true
series: ''
category: general
area: society-systems
project:
  - ai-impact-analysis
  - systems-thinking
people:
  - Martin Gonzalez
companies_orgs:
  - Google
  - Boston Consulting Group
  - Harvard
  - MIT
  - Wharton
products_models:
  - Google Maps
  - Waze
  - Large Language Model
media_books:
  - The Bonfire Moment
status: evergreen
---
### AI在职场中的三大挑战

我是Martin Gonzalez，谷歌（Google: 全球知名的互联网科技公司）的组织与领导力发展负责人，也是**《篝火时刻》**（The Bonfire Moment: 一本探讨团队协作与创新的书籍）的作者。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I'm Martin Gonzalez, a principal of organization and leadership development at Google and the author of The Bonfire Moment.</p>
</details>

这本书探讨了“团队比技术更难”的理念。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The book explores the idea that teams are harder than tech.</p>
</details>

在创新过程中，领导者、首席执行官和创始人关注业务中“人”的方面至关重要，因为忽视这一点很容易让精心制定的计划脱轨。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">In the process of innovation, it's crucial for leaders, CEOs, and founders to pay attention to the people's side of the business because neglecting it could easily derail their best-laid plans.</p>
</details>

我们知道许多员工和组织正开始在工作中使用**AI**（Artificial Intelligence: 人工智能）。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">We know many employees and organizations are starting to use AI for their work.</p>
</details>

我们也注意到，人们在两种截然不同的叙事之间摇摆不定：一种是“替代”叙事，认为由于AI，我们的工作将会消失，我的角色将被取代，从事我这类工作的人会越来越少；
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">We also observe a constant shift between two intense narratives: one of substitution, where jobs will disappear, roles will be replaced, and fewer people will perform certain types of work due to AI;</p>
</details>

另一种是“增强”叙事，认为这些工具赋予我超能力，让我在自己的角色中能做更多事情，如果我能适应这些新技术，未来就能成功并表现出色。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">and another of augmentation, where these tools provide superpowers, enabling individuals to achieve more within their roles. This suggests that future success depends on adapting to these new technologies.</p>
</details>

然而，当我们考虑“增强”模型时，有很多事情需要思考，因为早期研究表明，当这些工具被引入职场时，我们尚未看到AI发明者所宣称的那种变革性潜力。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">However, there's much to consider regarding the augmentation model, because early research indicates that as these tools are introduced into the workplace, we are not yet seeing the transformative potential of AI that its inventors have proclaimed.</p>
</details>

因此，我开始思考在将这些技术引入组织时，我们需要解决的三个难题。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Therefore, I've begun to consider three puzzles we need to solve as we integrate these technologies into our organizations.</p>
</details>

### 难题一：选择性升级之谜

将**AI**引入组织面临的挑战之一，就是我所称的“选择性升级之谜”。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">One of the challenges in bringing AI into an organization is what I've started to call the selective upgrade puzzle.</p>
</details>

这种情况发生在这些工具赋予部分用户超能力，而非所有用户，从而在组织内部产生一种选择性的升级效应。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">This occurs when these tools endow some users with superpowers but not all, creating a selective upgrade effect within an organization.</p>
</details>

例如，一项由哈佛大学（Harvard: 位于美国马萨诸塞州剑桥市的私立研究型大学）和麻省理工学院（MIT: Massachusetts Institute of Technology，位于美国马萨诸塞州剑桥市的私立研究型大学）等机构的研究人员进行的**随机对照实验**（Randomized Control Experiment: 一种科学研究方法，通过随机分配受试者到实验组和对照组来评估干预措施的效果），就与波士顿咨询集团（Boston Consulting Group: 全球知名的管理咨询公司）合作，将他们的初级顾问分为对照组和实验组。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">For example, one randomized control experiment conducted by researchers from institutions like Harvard and MIT involved the Boston Consulting Group. They divided junior consultants into control and experimental groups.</p>
</details>

实验组获得了使用**大型语言模型**（Large Language Model, LLM: 一种基于深度学习的人工智能模型，能够理解和生成人类语言）的权限，并被要求完成两类任务：
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The experimental groups were given access to a large language model and they were assigned two types of tasks:</p>
</details>

第一个任务是创意构思任务，他们需要帮助一个虚构的客户想出不同的产品创意，以便推向市场；
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">first, a creative ideation task, where they had to help a fictitious client generate product ideas for market;</p>
</details>

第二个任务是业务分析任务，要求他们分析一家陷入困境的企业并提出建议。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">second, a business analytics task, requiring them to analyze a struggling business and propose recommendations.</p>
</details>

这项研究发现，当他们观察表现优异的员工时，他们往往表现得更好；而当观察表现较差的员工时，他们往往表现得更糟。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The study found that top performers tended to do much better with the AI tools, while lower performers tended to do much worse.</p>
</details>

如果这种选择性升级效应在一段时间内扩散到数千名员工身上，我们可能会看到最佳表现者和最差表现者之间的差距不断扩大。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">If this selective upgrade effect is extrapolated across thousands of employees over time, we might observe an ever-growing gap between the best and worst performers.</p>
</details>

这种差异性将被归因于**AI**工具的使用，而在此类工具部署之前，这种差距是不存在的。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">This variability would be attributed to the use of AI tools, a gap that didn't exist before their deployment.</p>
</details>

领导者在组织中部署**AI**时可以考虑几件事。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Leaders should consider a couple of things when deploying AI in their organization.</p>
</details>

首先，要为这些工具应该和不应该用于何处设定非常明确的“护栏”。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">First, establish clear guardrails for what these tools should and shouldn't be used for.</p>
</details>

随着这些工具变得更加有效，这些护栏可能会随着时间推移而减少。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">These guardrails might diminish over time as the tools become more effective,</p>
</details>

但重要的是要经历这个实验期，了解它在何处真正增强了工作，又在何处削弱了工作。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">but it's crucial to use this experimental period to understand where AI truly augments work and where it detracts from it.</p>
</details>

另一个需要考虑的因素是，对于在特定领域使用这些工具的用户来说，他们具备该领域一定的基本专业知识非常重要。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Second, it's important that users leveraging these tools in specific domains possess a basic level of expertise in those areas.</p>
</details>

这使得用户在工具将他们引向错误方向时，以及在工具确实增强工作时，能够做出良好判断。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">This enables them to exercise good judgment when a tool might be leading them in the wrong direction versus genuinely augmenting their work.</p>
</details>

在对某个领域一无所知的情况下使用工具，是一个非常危险的提议。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Using a tool with zero domain knowledge is a very dangerous proposition.</p>
</details>

### 难题二：自主偏好之谜

当我们考虑将**AI**引入组织时，我们需要思考“自主偏好之谜”。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">As we consider integrating AI into our organizations, we must address the agentic preference puzzle.</p>
</details>

我们人类天生倾向于掌控，当这些工具削弱了我们对工作的控制权时，我们就会看到采用率下降。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Humans inherently tend towards control, and when these tools diminish our control over work, we observe a drop in adoption rates.</p>
</details>

沃顿商学院（Wharton: 宾夕法尼亚大学的商学院，全球顶尖商学院之一）的一些引人入胜的研究探讨了他们称之为**算法厌恶偏见**（Algorithmic Aversion Bias: 指人们倾向于不信任或拒绝算法的建议，即使算法表现优于人类）的理念。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Fascinating studies from Wharton explore this concept, which they termed Algorithmic Aversion Bias.</p>
</details>

例如，你上次决定不听从**Google Maps**（谷歌地图: 谷歌公司开发的网络地图服务）或**Waze**（位智: 一款基于社区的实时交通和导航应用程序）给出的回家路线是什么时候？
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">For instance, when was the last time you decided to override the directions given by Google Maps or Waze on your way home?</p>
</details>

我们有时会相信自己的判断比这些机器更好，或者错误率更低。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">We sometimes believe our judgment is superior or has a lower error rate than these machines.</p>
</details>

这项研究分支深入探讨的是，当个体实际感知到算法犯错时，即使该算法的错误率仍然低于人类的错误率，我们宁愿相信人类的判断而非算法。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">This field of study investigated how individuals react when they perceive an algorithm making an error; even if the algorithm's overall error rate remains lower than the human error rate, we often prefer to trust our human judgment over the algorithm.</p>
</details>

它进一步解释说，或许思考这个问题的一种方式是，当我们想到算法和这些**AI**机器人时，它们的错误率是可知且静态的；
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The explanation suggests that while the error rates of algorithms and AI bots are knowable and static;</p>
</details>

但人类的直觉和智慧是可完善的，因此我们或许相信自己能够在某些任务中完善判断。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">but human intuition and intelligence are perfectible. Therefore, we might trust our ability to perfect our own judgment in certain tasks.</p>
</details>

研究接着试图找出正确的对策，在一项研究中，他们允许这些算法的用户稍微调整算法的不同参数。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The research then sought to find an antidote, and in one study, they allowed users of these algorithms to slightly tweak different parameters.</p>
</details>

当这些人被赋予控制算法的这种自由度时，结果正如预期，错误率会增加。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">When people were given this leeway to control the algorithm, the error rates predictably increased.</p>
</details>

但同时，采用率也显著增加，因为人们可以控制它。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">However, this also led to a significant increase in adoption rates because people felt in control.</p>
</details>

这突出了关于**AI**工具采用的一个非常有价值的观点。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">This highlights a valuable point regarding the adoption of AI tools:</p>
</details>

作为领导者，你可能会思考，如果仅仅意味着能在职场中带来更高的采用率，那么什么样的错误率是可以接受的？
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">as a leader, you might consider what level of error rate is acceptable if it means significantly higher adoption in the workplace.</p>
</details>

理想的情况是人们完全采用这些工具而不进行任何调整。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The ideal scenario is full adoption without any tweaking.</p>
</details>

但我们知道，这会以较低的采用率为代价。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But we know this often comes at the cost of lower adoption.</p>
</details>

我们是否愿意牺牲这些工具使用中的一些精确性，以换取采用水平的提高？
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Are we willing to sacrifice some precision in the use of these tools in exchange for improved adoption?</p>
</details>

### 难题三：自给自足螺旋

最后一个难题是“自给自足螺旋”。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The final puzzle is the self-sufficiency spiral.</p>
</details>

如果你将组织中所有的工作分类，它们可以分为独立工作和相互依赖的工作。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">If you categorize all the work done in an organization, it falls into solo work and interdependent work.</p>
</details>

你可能会说，未来**AI**工具将使我们能够进行更多的独立工作。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">One might argue that in the future, AI tools will enable us to perform significantly more solo work.</p>
</details>

而且，许多独立工作将侵蚀传统上相互依赖的工作领域。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And much of this solo work will colonize parts of what was traditionally interdependent work.</p>
</details>

届时，许多剩余的相互依赖任务，无论是撰写电子邮件、制作演示文稿还是主持会议，都将由这些**AI**工具进行中介。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Consequently, many remaining interdependent tasks, such as writing emails, preparing presentations, or conducting meetings, will then be intermediated by these AI tools.</p>
</details>

当你思考在组织中创建文化需要什么，或者领导者在围绕共同使命凝聚人心方面的作用时，很多都与互动任务有关。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">When considering what it takes to foster culture within an organization or the leader's role in uniting people around a shared mission, much of this revolves around interactive tasks.</p>
</details>

这更多的是关于作为一个群体共同努力，而不是独自或孤立地工作。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">It's about coming together as a group, not working in solitude or isolation.</p>
</details>

如果未来的职场变得更加独立和孤立，我有点担心这对于组织的未来以及我们创建文化和与组织建立认同感的能力意味着什么。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">If the future workplace becomes significantly more solo and isolated, I am concerned about what this implies for the future of organizations and our capacity to build cultures and a sense of identity with the organization.</p>
</details>

我们过去也曾看到其他技术带来了我们并不完全想要的未来。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">We've witnessed other technologies in the past deliver a future we didn't quite desire.</p>
</details>

以社交媒体为例：它曾承诺创造一个更互联的世界，但实际上却可能给我们带来一个更加碎片化、两极分化的世界，在这个世界里，我们或许对彼此的期望更低。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Take social media, for example: it promised a more connected world but instead arguably gave us a more fragmented, polarized world where we perhaps expect less from each other.</p>
</details>

正如麻省理工学院（MIT）的一位民族志学者曾经说过：“我们通过这些工具**孤单共处**（alone together: 指人们虽然通过技术连接在一起，但实际上却感到更加孤立）。”
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">As an MIT ethnographer once said, 'we are alone together' through these tools.</p>
</details>

因此，我们不希望职场出现这样的未来，我们需要思考通过不同的方式和方法将人们聚集在一起，以便在人们使用这些工具时，我们能够继续为他们创造蓬勃发展的工作环境。
<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Therefore, we don't want this future for the workplace, and we need to devise ways to bring people together through different means and approaches, ensuring we can continue to create thriving environments for individuals as they engage with these tools.</p>
</details>