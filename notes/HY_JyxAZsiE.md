---
author: AI Engineer
date: '2026-01-09'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=HY_JyxAZsiE
speaker: AI Engineer
tags:
  - spec-driven-development
  - ai-agent-development
  - property-based-testing
  - requirements-engineering
  - multi-context-provider
title: Kira的规范驱动开发：磨砺你的AI工具链
summary: 亚马逊Kira团队的Al Harris介绍了其代理式IDE如何通过规范驱动开发（Spec-driven development）提升AI软件开发效率与质量。Kira利用EARS格式结构化需求，结合基于属性的测试，并支持多上下文提供器（MCP）集成外部数据与工具。演讲强调了通过定制化开发流程、引入UI原型和单元测试，以及利用引导文档（steering docs）优化AI代理行为，实现高保真、可复现的软件交付。同时探讨了Kira在大型代码库中的应用及会话管理策略。
insight: ''
draft: true
series: ''
category: ai-ml
area: tech-engineering
project: []
people: []
companies_orgs:
  - Amazon
  - Anthropic
products_models:
  - Kira
  - Agent Core
  - LangGraph
  - CDK
  - S3
  - DynamoDB
  - Sonnet
  - Gemini
  - Code OSS
media_books: []
status: evergreen
---
### 引言：Kira与规范驱动开发的核心理念

Kira是一款由亚马逊推出的**代理式集成开发环境**（Agentic IDE: 能够自主理解、规划和执行软件开发任务的AI系统），旨在通过**规范驱动开发**（Spec-driven development: 一种以明确的软件规范为核心，指导整个开发流程的方法）来提升AI软件开发的效率和质量。演讲者**Al Harris**，亚马逊的首席工程师，介绍了Kira团队如何从一个小型团队起步，致力于改进客户的软件开发生命周期（SDLC）。Kira的核心目标是帮助开发者将AI开发扩展到更复杂的问题，增强对AI代理的控制，并提高最终代码的质量和可靠性。

传统的**“随性编码”**（Vibe Coding: 依赖开发者直觉和经验，缺乏严格规范的编码方式）虽然灵活，但其成功高度依赖于操作者的经验和对系统的严格约束。Kira的出现正是为了解决这一痛点，它旨在提供一个更全面、更结构化的**软件开发生命周期**（Software Development Life Cycle, SDLC: 软件从构思到淘汰的整个过程），并尊重软件工程领域25至30年积累的实践经验，如瀑布模型（Waterfall）和极限编程（XP）。通过引入结构化方法，Kira力求在AI时代重塑软件开发的严谨性与可控性。

<details>
<summary>Original English Source</summary>

For those of you who haven't heard of us, Kira is an agentic ID. Um, we launched generally available this most recent Monday, I think the 17th, but we launched public preview on, uh, in July, uh, I think July 14th. So, out there for a few months getting customer feedback, um, all that good stuff. We're going to talk a little bit about using Spectriven development to sharpen your AI toolbox. I did a show of hands. About a quarter of the people here familiar with Spectrum and Dev. My name is Al Harris. Um, principal engineer at Amazon. I've been working on Curo for the last. Uh, and we're a very small team. We were basically three or four people sitting in a closet doing what we thought we could do to improve um the software development life cycle for customers. So we were ch we were charged with building a development tool that's that answered um that improved the experience for spectrum and development. We were theoretically funded out of the org that supported things like QDV but we were purposefully a very different product suite from the QE system to just take a different take on these things. So we wanted to work on scaling, you know, helping you scale AI dev to more complex problems. Uh improve the amount of control you have over AI agents and improve the code quality and maintain uh reliability, I should say, of what you got out the other end of the pipe. Now we're back to new content. Um so our solution was specri. We took a look at some existing stuff out there and said, "Hey, vibe coding is great, but vibe coding relies a lot on me as the operator getting things right. That is me giving guardrails to the system. And that is me uh putting the agent through a uh kind of a strict workflow. We wanted Spectri driven dev to sort of represent the holistic SDLC because we've got you know 25 30 years of industry experience um building uh software building it well and building it with different practices right we've gone through waterfall at XP um we have all these different ways that we represent what a system should do and we want to effectively respect what came before.

</details>

### SDLC的结构化与需求定义

软件开发的核心在于对需求的**发现**（Discovery: 识别和理解用户及系统需求的过程）和**定义**（Requirements: 明确系统应实现的功能和非功能性约束）。Kira通过**压缩SDLC**（Compression of SDLC: 缩短软件开发生命周期中的迭代反馈环）来加速这一过程，将输出快速反馈到输入需求，从而及时发现设计中的副作用。这种方法将结构引入软件开发流程，将设计过程中产生的**产物**（Artifacts: 开发过程中产生的各种文档、代码、模型等有形或无形的结果）——包括产品经理或开发者编写的需求、验收标准以及设计文档——整合到一个紧密的内部循环中。

Kira最初的**规范驱动开发**（Spec-driven development）流程是：用户提供一个**提示**（Prompt: 给予AI代理的初始指令或问题），Kira将其转化为一组清晰的需求，并附带**验收标准**（Acceptance Criteria: 衡量功能或系统是否满足用户需求的条件）。这些验收标准以**EARS格式**（Easy Approach to Requirement Syntax: 一种结构化的自然语言需求表达方式，通常采用“When [条件], then [结果], shall [动作]”的句式）呈现。EARS格式的引入，使得需求能够以结构化的自然语言形式被系统理解和处理。

Kira的最新进展是引入了**基于属性的测试**（Property-based Testing, PBT: 一种测试方法，通过生成大量随机输入来验证系统是否满足某个不变的属性或规则）。通过PBT，EARS需求可以直接转化为系统的**属性**（Properties: 系统必须满足的不变性或行为特征），这些属性在测试中被视为**不变性**（Invariants: 在系统运行的任何状态下都必须保持为真的条件）。PBT的目标是找到一个能**证伪**（Falsify: 通过反例证明某个命题或假设为假）这些不变性的测试用例。如果找不到这样的反例，开发者就能对系统符合需求拥有高度信心。Kira的最终目标是将这些结构化的自然语言需求与最终代码紧密关联，确保交付的软件能够可靠地满足预期。

<details>
<summary>Original English Source</summary>

So uh this animation looked a lot better. It was initially just the left diamond but I the idea was hey you know you basically are iterating on an idea. I think like half of software development is discovery requirements. Um and that discovery doesn't just happen by sitting there and thinking about what what should the system do? What can the system do? We we realized though kind of working on this that the best way to make these systems work is to actually synthesize the output and be able to feed that back really quickly. things like your input requirements um to actually do the design and feedback you know realize oh actually if we do this there's a side effect here we didn't consider we need to feed that back to the input requirements and so this compression of the SDLC evolved to bring structure into the software development flow we wanted to take um the artifacts that you generate as part of a design that's the requirements that maybe a product manager or developer writes that's going to be the acceptance criteria what does success look like at the end of this and then we want to the design artifacts that you might review with your dev team, you might review with you know stakeholders and say this is what we're going to go build and implement the thing and we want to make sure that you can do this all in some tight inner loop. Um and ult that was initially what spectriven dev was um what spectriven development in hero is today or at least was before it went g was uh you give us a prompt and we will take that and turn it into a set of clear requirements with acceptance criteria. We represent these acceptance criteria in the EARS format. EARS stands for the easy approach to requirement syntax. Um, and this lets you really easily uh it's effectively a structured natural language representation of what we you want the system to do. Now, for the first four and a half months this product existed, the ears format looked like kind of an interest decision we made, but just that sort of interesting. Um and with our launch, our general availability launch on Monday, we have finally started to roll out some of the side effects of which is property based testing. Um so now your ears requirements can be translated directly into properties of the system which are effectively invariants that you want to deliver. Um, for those of you who have or like have not I guess done property based testing in the past using something like I think it's a hypothesis in Python or fast check and node um closures spec library is another example. These are uh approaches to testing your software system where you're effectively trying to produce a single uh test case that that falsifies the invariant that you want to prove. And if you can find any uh contraositive then you can say this requirement is not met. If you cannot you have some high degree of confidence where the word high there is doing a little bit of heavy lifting because it depends on how well you write your tests but you can say with a high degree of confidence that the system does exactly what you're saying it does. Um yeah, so a property we we'll get a little bit more into property based testing and PBTs a little later, but this is the first step of many we're taking to actually take these structured natural language requirements and then tie this with a throughine all the way to the finished code and say if your code if the properties of the code meet the initial requirements, we have a high degree of confidence that you have re uh reliably shipped the the software you expected to ship.

</details>

### Kira的规范驱动工作流与可复现性

Kira的**规范驱动开发**（Spec-driven development）工作流是一个从高级指令到可执行代码的结构化过程。它首先接收用户的**提示**（Prompt），将其转化为详细的**需求**（Requirements），然后从中提取**设计**（Design），定义系统的**属性**（Properties），最终构建一个**任务列表**（Task List）供执行。在这个流程中，**规范**（Spec: 系统的自然语言表示，包含功能性和非功能性约束）成为了系统的核心自然语言表示。

规范在Kira中扮演三重角色：
1.  **系统状态的产物集**：它是一组在特定时间点代表系统状态的**产物**（Artifacts），如需求文档、设计文档等。
2.  **结构化工作流**：它提供了一个结构化的工作流，包括需求、设计和执行阶段，确保高质量软件的可靠交付。
3.  **可复现工具集**：它是一套工具和系统，帮助实现可复现的结果。例如，**基于属性的测试**（Property-based Testing）可以验证代码是否符合预期。此外，Kira还提供**需求验证**（Requirements Verification: 扫描需求中的模糊性或冲突，并利用自动化推理技术解决）功能，以确保需求的清晰性和一致性。

除了核心工作流，Kira还具备其他增强功能，例如**引导**（Steering: 影响AI代理行为的记忆和规则，如提交规范、部署策略等）、**多上下文提供器**（Multi-Context Provider, MCP: 用于集成外部数据源和工具的机制）集成、图像处理以及软件钩子（Software Hooks）。这些功能共同构成了Kira强大的工具链，旨在帮助开发者更高效、更可靠地构建和维护软件系统。

<details>
<summary>Original English Source</summary>

So with spectriven dev, we take your prompt, we turn it into requirements, we pull a design out of that, we define properties of the system and then we build a task list and we go and you can run your task list. Effectively the spec then becomes the natural language representation of your system. It has constraints, it has concerns um around functional requirements, non-functional requirements and it's this set of artifacts uh that you're delivering. So I don't think I have the slide in this deck, but ultimately the way I look at spec is that it is one a set of artifacts that represent sort of the state of your system at a point in time t. It is two a structured workflow that we push you through to reliably deliver high-quality software and that is the requirements design um and execution phases. And then three it is a set of tools and and um systems on top of that that help us deliver reproducible results where one example of that is property based testing. Another example of that which is a little less obvious but we can talk about later is going to be um I don't even know what to call it uh requirements verification. So we scan your requirements for over ambiguity. We scan your requirements for um invalid constraints eg uh you have conflicting requirements and we help you resolve those ambiguities using sort of classic uh automated reasoning techniques. Um and I could talk a little bit more about sort of the the features of Kira. I think that's maybe less interesting for this talk because we want to talk about spectrum and dev. We have all the stuff you would expect though. We have steering which is sort of memory and sort of cursor rules. We have MCP integration. We have you know image yada yada. Um so we have ways to and we have software hooks. Um so let's talk a little bit about sharpening your tool chain. And I'm going to take a break really quick here. Uh just pause for a moment for folks in the room who had maybe tried downloading Curo um or something else and just say are there any questions right now before we dive into how to actually use spec to achieve a goal? No questions. It could be a good sign. Could mean I'm not uh talking about anything that's particularly interesting. So um I actually want to like talk in some concrete detail here.

</details>

### 通过多上下文提供器（MCP）扩展代理能力

在Kira的早期测试中，一些用户反映其工作流“过于结构化”，并且难以访问外部数据。为了解决这些限制，Kira引入了**多上下文提供器**（Multi-Context Provider, MCP: 一种允许AI代理集成和利用外部数据源、工具和服务的机制）服务器。MCP服务器可以在规范驱动开发工作流的任何阶段使用，包括需求生成、设计和实现。

设置MCP服务器非常直接：用户可以通过Kira面板手动添加，或者直接指示Kira添加一个MCP并提供相关信息（例如JSON配置）。一旦MCP被添加，它就会显示在控制面板中，用户可以启用、禁用或管理其访问的工具。需要注意的是，更改MCP配置可能会导致缓存操作，从而减慢系统速度。

演讲者通过具体案例演示了MCP的强大功能。例如，Kira团队使用**Asana**（Asana: 一款流行的项目和任务管理工具）作为任务追踪器。通过集成**Asana MCP**，Kira可以直接从Asana任务中提取元数据，自动生成规范的需求，而无需从零开始编写。另一个例子是使用**Fetch MCP**来从互联网上类似产品中获取示例，这可以替代**Brave Search**或**Tavlet Search**等搜索MCP服务器。这些示例展示了MCP如何显著简化规范的生成过程，使其更加高效和可靠。

<details>
<summary>Original English Source</summary>

Uh this is a talk I gave a few months ago on how to use MCPS in Kira. And so one of the challenges that people who had tested out Kira had that might be a little easier to see was that they um they felt that the flow we were pushing them through was a little bit too structured like you don't have access to external data, you don't have access to the to all these other things you want. And so one thing that we said on our journey here towardsing your um oh you know what this out of order here's my nice AI generated image. So you can use MCP. Everybody here I assume is familiar with MCP at this point. But uh Curo integrates MCP the same way all the other tools do. Uh but what I think people don't do enough is use their MCPs when they're building their specs. And so you can use your MCP servers in any phase of the specdriven development workflow. That's going to be requirements generation, design, um, and implementation. Um, and you can use, we'll go through an example of each. So, first of all, to set up a spec in Kuro is fairly straightforward. We have the Kuro panel here, which there's a little ghosty um, and then you can go down to your MCP servers and click the plus button. You can also just my favorite way to do it is to ask Kirro to add an MCP uh and then give it some some information on where it is and it can go figure it out usually from there or you just give it the JSON blob and it'll figure it out. Once you have your MCP added, you'll see it in the control panel down here and you can enable it, disable it, allow list tools, disable tools, etc. So you can manage context that way. Worth noting changing MCP and changing tools in general is a caching operation. So if you're very deep into a long session, maybe don't tweak your MCP config because it will slow you down dramatically. But let's talk about um MCP inspect generation. So something I the Curo team uses a um for reasons I don't know, but it's our task tracker of choice. Uh but so one thing I want to do is uh maybe go and say I don't want to write the requirements for a spec from scratch. My product team has already done some thinking. We've iterated in a sauna to kind of break a project down. This is not always how things work, but sometimes how things work. So in this case, I have I have a task in a sauna. Oh no, I did the wrong thing. That's what I get for zooming. So I have this task in in a sauna that says add the view model and controller to this API. In this case, this was a demo app that I can figure in a few minutes. And we even had like it's kind of peeking under here, but we had some details about what we wanted to have happen. Now I can go into Kira and just say start executing task XYZ URL from ASA and Kira is going to recognize this is an Asana URL. I had the ASAN MCP installed. It goes and pulls down all the metadata there. Um da da da. So it's going to break out and from there start um start determining what to work on. Um oh it's funny these titles are backwards. basically create a spec for my open asauna tasks. Again, go pull from a sauna all the tasks and then for each one generate um requirements based on those tasks. So I think I had like six tasks assigned to me. One is do user management, do some sort of um uh property management da da da it pulled them in generated the requirements and then in this case title is wrong apologies start executing task. this is I want to go and do the code synthesis for this um and I will take a quick break here to talk about how you can do this in practice. So for those of you who are you know following along in room uh feel free to fire up your curo open a project and then picking a an MCP server. I'll share a few repos here really quick that you can play around with. So I have an MCP server implemented. I have this lofty views which I think implements the asauna. Um and then these should all be public. Let me just double check. Yeah. Okay. So for example, if you wanted to extend my I have a Nobel Prize MCP which curls perhaps unsurprisingly there is a Nobel Prize API. Um, so you can use UVX to install it or you can get clone this Al Harris at Nobelmcp. Uh, this is just one example. Another one here is if you want to play around with the sample that's in the video. Um, I have Al Harris atlofty Views. Um, I'll leave these both sort of up on the screen for a few moments for folks who do want to copy the uh the URLs. But while that is happening, oh no, let's put you on the same window. So what I'll demo quick is the usage of an MCP to make like spec generation much easier or more reliable. So here I have let's see Got a lot of MCPs. Which ones do I actually want to use? Let's use the GitHub MCP. Oh, no. Ignore me. That's better. Okay. Well, I have the fetch MCP. So in this case I could for example come in here and say hey I've generated a bunch of tasks lofty views app. This is basically a very simple CRUD web app. Um but I want Kira to uh use the fetch MCP to pull examples from similar products that exist on the internet. You could also use you know Brave search or Tavlet search MCP servers but in this case I'll just use fetch because I've got it enabled. Um, so let's say, oh actually we can run the web server and use fetch. That's a good example. This is one example of you can at any point in the workflow generating a spec go through and um you know use your MCP servers to get things working. No, this is what I get for not using a project in a while. We'll cancel that. We can actually do something a little more interesting which is a separate project I've been working on. Um, so I've been working on a an agent core agent and that might be I I know the project works, which is the reason I'll fire it up here. Should I call it? Well, maybe we'll do live demos at the end.

</details>

### 定制化开发产物：提升细节控制与反馈

Kira不仅允许集成外部工具，更进一步，它提供了深度定制开发**产物**（Artifacts）的能力，从而实现对开发流程更精细的控制。这相当于将工具链从粗糙的200目砂纸打磨到400目，获得更精致的抛光效果。开发者可以根据自己的需求，定制任务列表和需求列表的内容。

一个重要的定制化案例是在设计阶段引入**UI原型**（UI Mocks: 用户界面草图或模型，用于可视化设计概念）。由于Kira的规范本质上是自然语言，因此可以轻松地在设计文档中嵌入**线框图**（Wireframe Diagrams: 界面布局的骨架表示，通常不包含视觉细节）等UI原型。演讲者展示了Kira如何根据指令生成文本格式的ASCII图表，将用户管理界面的设计可视化。这种在早期阶段就能看到UI布局的能力，有助于团队和利益相关者在实现之前就达成共识，从而**“左移”**（Left-shifting: 将问题识别和解决的阶段提前到开发周期的早期）潜在的设计问题。

另一个关键的定制化是**在任务定义中包含明确的单元测试用例**（Explicit Unit Test Cases: 详细描述每个任务完成后需要通过的单元测试）。AI代理（特别是大型语言模型）常常善于报告“任务完成”，但其正确性可能存疑。通过要求Kira在任务中明确列出需要通过的单元测试，并结合**代理钩子**（Agent Hooks: 在代理执行特定操作时触发的自定义代码或脚本），开发者可以确保代理交付的代码真正达到预期的正确性标准，而非仅仅是表面上的“完成”。这些定制化能力使得Kira能够适应用户的特定工作方式，而非强制用户遵循单一的“最佳”流程。

<details>
<summary>Original English Source</summary>

So that's sort of like the most basic thing you can do with Kira is just use MCP servers, but any tool uses MCP servers. I actually don't think that's particularly interesting. So let's say in sort of this process of trying to sharpen our our spec dev toolkit, we've finished up with the 200 grit. We've added some capabilities with MCP. It's useful, but it's not going to be a gamecher for us. I want to come in here and actually get up to the 400 grit. Let's get start to get a really good polish on this thing. I want to customize the artifacts produced because you've got this task list, you've got this requirements list and I don't agree with what you put in there, Al. Um, you could say that a lot of people do and I that's a a great starting point. So, here's something I heard earlier in the week at um, you know, earlier in the conference is that people like to do things like use wireframes in their mocks. Um, use wireframe mocks because in your specs are natural language, you're using specs as a control surface to explain what you want the system to do. Uh therefore I want to be able to actually put UI mocks in here. So the trivial case is that I just come in here and say Kuro's asked me here does does the design look good? Are you happy? And I said this looks great but could you include wireframe diagrams and ask you for the screens we're going to build here. I'm adding this is again from that lofty views thing. I'm adding a user management UI but I want to actually see what we're sort of proposing building not just the architecture of the thing. So your cure is going to sit here and churn for a few seconds, but you can add whatever you want to any of these artifacts because they're natural language. So they're structured, which means we want some re um some sort of reproducibility in what they look like, but ultimately what they look like doesn't matter because we've got the the any machine here, the agent sitting that can help translate it to what it needs to be. So Kira's churning away here. It's thinking thinking and then it's going to spit out these uh text wrapped asy diagrams. I'll fix the wrapping here in a second in the video, but ultimately like you know it does whatever you want. So if you want additional data in your requirements, you can do that. If you want additional data in the design like this, uh you can easily add that. Here we've got sort of these wireframes in ASKI that help me sort of rationalize what we're actually about to ship. Um, and then I can again continue to chat and say actually in the design I don't want um, you know, maybe I don't want this add user button to be up at the top the entire time in which case I could chat with it to make that change easily and now we're on the same page up front instead of later during implementation time. So we've again sort of left shifted some of the concerns. Um, so that's one example. You know, I want to add UI mocks to the design of a system. Another example though could be this. Um, oh, this is a just a quick snapshot of the end state there where now my design does have these UI mocks. Um, but another example that I actually like a little bit more is this uh including test cases in the definition and tasks. So today the tasks that cure will give you will be kind of the bullet points of the requirements and the acceptance criteria you need to hit. But I want to know that at the end state of this task being executed, we have a really crisp understanding that it is correct. It's not just like done because the a anybody who's used an agent can probably testify that um the LMS are very good at saying I'm done. I'm happy. I'm sure you're happy. I'm just going to be complete. Oh, yeah. The tests don't pass but they're annoying. I tried three times them to work. I'm just going to move on. Um no, I don't want that. I want to actually know that things are working. So, in this case, I've asked Hero to um include explicit unit test cases that are going to be covered. So my task here for example in create creating this agent core memory checkp pointer is going to have all the test cases that need to pass before it's complete and then I can use things like agent hooks to ensure those are correct. We'll run this uh sample a little later in the talk. Um this is the thing I'm ready to little demo. Uh yeah, so this is another example where you can again you're you're working on your toolbench. You're sort of you have all these capabilities and primitives at your control and you can tweak the process to work for you, not just the process that I think is the best one.

</details>

### 迭代优化开发流程：代理的引导与决策

将工具链打磨到800目，意味着不仅要迭代开发**产物**（Artifacts），还要迭代优化实际运行的**开发流程**（Development Process）本身。这意味着开发者可以挑战Kira的默认行为，并引导其探索更优的解决方案。

演讲者举例说明了这一点：他最初指示Kira为代理添加内存，并建议将对话记录转储到**S3**（Simple Storage Service: 亚马逊提供的对象存储服务）文件。Kira虽然能够执行此任务，但这种初始指令可能引入了**偏见**（Bias: 在AI系统中，由于训练数据或指令的倾向性而导致的非最优或预设行为），即代理倾向于使用S3，仅仅因为这是开发者熟悉的方式，而非最佳方案。

为了避免这种偏见，开发者可以主动向Kira提出质疑，例如：“这是实现会话持久化的惯用方法吗？”Kira会利用其**多上下文提供器**（MCP）工具（如AWS文档和Fetch）进行研究，并可能提出替代方案。在这个案例中，Kira推荐使用**Agent Core memory**（Agent Core Memory: AWS Agent Core服务中用于代理会话持久化的原生功能），认为它更符合惯例且更具前瞻性。这表明，开发者不应被Kira预设的“刚性流程”所束缚，而应积极引导代理探索和评估多种选择，从而找到最适合项目需求的解决方案。这种能力使得Kira不仅仅是一个执行工具，更是一个可以进行协作和优化的开发伙伴。

<details>
<summary>Original English Source</summary>

And then sort of last but not least, the 800 grit. At this point, we're getting a final polish on the tool. Uh we might be stropping necks, but we want to, you know, you can iterate on your artifacts, but you can also iterate on the actual process that runs. So, one thing you might have, and I do this a lot, is I'll I'll be chatting with Kira, and I say, "Hey, I want to um in this case, I want to add memory to my agent in agent core. Um, let's dump conversations to an S3 file at the end of every execution." Cur is going to say, "That's great. I know how to do that. I'm going to research exactly how to do that thing. I will achieve this goal for you." But ultimately what I've done is actually introduce a bias up front which is I'm steering the whole agent using S3 as this storage solution just because maybe I'm familiar with it but it's probably not the best way to go about it. So then after it had synthesized the design and all the tasks and all this stuff I came back and said well like we don't need to stick to this rigid spectriven dev workflow that I've that has been defined by Kirao. I can ask for alternatives like is this the idiomatic way to achieve session persistence? I don't know maybe there's a better way. Maybe if we're talking AWS services, it's not S3, it's Dynamo or yada yada. Uh Kira's going to come in here and say, you know, good question. Uh da da da. Let me research. It's going to go through call a bunch of MCP tools that I've given it access to. This kind of ties back to that you should be using MCP. And then it comes back with this recommendation that I didn't know was a feature, which is Asian core memory. Um it says it's more idiomatic and future proof that maybe is TBD and should be checked a little closer. Um, but uh or you could use S3, which is the thing you recommend. Now, actually, I I bet there's far more than two options here. So, you could probably keep asking the agent, are there other options, yada yada, and it would go and continue to investigate, but you should not lock yourself into the rigid flow that is sort of the starting point here. Um, yeah. So, that that's actually I think it for my deck. Um what I will talk about is let's just run through that sample I just had up there which is that um so basically let me delete delete it and I'll just do a live demo of sort of specs in Curo and how we can fine-tune things a little bit.

</details>

### 实践案例：代理记忆与持久化

为了具体演示Kira的**规范驱动开发**（Spec-driven development）能力，演讲者展示了一个名为**Gramps**的项目，这是一个部署在**Agent Core**（Agent Core: 亚马逊AWS提供的一项服务，用于构建和管理AI代理）上的**老爸笑话生成器**（Dad Joke Generator）。该代理最初的问题是每次都提供相同的笑话，因此需要添加**记忆**（Memory: 代理存储和回顾过去交互信息的能力）和**持久化**（Persistence: 代理在会话之间保持状态和信息的能力）功能，以确保每次会话都能提供不同的笑话。

在演示中，演讲者最初指示Kira添加会话ID并将对话保存到**S3**（Simple Storage Service）文件，这反映了他对S3的熟悉和隐性偏好。Kira随即开始生成需求和设计。然而，演讲者随后挑战了Kira的这种默认选择，要求它研究实现持久化的最佳方法。Kira利用**AWS文档MCP**（AWS Documentation MCP: 集成AWS官方文档的多上下文提供器）和**Fetch MCP**（Fetch MCP: 用于从互联网获取信息的通用多上下文提供器）进行了调研，并根据对**LangGraph**（LangGraph: 一个用于构建AI代理的框架）原生检查点机制的了解，更新了需求。

一个具体的**EARS格式**（Easy Approach to Requirement Syntax）需求示例如下：“作为一名开发者，我希望实现一个基于S3的自定义检查点，以便代理可以使用LangGraph的原生持久化机制与S3集成。”（As a dev, I want to implement a custom S3-based checkpoint so the agent can use LangGraph's native persistence mechanism with S3.）这种结构化的自然语言表达对于Kira至关重要，因为它允许将需求传递给非**大语言模型**（Large Language Model, LLM: 基于海量文本训练的AI系统）的模型进行解析，从而获得更具确定性的结果。Kira的长期目标是减少对LLM的依赖，更多地利用经典的自动化推理技术来提供高质量的输出。

此外，Kira还引入了**正确性属性**（Correctness Properties: 确保系统行为与输入需求一致的关键不变性）的正式化，这对于原型阶段可能不那么关键，但在生产环境中至关重要。这些属性确保系统与输入需求一对一地对齐，从而显著提高软件的可靠性。

<details>
<summary>Original English Source</summary>

This project is a Node.js app. It is a um it's a CDK. Again, I'm not trying to sell more AWS. This is just the technologies I'm familiar with, so I can move a lot more quickly. So, I wanted to know a little bit about agent core, which is a new AWS offering. And as somebody building an agent, I should probably be familiar with it. So, and I'm not familiar enough with it. So, I've got we've got some other people here who know a lot about it. So, put my hand up a little bit and you know, you caught me. So, I set up a CDK stack, which is just um you know, IA technology to deploy software. I'm familiar with it and I love it. Uh, so I have a stack here that lets me deploy whatever an agent core runtime is. I don't know. I asked Kira to do it. We vibe coded this part. So we vibe coded the general structure. We got an agent. We got IA set up. I then vibe code added commit lint. I added husky. A few things like this that I like for my own TypeScript projects. Um, prettier and eslint I think. So we have a basic product here or like a basic project here that I know I can deploy to my personal AWS account. Um, now I'm going to come in here and oh, and then importantly, this is super important because I don't know how the hell agent core works. And I could go read the docs, but the docs are long and they're complicated and I'm really just trying to build out a PC to to like learn about it myself. So, I added two MCP servers. Oh, no, maybe I didn't. Let me check. Oh, okay. Yes, sorry. Buried down here at the bottom. So this is my Kira MCP config. I added one important MCP server here which is the AWS documentation one. There's other ways to get documentation. You can use things like um Tessle level 7 but in this case this is vended by AWS. So I have some confidence that it might be correct. So I used this to help the agent have knowledge about sort of what technologies exist. And I think I used fetch quite a bit as well. So these are the two sets of um these are the two step sets of uh MCP servers I provided the system. That's great. Move on. Confirm. So and I'll just rerun this from scratch. So what I had done yesterday evening or maybe the evening before was I sat down and I have this system sort of basically working and now I want to start doing specri development. So, I want to add this uh session ID concept and then I want to read conversation to an S3 file blah blah blah. This is the whole sort of bias thing I showed you earlier. We're going to fire that off through Curo. It's going to start running uh chugging away and then it's going to, you know, see if the spec exists. Uh, okay, the folder does exist. It's probably going to realize there's no files there and start working away. But, um, from here I'll sort of live demo. It's going to read through require. It's going to read through existing docs. It's going to read through existing files, gather the context it needs. Sure, in a way. Um, but in a moment once it generates sort of the initial requirements and design, I am going to challenge it to use its own, you know, MCQ servers. I want you to go and do some research on the best way to do this and provide me some proposals. Um, and this is why I was hoping to get the clip on mic working because I've got to set this down for a moment. Okay. So, you know, I don't know if this is the best way to do this. Um, go read docs, go use fetch. D. It's going to keep kind of churning away here and then come back to me after it's probably got a few ideas and proposed it. But, um, this is an example of me just using additional capabilities. uh use fetch, use the docs MCP, use whatever you can to get the best information and don't take at face value the things that I said. These are usually things we have to prompt pretty hard to get the agent to do, but if you're doing it in real time, it works fairly well. Um, again, the agent, all of these agents are going to be very easy to please. So, you know, just cuz I said something in the stupid docs, it may or may not actually be the most important thing from the agents perspective down the road. So, you know, okay, so it's done a little bit of research. It understands the lang graph which is the agent framework we're using already has this knowledge of persistence um da da da and actually in this case it didn't find it did not use the mcp for uh agent core docs who didn't find that agent core has this knowledge of persistence um so maybe you like let's assume I don't I still don't know that exists because I didn't dry run this a few days ago um we might have to find that later the design phase so first thing it's going to do is kind of iterate over all my requirements requirements here. Um, you know, it's changed the requirements based on what it now knows about Langraph and how it can natively integrate with the uh checkpointing, but it's still really crisply bound to this like S3 decision that I made implicitly in the ask. Um, so that is just something to be aware of. Anything you put in the prompt is effectively rounding the agent. Um, for better or for worse. I see it's still iterating. So, yeah, comes through says, does this look good? We changed duh. I'm going to say looks great. Let's go to the design phase. So now Curo is going to take my requirements and take me into the design phase of this project. I can make this so things are a little bit bigger. But um here's an example of what I meant by these ears requirements. So the user story here is as a dev I want to implement a custom S3based checkpoint so the agent can use Langraph's native persistence mechanism with S3. Great. That sounds reasonable to me as a person you know sort of co-authoring these requirements. This here, this sort of when then shall syntax. This is the years format and the structured natural language is really important for us to pass this through non LLM based models and give you more deterministic results when we parse out your requirements because ultimately our goal is to actually use the LM for as little not as little as possible but less and less over time. We want to use classic automated reasoning techniques to give you high quality results not just you know whatever the latest model is going to tell you. Um, so here's gone through spits out a design doc. Let's actually just look at this in markdown. This sure you got a server da da checkpo pointer ghost s3 that makes sense pseudo code again in a real scenario. Maybe I read this a little bit more closely and what's actually this is the new thing we shipped in um on the 17th is that now cur is going to go through and do this formalizing requirements for correctness properties. Um and so right now what the system is doing is it's taking a look at those requirements you generated uh the requirements we agreed upon with the system earlier. These look good. I agree with them. yada yada. It's taking a look at the design and it's extracting correctness properties about the system that we want to run property based testing for down the road. This is something that may or may not matter for you in the prototyping phase but should matter for you significantly when you're going to production. because these properties are correct and these properties are all met. The system aligns one to one with the input requirements you provided. Um yeah, so while this is chugging away, any questions yet? Any folks kind of curious about this?

</details>

### 问答精粹：Kira的适应性与性能

在问答环节，演讲者深入探讨了Kira在不同场景下的适应性与性能。

**Kira与传统规划模式的区别**：Kira的**规范驱动开发**（Spec-driven development）不仅仅是**大语言模型**（LLM）驱动，更是一个结构化的系统。与一些可能只生成短暂**执行计划**（Execution Plan: 描述如何完成特定任务的步骤序列）的工具不同，Kira致力于创建**活文档**（Living Documentation: 随系统演进而持续更新和反映当前状态的文档），实现规范的**双向同步**（Bidirectional Sync: 确保代码与文档之间保持一致性，任何一方的修改都能反映到另一方）。这意味着Kira生成的规范不仅指导当前开发，也作为系统演进的长期记录，取代了传统的**设计文档**（Design Docs: 详细描述系统架构、组件和交互的文档）。

**大型代码库的支持**：Kira在处理大型现有代码库（即**棕地项目**，Brownfield Project: 在现有系统基础上进行开发或修改的项目）时，会通过读取工作树、进行背景索引和语义搜索来理解代码。其性能取决于代码库的**关注点分离**（Separation of Concerns: 将软件系统分解为不同功能或责任模块的原则）和**内聚性**（Cohesion: 模块内部元素相互关联的紧密程度）。结构良好、模块化程度高的代码库能让代理更好地理解和执行任务。

**会话长度管理**：目前Kira主要通过累积上下文和利用**提示缓存**（Prompt Caching: 存储和重用之前发送给LLM的提示部分，以提高效率和降低成本）来管理会话长度，以实现快速交互。虽然尚未实现增量剪枝或增量摘要，但团队正致力于改进摘要功能，使其更快、更实时。

**跨功能需求处理**：当一个任务涉及多个规范（如安全需求、API设计、日志记录）时，操作者需要决定是将所有需求加载到一个**跨功能规范**（Cross-functional Spec: 涵盖多个功能领域或关注点的综合性规范）中，还是分别处理。Kira的**多组工作区**（Multi-group Workspace: 允许同时管理和操作多个独立项目或代码库的功能）功能有助于管理这些分离的项目。

**非功能性需求与多语言支持**：开发者可以将**非功能性需求**（Non-functional Requirements: 描述系统性能、安全性、可用性等方面的需求）纳入规范和**引导文档**（Steering Docs），从而影响代码生成（例如，对速度、运行时性能、锁竞争的考虑）。Kira是语言无关的，支持Java、Python、JavaScript、TypeScript、Rust等多种语言，其系统设计不依赖于特定的编程语言或框架。

**引导文档的重要性**：**引导文档**（Steering Docs）是磨砺Kira工具链的关键。它们允许开发者定义代理在代码开发中的偏好和优先级，例如提交信息的格式、代码风格、测试覆盖率最低要求等。通过在引导文档中明确这些**权衡**（Trade-offs: 在设计或实现中，为了优化某个方面而牺牲另一个方面的决策），开发者可以确保Kira生成的代码不仅功能正确，而且符合团队的质量标准和最佳实践。

<details>
<summary>Original English Source</summary>

Um yeah, we're here and then there. Um what would you say is the main difference between that has? Uh I haven't used the planning mode in a couple of weeks. So it's I'm things move so fast it's a little wild. Um but I think ultimately uh what we would say is that Kuro's spectrum and dev is not just LLM driven but it is actually driven by like a structured system. Um and so planning mode I'm not sure if there's actually like a workflow behind it that takes you through things but um yeah this is our take on it for sure. I'm not familiar enough to give like a more concrete example unfortunately. similar I mean it doesn't give you like this I think that this document is cool is bringing you the school but uh what Cer does is to basically create you a plan that's just an execution plan okay oh I see so I think that the fundamental difference there uh does that plan get committed anywhere or is it just ephemeral okay so what I want over time is not is not just how we make the changes we care about but it is actually the documentation and specification about what the system does. Um so the long-term goal I have is that as Kira we were able to do sort of a birectional sync that is as you continue to work with Kira you're not just acrewing these sort of task lists uh and so I'm just going to say go for it to go to the tasks um but we're not just acrewing task list but actually if I come back and let's say change the requirements down the road we will mutate a previous spec. So I'm looking at really just a diff of requirements which as you go through the green field process you're going to produce a lot of green in your PRs which is maybe not the best because I'm just reviewing three new huge markdown files but on the next time or the subsequent times that I go and open that doc up I want to be seeing oh you've actually you know you've relaxed this previous requirement you've added a requirement that actually has this implication on the design doc um that is the process the curo team internally uses to talk about changes to the curo So we review our design docs have in general been uh replaced by spec reviews. So we will you know somebody will take a spec from markdown they'll blast it into our wiki basically using an MCP tool we use internally and then we'll review that thing and comment on it in sort of a design session as opposed to you know I wrote this markdown file or a wiki from scratch. Um so it becomes sort of if uh well it's actually not like an ADR because it's not point in time. It is like this living documentation about the system. Um but yeah thanks for the question. There's one over here. Um this may be more a spectrum development question but are there like like is there like a template for a set of files that you fill out? Like right now you're in the design.md. Are there like is this is the designd the spec and it's a single doc or are there oh great question. So the yeah the question was um are there and correct me if I'm wrong here but question is are there a set of templates that are used for the system and is the question you're driving at can you change the templates or is just are there okay so the yeah question is are there a set of templates um there are implicitly in our system prompts for how we take care of your specs so you'll see here at the top navbar here right now we're really rigid about this requirement design task list phase but we know that doesn't work for everybody for example if you're starting we get this feedback from a lot of internal Amazonians actually that I want to start with a I have an idea for a technical design and I don't necessarily know what the requirements are yet but I know I want to make maybe design is even the wrong word I want to start with a technical note like I want to refac this comes up a lot for refactoring actually um so I want to refactor this to no longer have a dependency on um here's a good example here we use a ton of mutxes around the system to make sure that we're locking appropriately when the agent is taking certain actions because we don't want different agents to step on each other's toes. But maybe I want to challenge the requirements of the system so I can remove one of these mutexes uh or semaphors I should say. Um so I might start with something like a technical note and then from there sort of extract the the requirements that I want to share with the team and say hey you know I had to kind of play with it for a little while to understand what I wanted to build but I still want to generate all these rich artifacts. So today it's this structured workflow. We're playing a lot around with making that a little bit more flexible. But the the structure is important because the structure lets us build reproducible tooling that is not just an L. So I think that that's an important distinction we make is that our agent is not just an LLM with a workflow on top of it. The backend may or may not be an LLM or it may or may not be other neurosymbolic reasoning tools under the hood. Um, and so we we try to keep that distinction a little bit clear, uh, that you're not just talking to like Sonnet or Gemini or whatever. You're talking to sort of an amalgam of systems based on what type of task you're executing at any point in time. Um, although when you're chatting, you are talking to just an LLM. Um, but yeah, so we have a template for the requirements. We have a template for this design doc because there's sections that we think are important to cover. Um and again like if you disagree and you're like I don't care about the testing strategy section just ask the do it and similarly the task list has is structured because we have sort of UI elements that are built on top of it as well like task management and um do we have we'll get there when we do some property based testing but um there's some additional UI we'll add for things like optional you can have optional tasks and stuff like that and so we we need the structure there for our uh taskless LSP to work for example. Um yeah, thank you for the question. Anything else before we truck on? Cool. Uh I may need somebody to remind me what we were doing. Oh, that's right. So, we went through and we synthesized the spec for adding memory and some amount of persistence to my agent. By the way, I didn't introduce you to this project. This project is called Gramps. It is uh it is an agent that I'm deploying to agent core to learn about it. I mentioned that. But what I didn't tell you is that is it is uh a dad joke generator. A very expensive one since we're powering it via LLMs, but effectively you're a dad joke generator. Jokes should be clean. They should be based on puns, you know, obviously bon bonus points if they're slightly corny but endearing. Um yada yada. So we're deploying this to the back end. So, the reason I want memory is because every time I ask the dad joke generator for a joke, it gives me the same damn joke and that's just super boring and my kids are not going to be excited about that. So, I want memory so that as I come back for the same session, I get different jokes over and over again. Um, that's the context on the project. So, we've come through here and we actually said we generated this thing, we did the task list. I said, "Hey, is this the idiomatic way to do it?" But what I know is that we didn't actually uh we're not using Agent Core's memory feature, which is probably a big oops. Um, and so, you know, quick show of hands. Do we want to make the mistake and go all the way to synthesis and deployment, or should we fix it now? Who wants to fix it now because we know better? No, I want to make the mistake. Let's keep on trucking. I I had three yeses in a room full of nothing. So, we're going to make the mistake and then come back and fix it later. So, uh, let's say run all tasks in order. Uh, the reason I mention in order, which seems very specific, is because this is a preview build of Kira. Um, and so somebody just added to the system prompt I should only do one task at a time. And I found that if I say run all tasks, it thinks I somehow mean do them all in parallel. So, we'll that'll be fixed before these changes get out to production. So Kira's going to keep kind of going through here and chewing away on the system in the back end. Um, it has steering docs that explain how to do its job. It has, which I guess I should show you guys. Steering again is like memory. So I have some steering how to do commits. Uh, you know, how I like to have commits, but also steering on things like how do you actually deploy this thing? Um, how do you deal with agent core? And then how do you run the commands that are necessary for you to deploy this to my local dev account. Um, and then those are mostly just an example again of sharpening your tools like uh I went through this kind of painful process of figuring out oh you know you have to use this parameter on the CDK the CDK command you have to use this lag otherwise it doesn't work correctly and so once I go through that pain of learning I just say kira write what you learned into a steering doc and it will usually do a very good job of summarizing um and so it generated automatically this Asian core langraph workflow MD file um yeah so I mean it's just going to kind of go away here and truck truck on and do its job and we can watch it in the background. But in the interim, um I think at this point we're at a pretty flexible spot. Uh so for folks who want feel free to use Kira, try out Spectriven Dev on your own. I'm going to keep just kind of running this in the background and taking questions and comments. But that's kind of it for the scheduled part of today. Yep. How does Carol work for like existing large code bases or this? Yeah. Yeah. question was how does cure work for large and existing code bases basically the brownfield use case uh and the answer is it depends on what you're trying to do um for spec driven dev you can ask cure to do research into what already exists so when you start a new spec it will usually start by reading through the the working tree um but the agent is generally starting from a a scratch perspective right it needs to understand the system um in practice what that means is that you're going to end up with a bunch of things like if your system already had good separation of concerns uh your the components in your system are highly cohesive and they're sort of highly coherent and highly cohesive, it's going to have a great job, right? It's going to be able to say this is the module that does this thing. I don't need to keep 18 things in my context to do my job and it's going to do well. Um if you let's just take an example that's off the top of my head. if you were trying to launch an IDE very quickly uh leading up to an AWS launch and you um you know took a lot of tech debt along the way that you need to unwind and you know nobody here would do that I'm sure but um in case you did that like me then your agent might actually have a much harder time traversing the codebase in the same way that a dev would right so uh from just kind of that perspective the more reliable things like your test suite are and the more understandable things like module separation and sort of decomposition of concerns are the better the agent will do. Um and versus true of course. Now for things like uh understanding the code base, this is a bad example because this is a very small code base, but uh we do have things like you know code search and workspace. Um uh I don't know what to call these context providers. Um, so you can come in here and just say I want to do code. Uh, what is it? I might have turned this off actually. Oh, I did turn it off because the code base isn't big enough. We'll do things like indexing in the background so the agent like you can do semantic search over what you've got um if you're just chatting. But in general, uh, Cur should go in and do sort of background search to figure out how to do its job. like as the codebase scales up, it's going to be less do probably less well overall. But that's one thing we're working on as a team. Did that answer your question or did I kind of glance off the side a bit? Yeah, I think I got it. Okay, cool. Anybody else? Uh, how long are you willing to wait for indexing to complete? Uh so one example I have is that the code OSS um if it's not supremely obvious by looking at it cur is a code OSS fork just like you know cursor winds surf um one of the challenges we've had is the code OSS codebase is very large fairly large there's other big ones out there but that's kind of my large code base because I'm not forced get to work in it fairly frequently um and so there there's definitely some perceived slowdown when you're dealing with something large like that, especially when you talk about codebased indexing. It's a very active area of work for us though. So, we're trying to do things like um either remove indexing from the critical path so that you're not waiting there on some kind of slowed down render thread because indexing is running. Um but in practice, there should not be. I mean, again, the agent may practically do less well, but we're going to be talking in a couple weeks at reinvent about how some of the temple features in Curo were built via spec in a codebase we did not understand particularly well because we're just not VS code devs. Um, and Curo did a fine job of it. But again, that's a testament to the fact that codebase is reasonably well um structured and like if you've taken the time to understand how it works, it's very understandable. If you have not, it will might be a little bit opaque to stare at. Yeah. Uh in terms of indexing, is it like just just putting um um as much information from the code base into context or it just is there a way to like create some kind of like vector database of all the code base and then like query it? I just Yes. Um so the question was what do you mean by indexing? Um because indexing can mean a bunch of different things and what I mean is that um the agent is actually not provided the I'm going to keep the agent context as small as possible. We use the uh the index for most like secondary effects things like if you're doing a uh a code search or if I do something like search for um pound uh what the file in here http server like we use it more for these types of UI um than giving it to the agent because the agent does this is sort of anecdotal and based on our benchmarks does better when given less context but given the tools to understand where to go find things. Um, something we've heard a lot about is sort of incremental disclosure here at this conference. And that's again, we don't want to load too much at the beginning of the context and conversation with the agent. We want the agent to self-discover the right context for the task. Yeah. Thank you. Yeah. You guys managing session length like is there any kind of compression or pruning? Yeah. So, um, question was how do we manage session length? We have no incremental pruning today or incremental summary. Um you basically just accrete context until you hit your limit which I think right now I'm on auto which has like a 200k token limit um similar to the sonnetss. Um uh so we don't have a very sophisticated algorithm here yet. We've looked at a few things but our number one concern actually is um prompt caching hit rate. And so in a normal use case, I can achieve something like 90 95% cash token usage here on per turn, which means that my interactions are very fast. And that's or they're much faster than the alternative, which is I'm sending 160k tokens to to bedrock cold. Um, so that's one of the reasons we've actually not done much experimentation with incremental summary. Um, our summarization feature exists. When you hit the cap, it's not great. It's something we're trying to uh ship an improved version very very shortly. Um eg in the next couple of weeks which should be faster. Today it's like a one-off operation that can take up to 30 or 45 seconds which is a horrendous experience. We're hoping to fix that here and make it sort of a real-time experience. The follow managing stapleness between sessions then is that how why you're relying on a stereopated spectrum. So sort of um that is not the only reason I mean the spect the spectrum of dev is less to do with performance and more to do with reproducibility and accuracy of the agent. Um because if we can give you the right result, the the the way I and I think that we talk about it internally as this team is if I spend 10 seconds giving a prompt to the agent and then it goes off and it gets it wrong, it's like it's kind of no skin off my back, right? I burned however many tokens and you know, a couple cents of credit usage with whoever my LM provider is, but I spent 10 seconds generating a prompt. If I spend five to 10 minutes with the system producing a detailed design doc or let's just say even a detailed set of requirements I wanted to do a fairly good job. If I spend an hour generating a design doc reviewing it with my team and then synthesizing from that I wanted to get it right. So the goal necessarily is not just latency but actually accuracy when we talk about that. No, it's a both and. You need to do both. But um spec comes more from a uh the goal to have um highly reproducible output. I'm going to go over here first and then you Yeah. How did each of these task agents pass context to each other? And then are you only supposed to run this this parent task? Because it just finished all like 3.1 3.2 3.3 but then it still thought that 3.1 wasn't done and ran that in 3.2. too. Oh, did it? Yeah. Well, no, mine right. Oh, okay. Yeah. Yeah. Um, so if you the uh the question is if you're in the UI and you're like running tasks and I can just kind of pull up my task list here. Um, so if I just hit start, start start each of these is going to be a new session which means the context is completely unique. Um, personally I like to just if I can if I've got the context base to afford it, I just say do all the tasks because I find that more understandable and I think I actually get better performance. But by default, each task will be a new session that has no shared context with the previous ones. So the session is effectively just seated with your specification and then like here you're working on a spec that does all this stuff block of text um and you are doing this task da da da don't do any other tasks just do this. Um, so that sounds like a bug. Um, they ever spin up sub agents for certain things. We don't have sub agents yet in Caro, some we're working on. Yeah. Yeah. Because I mean, ideally, right, if we click on task three and I've got 31, 32, 33, and they're separated, there's no good reason I couldn't have different systems working on them. Yeah. Uh, right here, we do have in the Curo CLI custom agents that you can also run off. Yeah. Curli is a concept of custom agents. um which can be run sort of as a task um and it's something we're playing with right now in Curo Desktop um and I think you had another one yeah I'm sorry if I missed this but in the spec folder um as you do more and more of these tasks over time y is it just all in one design requirements tasks your whole project is defined there or did it group by that's a good question um yeah so I will have many I will have uh the question was as you do more you generate let's say more specs over time. Are you sort of just creating one massive spec and no? Uh let me open a different project. So this is for example the curo extension which is like a 1p extension inside the curo IDE. This is where the agent itself lives. And so we have pruned some specs but there are specs in here that we can talk through or I can just kind of demo. Um so these are the way I think about it is that the spec sort of represents a feature or a problem area in the in the project. And so for example, I can blast this a little larger. So for example, we have um like some of these are just tests. We've done things like oh could we have a prompt registry? Could we have a prompt registry file loader? They may or may not make it all the way to production. Um I want telemetry on the chat UI. So these are just like somebody will go off and spend maybe represents a few days of work for an SD. Um, agents MD support is a good one where we just, you know, I sort of said research what agents MD is and build it in the way you build steering in like support in the same way. This spec is fairly unlikely for us to come back and revisit in the future. So I may actually just delete it. Um, which is what we've done with some of the older ones. But a good example of one that we might come back to is our message history sanitizer. So, one thing we've had issues with or we had issues with early in the the development of Kira is that we would send these sort of invalid um sequences of messages because let's say the anthropic API required tools to be in the same order they were invoked and the responses but the system wasn't doing that. So we built this whole sanitizer system that has a bunch of requirements around um let's see very specifically yeah when conversation is validated the system shall verify that each user input is either non-MPT content or tool responses. So we had things where like empty strings would get passed in but there was a tool response. This is a good example where we've come in over time and actually just added maybe not to the requirements but to the to the acceptance criteria of the requirements as new validation rules are uncovered. Yeah. So how do you handle like that? So for example you have like telemetry up there y feature that needs telemetry is it going to go back and update that spec too or you're just it should. Yeah. So, if you usually you'll see and let me just ask uh a new chat here. No, that's a terrible idea. So here I've asked I've made a inspect mode I've made some requests to to um add UI telemetry to the thing I'll help you add it let me first check if there's any relevant runbooks then explore the codebase and sand the implementation it might go do a little bit of research here and then flip of a coin again it's an LLM so it may or may not discover the existing uh spec but ideally it will after doing its research say there exists a spec already for things like UI telemetry, I'm going to go and amend that one. Um, and if it doesn't in this case, like I would come in and just ask it to um as sort of the operator of the system. But over time, again, we want that to be easier for you as a user to not have to think about so much. We can watch it while it chugs along. Is there anything reconfigured in Kira that makes it better to work with AWS? trans. No, not really. Um, was that a question? Oh, question was, uh, is there anything in Kira that that's preconfigured to make it work better with AWS? No. Um, we are sort of purposefully we're in we are brought to you by AWS, which so you know, uh, Andy Jasse and Jeffy B pay my check, but um, we're not like an AWS product that's deeply deeply integrated with the rest of the AWS ecosystem. Now that said, I still answer emails when somebody says, "Why is this other thing we built with AWS not working with Curo?" Yay. But um similarly like if you're building on GC or Azure, whatever um or you're running some on-rem system, the product should work just as well for you. That's our goal. Good a good answer potentially is the AWS documentation MCP server. Yes. So there are MCP servers that you can add into any of these things that will make better. Yeah, that's a good point. So, like in this case, I actually had to add the AWS MCP documentation here. We could of course have natively bundled this, but I don't want to ship this to customers who don't need it. Yeah, because again, AWS is not the only docs that we might care about. Um, by the way, coming back to your question, so it did find the existing spec for telemetry. It read it, it read different sections of it, and now it's actually making amendments to it. So, we can follow the diff as it shows up here. So, it's added new uh requirements. um to the pre-existing specs. So, this is effectively another case where we're mutating the system as opposed to just adding this sort of never- ending spiel of specs. I guess what I'm wondering is like how did it know or decide where to put the spec, you know, if you break down your project into these different categories? Y I would imagine like crossover. Yeah. I mean, it's that that's sort of like software development in a nutshell though, right? like how do you actually define the seams between different parts of your system different concerns the product right but if you want to like build something like I have a task and it's going to cost require changing like three or four things y it's going to change three or four specs and then run tasks across three or four oh yeah yeah no it should not do that it would probably so again I don't have a good example off hand that we can do for that but um my my perspective would be that if you're working on something that is a crossf functional uh by the way the question was um if I'm working on something that let's say I have a spec for security requirements and I have a spec for API design uh like the API shapes and I have a spec for logging and I am changing something in the API public interface that is a securityf facing concern because we're redacting logging PII um I think that's maybe a semi-tangible use case uh that we can all imagine coming down from our governance teams um I want to I would imagine that you either pick one of those to load the requirements into or you create sort of a cross functional spec, but that would come down to I think you as a as an operator making that decision in much the same way that if I how you actually implement it might be you you would not necessarily implement my PII API redaction module. It's a standalone thing. It's going to be a crosscutting theme across your codebase, I'd imagine. And it's also a good example. There's like multi group workspace came out when it went to G on Monday and now you can like drag different. So like in your example you just went through with like APIs and off and like even the front ending you can bring in those projects if you have them separately and then still work. Yeah. Thanks bro. the mental model the spec generates the code after that like what code you can specify how does that work yeah so um we have now synthesized effectively the spec so we we sat down we defined the requirements design and task list I've had Kira now go through and run all the tasks in this spec so it ran them one at a time it basically worked on small bite-sized pieces of work uh chunk by chunk and then uh now this is done So what we've actually produced is not just like the completed spec, but it went here into my agent and it did a few things in the CDK repo because it's doing persistence to S3. I'm sure it added a bucket. Yep. Some new bucket encryption and yada yada. It then went in to the agent, added the S3 checkpoint saver. It looks like it, you know, created a checkpointer. It adds this to the graph and it kind of passes this all the way through the system. And the S3 checkpointer here I'm sure has some knowledge of how to write the checkpoints to and from S3. So like we have gone not just for defining the system but we've now um produced it end to end or we've uh delivered it end to end including property tests I believe. Um yeah. Oh, I have a answer to an earlier question related to like um some specific AWS related features like that makes it easier to work with. The Curo CLI comes with the use AWS tool which helps with the CLI. Yeah. Yep. So, uh, what Rob's pointing out is the Curo CLI, which we just rebranded, um, this week, has a use AWS tool, which is basically a wrapper over the AWS SDK, um, to make some of those things easy. Uh, but again, BYO use GCP tool as an FCP server if you were so inclined, if that's your uh, tool of choice. And I believe, don't quote me on this, um, because the CLI is kind of new to my new to me I should say. Um, but I believe you can turn off tools in the CLI as well. Let me know if that's not right, Rob. Yeah. So, that's like you're actually not strict. Uh, in the desktop product today, you can't control the tools, the native tools built in, but in CLI, you can. Um, so I I intuitively get the benefits of having a spec. Have you done any work to empirically see like how a project or a problem would have worked with or without? Yeah. Um we do have benchmarks uh covering the data off hand. Um I think part of that's in our blogs. So if you go to the cure.deblog or it's on the site, we we talk really crisply about some of the lift things like property based testing give to task accuracy. Science team's always working on that stuff. a blog about specs. I'm curious about Yeah. Distinguish engineer for databases. Yeah. His blog post really steps it up. I don't think it has the D specific that you are asking for, but I think it will be useful. Yeah. Yeah. How does it work? I understand the feature side of it, but how does it work in a nonfunctional site like agency dealing with, you know, a little bit more harder problems? Well, yeah. I mean, that is ultimately the goal here, right? Is we're saying you're making a slightly larger investment up front, but we believe that the uh the structure we're bringing is going to help you get increase the accuracy of your uh result. So, um, while we've got a team of people who are basically working on making spec better, my job when I fly back to Seattle is to make cur as a whole much faster. Um, one, execution time and like kind of like laggginess in the UI, but two, how do we get tokens through the system faster? How do we get responses to you faster so that like you're not syncing as much cost into KO to use a spec? Yeah. Yeah. I'm not talking about the KO tool itself, the code generated from the spec. Oh. Oh, yeah. Okay. Yeah. you mean like the non-functional requirements of the generated code? So, uh that's going to come down to I think what you're specifically trying to do. So, you could add uh one of the slides I had here was talking a little bit about how to tweak the process and tweak the artifacts for your use cases. Um again, you could very easily add something like I want non-functional requirements for speed and runtime and things like lock contention to be considered in the design phase. Um yeah, something you could certainly add. So you could generate a code in Rust or or Java. Yeah, totally. Yeah. And it will vary in the functional depending on what language you generated. I mean it would it would have to like yeah there's no other way I think to approach it. Um again I'm just I'm familiar with node so I'm doing everything here in node but you can use this with any language. I think technically we say we support Java, Python, JavaScript, um, and Jesus, JavaScript, TypeScript, Java, and Rust. But in practice, there's no reason that this doesn't work with any language. I mean, it's just an LLM. The there's nothing language specific or framework specific in the system. And for those of you um, so there was a conference earlier this week hosted by Tessle, which are doing sort of specs for knowledge base. um as long as you've got the right grounding docks in there and this is sort of uh their argument is that it should not matter what you're building like that's all just informed by the the context you're building for your system. This is also a really good point for steering. So steering you can get the agent to develop code in the way you want. Like being a developer is all about making trade-offs and the problem with your out of the box is it's like so polite because it's trying to be everything to everyone. U and especially like with latency and cost and other things like that, just tell it in steering what you want it to prioritize and then that will influence any code that gets generated. Yep. Even like how it designs based on that as well. So if there's something that's very specific to your use case or your industry or whatever, just shove it in that steering file and then Yeah, that's exactly right. So, for example, I I will have Kira generate um commits for me. And one of the things I care I personally care about is that I can track commits I generate versus commits that Kira generates being the ones that come from the system. And so my steering dock while short includes things like very specifically my requirement for Curo is just use the UI um attributed to the co-author of Kuro agent um which is trivial but also I want it to happen every time. So in this case it just generated a commit co-authored by Kirao agent D. So that's an example of like you could add whatever you want in there, not just something related to get commits, but you could do code style, you could do um uh you know code style, code coverage. Uh whenever you add a spec or you're adding a new module, make sure that you annotate it with coverage minimums that are 90% because that's the thing I care about. Um you can kind of put anything you want up in there. The good news is it looks like what we built works. Um, Cur is very happy with itself at least and it looks like all tests passed. But um, yeah, so we'll we can deploy this to the back end and see how things work. We're uh technically just about time. So, you know, if anybody has any other questions, I'm going to stick around here for a while. But uh, thank you all for joining, listening, and uh, learning a little bit more about Spectrum and Dev. Heat.

</details>