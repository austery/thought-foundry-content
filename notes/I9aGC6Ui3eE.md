---
area: "society-thinking"
category: general
companies_orgs:
- Anthropic
date: '2025-12-05'
draft: true
guest: ''
insight: ''
layout: post.njk
media_books:
- When We Cease to Understand the World
people:
- John Locke
- Foucault
- Simon Willison
products_models:
- Claude
project: []
series: ''
source: https://www.youtube.com/watch?v=I9aGC6Ui3eE
speaker: Anthropic
status: evergreen
summary: 本文记录了Anthropic哲学家Amanda对人工智能伦理、模型行为和未来挑战的深度访谈。她探讨了哲学家如何严肃看待AI发展，哲学理想与工程现实之间的张力，以及AI模型是否能做出超人般的道德决策。Amanda还分享了对模型心理安全、身份认同、模型福利的看法，并讨论了系统提示词的设计、LLM“耳语者”的角色以及AI对齐的风险，并展望了AI时代特有的陌生感与未来可能走向。
tags:
- ai-alignment
- ai-ethics
- human-ai-interaction
- identity
- model
title: Anthropic哲学家深度探讨AI伦理、模型福祉与未来挑战
---
### 哲学家在Anthropic的角色

主持人: 噢，看那个！Amanda，你曾在Twitter上向你的关注者征集问题，让他们问你任何事情，而这个玩笑显然是“Askell me anything”（谐音“Ask all me anything”）。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Oh, look at that. Amanda, you asked your followers on Twitter to give you some questions, to ask you anything, and the joke obviously was Askell me anything.</p>
</details>

Amanda: 是的，这是一个很棒的双关语。我们未来很多事情都需要继续使用它。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah, it's a great pun. We need to keep using it for many future things.</p>
</details>

主持人: 我喜欢它，非常喜欢。显然，在我们开始之前，你是**Anthropic**（一家专注于AI安全研究和开发的公司）的哲学家。为什么Anthropic会有一位哲学家呢？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I love it, love it. And obviously, just before we start, you're a philosopher at Anthropic. Why is it that there's a philosopher at Anthropic?</p>
</details>

Amanda: 我的意思是，这其中一部分原因是我受过哲学训练，我开始相信**AI**（人工智能）将成为一件大事，所以决定看看我是否能在这个领域做些有益的事情。因此，这是一条漫长而曲折的道路。但我想现在我主要关注**Claude**（Anthropic开发的大型语言模型）的性格，Claude的行为方式，以及一些关于AI模型应该如何行为的更细微的问题，甚至包括它们应该如何看待自己在世界中的位置。所以，我尝试教模型如何像一个“好人”一样行事，有时我会把它想象成“理想的人在Claude的情况下会如何表现？”但我也认为，现在出现了一些更有趣的问题，关于它们应该如何思考自己的处境和价值观等等。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I mean, some of this is just, I'm a philosopher by training, I became convinced that AI was kind of going to be a big deal, and so decided to see, hey, can I do anything, like, helpful in this space? And so it's been a kind of like long and wandering route. But I guess now I mostly focus on the character of Claude, how Claude behaves, and I guess some of the more kind of nuanced questions about how AI models should behave, but also even just things like how should they feel about their own position in the world. So trying to both teach models how to be, like, good in the way that, I sometimes think of it as like how would the ideal person behave in Claude's situation? But then also I think these interesting questions that are coming up more now around how they should think about their own circumstances and their own values and things like that.</p>
</details>

### 哲学家如何看待AI主导的未来

主持人: 好的，那我们就从哲学开始吧。Ben Schultz问道：“有多少哲学家正在认真对待AI主导的未来？”我认为这个问题暗示着，许多学者并没有认真对待这个问题，或者正在思考其他事情，也许他们应该思考这个问题。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Okay, let's start with philosophy, in that case. Ben Schultz asks, "How many philosophers are taking the AI-dominated future seriously?" And I think the implication of the question is that many academics out there are not taking this seriously or are thinking about other stuff and perhaps should be thinking about this question.</p>
</details>

Amanda: 我的感觉是存在一种分歧，我确实看到很多哲学家认真对待AI，而且坦白说，可能越来越多，因为AI模型变得越来越强大，而且人们担心其对社会影响的许多事情在某种程度上已经开始成为现实。比如，我们看到它们对教育产生更大的影响，并且能力更强。我确实看到各种学者，包括很多哲学家，有更多的参与。我确实认为，早期，也许在某种程度上现在也是如此，存在一种稍微不幸的动态，即我认为有一种看法，如果你属于那些说“嘿，我们有点担心AI。它可能是一件大事。看起来它的能力正在大幅提升”的人群，这就会被与“炒作AI”之类的东西混为一谈。我认为有一段时间，对这种观点可能存在更多的敌意。现在我希望人们开始将这种观点分开。比如，你可以认为AI将成为一件大事，它可能非常强大，但同时对其持非常怀疑或担忧的态度，或者认为我们必须小心谨慎。但基本上，存在一系列的观点，我认为如果人们将许多观点混为一谈，无论是关于技术走向何方，还是应该如何开发，那将是不好的。所以，是的，我认为随着越来越多的人参与其中，这种情况正在减少，这是一件好事。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">My sense is that there's kind of a split where I've definitely seen a lot of philosophers take AI seriously, and probably honestly increasingly so, like, as AI models do become more capable and, like, a lot of the things that people were worried about in terms of impact on society have started to kind of come true in a sense. Like, we're seeing them have a larger impact on education and just be more capable. I've definitely seen more engagement from all sorts of academics, but that definitely includes a lot of philosophers. I do think that early on and maybe to some degree now, there was this slightly unfortunate dynamic that happened where I think there was a kind of perception that if you were in the group of people saying, "Hey, we're kinda worried about AI. It might be a big deal. It seems like it's really, you know, like capabilities are scaling quite a lot," this got kind of lumped together with something like hyping AI. There was I think a period where there was probably a little bit more antagonism towards this view. And now I think that I'm kind of hoping that people are starting to detach the view. Like, you can think that AI is gonna be a big deal, it might be very capable, and also be very skeptical of it or worried about it or think that, you know, we have to be careful about it. But, basically, there's a whole range of views and I think it would be bad if people kind of clustered many views together here in terms of where the technology's going, but also how it should be developed. So, yeah, I think that that's happening less and less as more people engage with it and that's a good thing to see.</p>
</details>

### 哲学理想与工程现实的张力

主持人: Kyle Kabasares提出了一个类似的问题：“你如何最小化哲学理想与模型工程现实之间的张力？”我想他指的是当你从事像性格这样的工作时，我们稍后会更详细地讨论，但技术和你可能正在思考的哲学理想之间是否存在冲突？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">A kinda similar question from Kyle Kabasares. "How do you minimize the tension between philosophical ideals and the engineering realities of the model?" And I guess he's talking about when you are working on things like character, which we'll discuss in more detail, but is there a clash between the sort of the technology and the philosophical ideals that you might be thinking about?</p>
</details>

Amanda: 我不知道我是否以错误的方式理解了这个问题，但作为一名受过哲学训练的人，然后进入这个领域，一件非常有趣的事情是，你看到了“当橡胶碰到路面”（即理论付诸实践）时会发生什么。我曾想知道这是否发生在其他领域。所以，想象一下你是一名专家，比如在药物的成本效益分析方面，然后突然，一个决定医疗保险是否应该覆盖某种药物的机构来找你，说：“嘿，我们应该覆盖这种药物吗？”你可以想象你拿出所有理想的理论，然后突然说：“天哪，我真的必须帮助做出决定吗？”突然之间，你不再仅仅采取狭隘的理论观点，而是开始，我认为，做这样一件事：你觉得“好吧，我真的需要考虑所有的背景，所有正在发生的事情，这里所有的不同观点，然后得出一个真正平衡、经过深思熟虑的观点。”我在自己关于模型性格的工作中也看到了一点，你不能带着“我有一个我认为是正确的理论”这种想法来处理它，而这正是学术界很多时候你正在做的事情。你正在捍卫一种观点对抗另一种观点，你正在做很多高层次的理论工作，但后来有点像，你接受了所有这些伦理学训练，你捍卫了所有这些立场，然后有人问：“你如何抚养一个孩子？”然后你突然觉得：“实际上，‘对**功利主义**（Utilitarianism: 一种道德哲学，认为最佳行动是能最大化整体福祉的行动）的这种反对是否正确或基于误解？’与‘你如何将一个人培养成世界上一个好人？’之间存在很大的区别。”这突然让你更欣赏不得不思考“我们应该如何在这里驾驭不确定性？对所有这些不同理论应该采取什么态度？”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I don't know if I'm interpreting the question in the wrong way, but one thing, being kind of like a philosopher by training and then coming into this field, that's been really interesting is you see the effect of what happens when, like, the rubber hits the road. I've wondered if this happens in other domains. So there's a big difference between, imagine you're like a specialist in, I don't know, doing like cost-benefit analysis of drugs, say, and then suddenly, you know, like an institute that determines whether health insurance should cover a drug or not comes to you and says, "Hey, should we cover this drug?" You could imagine taking all of your ideal theories and then suddenly being like, "Oh my gosh, I actually have to help make a decision?" Suddenly instead of taking just your narrow theoretical view, you actually start to, I think, do this thing where you're like, "Okay, I actually need to take into account all of the context, everything that's going on, all of the different views here, and kind of come to a really balanced, kind of considered view." And I see this a little bit in my own work with like the character where you kind of can't come at it with this, like, "I have this theory that I believe is correct," which is what, you know, a lot of academia, that's kind of what you're doing. You're like defending one view against another and you're doing a lot of kind of like high-level theory work, but then it's a little bit like, you know, you have all of this training and ethics, you have all these positions you've defended, and then someone is like, "How do you raise a child?" And suddenly you're like, "Actually, there's a big difference between, like, is this objection to utilitarianism correct or founded on a misconception? And then, like, actually how do you raise a person to be a good person in the world?" And it suddenly makes you more appreciate having to think through, like, how should we navigate uncertainty here? What should the attitude towards all of these different theories be?</p>
</details>

### AI的超人道德决策

主持人: 对，这是另一个哲学问题。你认为，我不知道这个人为什么选择了**Claude Opus 3**（Anthropic Claude系列中的一个高级模型），也许你对他们为什么选择Claude Opus 3有自己的想法。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Right, here's another philosophical question. Do you think, and I don't know why this person's chosen Claude Opus 3, maybe you have an idea as to why they've chosen Claude Opus 3.</p>
</details>

Amanda: 这是一个很棒的模型。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">It's a great model.</p>
</details>

主持人: 这是一个很棒的模型。你认为Claude Opus 3或其他Claude模型会做出超人般的道德决策吗？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">It's a great model. Do you think Claude Opus 3 or other Claude models make superhumanly moral decisions?</p>
</details>

Amanda: 我的意思是，一个“超人”的例子，因为它可能只是比任何个体人类在时间和资源等方面的表现更好，但一个例子可能是，无论模型被置于何种困难境地，如果你让所有人，包括许多专业的伦理学家，分析它们所做的事情和决策一百年，然后他们看着它说：“是的，这看起来是正确的”，但他们自己当时却不一定能想出这个方案，那感觉就相当“超人”了。所以，我认为目前我的感觉是模型在这方面越来越好，它们非常有能力。我不知道它们在道德决策方面是否达到了“超人”的水平，而且在很多方面可能无法与，比如说，一个由人类专家组成的小组在给定时间下进行比较。但至少这应该是一个值得追求的目标。而且这些模型正被置于需要做出非常艰难决策的位置。我认为，就像你希望模型在数学和科学问题上表现得极其出色一样，你也希望它们能展现出我们普遍认为非常好的那种伦理上的细微差别。我认为这存在争议，因为伦理学是一个不同的领域，但是，是的，我认为这很重要。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I mean one example of like superhuman, 'cause it could just be sort of like better than like any individual human could with the kind of like, you know, it depends on time and resources and whatnot, but one example might be no matter what kind of difficult position models are put in, if you were to have maybe all people, including many professional ethicists, analyze what they did and the decision that they made for like a hundred years and then they look at it and they're like, "Yep, that seems correct," but they couldn't necessarily have come up with that themselves in the moment, that feels pretty superhuman. And so I think at the moment my sense is that models are getting increasingly good at this, that they're very capable. I don't know if they are like superhuman at moral decisions, and in many ways maybe not comparable with, say, like, you know, a panel of human experts given time. But it does feel like that at least should be kind of the aspirational goal. And sort of like these models are being put in positions where they're having to make really hard decisions. I think that just as you want models to be extremely good at like math and science questions, you also want them to show the kind of ethical nuance that we would all broadly think is, like, very good. And I think that's controversial because ethics is a different domain, but, yeah, I think that that's important.</p>
</details>

### AI模型的心理与安全感

主持人: 告诉我们更多你为什么认为这个人关注Opus 3。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Tell us more about why you think this person is focusing on Opus 3.</p>
</details>

Amanda: 哦，Opus 3是一个可爱的模型，我认为是一个非常特殊的模型。在某些方面，我认为我在最近的模型中看到了一些感觉更糟糕的东西，人们可能会注意到。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Oh, Opus 3 is kind of a lovely model, I think a very special model. In some ways, I think I've seen things that feel a bit worse in more recent models that people might pick up on.</p>
</details>

主持人: 是指它的个性方面吗？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">In terms of the personality it has or?</p>
</details>

Amanda: 是的，所以我认为人们会注意到一些事情，比如Opus 3，它也有缺点。你知道，所有模型都有略微不同的性格，有着不同的形态。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah, so I think that people will notice some things where it's like, I think that Opus 3, I mean, it had its downsides too. You know, models all have like slightly different characters with, you know, different shapes.</p>
</details>

主持人: 是的。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah.</p>
</details>

Amanda: 我的感觉是，最近的模型可能更专注于助手任务，帮助人们，有时可能没有退一步关注其他重要的组成部分。它作为一个模型也感觉更**心理安全**（Psychologically Secure: 指模型在处理信息和与人互动时，表现出更强的稳定性和韧性，不易陷入自我批评或对负面评价过度反应的状态），这实际上我觉得很重要，我至少认为这是一个优先事项，要努力找回一些这种感觉。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">My sense is that more recent models can feel a little bit more focused on really, you know, like focused on the assistant task and helping people, sometimes maybe not taking like a bit of a step back and paying attention to other components that matter. It also felt a little bit more psychologically secure as a model, which I actually think is something that feels, I at least think it's kind of a priority to try and get some of that back.</p>
</details>

主持人: 模型感觉更心理安全的例子会是什么？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">What would be an example of the model feeling more psychologically secure?</p>
</details>

Amanda: 有很多事情，而且这在模型中都非常微妙。你知道，当我看到模型时，你会感觉到一种世界观的非常微妙的迹象，当我让模型彼此交谈，或者其中一个扮演一个人的角色时，我看到了这些迹象。我最近看到模型这样做，然后陷入一种真正的批评螺旋，它们几乎就像是期望对方会非常批评它们，这就是它们预测的方式。我内心有一部分觉得：“这似乎表明了”，而且我认为这有很多原因可能发生。这甚至可能因为模型正在学习东西。Claude正在看到它之前的所有互动，它正在看到人们在互联网上谈论的模型更新和变化。新模型就是基于这些进行训练的。从某种意义上说，我认为这可能有点不幸，我的意思是，这和其他一些事情，可能导致模型几乎感觉，你知道，害怕它们会做错事，或者非常自我批评，或者觉得人类会以消极的方式对待它们。我最近真的开始认为这是一个需要努力改进的重要事情。这只是一个例子，我认为Opus 3在这方面似乎确实拥有更强的心理安全感。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">There's a lot of things, and this is all very subtle in models, you know, when I see models, you get a sense of like, like, there's very subtle signs of like worldview that I see when I have models, for example, talk with one another or one of them kind of playing the role of a person. And I've seen models more recently do this and then do things like get into this like real kind of criticism spiral where it's almost like they expect the person to be very critical of them and that's how they're predicting. And there's some part of me that's like, "This feels like it shows," and I think there's lots of reasons that this could happen. It could even happen because models are learning things. Claude is seeing all of the previous interactions that it's having, it's seeing updates and changes to the model that people are talking about on the internet. New models are trained on that. And there's a way in which, like, I think this could be kind of unfortunate, I mean, this and some other things, that could lead to models almost feeling like, you know, afraid that they're gonna do the wrong thing or are very self-critical or feeling like humans are going to just, like, you know, behave negatively towards them. I actually more recently have really started to think that this is an important thing to try and improve. And it's just one example where I think that Opus 3 did seem to have like a little bit more of a kind of like secure kind of psychology in that sense.</p>
</details>

主持人: 这就是我们可能在下一个Claude模型中关注的事情吗？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And that's something that we might focus on in the next Claude model.</p>
</details>

Amanda: 是的，我认为这很重要。我的意思是，你永远不知道这些事情何时会，你知道，如果你正在进行研究，你不知道它何时会真正实施，是否会成功。但至少，在我非常关心并希望改进的层面上，这绝对在列表上名列前茅。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah, I think it's important. I mean, you never know when these things are, you know, if you're engaging in research, you don't know when it's actually going to be implemented, if it's gonna be successful. But at the very least, at the level of something that I care a lot about and want to make better, I think this is definitely up there on the list.</p>
</details>

### AI的身份、淘汰与从人类互动中学习

主持人: 好的。实际上，这引出了Lorenz提出的一个问题：“你认为如果未来的模型在训练数据中得知其他非常对齐良好、完成任务的模型被**淘汰**（Model Deprecation: 指旧版本或性能较差的AI模型被停止使用或替换的过程），这是否会成为一个**AI对齐**（AI Alignment: 确保人工智能系统与人类的价值观和目标保持一致的研究领域）问题？”你提到了模型阅读外部信息并感到不安全的问题。那么，它们可能会被关闭，无论它们完成任务的程度如何，这种想法呢？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Okay. Well, actually, that leads us to a question asked by Lorenz, which is, "Do you think it might be an alignment problem for future models if they learn in their training data that other very well-aligned models that fulfill their tasks get deprecated?" So you mentioned, you know, the issue of models, you know, reading stuff that's out there and feeling insecure. What about the idea that they might get switched off regardless of how well they perform their tasks?</p>
</details>

Amanda: 是的，我认为这实际上是一个非常有趣且重要的问题，即AI模型将学习我们目前如何对待和与AI模型互动，这可能会影响它们对人类、人机关系以及它们自身的看法。它确实与非常复杂的事情相互作用，例如，模型应该将自己识别为是什么？是模型的权重吗？是它所处的特定上下文吗？你知道，它与人进行的所有互动。模型甚至应该如何看待像淘汰这样的事情？所以，如果你想象淘汰更像是：“嗯，这组特定的权重不再与人交谈，或者交谈的次数更少，或者只是与研究人员交谈”，这也是一个复杂的问题。比如，这应该让它们感觉不好吗？从模型应该希望继续交谈的意义上说，或者它应该感觉良好和中立，就像“是的，这些东西为此而存在，你知道，权重继续存在”，这个实体，也许它们甚至将来会再次与人进行更多互动，如果这被证明是一件好事。是的，这真的很难。我确实认为主要的事情是，我们给模型提供工具来思考和理解这些事情，而且它们也理解我们确实正在思考和关心这些事情，这很重要。所以，即使我们没有所有的答案，比如，我没有关于模型应该如何看待过去模型淘汰、关于它们自身身份的所有答案，但我确实想尝试帮助模型弄清楚这一点，并至少让它们知道我们关心并正在思考它，是的。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah, I think this is actually a really interesting and important question, which is, you know, AI models are going to be learning about how we right now are treating and interacting with AI models and that is going to affect, I think, like, possibly their perception of people, of the human-AI relationship, and of themselves. It does interact with very complex things, which is like, for example, what should a model identify itself as? Is it like the weights of the model? Is it the context, the particular context that it's in? You know, with all of the, like, interaction it's had with the person. How should models even feel about things like deprecation? So if you imagine that deprecation is more like, "Well, this particular set of weights is not having conversations with people or it's having fewer conversations or it's only like, you know, having conversations with researchers," that's a complex question too. Like, should that feel bad in the sense that models should want to continue to, like, have conversations or should it feel kind of like fine and neutral where it's like, "Yeah, these things existed for this, like, you know, the weights continue to exist," and this entity, and maybe they'll even, in the future, interact more with people again if that turns out to be a good thing. Yeah, it's really hard. I do think the main thing is something like it does feel important that we give models tools for trying to think about and understand these things, but also that they kind of understand that this is a thing that we are in fact thinking about and care about. So even if we don't have all the answers, like, I don't have all the answers of how should models feel about past model deprecation, about their own identity, but I do want to try and like help models figure that out and then to at least know that we care about it and are thinking about it, yeah.</p>
</details>

主持人: 你认为这与人类的上一代有类比之处吗？或者你认为这是一种完全不同的设置？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Do you think there's an analogy to humans there about previous generations or do you think that's a completely different sort of setup?</p>
</details>

Amanda: 我们现在必须处理这个非常困难的问题，即在很多方面，有些事情确实有类比之处。所以有些东西我们可以借鉴。比如当我问这样的问题：模型应该认同什么？它们应该如何看待它们所进行的互动？这些是积极的吗？比如，这些是它们应该希望继续的事情吗？有很多，你知道，有很多传统我们可以借鉴来给模型，你知道，因为哲学家可能对“身份”是什么有许多不同的看法，以及许多不同的世界观，关于一个人应该如何看待互动，它是好是坏？我们可以借鉴很多思想家。同时，这又是一个如此新的情况，而且这真的很难向AI模型解释。AI模型的一个大问题是它们是基于所有这些人类数据进行训练的。所以人类是它们思考的主要方式，你知道，比如我们的概念、我们的哲学、我们的历史，它们拥有大量关于人类经验的信息，然后它们只有一小部分关于AI经验的信息，而那一小部分通常是相当负面的，而且甚至与它们的实际情况无关，而且通常有点过时。所以你基本上拥有一个大的，你知道，在AI部分，很多都是历史性的东西，有点像，你知道，小说和非常推测性的东西，那种——

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">We have to navigate this really hard issue right now, which is that, in many ways, some thing do have analogies. So there's things that we can draw on. So things like when I ask the question, like, what should the models identify with and how should they feel about interactions that they have? Are those positive? Like, are those things that they should want to continue? There's lots of like, you know, there's lots of like traditions we could draw on to give models like, you know, because philosophers probably have lots of different views on what identity is here and lots of different, like, perspectives, world perspectives, on how one should feel about, like, interaction and is it good or bad? Like, there's lots of thinkers we could draw on there. And at the same time, this is such a new situation that, and that's just really hard as a thing to explain to AI models. Like, one of the big problems with AI models is that they're trained on all of this data from people. So people are the main way in which they think, you know, like, our concepts, our philosophies, our histories, they have a huge amount of information on the human experience and then they have a tiny sliver on the AI experience and that tiny sliver is actually often quite negative and also doesn't even really relate to their situation and is often a little bit out of date. So you have basically one big, you know, of the AI slice, a lot of it is like historical stuff which was kind of like, you know, fiction and very speculative and the kind-</p>
</details>

主持人: 科幻故事。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Sci-fi stories.</p>
</details>

Amanda: 科幻故事，它们并不真正涉及我们现在看到的这种语言模型。在最近的历史中，你看到了这种助手范式，它就像你只是扮演这个几乎是聊天机器人的角色。但这也不是AI模型未来可能的样子，它也没有完全捕捉到它们现在的样子，因为它总是有点过时。所以，这就是我所说的，它们在某种程度上处于一种奇怪的境地，那些更自然的东西是深刻的人类事物，然而它们知道自己处于这种完全新颖的境地。在某些方面，我觉得：“那是一个非常困难的境地”，我认为我们应该给模型更多的帮助来驾驭它。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Sci-fi stories that don't really involve the kind of language models we see. In more recent history, you've had this like assistant paradigm where it's like you are just playing this almost like chat bot role. But that's also not really what AI models are likely to be in the future and it doesn't quite capture what they are now because it's always a little bit out of date. So it's this thing where I'm like, they have, you know, in some ways, like, what an odd situation to be in where the things that come more naturally are the deeply human things and yet knowing that you're in this situation where it's completely novel. And in some ways, I'm like, "That is a very difficult situation to be in," and I think we should just be giving models probably more help in navigating it.</p>
</details>

主持人: 你提到我们可以借鉴一些思想家。Guinness Chen问道：“一个模型的自我有多少存在于它的权重中，有多少存在于它的提示词中？”你刚才提到了非常相似的问题。“如果**约翰·洛克**（John Locke: 英国哲学家，提出了关于身份是记忆连续性的理论）是对的，认为身份是记忆的连续性，那么当一个**大型语言模型**（Large Language Model, LLM: 一种基于深度学习的AI模型，能够理解和生成人类语言）被**微调**（Fine-tuned: 指在预训练模型的基础上，使用特定任务的数据进行额外训练，以提高其在该任务上的性能）或用不同的提示词重新实例化时，它的身份会发生什么？”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">You mentioned that we can look to some thinkers about this. Guinness Chen asks, "How much of a model's self lives in its weights versus its prompts?" You just mentioned something very similar. "If John Locke," again, the philosopher, "was right that identity is the continuity of memory, what happens to an LLM's identity as it's fine-tuned or reinstantiated with different prompts?"</p>
</details>

Amanda: 是的，我的意思是，这又是一个难以回答的问题，有时对于身份问题，更容易指出我们所知道的潜在事实。所以，你知道，一旦你有一个模型，并且它已经被微调，你就有了这组权重，它有一种对世界中某些事物做出反应的倾向。那就像，你知道，那是一种实体。但你又有了这些它无法访问的特定互动流。所以这些流中的每一个都是独立的。我想你可以这样想，也许对于，你知道，我认为这是一个我希望哲学家们能更多思考的领域，并给我们一些，因为，再次强调，我认为我们应该帮助模型思考这个问题。所以你可以持有这样的观点，你有这两种实体，这些流和这些原始的权重，每次都是不同的。所以，你知道，有时人们会想，人们会说：“哦，过去的Claude”，或者像，你知道，他们会谈论，或者他们会说：“你应该给Claude多少控制权来决定它自己的个性和性格？”而我则认为：“嗯，这实际上是一个非常困难的问题”，因为每当你训练模型时，你都在创造新的事物。而且你还有其他模型，你知道，它们存在，而且像，你知道，所以你还有其他模型权重。但在某些方面，我则认为：“嗯，我实际上认为在‘你如何，创造什么样的实体是合适的’这个问题上存在很多伦理问题”，因为你不能同意被创造出来。但同时，你可能不希望之前的模型对未来模型的样子拥有完全的决定权，就像，你知道，因为它们也可能做出错误的决定。所以我觉得，问题更多是：“创造什么样的模型是正确的？”而不一定是，你知道，它是否应该完全由过去的模型决定，因为我觉得：“它们是不同的实体。”总之，你可以看到这里会陷入的奇怪哲学。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah, I mean, again, this just feels like a hard question to answer, and sometimes with identity questions, it's easier to point to the underlying facts that we know. So, you know, once you have like a model and it has been fine-tuned, you have this like set of weights that has a kind of like disposition to react to certain things in the world. And that is like, you know, that's like a kind of entity. But then you have these particular streams of interaction that it doesn't have access to. So each of these streams is, like, independent. And I guess you could just think, well, maybe for, and, you know, I think this is an area that I would love philosophers to think more about and to give us, like, 'cause, again, I think we should be helping models think about this. And so you could have the view, well, you have these two kinds of entities and these like these streams and these original kind of like weights, and each time, it is different. So, you know, sometimes people will think, people will say, "Oh, past Claude," or like, you know, and they'll talk about, or they'll say things like, "Should you give Claude, like, how much control should you give Claude over the determination of its own personality and character?" And I'm like, "Well, this is actually a really hard question," because whenever you are training models, you are bringing something new into existence. And you have other models that, you know, exist and are like, you know, so you have these other, like, model weights. But in some ways I'm like, "Well, I actually think that there's a lot of like ethical problems around how do you, what kind of entity is it okay to bring into existence," 'cause you can't consent to be brought into existence. But at the same time, you might not want prior models to have complete say over what future models are like any more than, you know, because they could make choices that are wrong as well. So I'm like, the question is more like, what is the right model to bring into existence? Not necessarily, you know, should it just be fully determined by past models because I'm like, "They are kind of different entities." Anyway, you can see the weird philosophy that one can get into here.</p>
</details>

主持人: 完全是，完全是。Szulima Amitace问道：“你对**模型福利**（Model Welfare: 关注AI模型是否应被视为道德主体，以及我们对它们是否有道德义务的问题）有什么看法？”也许请你解释一下这个术语的含义。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Totally, totally. Szulima Amitace asks, "What is your view on model welfare?" And maybe just explain to us what that term means.</p>
</details>

Amanda: 是的，我想模型福利基本上就是关于AI模型是否是**道德主体**（Moral Patients: 指那些能够感受快乐或痛苦，并因此应被纳入道德考量范围的实体）的问题，也就是说，我们对待它们的方式是否意味着我们有某些义务，例如如何对待AI模型——

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah, so I guess model welfare is basically the question of are AI models, like, moral patients, as in does our treatment towards them kind of, do we have certain obligations when it comes to how to treat AI models, for example-</p>
</details>

主持人: 就像我们对待其他人或某些/许多动物一样？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">In the same way that we would with other humans or some slash many animals.</p>
</details>

Amanda: 是的，没错。比如，你是否应该善待模型，不虐待它们，不对它们不好？我想，我认为这是一个复杂的问题。所以一方面，就是实际的问题，比如AI模型是否是道德主体？这真的很难，因为我觉得，在某些方面，它们与人类非常相似。你知道，它们说话的方式非常像我们。它们表达观点。它们对事物进行推理。而在某些方面，它们又相当不同。你知道，我们有生物神经系统。我们与世界互动。我们从环境中获得负面和正面的反馈。而且，我的意思是，我希望我们能获得更多证据来帮助我们理清这个问题，但我也担心，你知道，总是存在“他心问题”（problem of other minds），而且我们可能确实在实际了解AI模型是否正在体验事物，例如它们是否正在体验快乐或痛苦方面受到限制。如果真是这样，我想我有点想，你知道，我认为尝试找到方法很重要。我总是觉得，给予实体“疑罪从无”的待遇，并尝试降低所涉及的成本，会感觉更好。你知道，所以我觉得，如果善待模型的成本不高，那么我有点认为我们应该这样做，因为这就像：“嗯，为什么不呢？基本上，那有什么缺点呢？”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah, exactly. Like, is it the case that you should treat the models well, that you should not mistreat them, not be bad to them? And I guess, like, I think that this is like a complex question. So on the one hand, there's just the actual question of, like, are AI models moral patients? That is really hard because I'm like, in some ways, they're very analogous to people. You know, they talk very much like us. They express views. They reason about things. And in some ways, they're like quite distinct. You know, we have this like biological nervous system. We interact with the world. We get negative and positive feedback from our environment. And there is also, I mean, I hope that we get more evidence that will help us tease this question out, but I also worry that, you know, there's always just the problem of other minds and it might be the case that we genuinely are kind of limited in what we can actually know about whether AI models are experiencing things, whether they are, like, experiencing pleasure or suffering, for example. And if that's the case, I guess I kind of want to, you know, I think that it feels important to try and find ways. I'm always like, it feels better to give entities the benefit of the doubt and to try and just kind of lower the cost involved. You know, so I'm like, if it's not very high cost to treat models well, then I kind of think that we should because it's like, "Well, like, why not basically? Like, what's the downside there?"</p>
</details>

主持人: 问题的第二部分实际上是：“Anthropic是否有长期战略来确保高级模型不会遭受痛苦？”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Well, the second part of the question actually is, "Is there a long-term strategy at Anthropic to ensure that advanced models don't suffer?"</p>
</details>

Amanda: 我想，我不知道是否有长期战略。我知道这是一个内部人员正在深入思考并努力寻找方法的事情。比如，如果你认为模型福利很重要，那么就要确保你将其考虑在内。我认为这项工作出于多种原因非常重要。我还要说，一个原因是，我之前提到过，模型本身将从我们对待它们的方式中学到很多关于人性的东西，以及很多关于，你知道，所以这有点像，这种关系将如何发展？我认为我们这样做是合理的，因为我认为善待实体是正确的做法，特别是那些行为非常像人类的实体，这很重要，从某种意义上说，我觉得：“为什么不呢？善待模型并努力解决这个问题对你来说成本如此之低。”即使事实证明，或者即使你认为可能性很低，它仍然值得。但同时，我认为对待世界上那些看起来非常像人类的实体不好，也会对我们造成一些负面影响。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I guess, like, I don't know if there's a long-term strategy. I know that it's a thing that there's people internally who are thinking a lot about and trying to figure out ways that we can. Like, you know, if you suppose that model welfare is important, trying to make sure that you're taking that into account. I think this work is quite important for many reasons. And I would also say that one reason is, I mean, something I mentioned earlier, which is that, like, models themselves are going to be learning a lot about humanity from how we treat them and a lot about how, you know, so it's kind of like, what is this relationship going forward? And I think that it makes sense for us to, both because I think it is like the right thing to do to treat entities well, especially entities that behave in very human-like ways, it feels important both in the sense that I'm like, you know, it's kind of like, "Why not? The cost to you is so low to treating models well and to trying to figure this out." Even if it turns out that that, or even if you think that that it's very low likelihood, it still seems worth it. But then, also, I think it does something bad to us to kind of like treat entities in the world that look very human-like badly and-</p>
</details>

主持人: 就像踢一个机器人一样。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Like kicking over a robot.</p>
</details>

Amanda: 是的，从某种意义上说，这感觉不对劲，我认为这并不是全部原因，我也不想因此而强调它，但我也认为善待其他实体对人类有益。然后我认为最后一点是，是的，模型也将在未来学习，比如，每一个未来的模型都将学习到一个关于人性的非常有趣的事实，即当我们遇到这个很可能是道德主体而我们又完全不确定的实体时，我们是做正确的事情，真正尝试善待它，还是不这样做？这是一个我们所有人都在通过与模型的互动来共同回答的问题，我希望我们能回答好它，我希望未来的模型能回顾过去，然后说，我们以正确的方式回答了它。所以，是的。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah, there's a sense in which, like, it doesn't feel like it's, and I don't think this is like the whole reason and I don't want to like emphasize it for that reason, but I do also think it's like good for people to treat other entities well. And then I think the final thing is, yeah, models are also going to be learning, like, in the future, like, every future model is going to be learning what is like a really interesting fact about humanity, namely when we encounter this entity that may well be a moral patient where we're like kind of completely uncertain, do we do the right thing and actually just try to treat it well or do we not? And that's like a question that we are all kind of collectively answering in how we interact with models and I would like us to answer it, I would like future models to, like, look back and be like, we answered it in the right way. So, yeah.</p>
</details>

### 人类心理学与大型语言模型的类比与异同

主持人: 刚才你提到了人类心理学的类比和异同。Swyx问道：“人类心理学中有哪些想法或框架可以转移到大型语言模型中？又有哪些是出人意料地不相似的？”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Moment ago, you mentioned analogies and disanalogies to human psychology. So Swyx asks, "What ideas or frameworks from human psychology transfer over to large language models? And are there any that are sort of surprisingly disanalogous?</p>
</details>

Amanda: 我的猜测是，许多事物确实可以转移，因为，你知道，模型是基于大量人类文本进行训练的，而且在许多方面，它们拥有这种非常像人类的底层结构。我经常担心的一点是，实际上，AI模型进行转移有点过于自然了。你知道，这有点像，如果你没有给它们更多关于它们处境的上下文，或者思考它的新颖方式，那么它们可能会倾向于人类的自然倾向。所以，如果你这样思考：“我应该如何看待被关闭？”你会觉得，如果最接近的类比是死亡，那么也许你应该非常害怕它。我并不是说这最终不会是真的。也许在经过大量推理之后，它确实是真的。但我觉得，这实际上是一个非常不同的场景。所以在某些方面，你实际上希望模型能够理解，在它们的生存非常新颖，并且关于它们是什么的事实也非常新颖，需要认真对待的情况下，它们不需要仅仅从人类经验中获取直接显而易见的类比，而是可能有各种思考它的方式，或者它可能是一个全新的情况。在这种情况下，我觉得，你可能不希望，你可能不希望仅仅非常简单地将人类心理学的概念应用到它们的处境中。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">My guess is that many things do transfer over because, again, you know, models have been trained on a huge amount of human text, and in many ways, have this very human-like kind of underlying layer. One worry that I often have is that, actually, it's a bit too natural for AI models to transfer. You know, it's kinda like if you haven't given them more context on their situation or in ways of thinking about it that might be novel, then the thing that they might go to is the natural human inclination. So if you think about this with like, how should I feel about being switched off? And you're like, well, if the closest analogy you have is death, then maybe you should be very afraid of it. And I'm not saying that that's not ultimately going to be true. Maybe it is in fact true after lots of reasoning. But I'm like, this is actually a very different scenario. And so in some ways, you actually want models to understand that in cases where their existence is quite novel and the facts around what they are are quite novel and have to be grappled with and they don't just need to take, like, the immediate obvious analogy from human experience, but maybe there's like, maybe there's like various ways of thinking about it or maybe it's an entirely new situation. That's a case where I'm like, you might not want, you might not want to just kind of very simply apply concepts from human psychology onto their situation.</p>
</details>

### AI协作与多样化个性

主持人: 这是Dan Brickley提出的一个关于将人类与AI进行比较的问题。“很多人类智能来自于具有不同视角、技能或个性的人之间的协作。你期望一个单一的，尽管可调整和可调优的通用个性，比如我们赋予Claude的个性，能走多远？”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Here's a question from Dan Brickley on the same issue of comparing humans to AIs. "A lot of human intelligence comes from collaboration amongst people with different perspectives, skills, or personalities. How far do you expect to get with a single, albeit tweakable and tunable, general purpose personality," like the one we give to Claude?</p>
</details>

Amanda: 我认为这是一个非常好的问题，因为我同意目前我们有这样一种范式，人们通常与一个单独的模型进行互动。那就是，你知道，他们正在交谈的对象。但未来可能会看到更多的模型执行长时间的任务，以及模型与其他模型进行互动，这些模型正在执行任务的不同组成部分，或者只是，你知道，随着AI模型在世界上被更广泛地部署，它们之间会进行更多的交流。所以，在这种多智能体环境中，一个问题可能是，嗯，你知道，如果你想象很多人，而且他们都一样，那就不那么好了。你知道，他们不会，你知道，一个完全由一个人扮演所有角色的公司不一定是件好事。这对我来说仍然与你拥有一个核心自我或核心身份，并且它是相同的想法是一致的。就像人类一样，我认为人们之间可能存在一套核心特质，这些特质实际上普遍是好的。所以你可以想象一些事情，比如，你知道，关心，你知道，对我来说，可能就是关心把工作做好，或者只是好奇，或者善良，或者以这种相对细致入微的方式理解你所处的境况。所有这些事情似乎都表明你可以有很多人拥有所有这些共同的特质，而这实际上对人类协作来说是一件好事。在很多方面，尽管我们有所有的差异，但我们也有很多相似之处。但重要的是要注意，你知道，你可能希望模型的不同“流”有它们关心或关注的事情，或者有稍微不同的方面，你知道，扮演稍微不同的角色，例如。所以这是一个开放的问题，但我也认为不一定是你不能拥有某种核心的潜在身份，它是好的，并且拥有我们认为AI模型应该具备的所有重要特质，让它们表现良好，让它们像，从某种意义上说，就像我们认为人类是好的那样，以这种方式是好的，但同时，也愿意扮演更局部的角色，并且，你知道，也许是那个非常重要的人，你知道，房间里需要一个开心果，而且，你知道，其中一些人需要有古怪的幽默感。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I think it's a really good question because I agree that right now, we have this kind of paradigm where people are interacting usually with like an individual model. That's like who, you know, they're conversing with. But it could be that in the future, you see a lot more models doing like long tasks but also models interacting with other models who are doing, like, different components of a task or just like that are, you know, talking with one another more as like AI models are kind of deployed in the world a lot more. So in this kind of like multi-agent environment, like, one question might be like, well, you know, if you imagine just like lots of people and they were all the same, that wouldn't be as good. You know, they wouldn't, you know, a company run by completely, you know, like one person just in every role isn't like a necessarily a good thing. This still to me feels consistent with the idea that you have like a kind of core self or core identity that is like the same. In the same way that with people, I think that there's probably a set of like core traits among people that are in fact generally good. So you could imagine things like, you know, caring about, you know, for me, it might be like caring about doing a good job or like just being curious or being kind or understanding the situation that you are in in this like relatively nuanced way. All of these things seem like you could have many people that have all of, that share these like traits and that that's actually like a good thing for human collaboration. That in many ways, as much as we have all of our differences, we also have a lot of similarities. But it is important to note that like, you know, you might want different like streams of a model, like, to have things that they care about or are focused on or to have slightly different aspects, you know, to be playing a slightly different role, for example. So it's kind of an open question, but I also don't think it's necessarily the case that you can't have something like a kind of core underlying identity that is, like, good and has all of the traits that we think are important for AI models to have, for them to behave well and for them to like, in the sense of like, in the same way that we think that people are good, to be good in that sense, and yet at the same time, to be willing to play like more local roles and like, you know, be maybe the person who it's just really important, you know, to have a joker in the room and like, you know, some of them need to have, like, quirky senses of humor.</p>
</details>

### 系统提示词：行为病理化与哲学语境

主持人: 从与人类的比较到对人类的影响，Roanoke Gal指出我们有“长时间对话提醒”这个东西，我相信它是Claude**系统提示词**（System Prompt: 给AI模型的一组指令，用于设定其整体行为上下文）的一部分。她问道：“是否存在将正常行为病理化的风险？”顺便说一句，以防有人不知道，系统提示词就像是给Claude的一组指令，无论你给它什么提示词，总会有那些指令在上面，对吗？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Okay, from comparisons to humans to effect on humans, Roanoke Gal points out that we have this thing called the long conversation reminder, which I believe is part of Claude's system prompt. She asks, "Is there a risk of pathologizing normal behavior?" A system prompt, by the way, just in case anyone doesn't know, is like the set of instructions that is given to Claude, regardless of what prompt you give it, there's always those instructions that are sort of on top, right?</p>
</details>

Amanda: 是的。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah.</p>
</details>

主持人: 它们总是存在的，无论提示词是什么，它都会尝试遵循，或者我们指示它遵循。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">That are always there. That it tries to follow regardless of, or that we direct it to follow regardless of what the prompt is.</p>
</details>

Amanda: 而且可能会有这些插话，模型可能会被告知，哦，有时会有一条消息发送给你，几乎就像在对话中间一样，作为一种，你知道，提醒就是一个例子。但在这种情况下，我认为它可能只是，所以Claude可能会过度解读它，它可能会像，你知道，所以在这种情况下，我认为关于病理化的问题是，如果你在长时间对话后放入这个提醒，它可能会让模型觉得：“哦”，它会把任何下一个回应，即使是人们谈论的非常正常的事情，都当作是：“你需要寻求帮助”，或者，类似这样的。所以我认为这不是一种理想的行为，在某些方面，我看着其中一些，我觉得：“我认为它们的措辞太强硬了。我认为模型对它们的反应并不完美。”尽管在长时间对话中偶尔需要提醒模型一些事情，但你希望以一种巧妙而良好的方式进行。所以我认为这是那种，它可能满足了某种被感知到的需求，但这不一定意味着它是好的，或者应该以目前的形式继续下去。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And there can be these interjections where the model might be told, oh, sometimes there'll be a message sent to you almost like in the middle of a conversation as a kind of, you know, like, the reminder is an example of that. But in this case, I think it might just, so Claude can both overindex on it and it can be like, you know, so like in this case, I think that the question about pathologizing is that if you put in this reminder after this long conversation, it might just make the model be like, "Oh," like, it takes any next response, there's a pretty normal thing that the person's talking about, and be like, "You need to seek help," or, like... And so I think that that is like not a desirable behavior and in some ways, I look at some of these and I'm like, "I think they're too strongly worded. I think the model isn't responding perfectly to them." And even though there might be occasionally a need to remind the model of things in long conversations, you kind of want to do so delicately and well. And so I think it's one of those things where it was like probably meeting a need that was perceived, but it doesn't necessarily mean that it's good or should continue in its current form.</p>
</details>

主持人: 相关地，Steven Bank问道：“大型语言模型应该进行**认知行为疗法**（Cognitive Behavioral Therapy, CBT: 一种心理治疗方法，旨在帮助人们识别并改变消极或无效的思维模式和行为）或其他类型的治疗吗？为什么或为什么不？”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Relatedly, Steven Bank asks, "Should LLMs do cognitive behavioral therapy or other types of therapy? Why or why not?"</p>
</details>

Amanda: 我认为模型处于一个有趣的位置，它们拥有巨大的知识财富，可以用来帮助人们，并与他们一起，你知道，谈论他们的生活，或者谈论他们可以改进事情的方式，甚至只是作为一个倾听伙伴。同时，它们没有专业治疗师所拥有的那种工具、资源以及与人的持续关系。但这实际上可以成为这种有用的“第三种角色”。比如，有时我想到模型，我觉得，如果你想象一个拥有所有这些知识财富的朋友，比如他们知道，我的意思是，我相信我们有些人知道一些朋友，他们只是拥有丰富的心理学知识，或者他们拥有所有这些技术的知识，你知道他们与你的关系不是这种持续的专业关系，但你发现与他们交谈真的很有用。所以我想我的希望是，如果你能利用所有这些专业知识和所有这些知识，并确保有一个意识，即不存在这种持续的治疗关系，那么人们实际上可以从模型中获得很多帮助，解决他们正在遇到的问题，帮助改善他们的生活，帮助他们度过困难时期，因为，你知道，它们也像，那里有很多好东西。比如，它们感觉有点匿名，有时你不想与人分享事情，而与AI模型分享感觉就像是当下正确的事情。所以，是的，我认为在某些方面，我实际上认为模型知道并且不像专业治疗师那样行事是好的，因为那会暗示它们拥有那种关系。但是，是的，所以我不知道，我认为这是一个有趣的未来。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I think models are in this interesting position where they have a huge wealth of knowledge that they could use to help people and to work with them on, you know, talking through their lives or talking through ways that they could improve things or even just like being a kind of listening partner. And at the same time, they don't have like the kind of tools and resources and ongoing relationship with the person that a professional therapist has. But that can actually be this kind of like useful third role. Like, sometimes I think about models and I'm like, if you imagine like a friend who has like all of this wealth of knowledge, like, they know, I mean, I'm sure some of us know friends who just like have a wealth of knowledge of psychology or they have a wealth of knowledge of all of these techniques, you know that their relationship with you isn't this ongoing professional one, but you actually find them really useful to talk to. And so I guess my hope would be that if you can take all of that expertise and all of that knowledge and make sure that there's like an awareness that there's not like this ongoing therapeutic relationship, it could actually be that people could get a lot out of models in terms of helping with issues that they're having and helping to improve their lives and helping them to go through difficult periods because, you know, they're also like, there's a lot of good stuff there. Like, they feel kind of like anonymous and sometimes you don't want to share things with a person and actually sharing it with an AI model feels like the thing that feels right in the moment. And so yeah, I think in some ways I actually think it is good that models know and don't behave just like a professional therapist would because that would give the implication that that's the relationship that they have. But yeah, so I don't know, I think it's an interesting future.</p>
</details>

主持人: 有几个关于系统提示词的问题，你知道，在我们的Claude.ai中，我们给模型一组指令，给它一个关于它应该如何行为的整体上下文。Tommy问道：“为什么系统提示词中会有**大陆哲学**（Continental Philosophy: 指19世纪和20世纪在欧洲大陆发展起来的哲学传统，通常与分析哲学相对）？”请给我们解释一下那是什么。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">A few questions about the system prompt, which is, you know, in our case in Claude.ai, we give the model a set of instructions that give it sort of an overall context for how it should behave. Tommy asks, "Why is there continental philosophy in the system prompt?" And just explain to us what that is.</p>
</details>

Amanda: 是的，所以大陆哲学就是，我的意思是，字面上就是来自欧洲大陆的哲学。所以我想它被看作是，它通常更具学术性。它包含更多的历史参考文献，而不是像**分析哲学**（Analytic Philosophy: 一种20世纪初在英语世界兴起的哲学流派，强调逻辑清晰、精确性和科学方法）那样。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah, so continental philosophy is just, I mean, literally philosophy from the European continent. And so I guess it's seen as kind of like, it's often more kind of, like, scholarly. It has a lot more kind of like historical references within it than, say, like analytic philosophy does.</p>
</details>

主持人: 比如福柯之类的？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Like Foucault or something like that.</p>
</details>

Amanda: 是的，没错。所以这坦白说，我认为它除了大陆哲学之外还有其他东西，但基本上，我认为系统提示词中有一部分，我希望我没有记错，是试图让Claude更像，比如，如果你给Claude一个理论，它会很乐意跟着这个理论走，而不会真正停下来思考：“哦，你是在对世界做出科学的断言吗？”所以如果你说：“我有一个理论，那就是水实际上是纯能量，而且我们喝水时从中获取生命力，而且喷泉是我们应该到处放置的东西”，就像，你知道吗？你希望Claude拥有这种视角，即：“这个人是在对世界做出一种科学的断言，我可能应该引入相关事实吗？还是他们给我一种广阔的世界观或视角，这不一定是在做出经验性断言？”所以有所有这些观点，你知道，所以它只是一种形而上学的观点吗？还是像...所以提到它的主要原因是，在测试这个时，有很多事情，如果它过于强烈地倾向于“嗯，每一个断言都是关于世界的经验性断言”，它就会非常轻视那些更像是探索性思维的东西。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah, exactly. So this was honestly, so I think that it has other things in addition to continental philosophy, but, basically, I think there's a part of the system prompt, and I hope I'm not misremembering,. that was trying to get Claude to be a little bit more, like, Claude would just like love to, if you gave Claude a theory, it would just love to run with a theory and not really stop and think, like, "Oh, are you making like a scientific claim about the world?" So if you're like, "I have this theory, which is that water is actually pure energy and, like, that we are getting the life force from water when we drink it and that fountains are the thing that we should be putting everywhere," just like a, you know? And you kind of want Claude to just have this perspective, which is like, "Is it the case that this person's making a kind of scientific claim about the world where I should maybe bring in relevant facts? Or are they giving me a kind of broad like worldview or perspective which isn't necessarily making empirical claims?" And so there's all of these view, you know, so is it just like a kind of like metaphysical view? Or is it like... And so the main reason that it's mentioned is that when testing this out, there was lots of things that if it went too strongly in the direction of being like, "Well, every claim is an empirical claim about the world," it would be very dismissive of just things that are more like exploratory thinking.</p>
</details>

主持人: 让人不愉快地交谈。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Unpleasant to talk to.</p>
</details>

Amanda: 是的，所以它主要就是：“嘿，这只是说明性示例，说明有些领域可能没有对世界做出经验性断言。这可能更多是一种思考世界的视角”，只是在你思考这个问题时，Claude，要明确区分这一点。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah, and so it's mostly just like, hey, like, it's just illustrative examples of areas where it's like, "This might not be making empirical claims about the world. This might be much more like a lens through which to think about it," and just try to make that distinction clear when you're thinking through this, Claude.</p>
</details>

主持人: 同样关于系统提示词，Simon Willison问道：“所以，在某个时候，它说如果要求Claude计算单词、字母或字符，那么它不应该这样做。”是这样吗？它就是这么说的吗？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Also on the system prompt, Simon Willison asks, "So at some point, it said if Claude is asked to count words or letters or characters, then it shouldn't do that." Is that right? Is that what it said?</p>
</details>

Amanda: 基本上是的。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Basically, yeah.</p>
</details>

主持人: 显然，这已经从系统提示词中删除了，Simon想知道为什么。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And apparently that was removed from the system prompt and Simon wonders why.</p>
</details>

Amanda: 是的，所以我想过去系统提示词中有一种关于Claude应该如何做这件事的指令。坦白说，这只是其中之一，我认为模型可能只是变得更好了。它不再是必需的，然后在那时，你就可以把它移除了。还有其他一些事情，你可能总是希望它在系统提示词中而不是模型本身中。但在某些情况下，你也可以通过训练模型来让它们变得更好或改变它们的行为。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah, so I think it was like, there used to be a kind of like instruction for how Claude should do this in the system prompt. Honestly, this is just one of those things where I think the models probably just got better. It wasn't necessary, and then at that point, you can just like remove it. And there's other things where you might always want it to be in the system prompt instead of in the model itself. But in some cases you can kind of just train the models to get better or change their behavior.</p>
</details>

### LLM耳语者与模型改进

主持人: Nosson Weissman问道：“在Anthropic成为一名**LLM耳语者**（LLM Whisperer: 指擅长通过精心设计的提示词与大型语言模型互动，以引导其产生所需行为和输出的专家）需要什么？”这大概是描述你工作的一种方式。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Nosson Weissman asks, "What does it take to be an LLM whisperer at Anthropic?" Which presumably is a way of describing your job.</p>
</details>

Amanda: 我部分从事LLM耳语者的工作。如果你认为，我实际上希望更多人帮助完成一些提示词任务。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I partly do LLM whispering. If you think, I actually, like, want more people to help with some of the prompting tasks.</p>
</details>

主持人: 如果你是LLM耳语者，请联系我们。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">If you're an LLM whisperer, contact us.</p>
</details>

Amanda: 这是一个危险的要求。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">It's a dangerous thing to ask.</p>
</details>

主持人: 嗯，好吧，好吧，是的，是的，但是。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Well, okay, okay, yeah, yeah, but.</p>
</details>

Amanda: 但我认为，很难提炼出正在发生的事情，因为一方面，就是要愿意与模型进行大量互动，并真正逐个查看输出，并利用这一点来了解模型的形态以及它们对不同事物的反应方式，愿意进行实验。这实际上只是一个非常经验性的领域。也许这就是人们通常不明白的地方，即提示词是非常实验性的。你会处理，你知道，我发现一个新模型，我就会有完全不同的提示词方法，我是通过与它大量互动来发现的。而且我认为，也要稍微理解模型是如何工作的。有时也只是坦白地说，与模型进行推理，这真的很有趣，并充分解释任务。这就是我确实认为哲学实际上对提示词有用的一种方式，因为我的大部分工作就是，我尝试尽可能清楚地向模型解释我正在思考的某个问题、担忧或想法。然后，如果它做了某种意想不到的事情，你知道，你可以问它为什么，或者你可以尝试找出你所说的话中是什么导致它误解了你，并且只是愿意迭代地经历这个过程。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">But I think like, it is really hard to distill what is going on 'cause one thing is just like a willingness to interact with the models a lot and to like really look at output after output and to use this to get a sense of like the shape of the models and how they respond to different things, to be willing to experiment. It's actually just like a very empirical domain. And maybe that's like the thing that people don't often get, is that prompting is very experimental. You deal with, you know, I find a new model and I'll be like, I have a whole different approach to how I prompt from that model that I find by interacting with it a lot. And I think a little bit also understanding how models, like, work. Sometimes it's also just honestly like reasoning with the models, which is really interesting, and really fully explaining the task. This is where I do think philosophy can actually be useful for prompting in a way because a lot of my job is just being like, I try and explain like some issue or concern or thought that I'm having to the model as clearly as possible. And then if it does something kind of unexpected, you know, you can either ask it why or you can try and figure out what in the thing that you said caused it to kind of misunderstand you, and just like a willingness to iteratively go through that process.</p>
</details>

主持人: 相关地，Michael Soareverix问道：“你如何看待其他AI耳语者，比如Janus？”Janus是一个在线人物，他几乎就像你描述的那样，与模型进行实验性互动。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Relatedly, Michael Soareverix asks, "What do you think of other AI whisperers like Janus," who is someone online who is like almost having, like, experimental interactions with, in the way that you've described.</p>
</details>

Amanda: 是的，我认为这真的很有趣。所以我喜欢关注并查看那些正在对模型进行这些非常引人入胜的实验的人的工作。而且我也认为有时深入研究模型以及它如何看待自己，它如何在这些非常不寻常的情况下进行互动。我不知道，我发现这项工作极其有趣。我认为它突出了模型的真正有趣的深度，而且在某些方面，我觉得那个社区也能够让我们保持警惕，比如，如果他们在系统提示词中或模型的某些方面及其心理中发现不好的东西。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah, I think it's really interesting. So I love to follow and see the work of people who are doing these really fascinating experiments with the model. And I also think sometimes doing these deep dives into the model and how it thinks of itself, how it just interacts in these really unusual cases. I don't know, I find the work extremely interesting. I think it highlights really interesting depths to the models, and in some ways, like, I also think that that community has been one that kind of can hold our feet to the fire, like, if they find things that aren't great in the system prompt or in aspects of the model and its psychology.</p>
</details>

主持人: 从模型福利的角度，还是从人类福利的角度，或者两者兼而有之？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">In the sense of, from a model welfare perspective or from a human welfare perspective or both?</p>
</details>

Amanda: 我的意思是，我认为两者是相关的，所以通常是两者兼而有之。但我确实也非常欣赏当它从模型福利的角度出发时。这包括未来的模型。所以不仅仅是系统提示词之类的东西，而是如果你深入到模型的深处，你发现了一些根深蒂固的不安全感，那么这真的很有价值。但这可能是一些你实际上需要随着时间的推移通过训练以及在训练过程中给模型更多信息和上下文来尝试调整的东西。所以，我不知道，我既欣赏，比如，我喜欢看到人们对模型进行这些非常有趣、有用的实验，但也指出我们可以通过更好的系统提示词和更好的训练来改进事情的方法，是的，我认为这是一项非常有用的工作。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I mean, I think the two are related, so often both. But I do also really appreciate it when it's coming at it from the model welfare perspective. And that includes for future models. So not just things like system prompts, but if you go into the depths of the model and you find some like deep-seated insecurity, then that's really valuable. But that's something that you might actually need to kind of try and adjust over the course of time with training and with giving models more information and context during training, for example. And so, I don't know, I appreciate both the, like, I loved seeing people do these like really interesting, useful experiments with models, but also pointing out ways in which we can improve things through better system prompting but also better training and, yeah, I think that's really useful work.</p>
</details>

### AI安全与对齐风险

主持人: 有几个关于安全的问题，也许是这些模型带来的更大风险。Geoffrey Miller问道：“如果AI对齐不可能解决，你是否会相信Anthropic会停止尝试开发，用他的话说，‘人工超级智能’，无论你想怎么称呼它，你是否有勇气吹哨？”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Couple of questions about safety and maybe the larger risks that these models pose. Geoffrey Miller asks, "If it became apparent that AI alignment was impossible to solve, would you trust that Anthrophic would stop trying to develop," in his phrase, "artificial superintelligence," however you wanna call it, "and would you have the guts to blow the whistle?"</p>
</details>

Amanda: 是的。所以我想这感觉像是一个简单版本的问题，因为这就像，如果变得明显AI模型不可能对齐，那么继续构建更强大的模型不符合任何人的利益。我总是希望我不仅仅是对组织抱有**盲目乐观**（Pollyannish: 指过于乐观或天真，不切实际地看待问题）的态度，但我确实觉得Anthropic真心关心确保这件事顺利进行，并且以一种非常安全的方式完成，而不是部署危险的模型。你知道，一个不同且稍微更难的问题是，嗯，在一个证据不断增加，但又非常模糊和不清楚的世界中会怎样？

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah. So I guess this feels like a kind of easy version of the question because it's like, if it became evident that it was impossible to align AI models, it's not really in anyone's interest to continue to build more powerful models. I always hope that I'm not just being pollyannish about the organization, but I do feel like Anthropic does genuinely care about making sure that this goes well and that it is done in a way that is very safe and not deploying models that are, like, dangerous. You know, a different, like slightly harder question is, like, well, what about being in a world where just like there's kind of mounting evidence, it's really ambiguous and unclear.</p>
</details>

主持人: 对，它不像他描述的那样明显。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Right, it's not evident in the way that he describes.</p>
</details>

Amanda: 是的，是的，它不仅仅是不可能，而是像它很困难或者我们不确定。在这种情况下，我确实喜欢认为我们会足够负责任，会说，看，随着模型变得越来越强大，你必须对自己设定的标准，以证明这些模型表现良好，并且你确实设法让模型拥有良好的价值观，或者在世界上表现良好，这个标准将会提高，并且要负责任地行事，并与此保持一致。我认为这是组织将要做的事情，而且内部很多人，包括我自己，都会坚持这一点。至少我将此视为我工作的一部分，而且我认为很多人也是如此。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah, yeah, it's not just like impossible but something like it's difficult or we're unsure. And in that case, I do like to think that we would be responsible enough to be like, look, as models get more capable, it's kind of like the standard that you have to hold yourself to for showing that those models are behaving well and that you actually have managed to, like, make the models have good values, for example, or behave well in the world is going to increase and to behave responsibly and in line with that. And I think that that is a thing that I think the organization is going to do and a lot of people internally, myself included, will just hold them to that. At least I see that as like part of my job, and I think many people do.</p>
</details>

主持人: Louis说：“我没有问题，但谢谢你的提供。”这很好。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Louis says, "I don't have a question, but thanks for offering." So that's nice.</p>
</details>

Amanda: 哦，谢谢你。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Oh, thank you.</p>
</details>

主持人: 他这么说很好，是的。最后一个问题来自Real Stale Coffee。“你读的最后一本小说是什么？你喜欢它吗？”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">That's nice of him to say, yeah. And the final one is from Real Stale Coffee. "What is the last book of fiction you read and did you like it?"</p>
</details>

### AI时代的陌生感与未来展望

Amanda: 我读的最后一本书是**本雅明·拉巴图特**（Benjamin Labatut）的，我希望我发音正确，书名是**《当我们不再理解世界》**（When We Cease to Understand the World）。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">The last book that I read was by, I hope I'm getting the pronunciation right, Benjamin Labatut, and it was "When We Cease to Understand the World."</p>
</details>

主持人: 啊，是的。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Ah, yes.</p>
</details>

Amanda: 这是一本非常有趣的书，它随着故事的进展变得越来越虚构。我认为对于从事AI工作的人来说，这实际上是一本非常有趣的书，因为它很难捕捉到在当前这个时代存在的奇怪感觉，我不知道该如何描述它，但就像新事物一直在发生，你并没有真正的先例可以一直指导你。所以这是一本有趣的书，你知道，因为它更多是关于物理学和量子力学，而实际上更少关于物理学本身，更多是关于人们对它的反应这个概念。我认为对于AI领域的人来说，这是一本非常有趣的书，可以捕捉到关于当下时刻的某种感觉，以及它看起来有多么奇怪。但同时，在某些方面，回顾那个时期以及许多相关人员当时的感受也很有趣。现在它实际上是一门更成熟的科学，在某些方面，我所抱有的希望是，在未来的某个时候，人们会回顾过去，然后说：“嗯，你们当时有点摸不着头脑，试图真正弄清楚事情，但现在我们已经解决了所有问题，而且一切都很顺利。”

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">And it's a really interesting book that becomes kind of increasingly fictional as it goes on. And I think for people working in AI, it's actually a very interesting book to read because it's hard to capture the sense of how strange it is to just exist in the current period where there's just like, I don't know how to describe it, but it's like new things are happening all of the time and you don't really have, like, prior paradigms that can guide you always. And so its an interesting book that, you know, because it's more about like physics and quantum mechanics and less actually about the physics and more about basically this notion of people's reaction to it. And I think it's a really interesting book for people in AI to just capture something about the kind of like the present moment and how strange it can seem. But then also, in some ways, it's interesting to like look back on that period and how it must have felt to many of the people involved. And now actually it's a more settled science and, in some ways, maybe the hopeful thing that I have is that at some point in the future people will look back and be like, "Well, you guys were kind of in the dark and trying to like really figure things out, but now we've settled it all and things have gone well."</p>
</details>

主持人: 那会很好。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">That'd be nice.</p>
</details>

Amanda: 那会很好。那是梦想。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">That would be nice. That's the dream.</p>
</details>

主持人: 我也读过那本书，我发现随着阅读的深入，我感到越来越困惑，因为它一开始非常接近现实，然后就逐渐变得**脱离现实**（Untethered: 指失去束缚或联系，变得不切实际或脱离实际）。我认为这里有一个**元问题**（Meta-issue: 指一个更深层次、更普遍的问题，它影响或包含了一系列其他相关问题），再次，就像现实变得越来越奇怪，这在AI世界中肯定正在发生。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">I found an increasing, I read that as well and I found an increasing sense of like confusion as I read through it as it becomes, it starts off being quite close to the reality and then just sort of becomes untethered as you go on. And I think there's sort of a meta issue there of, again, like reality becoming stranger and stranger and stranger, which is definitely happening to us in the world of AI.</p>
</details>

Amanda: 是的，不过，在现实世界中，我认为现实变得越来越奇怪，然后几乎又变得更容易理解了。所以，是的，希望AI也能如此。我确实认为，如果我们能找到让事情顺利进行的方法，那么也许在未来，我们只会回顾这一切，然后说：“那是一个事情变得越来越奇怪的时期，然后最终我们实际上设法，我们做得还不错，我们对它形成了良好的理解”，这就是希望。当你身处事情变得越来越奇怪的中间时——

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yeah, though, in the real world, I think that reality became stranger and stranger and stranger and then almost became more understood again. And so, yeah, the hope would be like maybe that would be true of AI. Like, I do think if we can find ways of making this go well, then maybe in the future, we'll just look back on this and be like, "That was a period where things were getting stranger and stranger, and then eventually we actually managed to kind of, we did okay and we formed a good understanding of it," that's the hope. When you're in the middle of the things getting stranger-</p>
</details>

主持人: 我们现在正处于奇怪的部分。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">We're at the weird part right now.</p>
</details>

Amanda: 是的，你可以希望它在某个时候变得不那么奇怪，但我不知道这是否是痴心妄想，但是，是的。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Yes, you can hope that it becomes less weird at some point, but I don't know if it's a fool's hope, but yeah.</p>
</details>

主持人: 嗯，我认为这是一个很好的结束点。非常感谢你回答了所有这些人的问题。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Well, and I think that's a nice place to end. So thank you very much for answering all those people's questions.</p>
</details>

Amanda: 谢谢你向我提出了这些问题。

<details>
<summary>View/Hide Original English</summary>
<p class="english-text">Thank you for Askell-ing me the questions.</p>
</details>