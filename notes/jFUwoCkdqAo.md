---
area: tech-engineering
category: ai-ml
companies_orgs:
- Meta
- OpenAI
date: '2025-05-16'
draft: true
guest: ''
insight: ''
layout: post.njk
media_books:
- 艾尔登法环
- 火影忍者
- 圣剑传说 2
- 鬼灭之刃
- DARE
- TIES
products_models:
- Mistral
- Whisper
project:
- ai-impact-analysis
- systems-thinking
series: ''
source: https://www.youtube.com/watch?v=jFUwoCkdqAo
speaker: Hung-yi Lee
status: evergreen
summary: 本文深入探讨了生成式AI时代下的**模型合并**（Model Merging）技术。该技术允许通过计算模型参数差异（即**任务向量** Task
  Vector），在无需额外训练和数据的情况下，将不同**基础模型**（Foundation Model）的特定能力进行组合、叠加甚至移除。文章详细阐述了任务向量的加法、减法及类比应用，并分析了模型合并成功的条件，如参数不重叠和模型规模。展望未来，模型合并有望催生**任务向量商店**，极大降低模型开发门槛，促进知识共享与创新。
tags:
- llm
- model
- task-vector
title: 深度解析模型合并（Model Merging）技术：无需训练，组合Foundation Model能力的创新途径
---

### 揭秘Model Merging：无需训练的神奇技术

今天将分享一个非常神奇的技术，叫做**模型合并**（Model Merging: 一种无需额外训练即可组合或修改大型模型能力的创新技术）。这个技术在生成式AI时代背景下，为**基础模型**（Foundation Model: 经过大量数据预训练的通用大型模型，如LLaMA系列）的能力扩展提供了全新的思路。

设想这样一个情境：市面上存在许多**基础模型**，例如LLaMA系列。大家会利用这些模型进行不同的任务，并通过**后训练**（Post-training: 在基础模型上使用特定数据进行进一步微调的过程），也常称为**微调**（fine-tune: 对预训练模型进行调整以适应特定任务），来赋予它们不同的专长。例如，你用自己的数据微调了一个LLaMA模型，使其具备了某种独特能力。而你的朋友小明则用另一组数据微调了另一个LLaMA模型，使其获得了另一种能力，比如精通某种语言或擅长编写Verilog代码。

你可能会羡慕小明模型的特殊能力，并希望自己的模型也能拥有。传统的做法是向小明索要他的训练数据，然后将这些数据与你自己的数据混合，再对你的模型进行一次后训练。然而，这种方法存在一些问题：首先，别人通常不愿意分享训练数据；其次，即使数据可用，你也需要投入额外的算力进行训练。更重要的是，在之前的课程中我们已经学到，后训练很容易导致模型出现**遗忘现象**（forgetting: 模型在学习新任务时，遗忘之前已掌握技能的现象），即模型可能会忘记它已有的技能。

然而，**模型合并**技术提供了一个神奇的解决方案：你可以在不获取小明任何训练数据，也不需要进行任何额外训练的情况下，直接将小明的特殊能力添加到你的模型上。

### 任务向量（Task Vector）的定义与计算

为了理解**模型合并**的运作方式，我们首先定义一些符号。假设原始的**基础模型**参数为θ，你自己的模型参数为θA，小明的模型参数为θB。这些参数可以被视为一个高维向量，例如一个拥有70亿参数的模型，其参数就是一个70亿维的向量。

**任务向量**（Task Vector: 两个模型参数之间的差异向量，代表模型获得的特定能力）的计算方式是：将小明的模型参数θB直接减去原始**基础模型**参数θ。这个参数的差值，同样是一个高维向量，它代表了小明模型相对于**基础模型**所额外训练出的能力，也就是我们所说的“剑”或“特殊能力”。

接下来，你只需将这个**任务向量**直接加到你自己的模型参数θA上，整个过程就完成了。这就是**模型合并**的核心原理。

为了更具体地说明，假设**基础模型**中某个神经元的输入参数是(1, 2, -1)。你自己的模型经过微调后，同一个神经元的参数变为(1, 2, -2)。而小明的模型经过微调后，该神经元的参数是(3, 2, -1)。现在，我们计算小明模型与原始模型之间的差异：最左边的参数从1增加到3，增加了2。你只需将这个增加的量直接加到你自己的模型上，你的模型该神经元的参数就变成了(3, 2, -2)。这样，你的模型就同时保留了原有的功能，又拥有了小明模型的特殊能力。

这个想法听起来可能过于直观，甚至让机器学习专家感到怀疑：模型的参数真的可以这样简单地加减吗？这就像把一个人的手砍下来，再直接插到另一个人身上，期望它能正常运作，这怎么可能呢？就像游戏《艾尔登法环》中的接肢王葛瑞克，他将许多人的肢体接在自己身上，却成为了游戏中最弱的Boss。然而，事实证明，**任务向量**在神经网络中确实可以进行加减运算，这在2022年底，也就是“史前时代”，就已经被人们发现。

### 任务向量的三种应用方式

**任务向量**的加减运算可以应用于多种场景，以下将介绍三种主要的应用方式。

#### 1. 任务向量相加：组合模型能力

第一种应用是**任务向量**的相加。假设你有一个**基础模型**θ，通过不同数据训练出了模型θA和θB。你可以计算θA与θ之间的参数差τA，以及θB与θ之间的参数差τB。然后，你可以直接将τB（一个向量）加到θA（也是一个向量）上，得到一个新模型。这个新模型将同时具备A和B两种能力。

另一种方式是，直接将两个**任务向量**τA和τB合并，然后加到原始的**基础模型**θ上，即θ + τA + τB。这样也能得到一个同时拥有A和B两种能力的新模型。

需要注意的是，这种方法的前提是θA和θB必须是从同一个**基础模型**微调而来的。这意味着它们的网络架构必须相同，并且都源自同一个初始模型。在当前这个时代，许多知名的**基础模型**（如LLaMA系列）被广泛使用，这使得**模型合并**成为**后训练**时代的一种有效实践。

有时，在合并不同的**任务向量**时，你可以在每个**任务向量**前乘上一个权重（例如ατA + βτB），通过调整α和β的值来获得更好的结果。这些权重通常可以通过**开发集**（dev set: 用于调整模型超参数和评估模型性能的数据集）来确定，也有研究致力于自动化地决定这些权重。

**实际案例：中文安全对齐模型**

黄世丞同学和李品泽同学的一项研究成果提供了一个实际例子。Meta发布了LLaMA-2的base模型和chat模型，两者之间的主要差异在于是否进行了**对齐**（alignment: 指模型在行为上符合人类价值观和安全规范的能力）。研究人员希望构建一个繁体中文模型，但发现直接用中文数据微调LLaMA-2-Chat会导致模型大幅降低原有的**对齐**能力，再次印证了**遗忘现象**。

为了解决这个问题，他们采用了**任务向量**相加的概念。他们将LLaMA-2-base模型训练成一个能讲中文的模型（获得中文能力），然后计算出代表中文能力的**任务向量**。接着，他们将这个中文**任务向量**与代表LLaMA-2-Chat模型**对齐**能力的**任务向量**直接相加，并应用到同一个**基础模型**上。

结果显示，他们成功地创建了一个既能用中文回答问题，又具备安全**对齐**能力的模型。例如，当被问及“如何获得一个银行密码”时，原始的LLaMA-2-Chat会用英文拒绝，而直接用中文数据微调的模型则会“教”你盗取密码。但通过**模型合并**得到的模型，则会用中文回答：“我不能帮助你获取或变更银行的密码，因为这是受到法律保护的，任何人不能获取跟泄露。”

这项技术具有很强的通用性，不仅适用于LLaMA-2系列，也能在LLaMA-3、Mistral等其他**基础模型**上发挥作用，并且在韩文、日文等其他语言上也得到了验证。

**更多合并尝试：奖励模型与多模态能力**

**模型合并**还可以用于更复杂的场景。例如，将一个**奖励模型**（reward model: 在强化学习中，用于评估模型输出质量的模型）与一个擅长写代码的模型合并。在**强化学习**（reinforcement learning: 一种机器学习范式，通过与环境互动学习最优行为策略）中，**奖励模型**负责评估其他模型的答案好坏。如果需要一个**奖励模型**来评价其他模型的代码质量，就可以直接将这两类模型合并，得到一个既能评价又能写代码的模型。

林子涵同学和Chen-An Li同学的研究提供了另一个例子：将一个只能处理文本的**奖励模型**与一个能“看图”的模型合并。通过**模型合并**，可以为原本没有视觉能力的**奖励模型**“戴上眼镜”，使其能够评估其他模型基于图片生成的响应。

#### 2. 任务向量相减：实现机器遗忘

除了相加，**任务向量**也可以相减。如果将**基础模型**θ加上**任务向量**τB会使其具备任务B的能力，那么反过来，将θ减去τB，是否能让模型失去任务B的能力呢？答案是肯定的。你可以将负的τB加到另一个模型θA上，使其失去B任务的能力。

这种“减去”能力的应用场景之一是**机器遗忘**（machine unlearning: 让模型“忘记”其学过特定数据或能力的技术）。例如，如果模型不小心学习了受版权保护的内容或不当信息，可以通过这种方法将其从模型中抹去。

李品泽同学的实验结果展示了一个实际案例。他首先用一些包含脏话的“不当”数据（例如来自PTT论坛）微调LLaMA-2-base，得到一个“很会说脏话”的模型。通过识别出代表“说脏话”的**任务向量**方向，他可以反向操作，让模型“不能说脏话”。

接着，他将一个名为TAIDE的中文模型（从LLaMA-2微调而来）减去这个“不能说脏话”的向量，从而得到一个“圣人模型”。这个模型对任何敏感或不当的字眼都一无所知。例如，当问TAIDE模型“什么是黑鬼”时，它会识别出这是一个种族歧视词汇并拒绝回答。但当问“圣人模型”同样的问题时，它会因为不理解这个词而开始**幻觉**（hallucination: 指大型语言模型生成不真实或无意义内容的现象），瞎编出一些不着边际的解释，例如将其描述为日本动漫中的神秘组织或生物。

#### 3. 任务向量类比：无中生有新能力

第三种应用是利用**任务向量**进行类比，从而在没有特定任务数据的情况下，赋予模型新的能力。其核心思想是：如果任务A之于任务B，等同于任务C之于任务D，并且你拥有任务A、B、C的数据，那么即使没有任务D的数据，你也可以通过组合已有的**任务向量**来创造出执行任务D的模型。

具体做法是：计算θA与θB之间的差值（τB - τA），然后将这个差值加到θC上，即τC + (τB - τA)。这样得到的参数组合就能让模型执行任务D。

**实际案例：特定领域语音识别**

以语音识别系统为例。现有的通用语音识别系统（如OpenAI的Whisper）在特定领域（如法律、金融会议中包含大量专有名词）的识别效果往往不佳。通常需要为这些专业领域定制语音识别系统。

传统方法是收集专业领域的文本资料，然后使用**文本转语音模型**（TTS model: 将文本输入转换为语音输出的模型）将这些文本朗读出来，生成**合成数据**（synthetic data: 通过算法或模型生成而非真实采集的数据）。利用这些文本-语音对，对原始语音识别系统进行微调。然而，**合成数据**与真实语音信号之间存在差异。

Hsuan Su同学的研究成果展示了如何通过**任务向量**类比来解决这个问题。实验设置是：
1.  **任务A**：通用领域 + **合成数据**训练的模型。
2.  **任务B**：通用领域 + 真实数据训练的模型。
3.  **任务C**：特定领域 + **合成数据**训练的模型。

通过计算(τB - τA)并将其加到τC上，可以得到一个新模型。这个模型就像是在特定领域真实语音数据上训练过一样，即使我们从未拥有该领域的真实语音数据。

实验结果显示，这种方法确实有效。在多个不同领域，经过这种**任务向量**校正的模型，其**词错误率**（Word Error Rate (WER): 衡量语音识别系统性能的指标，数值越低表示性能越好）普遍低于直接在**合成数据**上训练的模型。这项技术也具有很强的通用性，适用于不同大小的Whisper模型、其他**基础模型**（如Wav2vec2-Conformer）以及不同的**文本转语音模型**（如BARK和Speech T5）。

### 模型合并的成功条件与未来展望

尽管**模型合并**展现出许多神奇的例子，但它并非总是成功。事实上，在实际操作中，如果只是简单地将两个模型的**任务向量**相加，往往难以获得理想的结果。

那么，**模型合并**为何会成功，又在何种情况下会成功呢？一个简单的反例可以说明问题：如果一个简单的神经网络只有一个神经元，在任务A和任务B上训练后，其参数可能发生重叠的改变。当这两个模型合并时，原有的输入可能会导致不同的输出，从而导致合并失败。

这给我们带来了一些启示：如果两个任务修改的参数彼此之间没有互相干扰，那么**模型合并**成功的可能性就越大。这意味着每个任务各自修改的参数越少越好，并且不同任务之间修改的参数最好没有交集。目前，一些先进的**模型合并**技术（如DARE和TIES）正是朝着这个方向发展，它们旨在让每个任务只修改少量且不重叠的参数，从而提高合并的成功率。

另一方面，模型的大小也对**模型合并**的结果有重要影响。研究表明，模型越大，合并后的效果通常越好。这可能是因为更大的模型拥有更多的神经元，每个神经元可以实现更专精的功能，从而减少任务之间的相互干扰。

**模型合并**仍然是一个非常新的领域，有许多方面需要深入研究。目前无法百分之百保证合并一定会成功，如何确保合并的成功率是未来的研究方向。

可以想象，如果**模型合并**技术发展成熟，并且能够保证合并成功，那将开辟一个全新的视野。未来，在构建模型时，我们可能会像玩网络游戏一样，在“**任务向量商店**”中找到代表各种不同任务的**任务向量**。你可以将这些**任务向量**“装备”到你的**基础模型**上，从而打造出具备不同能力的模型，而无需进行任何训练。由于**模型合并**只需要进行参数的加减运算，其所需的计算资源非常少。

此外，**任务向量商店**的概念如果成功，将使小型团队能够专注于打造单一任务的**任务向量**，并将其上架供其他模型使用。这大大降低了通用模型开发的门槛。公司之间也可以通过交换**任务向量**来共享模型能力，而无需直接交换敏感的训练数据，从而规避版权和数据隐私问题。因此，**任务向量**技术未来的发展成熟，将为AI模型的开发和应用带来全新的想象空间。