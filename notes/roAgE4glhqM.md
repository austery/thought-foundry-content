---
title: AI Agent的经验时代：Meta“早期经验”范式深度解析
summary: 图灵奖得主Richard Sutton提出AI Agent将迎来经验时代。为解决当前AI Agent过度依赖专家数据、缺乏从试错中学习的问题，Meta研究团队提出“早期经验”范式，通过隐式世界建模和自我反思，让Agent在没有外部奖励下从自身经验中学习成长，显著提升了有效性、泛化性和RL兼容性。
area: tech-insights
category: technology
project:
- ai-impact-analysis
tags:
- ai-agent
- experience-learning
- imitation-learning
- meta-ai
- reinforcement-learning
people: []
companies_orgs: []
products_models: []
media_books: []
date: '2025-10-15'
author: Best Partners TV
speaker: Best Partners TV
draft: true
guest: ''
insight: ''
layout: post.njk
series: ''
source: https://www.youtube.com/watch?v=roAgE4glhqM
status: evergreen
---
### 引言：AI Agent迈向经验时代

前段时间，图灵奖得主Richard Sutton与谷歌RL大佬David Silver合作撰写了一篇文章《欢迎来到经验时代（Welcome to the Era of Experience）》，引发了广泛关注。他们在文中指出，人类数据已经接近极限，**AI Agent**（人工智能代理: 一种能够感知环境、进行决策并执行行动的智能实体）如果想要突破天花板，必须像人类和动物一样，通过与环境的持续互动来生成经验流，并通过**强化学习**（Reinforcement Learning, RL: 一种通过与环境互动，从试错中学习最优行为策略的机器学习范式）实现自主提升。这意味着AI Agent将迎来经验时代，这是一个重大的范式转变。

然而，在许多环境中，基于经验数据使用强化学习来训练Agent仍然面临诸多挑战。一方面，这些环境往往缺乏可验证或密集的奖励信号，尤其是在开放式场景中，例如在大多数网页环境中通常不会返回明确的任务反馈。另一方面，Agent可能需要在长时间跨度内进行低效的探索与泛化，比如多轮的工具使用或复杂交互流程。

目前，大多数语言Agent会采用**监督微调**（Supervised Fine-Tuning, SFT: 一种利用带有标签的专家数据对预训练模型进行进一步训练的方法），从专家示范中学习，从而避免过度依赖奖励信号。虽然这种方法的训练效率很高，但它缺乏环境交互，无法从失败中学习或主动探索。同时，它对高质量专家数据的依赖过强、成本过高，且泛化性也有限。因此，一个关键问题浮出水面：如何让Agent在没有外部奖励的情况下，从自身的经验中学习成长？

### “早期经验”范式：从自身试错中学习

上周末，一篇来自META超级智能实验室MSL、FAIR、俄亥俄州立大学的研究提出了一个名为“早期经验”（Early Experience）的中间路线。其核心思想很简单：让Agent在训练时，既从人类专家数据中学习，也从自己的试错中学习。具体来说，Agent在环境中提出替代行动，收集这些行动带来的未来状态（Future States），然后把这些未来状态直接变成监督信号。Agent无需等待外部奖励，也无需完全依赖专家，因为Agent自己的行动后果就是最好的老师。

### 理论基础：马尔可夫决策过程与模仿学习的挑战

要理解“早期经验”范式，需要先铺垫一点理论基础。研究团队把语言Agent的决策问题形式化成了一个**马尔可夫决策过程**（Markov Decision Process, MDP: 一种用于决策建模的数学框架）。一个MDP可以用一个元组M=(S, A, T, R, γ, ρ₀)来定义：
*   **状态空间**（State Space, S: Agent能感知到的环境信息集合），例如网页内容、工具输出、环境的文本描述。
*   **行动空间**（Action Space, A: Agent能执行的离散行动集合），例如点击网页按钮、调用搜索工具、生成文本回复等。
*   **转移函数**（Transition Function, T: 描述在特定状态下执行某个行动后，转移到下一个状态的概率），例如点击提交按钮后，页面可能跳转到成功页面或停留在原来的页面。
*   **奖励函数**（Reward Function, R: 定义Agent在执行行动后获得的反馈信号），但就像之前所说，很多场景下R是未知或不可验证的，这也是早期经验要解决的问题。
*   **折扣因子**（Discount Factor, γ: 用于衡量未来奖励在当前价值中的比重），用来权衡当前奖励和未来奖励的重要性。
*   **初始状态分布**（Initial State Distribution, ρ₀: Agent任务开始时环境可能处于的状态集合）。

而Agent的核心是**策略**（Policy, π_θ: Agent的决策逻辑，将状态映射到行动的概率分布），θ是模型参数。例如，在网页购物场景中，给定当前页面显示红色衬衫、预算20美元这个状态，策略会给出点击蓝色衬衫、点击价格筛选等行动的概率，这就是Agent的决策逻辑。

在无奖励的场景下，传统**模仿学习**（Imitation Learning: 一种通过学习专家示范来训练Agent的方法）的训练目标是最小化模仿损失，这里的(s_i, a_i)来自专家数据集D_expert，即人类专家在状态s_i下执行的正确行动a_i。这个损失函数意味着让Agent在状态s_i下尽可能选择专家行动a_i。然而，它存在两个致命问题：

1.  **分布偏移**（Distribution Shift: Agent在实际部署时，其策略与专家策略产生偏差，导致在未见过状态下性能下降的现象）。Agent在实际部署时，其策略π_θ肯定会和专家策略有偏差。例如，专家从来不会点无效按钮，但Agent可能会点。这导致Agent在遇到训练数据里没有的状态时，错误会不断累积。罗斯等人在2011年的研究指出，这种偏移会让模仿学习的性能越用越差。
2.  **缺乏行动后果的认知**。Agent只见过专家行动和专家未来状态的配对，从未见过自己行动和自己未来状态的结果。它不知道如果点错按钮会出现错误提示，也不知道填错日期会导致表单提交失败。因此，遇到错误时根本不知道如何修正，更无法理解为什么专家选择这个行动而不是那个。罗斯和巴涅尔在2010年的研究也强调，这种后果正是模仿学习泛化能力弱的核心原因。

“早期经验”范式正是为了解决这两个问题。

### 构建D_rollout数据集：行动与后果的经验

“早期经验”范式的核心第一步是构建D_rollout数据集，让Agent获得从行动到后果的经验。研究团队定义了一套清晰的流程：

1.  **选取专家状态与生成候选行动：** 从专家数据集D_expert中选取每个状态s_i，然后让Agent的初始策略π_θ生成一个候选行动集A_i，其中包含K个替代行动，同时也会把专家行动a_i包含进来做对比。这里的K是超参数，例如K=4意味着每个状态会生成4个替代行动。
2.  **执行行动并收集未来状态：** 执行专家行动a_i会得到专家的未来状态s_{i+1}；执行每个替代行动a_i^j会根据环境的转移函数T(s_i, a_i^j)得到对应的未来状态s_i^j。这些未来状态并非抽象符号，而是具体的环境反馈，例如点击错误按钮后出现的无效操作文本，填错日期后页面显示的“请输入正确日期格式”的提示。这些都是Agent能感知到的后果。
3.  **构建D_rollout数据集：** 将这些状态、替代行动、未来状态的三元组收集起来，就构成了D_rollout数据集。这个数据集的关键在于它不需要任何外部奖励，未来状态本身就包含了行动质量的隐含信息。错误提示意味着行动无效，页面正常跳转意味着行动有效，这些信息足以供Agent学习。

### 两种训练策略：隐式世界建模与自我反思

有了D_rollout数据集，研究团队提出了两种具体的训练策略，它们就像Agent学习的两条腿：一条负责感知环境规律，一条负责思考行动合理性。

#### 1. 隐式世界建模（IWM）

**隐式世界建模**（Implicit World Modeling, IWM: 一种让Agent通过预测下一个状态，在自身策略中内化环境运行规律的方法）的核心思路是让Agent把学习环境动态当作一个辅助的预测任务。通过预测下一个状态，在自己的策略里内化环境的运行规律，而不是单独建立一个外部模拟器。

语言Agent的状态都用自然语言表示，例如网页内容、工具输出、环境反馈都是文本。这意味着预测下一个状态可以直接转化为语言模型最擅长的预测下一个token的任务。例如，在网页订机票场景中，Agent输入“当前状态：机票查询页面，出发地北京，目的地上海；行动：输入无效日期2025-02-30”，那么下一个状态就是页面显示的“日期无效，请选择正确日期”。隐式世界建模就是让Agent学习从状态与行动的组合中，预测出这段下一个状态的文本。

其损失函数旨在让Agent在给定s_i和a_i^j的情况下，尽可能准确地预测出s_i^j。特别重要的一点是，用来预测下一个状态的模型参数θ和Agent执行行动时的策略参数θ是同一个。这意味着Agent在学习环境会如何反应的同时，也在优化自己该如何行动，两者深度绑定，无需额外训练一个独立的世界模型模拟器。

在实际训练时，研究团队采用了两阶段流水线：
1.  **第一阶段：** 用IWM的损失函数训练一个周期，让Agent先摸清环境的“脾气”，例如点击提交按钮可能会出现哪几种反馈，输入错误信息会得到什么提示。
2.  **第二阶段：** 再用专家数据集D_expert做监督微调，即最小化模仿学习的损失。

这样做的好处是，Agent在学习专家行动前，已对环境有了基础认知，能够理解专家选择某个行动是因为它会带来好的下个状态，而不是简单地死记硬背行动配对。而且，rollout数据集的规模通常会比专家数据集大一个数量级，能够给Agent提供更丰富的试错经验。

#### 2. 自我反思（SR）

如果说隐式世界建模是让Agent感知环境规律，那么**自我反思**（Self-Reflection, SR: 一种Agent通过对比自身行动与专家行动后果，生成解释并优化策略的方法）就是让Agent思考行动的合理性，通过对比自己的替代行动和专家行动的后果，生成为什么专家行动更好的解释，再用这些解释来优化策略。

其实现流程比隐式世界建模多了一步生成解释：
1.  **获取行动后果：** 仍然从专家状态s_i出发，执行专家行动a_i得到专家下一步状态s_{i+1}，执行替代行动a_i^j得到替代状态s_i^j。这一步与隐式世界建模相同。
2.  **生成反思：** 这是核心步骤。研究团队用**大语言模型**（Large Language Model, LLM: 拥有大量参数、能够处理和生成自然语言的AI模型）生成一段**思维链**（Chain of Thought, CoT: 一种提示技术，引导语言模型逐步推理以解决复杂问题）c_i^j，解释为什么专家行动a_i比替代行动a_i^j更优。解释的依据就是s_{i+1}和s_i^j的差异。研究团队设计了严格的提示词模板，要求解释必须包含任务目标、分析行动、对比专家行动合理性以及论证约束条件强调四个部分，确保反思的逻辑性和实用性。
3.  **构建自我反思数据集：** 将状态s_i、替代行动a_i^j以及反思c_i^j的三元组收集起来，形成D_refl。
4.  **训练策略：** Agent需要在给定s_i的情况下，同时预测出反思c_i^j和专家行动a_i。其损失函数旨在让Agent在看到s_i时，先想清楚为什么专家行动更好，再做出专家行动。

在实际训练中，研究团队会将D_refl和专家数据集D_expert混合起来。如果专家数据中本身就有专家写的行动解释，也会保留下来一起训练。这样做能够平衡专家示范的准确性和自我反思的泛化性，让Agent既不会偏离专家路线，又能理解行动背后的逻辑。

### 实验验证：有效性、泛化性与RL兼容性

任何理论都需要实验验证，研究团队做了一套全面的实验，覆盖8个环境、3种模型，从有效性、泛化性、RL兼容性三个维度验证了“早期经验”的价值。

#### 实验设置

为了确保结果的可靠性，研究团队选择了8个差异极大的场景：
*   **ALFWorld**（具身家庭环境: 一种模拟家庭场景的AI测试环境）
*   **ScienceWorld**（科学实验室环境: 一种模拟科学实验的AI测试环境）
*   **TravelPlanner**（长序列旅行规划环境: 一种模拟旅行计划制定的AI测试环境）
*   **SearchQA**（开放域多跳问答: 一种需要Agent在多个信息源中进行推理和回答问题的环境）
*   **BFCLv3**（多轮工具调用环境: 一种需要Agent连续调用多种工具以完成任务的环境）
*   **Tau-Bench**（客户服务场景: 一种模拟客户服务交互的AI测试环境）
*   **WebShop**（电商购物场景: 一种模拟在线购物流程的AI测试环境）
*   **WebArena-Lite**（多领域网页导航: 一种模拟在多个网页领域进行导航操作的AI测试环境）

模型方面，研究团队选择了3种不同规模、不同家族的指令微调模型，以验证方法的模型无关性。在训练和评估流程上，研究团队也做了严格控制：先为模仿学习基线找到每个环境的最优训练步数，同时为“早期经验”的两种策略使用与基线完全相同的总步数。最后在评估时，使用每个环境的原生指标，遵循官方评估标准，确保结果可比较。

#### 实验结果：三大核心维度

1.  **有效性：** 指“早期经验”在领域内任务上的表现。研究团队发现，两种策略在所有8个环境中都显著优于模仿学习基线，平均提升达9.6%。其中，WebShop环境的隐式世界建模提升最明显，TravelPlanner环境的自我反思提升最明显（+15.0%）。
2.  **领域泛化能力：** 表示“早期经验”在**分布外**（Out-of-Distribution, OOD: 指与训练数据分布不同的数据）数据上的表现。许多AI方法在训练数据上表现良好，但遇到稍微不一样的场景就性能下降，这就是泛化能力弱的表现。研究团队针对ALFWorld、BFCLv3、SearchQA三个环境设计了相应的分布外场景。结果显示，虽然所有方法在分布外场景下性能都会下降，但“早期经验”能够挽回更多的损失。更关键的是，部分场景下的分布外增益甚至超过了领域内得分，这说明“早期经验”的自身试错能够覆盖专家数据未包含的场景，泛化性更强。
3.  **RL兼容性：** 旨在验证“早期经验”模型能否作为RL的优质初始化，进一步提升性能。研究团队选择了三个有可验证奖励的环境，采用主流的**GRPO**（Generalized Policy Optimization: 一种强化学习算法）算法做RL训练，对比模仿学习初始化和“早期经验”初始化的效果。结果非常明确：“早期经验”初始化的RL模型最终性能远高于模仿学习初始化，并且“早期经验”的优势会持续到RL训练结束，全程保持差距甚至扩大。这说明“早期经验”不仅能自己提升性能，还能为后续的RL打牢基础，可以说是连接模仿学习和强化学习的桥梁。

#### 消融实验：进一步验证价值

除了这三个核心维度，研究团队还做了三个关键的**消融实验**（Ablation Study: 一种科学实验方法，通过移除或修改系统中的某个组件来评估其对整体性能的影响），进一步验证了“早期经验”的价值。

1.  **专家数据量的影响：** 如果专家数据很少，“早期经验”还能用吗？研究团队在WebShop和ALFWorld中减少了专家数据量。结果显示，在WebShop中只用1/8的专家数据就超过了使用100%专家数据的模仿学习；同样在ALFWorld中，只用1/2的专家数据就超过了使用100%专家数据的模仿学习。这说明“早期经验”能用更少的专家数据达到更好的效果，大幅降低了数据成本。
2.  **分支因子K影响：** 替代行动的数量对性能会有什么影响？研究团队测试了K=1、2、4、8四种情况。结果显示，隐式世界建模的性能会随K增大而稳步提升，因为更多替代行动能让Agent学习更全面的环境动态；而自我反思的性能在K=2-4时最优，K=8时略有下降，因为太多替代行动会让对比解释变得复杂，Agent难以聚焦核心差异。
3.  **模型规模的扩展：** 当模型从3B升级到70B时，“早期经验”还有效吗？研究团队用Llama-3.3-70B在WebArena-Lite环境进行了测试。结果显示，隐式世界建模和自我反思的成功率都要高于模仿学习，而且即使采用了**LoRA**（Low-Rank Adaptation: 一种参数高效微调大语言模型的技术），“早期经验”依然能带来稳定的增益，说明它在大模型、有限计算资源下也适用。

#### 与常见基线方法的对比

研究团队还对比了“早期经验”与两种常见的基线方法：

1.  **长思维链（Long CoT）:** 通过让Agent在推理时生成更长的思维链来提升决策能力。结果显示，Long CoT只能让模仿学习的性能提升一小部分，而且在复杂环境中会适得其反，这是由于长思维链会导致行动偏离专家路线。而“早期经验”的提升幅度较大，且无性能下降风险。
2.  **STaR风格的数据:** 通过让模型生成专家行动的解释，但不使用替代行动和未来状态。结果显示，**STaR**（Self-Taught Reasoner: 一种通过模型自我生成推理步骤并学习的AI训练范式）风格数据的解释匹配率很低，且解释不够接地气，导致模仿学习的性能下降。而“早期经验”的解释基于真实的行动后果，匹配率更高，能够稳定提升性能。

### 总结、局限与未来方向

#### 早期经验范式的核心价值

1.  **解决了无奖励学习难题：** 直接以行动-未来状态作为监督信号，无需外部奖励，可适配多数现实环境。
2.  **降低了数据依赖：** 用更少的专家数据就能达到甚至超过全量模仿学习的性能，大幅降低了数据成本。
3.  **连接了两大范式：** 既是模仿学习的增强版，又是强化学习的优质初始化，为大语言Agent从人类数据时代过渡到经验时代提供了可行的路径。

#### 局限性

目前的研究也存在一定的局限性：
1.  当前方法主要针对短交互序列，例如单步点击、单轮工具调用。对于几十步的长序列任务，如何分配“早期经验”的学习权重还需要进一步研究。
2.  如果环境存在危险行动，例如删除某些重要文件，Agent的试错可能带来风险。如何在安全约束下生成替代行动也是未来需要解决的问题。

#### 未来研究方向

研究团队也给出了一些未来研究的建议：
1.  结合自监督目标，例如将行动一致性、预测状态相似度计算等自监督任务融入“早期经验”，进一步提升环境的感知能力。
2.  进行跨环境的迁移，让Agent在A环境学到的“早期经验”能够迁移到B环境，减少重复训练。
3.  大规模的真实世界部署，通过在真实产品中收集自然的交互数据，用“早期经验”来实现持续学习，从而让Agent越用越好。

这篇论文让我们看到Agent在离自主学习的目标上更近了一步。未来的AI助手也许不用人类一遍遍地教，就能通过自己的试错和反思变得更加聪明。或许，萨顿等人提出的经验时代真的会在某个时间点到来。