---
area: society-systems
category: technology
companies_orgs:
- OpenAI
- SSI
- Google
- Anthropic
- GitHub
date: '2025-11-27'
draft: true
guest: ''
insight: ''
layout: post.njk
media_books:
- 《思考快与慢》
people:
- Ilya Sutskever
- Gary Marcus
- Yann LeCun
products_models:
- ChatGPT
- AlphaGo
- o1 model
- Neuralink
project:
- ai-impact-analysis
- systems-thinking
series: ''
source: https://www.youtube.com/watch?v=m1gnA_wL970
speaker: 北美王路飞
status: evergreen
summary: 本文深入探讨了AI发展模式的转变，从过去依赖海量数据和算力进行“预训练”的“卷王”模式，转向强调内在“价值函数”和“慢思考”的“研究时代”。文章引用OpenAI前首席科学家伊尔亚·苏茨克维的观点，指出当前AI在复杂问题上缺乏判断力，呼吁引入类似人类情绪的直觉机制以提高学习效率。同时，文章也讨论了超级智能带来的权力挑战、安全对齐的重要性，以及人类与AI融合的未来可能性。
tags:
- ai-development
- ai-safety
- human
- scaling-law
- thinking
- value
title: AI发展新范式：从“数据工厂”到“内在裁判”的智能跃迁
---

### AI现状与“缓慢的腾飞”

当我们环顾四周，世界似乎一切如常：快递员穿梭于大街小巷，红绿灯有节奏地闪烁，邻居们照常遛狗。然而，就在我们说话的这一刻，全球GDP约1%，即数千亿美元的真金白银，正被疯狂地投入到同一个领域——人工智能。按照常理，如此巨量的资源注入一项技术，我们应该能够看到某种惊天动地的爆炸式发展，我们想象中的AI革命，应该是机器人满街跑，或是超级大脑瞬间解决了癌症。

但现实是什么呢？现实是，你早上起来刷抖音，看到某某大公司又融了100亿，某某模型参数量又翻了一倍。然后，你关掉新闻，继续使用ChatGPT帮你写一份不太重要的邮件，或者让他帮你修补一段难搞的代码。伊尔亚·苏茨克维（Ilya Sutskever），这位曾经点燃了深度学习革命的男人，把这种现象称之为**缓慢的腾飞**（slow take off: 指技术进步巨大，但社会适应迅速，导致感知上的常态化）。这是一种非常诡异的常态化。这些新闻放在过去，每一条都能够作为当日的头版头条，但现在，它只是另一条科技新闻。我们人类适应得太快了，快到忽略了一个巨大的反差：即使我们今天造出了能够通过所有人类考试的AI，为什么在很多时候，它依然给人一种不够聪明的感觉呢？今天，我们就来好好拆解一下伊尔亚·苏茨克维最近做的一个播客节目。在这期播客中，伊尔亚分享了他是如何看待AI从模仿走向思考的“关键开关”，也就是那个“价值函数”。

### “卷王”模式的局限性：AI为何强大又脆弱

为了理解现在AI为什么既强大又脆弱，伊尔亚给主持人讲了一个非常精妙的比喻。想象一下，你要培养一个世界顶级的编程选手，你现在有两个学生。1号学生，我们称他为“卷王”，他的策略就是题海战术。他极度勤奋，练习了整整1万个小时，做过人类历史上所有的编程竞赛题，记住了所有的算法变体，背下了所有的解题套路。当你给他一道题，他能够瞬间调动记忆，快速精准地写出代码。在这个特定的考场里，他就像那个自带BGM的乔峰，在他的BGM中，他就是无敌的。2号学生，我们叫他“天赋型”，他只练了100多个小时，但他有一种特殊能力：虽然他做过的题少得多，但他似乎看透了题目背后的逻辑，他拥有某种说不清道不明的直觉或者品味。那么现在关键来了，如果你让他们离开考场，去解决一个现实生活中从未出现过的乱七八糟的烂摊子问题，谁会表现得更好呢？直觉告诉我们是那个天赋型的学生。而现在的AI，恰恰就是那个练了1万小时的“卷王”。我们现在的训练方式，也就是所谓的**预训练**（Pre training: 指模型在大量无标签数据上进行初步学习，以掌握通用特征和知识），本质上就是把人类互联网上所有的文本、代码、对话一股脑地塞进模型的脑子里，我们强行让他变成那个见过所有题的学生。所以，当我们看到AI在考试里拿高分时，千万别着急着膜拜，他可能并不是真的学会了思考，他只是把那道题见过太多次了，基本上所有变题都见过了。这就解释了一个巨大落差：为什么现在这些AI在某些超难测试里头表现得像一个神，但在某些简单的任务中却像一个复读机呢？

伊尔亚提到了一个非常具体的、让很多程序员抓狂的现象，完美地暴露了这种“卷王”模式的缺陷。如果你用现在的AI来辅助写代码，你可能会经历这样的崩溃时刻：你有一段有bug的代码扔给他，你说“帮我修好它”。这个AI会自信满满地说：“你说的太对了，这里确实有个错误，你看我怎么修。”然后，他给了你一段新代码，你运行，发现原来的bug确实没了，但是因为他修改，引入了一个新的bug。你就告诉他：“现在这个报错，是有bug b，你赶紧帮我修改一下。”ChatGPT会非常礼貌地说：“抱歉抱歉，我是个笨蛋，我马上就修。”然后，他把代码又改了回去，这个bug b没了，但是原来bug a又回来了。很多情况下，你可以跟ChatGPT在这两个bug中玩上一整天。为什么一个在编程竞赛里头能够打败99%人类的超级大脑会陷入这种非常低级的死循环呢？伊尔亚的诊断是，他并没有真正的判断力，他只有概率。在他的训练数据里，可能有1,000种修复bug a的方式，也有1,000种修复bug b的方式，但缺乏一个核心的东西：他不知道自己在宏观上是不是在往正确的方向走。他就像一个没有指南针的探险家，每一步做得都很自信，但整体是在原地打转。这就是我们必须面对的一个反直觉的事实：只是堆砌更多的数据，可能永远无法解决这样一个问题。

### “数据饥渴”与“缩放定律”的终结

要理解为什么我们会在这种问题上卡住，我们要看懂AI到底是吃什么长大的。在过去很长一段时间里，训练AI的逻辑非常简单粗暴，就叫做“预训练”。这个阶段的原则只有一条，就是九品芝麻官里头那个豹子头的那句话：“我全都要。”你想要教AI懂中文，没问题，把中文互联网所有的文章都喂给他；你想教他学编程，把GitHub所有的开源代码都喂给他。你不需要思考选什么数据，因为最好的策略就是给所有的数据。这就像那种1万小时定律的极致版：只要书读得足够多，甚至都不用理解，也能够熟读唐诗三百首，不会作诗也会吟。但是，这种暴力美学现在已经撞上了一堵墙了。首先，数据快不够用了。高质量的人类文本是有限的，我们快把这个互联网都吸干了。你看到各大领先的模型，他们基本上训练的这个数据集都是一样的。第二，就是有些东西是书本里头没有的。你在书上读了一万遍骑自行车要保持平衡，和你真正跨上自行车摔的那一跤，完全是两码事。书本数据能够教AI知识，但它不会教给他经验和手感。

伊尔亚在访谈中非常隐晦但也非常坚定地指出，那个简单粗暴地喂更多数据的时代，也就是那个**缩放定律**（scaling law: 指通过增加模型规模、数据量和计算资源，AI模型性能会持续提升的经验法则）躺着就能赢的时代，现在可能正在发生微妙的变化。这样的论点，其实之前Gary Marcus的节目中也强调了同样的观点。最近Yann LeCun也跑出来说，这个Scaling要不行了。如果说所有数据都喂完了，AI还是那个只会写死循环代码的“卷王”，那下一步我们应该怎么办呢？

伊尔亚在访谈中给了一个非常有意思的视角。他自己本人曾经就是这个扩展定律最坚定的信徒，这是他当年在OpenAI力排众议，喊出“我们大力出奇迹，把模型做大”的口号，才有了今天的ChatGPT的突破。但这次对话中，他抛出一个震耳欲聋的观点，他说这个Scaling已经吸干了房间里的所有空气。伊尔亚认为，2012年到2020年是研发的时代，那个时候算力是瓶颈。但是，从2020年到2025年，AI的进步就变成了工业制造，因为大家发现了一个非常简单的配方，就是算力乘以数据就等于智能。所以，大家都不用动脑子，只需要疯狂买显卡，疯狂抓数据，这个模型就会自动变强。这导致的一个结果，全世界的公司都比想法多，大家都拿同一个配方在“卷”。但现在，伊尔亚认为，我们又从那个扩展时代退回到了研究时代。这意味着，仅仅把模型做大100倍，可能也不会发生奇迹。我们需要新的配方，我们需要搞清楚那个练了100小时就能够学会开车的人类大脑里头到底藏着什么AI没有的秘密。为什么人类不需要阅读10亿篇关于驾驶的论文，只需要在这个停车场转个两圈，就能够学会倒车入库呢？为什么人类不需要把代码改错1,000次，就能在心里知道这个方向可能不对？

### AI学习的圣杯：价值函数与内在裁判

接下来这部分，我们就将触及AI领域最核心的圣杯。伊尔亚认为，解开这道谜题的钥匙，其实藏在一个如果你下过国际象棋就会懂的概念里：**价值函数**（value function: 在强化学习中，衡量在特定状态或采取特定行动后，预期能获得多少长期奖励的函数）。

如果你教过别人开车，或者你自己学过开车，你就知道这事其实没那么难。一个完全不会开车的青少年，大概只需要10-20个小时的练习，就能够及格上路了。在这个过程中，他可能连一次车祸都没有出过。现在来看看AI，为了让这个自动驾驶AI学会同样的事情，科技巨头为了它投入数十亿英里的真实路测数据，甚至加上更多虚拟的仿真数据。结果，它依然会被一个突然飞出来的塑料袋，或者是一个穿着玩偶的路人搞得不知所措。这就是AI领域目前最尴尬的软肋：这个**样本效率**（sample efficiency: 指一个学习系统在达到特定性能水平时所需的训练数据量）极低。人类大脑就像一块海绵，你滴几滴水就能够吸饱；而现在AI就像一块石头，你得把整条河流倒在它身上，它才能湿那么一点点。

有的科学家就试图用进化论来解释这个差距。他们说：“你看人类之所以学这么快，是因为我们祖先在几百万年的进化里，把怎么识别物体、怎么运动的本能都已经写进基因里了。那个你口中的青少年，他不是从0开始学的，他是带着几万年的外挂在学。”这个解释听起来很合理，对吧？但是伊尔亚并不认同。他在访谈中反驳道：“如果说视觉和运动是这个进化送的外挂，那数学和编程呢？我们的祖先可不需要解微积分，也不需要写Python代码。进化并没有给我们预装这些编程包。”但是现实是，一个聪明的人类只要看几本教材，做几十道题，就能够学会微积分；而AI，依旧需要把人类历史上所有的数学书都读一遍，才能够达到类似的水平。这意味着人类大脑中，一定运行着一种比现在这种深度学习更高级、更高效的学习机制。这个机制是什么呢？伊尔亚给出了他的答案：我们拥有一个内置的裁判。

要理解人类为什么学得快，我需要引入一个核心概念，叫做“价值函数”。这什么意思呢？伊尔亚用下国际象棋做了一个绝妙的例子。想象一下，你正在下一盘棋，你经过深思熟虑，走出了一步。就在你手指松开棋子的一瞬间，甚至松开那几毫秒，你心里会突然咯噔一下，你知道你搞砸了，你送掉了你的皇后。虽然对手还没有动，但是离你被将死可能还有20步，你已经知道这局棋输了。这“咯噔一下”的感觉，就是价值函数在工作。他不需要等这个游戏结束（game over）才告诉你这个输赢，他在中间过程就能够给你打分。他让你短路掉无数个错误的尝试，你不需要把这盘必输的棋下完，你脑子里的裁判直接吹哨停：“这个方向是死路，下次别这么干了。”这就是目前大多数AI所缺乏的能力。传统的AI训练，特别是早期的**强化学习**（reinforcement learning: 一种机器学习范式，通过让智能体与环境互动，根据奖励信号学习如何采取行动以最大化长期回报），往往要等到这个事情彻底做完了，比如说车撞了，游戏输了，代码运行报错了，才能够得到第一个反馈信号。这就说，你只有考完试拿到0分卷子，你才知道自己学错了。而人类，我们其实做题做到一半的时候，就会有一种感觉：这道题越算越复杂，我是不是公式用错了？这种过程中的直觉，其实极大地节省了我们试错的成本。如果说能给AI装上这种内置裁判，让它拥有这种预知好坏的直觉，它是不是就能够像人类一样高效地学习了呢？

### 情绪：高效的算法与决策的引擎

到这里你可能会问，人类这个神奇的价值函数到底长什么样呢？伊尔亚给出的答案既浪漫又硬核：在这个生物体中，它经常被称为是情绪。我们通常认为理智是高级的，而情绪是低级的，是这种原始的干扰。如果你看过一些行为心理学家的书，他会认为人类的这个情绪会导致人类做出错误的判断的主要原因。但是，从AI科学家的视角来看，这个情绪其实是一种极度高效的算法压缩。神经科学里头有一个著名的案例：有一些病人大脑的情感区域受损，但是逻辑区域完好无损。你以为他们会变成冷酷无情的逻辑机器吗？还并没有。他们变成了决策瘫痪机。比如早上选袜子，一个正常人大概需要2秒钟，凭感觉就选一双顺眼的；但失去了情感能力的病人，会站在衣柜前花费几个小时，他会列出所有的逻辑可能性：今天的温度、袜子的材质、颜色的光谱分析。因为他没有那个“喜欢”或者“感觉好”的价值函数，帮他快速剪断决策树，他就陷入了无限的计算死循环。

在伊尔亚看来，进化给我们大脑里头硬编码了一套价值函数。哪怕你还没有吃到那口发霉的食物，你的厌恶感，这个value function就已经让你想吐了，这就是为了防止你中毒。哪怕你还没有被踢出部落，你的羞耻感就已经让你脸红了，这就为了防止你社会性死亡。这其实给AI研究一个巨大的启示：如果说我们想制造这种超级智能，我们不应该试图消除他的直觉，反而应该教他形成某种类似于情绪价值判断机制。不是为了让他多愁善感，而是为了让他从海量的计算泥潭中拔出腿来，快速地做出决定。这个概念其实很有意思，就是我不知道大家有没有读过那本《思考快与慢》（Thinking, Fast and Slow）。这本书里头提到了两种思维模式：第一种**系统一**（System 1: 指人类快速、直觉、情绪化的思维模式），就是伊尔亚所说的这种用情绪去做快速决定；**系统二**（System 2: 指人类缓慢、理性、需要消耗认知资源的分析性思维模式），是非常耗时的，就是要进行逻辑分析和推理判断。那么在伊尔亚看来，人工智能恰恰缺的是人类的这个系统一的这样一个情感的判断。

### AI的自我学习：从“左右互搏”到“思维链”

既然我们不能够把人类的情绪直接拷贝到代码里，工程师是怎么做的呢？伊尔亚给出的答案就是让AI成为自己的老师。这其实并不新鲜，当年的AlphaGo之所以能够横扫围棋圈，就是因为他后面就不再看人类的棋谱了，开始**左右互搏**（self-play: 指AI通过与自身对弈来学习和提升性能的方法）。他自己跟自己下了几千万盘棋，在这一局里，黑棋赢了，这说明刚才黑棋某种新下法是好的，这就是一种极其清晰的价值信号。在这个封闭的棋盘宇宙里，他无师自通，进化出了人类从来没有见过的招数，比如说著名的第37手。

现在，伊尔亚和他的同行们正在做一件更疯狂的事情：他想把这种左右互搏从这个围棋扩展到所有领域。这也就是大家可能最近听到的OpenAI的o1模型，或者是类似模型背后的逻辑：**思维链**（chain of thought: 指大型语言模型在回答复杂问题时，通过生成一系列中间推理步骤来解决问题的方法）。现在AI在回答你的复杂问题之前，它不再是张口就来了，而是它会在心中，也就是后台先进行一场激烈的自我辩论：“我想这样解题……等等，这步好像有点逻辑漏洞，价值函数报警了，我再换个方法试试。嗯，这个方法看起来更靠谱。”虽然它目前还不完美，但这意味着AI开始拥有那个“虽然还没有交卷，但我知道我写错”的能力。一旦这个能力被打通，那个需要10亿英里数据才能够学会开车的笨拙时代，可能就要结束了。

### 从“背书模式”到“慢思考”：AI范式的质变

最后，让我们来看一张图。这张图解释了为什么伊尔亚说现在这些AI公司正在把钱花在不一样的地方。在过去几年，大家几乎把钱都花在预训练上，这就像给AI送进图书馆，花巨资让他把这些书都背下来。这是一次性投入，背完了就是背完了。但你现在看到这个飞速增长的区域，就是**推理计算**（inference compute: 指AI模型在部署后，用于处理新数据并生成预测或响应所需的计算资源）和强化学习的区域。这什么意思呢？就是说把AI从这个背书模式变换到了考试模式。如果你问AI一个简单问题，比如法国首都在哪，他会秒回答“巴黎”，他不需要思考，这就是预训练的知识。但如果你问他，如何设计一种不仅能够通过药监局审批，还能够在成本上打败现有竞品的新药分子，这就不是能靠背就能解决的。现在AI公司正在用算力，让模型思考得更久。不是让他马上回答，而是给他10秒、1分钟甚至1小时的算力，让他在后台进行成千上万次推演模拟试错，就像AlphaGo在脑子里下了1,000盘棋一样，最后再给你那一个经过深思熟虑的答案。这就是所谓的“系统二”慢思考。

伊尔亚认为，这个转变是质变的前夜。当AI开始花时间思考，而不仅仅是检索时，他就不再是一个在大英图书馆里翻书的图书管理员了，他开始变成一个坐在实验室里，眉头紧锁，在草稿纸上反复推导公式的研究员。而当这个研究员的思考速度比人类快1万倍时，我们之前所提到这个**经济奇点**（economic singularity: 指技术进步导致经济增长速度指数级提升，达到人类无法预测和理解的程度）可能就要真的降临了。

### 超级智能的阴影：权力、安全与人类未来

那么当这样一个超级大脑真的被制造出来时候，我们应该如何确保它还是站在我们人类这一边的呢？或者更激进一点，伊尔亚提出了一个让人毛骨悚然的问题：我们真的应该让他优先保护人类吗？Sutton也提过类似的观点，Sutton认为不需要，但是伊尔亚肯定是有不同的看法的。如果说伊尔亚关于价值函数和慢思考的预测成真，那么我们未来几年将看到AI的形态会彻底突变。现在AI本质上还是一个工具，你需要拿着它像一个锤子一样去敲钉子。但是未来的AI，更像一个超级实习生。想象一下，你招了一个实习生，他刚来的时候什么都不懂，但他拥有我们上一集所说的那种通用学习的能力。你把公司文档告诉他，让他看几个现有的案例，第一天他还是小白，到了第三天他可能已经像老员工一样能干活了，第五天他已经读完了行业内所有论文，变成了这个领域的专家。更可怕的是，这个实习生他不需要睡觉，而且你可以瞬间复制1万个他。

伊尔亚在访谈中描绘了这样一个图景：一旦AI打通了像人类一样学习的关卡，他就会以数字员工的身份大规模地进入经济系统。这不仅仅带来效率提升，而且会把这个经济增长的物理极限给打破。当研发、工程甚至是管理工作都可以被无限复制的算力所替代时，我们可能会经历人类历史上最疯狂的经济大爆炸。但紧接着，一个巨大的阴影就会笼罩下来：如果这个实习生比你聪明100倍，比你努力1,000倍，而且他还掌握着这个公司的所有命脉，那么谁才是真正的老板呢？这里，我们就要触及一下房间里的大象了：权力。

伊尔亚的对话中非常直白地指出，**AGI**（通用人工智能: Artificial General Intelligence，指能够理解、学习或执行任何人类智力任务的AI）的核心问题其实就是权力的问题。我们在地球上处于统治地位，并不是因为我们牙齿锋利，也不是因为我们力气大，纯粹是因为我们人类比其他物种聪明。我们就凭这一点点智力的优势，决定了老虎你得住在笼子里，决定哪片森林被保留，哪片被砍伐。那么如果我们亲手制造出一个智力远远凌驾于我们之上的东西，那这权力天平会发生怎样的倾斜呢？很多乐观主义者说：“没关系，这个AI只是软件，我们拔电源不就行了吗？”但当你面对一个比你聪明1,000倍的对手，你觉得他会让你有机会拔电源吗？他可能会通过操纵经济、操控舆论，甚至雇佣其他人类来保护自己。这就为什么伊尔亚离开了OpenAI，创造了SSI（Safe Superintelligence）。你看这个公司的名字：超级安全智能。它把这个安全放在了超级智能的前面，这不仅仅是一个商业决策，更像是一个生存赌注。

为了赢得这场赌注，伊尔亚采取了一个极其反共识的战略，叫做“直奔超级智能”（straight shot）。现在硅谷AI的战争就像一个赛车比赛，各家不停地推出各种新的模型，OpenAI、Google、Anthropic，大家都在争分夺秒地发布新模型。逻辑就是，我们必须先把这个产品推向市场，哪怕它不完美，我能从用户的反馈中学习，快速迭代。这确实也是互联网时代这个黄金法则。但是伊尔亚的新公司SSI，选择一条极其孤独的路。它称之为“直奔超级智能”。它逻辑是，如果你陷入了商业竞争这样的“老鼠赛跑”，你就身不由己了。为了抢夺市场份额，你会被迫在安全线上妥协，你会让模型早发一个月，而跳过那几个关键的安全测试。而在面对超级智能这种级别的力量时，一次微小妥协，可能都是全人类的终局。所以SSI的计划听起来非常复古，他们不准备像发iPhone一样每年搞个发布会，他们准备像当年的曼哈顿计划一样，在一个封闭的实验室里，用几年时间，利用我们之前所提到这个研究范式，直接攻克那个最终极、最安全的超级智能。在这期间，他们可能没有任何产品，也没有任何收入，甚至会被外界嘲笑“掉队了”。这其实需要巨大的定力，也需要承担巨大的风险。如果他们赌输了，他们就只是一个烧光了几十亿美金的笑话。但如果他们赌赢了呢？他们带回来的必须是一个好的神。

### AI伦理的颠覆性视角：对齐感知生命

问题是什么是好？我们应该给这个神设定什么样的道德标准呢？这可能是整个对话中伊尔亚最具颠覆性的观点。以前我们谈AI安全、**对齐**（alignment: 指确保AI系统行为与人类意图、价值观和目标保持一致的研究领域），大家第一反应就是让AI听人类的话，或者是让AI遵守人类的价值观。这听起来没有毛病，对吧？但是伊尔亚反问：“哪个人类的价值观？是美国人呢？还是中国人呢？是21世纪的人类价值观，还是19世纪的？人类的价值观本身就充满了矛盾、偏见和混乱。甚至我们人类自己经常为价值观打得头破血流。”如果你把这些混乱的规则写进超级人工智能的代码里，你可能会造出一个精神分裂的怪物。

这里，伊尔亚提出了一个更底层、更朴实的观点：对齐**感知生命**（sentient beings: 指能够感受痛苦和快乐，具有意识的生命体）。什么叫做感知生命呢？就是一切能够感受到痛苦和快乐的存在，包括人类、包括动物，甚至可能包括未来那个拥有自我意识的AI自己。伊尔亚认为，与其教AI复杂的人类礼仪或者是法律条文，不如教它一个最简单的元规则：关爱那些能够感知痛苦的存在。这听起来有点像佛教的慈悲，或者是某种宗教情怀。伊尔亚在采访过程中也经常提这个佛教的理念。但在工程上，这可能比人类价值观更加**鲁棒**（robust: 指系统在面对异常输入、错误或变化时，仍能保持稳定和正常运行的能力），因为这个痛苦他是生物学上的一个客观信号，比正义或者自由这种抽象的概念呢，更容易定义。如果一个超级智能，他不仅聪明，而且他真的在乎你不受痛苦，像一个慈悲的长者爱护孩子一样，那也许是我们唯一的生路。

### 人类的命运：融合与进化的下一阶段

当然了，即使AI真的爱我们，那还有一个更现实的问题，就是自尊。如果这个AI把一切都做完了，它治好了癌症，管理了经济，甚至写出比莎士比亚更好的文章，那人类干什么呢？我们是变成那种被精养在动物园里的宠物，还是变成整天戴着VR眼镜，混吃等死的废人呢？伊尔亚提供了一个不想接受，但是可能不得不接受的解决方案：融合。如果说我们要跟上这个AI的步伐，如果我们想要保持在这个世界上的参与感，我们人类可能要通过**脑机接口**（Brain-Computer Interface, BCI: 指在大脑与外部设备之间建立直接连接，实现信息交换的技术），比如像Neuralink这样的技术，直接把AI带宽接入我们的大脑。我们将不再是单纯的**智人**（Homo sapiens: 现代人类的学名），我们将变成这种**半机械人**（cyborgs: 指结合了生物与机械或电子部件的生物体）。通过这种方式，AI的超级智力变成了我们自己的扩展，这时候甚至人类和AI的界限都会消失，我们就是他，他就是我们。这听起来非常赛博朋克，甚至有点恐怖，但是就像伊尔亚说的，这个变化是唯一的永恒。这也是一个佛教的概念。几千年前，我们通过发明语言和文字，把大脑外挂到了书本上，这本身就是一种融合。或许，脑机接口只是进化的下一个必然阶段。

### 结语：质变前夜的清醒与准备

最后，伊尔亚给出了他对于超级人工智能到来的时间预测：5-20年。这在历史的长河中，不过是一眨眼的时间。也许就在你还房贷这几年里，也许就在你孩子上大学之前，这世界将经历一场比工业革命和电力革命加起来还要剧烈1,000倍的变革。我们今天所讲的一切，从预训练的平静，到价值函数的觉醒，再到爱众生的伦理，并不是为了吓唬大家，而是为了让我们在面对一些眼花缭乱的新闻时，多一份清醒。不要只盯着那些股价和融资数字，去关注真正重要的问题：AI开始学会慢思考了吗？算力的重心开始转移了吗？我们是否为对齐做了真正严肃认真的准备呢？正如伊尔亚所说，这不仅是关于制造更聪明的机器，这是关于我们如何定义文明的未来。