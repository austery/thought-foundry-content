---
author: Dwarkesh Patel
date: '2025-04-03'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=htOvH12T7mU
speaker: Dwarkesh Patel
tags:
  - llm
  - agi
  - superintelligence
  - ai-alignment
  - forecasting
title: AI 2027：月度AI进展预测与智能爆炸探讨
summary: 本播客讨论了“AI 2027”情景预测，这是一个关于AI进展、通用人工智能（AGI）和超级智能月度预测。斯科特·亚历山大和丹尼尔·科科塔伊洛阐述了他们的模型，包括智能爆炸动态、AI对齐挑战、地缘政治影响（中美竞赛）以及高级AI对社会的影响。他们还探讨了透明度的重要性和博客的未来。
insight: ''
draft: true
series: ''
category: ai-ml
area: tech-work
project:
  - ai-impact-analysis
people:
  - Scott Alexander
  - Daniel Kokotajlo
  - Sam Altman
  - Dario Amodei
  - Elon Musk
  - Eli Liflund
  - Thomas Larsen
  - Jonas Vollmer
  - Robin Hanson
  - Katja Grace
  - David Anthony
  - Bill Gates
  - Eliezer Yudkowski
  - Tyler Cowen
  - Joseph Henrich
  - Larry Summers
  - Jason Furman
  - Peter Navarro
  - Garry Kasparov
  - Scott Aaronson
  - Alvaro de Menard
  - Paul Krugman
  - Clara Collier
companies_orgs:
  - OpenAI
  - AI Futures Project
  - Samotsvety
  - Google DeepMind
  - Anthropic
  - XAI
  - NASA
  - TSMC
  - AMA
products_models:
  - GPT-4
  - ChatGPT
  - Claude
  - GPT3
  - GPT7
  - Grok
  - Bing
media_books:
  - 《Slate Star Codex》
  - 《Astral Codex 10》
  - 《What 2026 Looks Like》
  - 《Horse, the Wheel and Language》
  - 《Superintelligence》
  - 《Secrets of Our Success》
  - 《Situational Awareness》
  - 《Asterisk》
  - 《Works in Progress》
  - 《Unsong》
status: evergreen
---
今天我很荣幸能与**斯科特·亚历山大**和**丹尼尔·科科塔伊洛**交流。斯科特当然是博客《**Slate Star Codex**》的作者，现在是《**Astral Codex 10**》。正如你所知，邀请你上播客一直是我的一个重要心愿。这是我们第一次一起做播客，对吗？是的。丹尼尔是**AI未来项目**的主管。你们今天刚刚发布了一个名为**AI 2027**的项目。这是什么？

### AI 2027：未来AI进展情景预测

**AI 2027**是我们的情景预测，旨在预测未来几年AI的进展。我们想做两件事。首先，我们希望有一个具体的预测情景。现在有很多人，像**萨姆·奥特曼**、**达里奥·阿莫代**和**埃隆·马斯克**，都说“三年内会有**通用人工智能**（AGI: Artificial General Intelligence，指在各种智力任务上都能达到或超越人类水平的人工智能），五年内会有**超级智能**（Superintelligence: 远超人类智能水平的AI）”。人们觉得这很疯狂，因为目前我们有的聊天机器人大多只能进行谷歌搜索，能力有限。所以人们会问：“三年内怎么可能出现AGI？”我们想做的是提供一个故事，提供过渡性的“化石”，从现在开始，一直到2027年AGI出现，2028年可能出现超级智能，展示每个月会发生什么。用小说创作的术语来说，就是让它“水到渠成”。这是容易的部分。困难的部分是我们还希望预测准确，预测事情将如何发展，以何种速度发展。我们知道，通常这种预测的最终结果都是被彻底打脸，因为事情会完全不同。如果你阅读我们的情景预测，你肯定不会认为我们会是这个趋势的例外。

让我感到乐观的是，丹尼尔在2021年写了这篇情景预测的前传，名为《**2026年看起来会怎样**》。那是他对未来五年AI进展的预测，而且他预测得几乎完全准确。你应该现在就暂停这个播客，去读那份文件，它非常惊人。它看起来就像你让**ChatGPT**总结过去五年AI进展，然后得到了一份有一些幻觉但基本善意且正确的总结。所以当丹尼尔说他要做续集时，我非常兴奋，很想知道它会走向何方。它走向了一些相当疯狂的地方，我很高兴今天能更深入地讨论它。

我觉得你有点夸大其词了。是的，我确实建议大家去读我以前写的那篇博客文章。我觉得它有很多地方是对的，也有很多地方是错的，但总的来说表现得很好，这激励我再次尝试，做一个更好的版本。我认为，大家可以阅读这份文件，然后判断我们俩谁说得对。另外，最初的预测本不应该在2026年结束，它本应该一直讲到所有激动人心的部分，对吧？因为大家都在讨论AGI和超级智能会是什么样子。所以我当时试图一步步地从我们所处的时代，推演到这些事情发生，然后看看它们会是什么样子。但我基本上在2027年的时候退缩了，因为事情开始发生，自动化循环开始加速，一切都太混乱，有太多不确定性，所以我基本上删掉了最后一章，只发布了当时已有的内容，那就是那篇博客文章。

### 参与AI 2027项目

好的，那么斯科特，你是如何参与到这个项目中的？我被邀请协助写作，并且对项目中的一些人已经有所了解，他们中的许多人都是我的偶像。我认识丹尼尔，不仅因为我写过一篇关于他观点的博客文章，而且在他写《**2026年看起来会怎样**》之前我就知道他了，那篇文章非常棒。最近他还上了全国新闻，因为他从**OpenAI**辞职时，公司要求他签署一份不贬损协议，否则就会收回他的股票期权。他拒绝了，这出乎OpenAI的意料。这引发了一场重大新闻事件，一场丑闻，最终导致OpenAI同意不再对员工施加这种限制。

人们经常谈论在AI领域很难信任任何人，因为他们都投入了大量资金来炒作和提升自己的股票期权。而丹尼尔为了表达自己的信念，试图牺牲数百万美元，这对我来说是诚实和能力的极强象征。我当时想，我怎么能拒绝这个人呢？团队里的其他人也同样令人印象深刻。**伊莱·利夫隆德**是**Samotsvety**（全球顶尖的预测团队）的成员。他赢得了顶级的预测比赛，可以说至少在超预测社区使用的那些技术衡量标准上，他是世界上最优秀的预测者。**托马斯·拉森**和**乔纳斯·沃尔默**也都是非常出色的人，之前在AI领域做出了卓越的工作。

我非常高兴能与这个明星团队合作。我一直想更深入地参与到让AI发展顺利的实际尝试中。现在，我只是写写文章。我认为写作很重要，但我不知道。你总是会后悔自己不是那个能够解决所有问题的技术对齐天才。能与这样的人合作，并可能产生影响，这似乎是一个绝佳的机会。我没有意识到的是，我也学到了很多。我努力阅读AI领域的大部分进展，但这是一种非常低带宽的方式，能与一个像世界上任何人都思考得一样多的人交谈，简直太棒了。这让我真正理解了AI将如何快速学习这些事情。你需要对底层领域有深入的参与，我觉得我做到了。在与你交谈以及试图提出反驳的过程中，我可能已经三四次改变了对智能爆炸的看法，从支持到反对，再到支持。

不仅仅是改变了我的想法，第一次阅读这个情景预测时，它显然还没有写成文章，而是一个巨大的电子表格。我已经思考这个问题十年，甚至十五年了。有一个具体的故事让它变得更加具体。比如，哦，是的，这就是我们如此担心与中国军备竞赛的原因。显然，在这种情况下，我们会与中国展开军备竞赛。除了那些人，阅读这个情景预测真的说服了我。这东西需要更广泛地传播出去。

### AI 2027预测：月度进展

好的，现在我们来谈谈这个新的预测。因为你们对未来会发生什么做了逐月分析。那么，在这个预测中，你们预计2025年中期和2025年末会发生什么？预测的开头主要关注**智能体**（Agents: 能够感知环境、做出决策并采取行动的AI系统）。我们认为它们将从**智能体训练**（Agency Training: 训练AI系统以实现特定目标并采取行动的过程）开始，扩展时间范围，并使编码进展顺利。我们的理论是，它们在某种程度上是自觉地，在某种程度上是偶然地，朝着智能爆炸的方向发展，即AI本身可以开始接管部分AI研究，从而更快地发展。

所以，2025年，编码能力会略有提升；2026年，智能体和编码能力都会略有提升。然后我们重点关注2027年，这个情景预测也因此得名，因为那时智能爆炸将全面展开，智能体将变得足够好，可以协助（最初不是真正执行，而是协助）部分AI研究。我们引入了**研发进展乘数**（R&D Progress Multiplier: 衡量AI辅助下研发速度相对于无AI辅助的提升倍数）这个概念：在AI的帮助下，一个月能取得多少个月的进展。所以到2027年，我们预计（我不记得是字面上从那时开始，还是到3月左右）算法进展将达到**五倍乘数**。

我们网站上跟踪了故事的统计数据。我们之所以把它做成网站，部分原因是为了能有这些酷炫的小工具和部件。当你阅读故事时，侧边的统计数据会自动更新。其中一个统计数据就是进展乘数。你问的同一个问题的另一个答案是：2025年，没有发生什么特别有趣的事情，基本上与我们目前看到的趋势相似。

### 2025年计算机使用能力预测

计算机使用问题是完全解决了，还是部分解决了？到2025年底，计算机使用能力会有多好？我猜到2025年底，它们不会再犯像现在有时会犯的基本鼠标点击错误。如果你看《**Claude玩宝可梦**》（Claude Plays Pokemon），你会发现它有时会无法解析屏幕上的内容，甚至认为自己的玩家角色是**NPC**（Non-Player Character: 游戏中的非玩家角色），然后感到困惑。我猜这种问题到今年年底大部分会消失，但它们仍然无法长时间自主操作。

但是到2025年，当你说它无法长时间连贯地使用计算机时，如果我想在办公室组织一个欢乐时光，那大概是30分钟的任务？其中有多少部分是它能完成的，比如邀请合适的人，预订合适的**DoorDash**之类的？我猜到今年年底，会有一些东西能做到，但不可靠。如果你真的尝试用它来管理你的生活，它会犯一些搞笑的错误，然后会在**Twitter**上走红。但它的**最小可行产品**（MVP: Minimum Viable Product，指具有最少功能但足以满足早期用户需求的产品）可能会在今年出现。比如会有一些Twitter帖子说：“我把这个智能体接入来管理我的派对，它成功了！”

### 智能爆炸的触发点：编码能力

我们的情景预测特别关注编码，因为我们认为编码是智能爆炸的起点。所以我们不太关心“如何解决人类独有的最后几项任务”这类问题，而是更关心“AI何时能开始以一种帮助人类AI研究人员加速其AI研究的方式进行编码，然后，如果它们足够快地加速了AI研究，这是否足以通过一些惊人的速度乘数——10倍、100倍——来解决所有其他问题？”

我有一个观察：在2021年**ChatGPT**问世后，你本可以讲一个故事。我记得有些可信的AI思想家当时说：“看，现在有了编码智能体，问题解决了。现在**GPT-4**会四处进行工程设计，我们再在此基础上进行**强化学习**（RL: Reinforcement Learning，一种机器学习方法，通过让智能体在环境中采取行动并根据结果获得奖励或惩罚来学习最佳行为策略），我们可以将系统扩展100倍。”但这个过程的每一个层面都比最乐观的预期要困难得多。似乎在增加预训练规模方面存在显著困难，至少从关于现场训练运行或实验室训练运行效果不佳的传闻来看是如此。

从完全外部的角度来看（我对实际工程一无所知），构建这些**RL**（强化学习）系统显然在**GPT-4**发布后至少花了两年时间。而且这些东西的经济影响以及你根据基准测试立即期望它们特别擅长的那些事情，并没有那么压倒性，比如呼叫中心员工还没有被解雇。那么，为什么不直接说，看，在更高的规模下，它可能会变得更加困难呢？

### AI预测的乐观与悲观

等一下，听到你这么说我有点困惑，因为当我看到人们预测AI里程碑时，比如**卡佳·格雷斯**（Katja Grace）的专家调查，他们几乎总是过于悲观，从AI进展速度的角度来看。我记得2022年的调查，他们实际上说已经发生的事情需要10年才能发生，但那次调查——可能是2023年的，在**GPT-3**、**GPT-4**问世前六个月——**GPT-3**或**GPT-4**（无论是哪个）在六个月内完成的事情，他们当时仍然预测需要五到十年。我相信丹尼尔会有更详细的答案，但我完全不同意“每个人都一直过于乐观”这个前提。

是的，我认为总的来说，大多数关注这个领域的人都低估了AI进展的速度，也低估了AI向世界扩散的速度。例如，**罗宾·汉森**（Robin Hanson）曾打赌，到2025年AI带来的收入会低于10亿美元。我同意**罗宾·汉森**尤其过于悲观。但他是个聪明人。所以我认为，总体观点一直低估了技术进步和部署的速度。我同意有很多比我更乐观的人已经被证明是错的，但他们不是我。

等一下，我们不必猜测总体观点，我们可以看看**Metaculus**。**Metaculus**在2020年时，他们的预测时间线大概是2050年。三年前，它逐渐下降到2040年左右。现在是2030年，所以它也只比我们稍微领先一点。这可能再次被证明是错误的，但看起来**Metaculus**的预测者总体上一直过于悲观，倾向于长期预测而非过于乐观。我认为这是我们最接近中立聚合器的地方，我们没有选择性地挑选信息。

### AI对研究的实际帮助

是的，我昨天有一次有趣的经历。我们和一位资深AI研究员共进午餐，他每月收入大概数百万美元。我们问他：“AI对你有多大帮助？”他说：“在我非常了解的领域，它更像是更强力的自动补全，每周可以节省我四到八小时。”但他接着说：“在我不太熟悉的领域，如果我需要去处理某个硬件库，或者修改内核之类的，我了解较少的地方，那每周可以节省我大约24小时。”这是当前模型就能做到的。我发现真正令人惊讶的是，在AI的帮助更像新颖贡献而非自动补全的领域，它的帮助更大，生产力提升也更显著。

是的，这很有趣。我猜想，当你对某个领域不熟悉时，很多过程都像是在**Google**上搜索并学习更多关于该领域的信息。而语言模型在这方面非常出色，因为它们已经阅读了整个互联网，了解所有细节。这不是一个讨论我问**达里奥**，你回应的某个问题的好机会吗？你在想什么？

### AI的发现能力与人类智慧

我问了这个问题，正如你所说，这些模型知道所有这些东西。我不知道你是否看到过。我问的这个问题是，看，这些模型知道所有这些东西。如果一个人知道人类在互联网上写下的每一件事，他们就能在不同的想法之间建立所有这些有趣的联系，甚至可能因此找到医疗方法或科学发现。有一个人注意到镁缺乏会导致大脑中发生一些与偏头痛类似的事情。所以他只是说：给你补充镁，这治愈了很多偏头痛。那么，为什么它们无法利用这种巨大的不对称优势来做出像这样的一个新发现呢？

然后我举的例子是，人类也做不到这一点。对我来说，最突出的例子是词源学。英语中有很多词非常相似，比如“happy”和“hapless”、“happen”、“perhaps”。我们从不思考它们，除非你读一本词源词典，然后你会发现，哦，显然这些词都来自某个古老的词根，意思一定是“运气”或“发生”之类的。所以这有点像“发现”与“验证”的区别。如果我告诉你这些，你会觉得“这听起来很合理”。当然，在词源学中，也有很多“假朋友”，它们听起来合理但实际上没有关联。但你真的需要有人把它摆在你面前，你才会开始思考并建立所有这些联系。

我其实不同意这一点。我们知道人类可以做到，我们有例子表明人类可以做到。我同意我们没有**逻辑全知**（Logical Omniscience: 指一个人或AI系统能够立即知道所有逻辑推论和事实，没有知识上的限制），因为存在组合爆炸，但我们能够利用我们的智能。我最喜欢的一个例子是**大卫·安东尼**（David Anthony），他写了《**马、轮子和语言**》（Horse, the Wheel and Language）。他在我们获得遗传学证据之前，大约十年前，就做出了一个非常了不起的发现。他说，看，如果我观察印度和欧洲的所有这些语言，它们都共享相同的词源。我的意思是，像“轮子”、“马车”和“马”这样的词，它们的词源完全相同。而这些技术只存在了大约6000年，这意味着这些群体至少在语言上都源自某个共同的群体。现在我们有了**亚姆纳亚人**（Yamnaya: 约公元前3500-2500年存在于东欧大草原的文化群体，被认为是印欧语系扩散的重要来源）的遗传学证据，我们认为这就是那个群体。斯科特，你有一个博客就是做这个的！所以我们为什么不应该更严厉地看待语言模型无法做到这一点的事实呢？

是的。所以对我来说，他似乎不是坐在那里逻辑全知地得出答案。他似乎是一个天才，他为此思考了多年，可能在某个时候，他同时听到了几个印度词和几个欧洲词，它们之间产生了某种联系，然后灵光一现。所以这与其说是将所有信息存储在记忆中，不如说是正常的发现过程，这个过程有点神秘，但似乎源于拥有良好的**启发式方法**（Heuristics: 解决问题或做出决策的经验法则或捷径，不保证最优解但通常有效）并不断尝试，直到你偶然获得成功。

我猜测，如果我们有真正优秀的AI智能体，并将它们应用于这项任务，它会像一个脚手架一样：思考你所知道的每一个词的组合，进行比较。如果它们听起来非常相似，就写在这个草稿板上。如果草稿板上出现很多相同类型的词，那很奇怪，就围绕它进行一些思考。我只是觉得我们甚至没有尝试过。而且我认为现在如果我们尝试，就会遇到**组合爆炸**（Combinatorial Explosion: 指随着问题规模的增加，可能解的数量呈指数级增长，导致计算资源迅速耗尽的现象）。我们需要更好的启发式方法。人类有如此好的启发式方法，以至于即使是我们意识中出现的大部分事物，而不是在某种无意识处理层面发生的，至少也是可能真实的事物。你可以把它想象成一个国际象棋引擎。你有无数种可能的下一步棋，你有一些启发式方法来选择哪些是正确的。然后，你逐渐让国际象棋引擎思考，通过它，得出更好或更差的走法，然后在某个时刻，你可能会比人类更强。我想，如果你强迫AI以合理的方式做到这一点，或者你训练AI，使其本身能够以某种启发式驱动的方式来完成这个计划，你就有可能与人类匹敌。

### LLM的局限性与未来发展

我再补充几点。我认为，人们长期以来一直有一个糟糕的习惯，就是看到当前**大型语言模型**（LLM: Large Language Model，指拥有大量参数、通过海量文本数据训练的深度学习模型）的某些局限性，然后就大肆宣称整个范式注定失败，因为它永远无法克服这个局限性。然后一两年后，新的LLM就克服了那个局限性。关于“为什么它们没有通过结合已有的知识并注意到有趣的联系来做出这些有趣的科学发现？”这个问题，我想说，首先，我们是否认真尝试构建脚手架来让它们做到这一点？我认为答案大多是否定的。

我认为**Google DeepMind**尝试过这个。也许吧。其次，你有没有尝试把模型做得更大？过去几年他们确实做得更大了一些，但到目前为止还没有奏效。也许如果他们做得更大，它会注意到更多的联系。第三点，我认为这是最特别的一点：你有没有尝试训练模型来做这件事？预训练过程并没有强烈地激励这种建立联系的能力。

总的来说，我认为我使用的一个有用的启发式方法是问自己：提醒一下，AI是受训来做什么的？它的训练环境是怎样的？如果你想知道为什么AI没有做到这一点，问问自己，训练环境是否训练它做这个了？通常答案是否定的。而且我通常认为，这就是为什么AI不擅长它的一个很好的解释，因为它没有被训练去做这件事。我的意思是，这看起来是如此具有经济价值……但是你如何设置训练环境呢？尝试设置一个**强化学习**（RL: Reinforcement Learning，一种机器学习方法，通过让智能体在环境中采取行动并根据结果获得奖励或惩罚来学习最佳行为策略）环境来训练它做出新的科学发现，难道不会非常棘手吗？也许这就是为什么你应该有更长的时间线。这是一个棘手的工程问题。

### 智能爆炸的迭代路径

在我们的情景预测中，它们并非一蹴而就地解决这个问题。相反，它们只是迭代地改进编码智能体，直到基本上解决了编码问题。但即便如此，它们的编码智能体仍然无法完成某些任务。在我们的故事中，2027年上半年基本上是这样的：它们拥有这些出色的自动化编码器，但仍然缺乏研究品味，可能还缺乏组织技能等。

因此，它们需要克服这些剩余的瓶颈和差距，才能完全自动化AI研究周期。但它们能够比平时更快地克服这些差距，因为编码智能体正在非常快速地完成所有繁重的工作。是的，我认为将我们的时间线想象成2070年、2100年可能会很有用。只不过，那50到70年的进展都发生在2027年到2028年之间，因为我们正在经历智能爆炸。我想，如果我问你，我们能否在2100年解决这个问题？你会说，哦，是的，到2100年？绝对可以。而我们只是说，2100年可能会比你预期的更早到来，因为我们有这个研究进展乘数。

### AI能力与人类知识的结合

然后我稍后会谈到这一点。但关于这个话题的最后一点思考是：这里存在一种**肯定前件**（Modus Ponens: 一种逻辑推理形式，如果P则Q，P为真，则Q为真）和**否定后件**（Modus Tollens: 一种逻辑推理形式，如果P则Q，Q为假，则P为假）的关系。你可以说，看，AI——不仅仅是LLM，而是AI——将拥有这种根本性的不对称优势，它们知道所有这些东西。为什么它们不能利用其通用智能来利用这种不对称优势，从而获得巨大的能力优势呢？

现在，你也可以通过说，好的，一旦它们拥有了通用智能，它们就能够利用它们的不对称优势来取得所有这些人类原则上无法取得的巨大进步，从而推断出同样的说法，对吗？所以基本上，如果你确实认同AI只要拥有通用智能就能做到所有这些事情的观点，你就会觉得，一旦我们真正获得了**通用人工智能**（AGI），它将是完全变革性的，因为它们将记住所有人类知识，并能利用这些知识建立所有这些联系。

我很高兴你提到我们目前的情景预测并没有太多地考虑到这一点。所以这是一个例子，说明我们的情景预测可能低估了进展速度。丹尼尔，你太保守了。这是我与团队合作的经验，我指出了五件不同的事情。“你确定你考虑到了这一点吗？你确定你考虑到了这一点吗？”首先，99%的时候他会说：“是的，我们有补充说明。”但即使他没有这么说，他也会说：“是的，这是一个可能让事情变慢的原因。但这里有10个可能让事情变快的原因。”它试图成为我们的中位数猜测。所以我们可能低估了很多方面，也可能高估了很多方面。我们希望之后能继续对此进行更多思考，并继续迭代完善我们的模型，提出更好的猜测等等。

### AI发展速度的预测与历史参照

所以，如果我回顾过去的AI进展，假设我们回到2017年。假设我们在2017年拥有这些超人编码器；那么从那时起我们所取得的进展，也就是我们目前在2025年的水平，我们本可以提前多久达到？很好的问题。我们仍然需要摸索2017年以来我们所做的所有发现。我们仍然需要弄清楚语言模型是什么，我们仍然需要弄清楚可以用**强化学习**（RL）来微调它们。

所以所有这些事情仍然需要发生。它们会快多少？也许快5倍，因为这些人为了在进行大型训练之前快速测试想法而进行的大量小规模实验会快得多，因为它们会迅速地被吐出来。我对这个5倍的数字不是很确定，它可能更低，也可能更高，但这大致是我们的猜测。顺便说一下，我们的5倍是针对算法进展部分，而不是整体。所以在这个假设中，我认为事情基本上会快2.5倍，其中算法将以5倍的速度发展，但计算能力仍然停留在通常的速度。

这对我来说似乎是合理的。你在某个点上达到5倍，然后，然后，在一年之内AI进展达到1000倍。也许这就是我感到疑惑的部分，这到底是怎么发生的？那么，故事是怎样的？我们预测“**起飞**”（Takeoff: 指AI能力快速增长，可能在短时间内超越人类智能的时期）的方式，我们稍后会提到，基本上是将我们认为的智能爆炸过程分解为一系列里程碑。首先是自动化编码，然后是自动化整个研究过程，但方式与人类非常相似，由接近人类水平的智能体团队完成，然后达到超人水平等等。

所以我们将其分解为这些里程碑，超人编码器、超人AI研究员，然后是超智能AI研究员。我们预测的方式是，对于每一个里程碑，我们都会想，要制造一个达到该里程碑的AI需要什么？一旦达到该里程碑，你的整体加速会是多少？然后，要达到下一个里程碑需要什么？将其与整体加速结合起来，就能得到达到该里程碑所需的时间距离，然后，好的，现在你达到了那个里程碑。你的整体加速是多少？假设你也达到了那个里程碑，下一个是什么？达到下一个需要多长时间？所以我们就是这样一点一点地推演，在每个阶段都做出我们最好的猜测。

### 智能爆炸的量化预测

所以从定量角度来看，我们认为超人编码器能使算法进展加速约**5倍**，而超人AI研究员能使算法进展加速约**25倍**。因为到那时，整个堆栈都已自动化，这比仅仅自动化编码要有用得多。然后我忘了我们对超智能AI研究员的说法，但凭印象大概是整体加速数百倍，甚至可能达到**1000倍**。

所以，也许我对智能爆炸的宏观看法是……我们可以深入探讨自动化编码器能做多少，以及超人AI编码器能做多少的具体论点。但从先验来看，这本身就是一件非常疯狂的事情。因此，在我们深入所有具体论点之前，也许你可以先谈谈这个想法：为什么不先从0.01%的可能性开始，然后需要极其、极其强有力的证据才能将其作为你的主流观点？

我认为这是一个关于你的默认选项是什么，或者你将其与什么进行比较的问题。我认为人们天真地认为，好吧，每件事都可能出错。所以我们不如设定一个默认路径，即什么都不会发生。而我认为，这始终是最错误的预测。我认为，要让什么都不发生，实际上需要发生很多事情。比如，你需要AI的进展突然停止，而这种进展已经以恒定速度持续了很长时间。它为什么会停止？

### 历史趋势与AI进展的保守预测

我们不知道。你对此提出的任何主张，都可能存在大量的模型外误差。你会期望有人提出了一个相当明确的主张，而你想挑战它。所以我认为不存在一个中立的立场，你可以说，好吧，鉴于模型外误差非常高，我们什么都不知道，所以就选择那个。我认为我们正在尝试采取——我知道这听起来很疯狂，因为如果你读我们的文件，会发生各种奇怪的事情。这可能是史上最奇怪的几年。但我们试图在某种意义上采取一种**保守立场**（Conservative Position: 指在预测或决策中倾向于低估可能性或风险，以避免过度乐观或冒险），即趋势不会改变，没有人会做疯狂的事情，我们没有证据表明会发生的事情也不会发生。而AI智能爆炸的动态是如此奇特，以至于要让什么都不发生，你需要发生很多疯狂的事情。

我最喜欢的表情包图片之一是这张世界**GDP**（Gross Domestic Product: 国内生产总值，衡量一个国家或地区经济活动总量的指标）随时间变化的图表。你可能见过，它在2010年左右达到顶峰，然后顶峰上有一个小小的思想泡泡，写着：“我的生活很正常，我很清楚什么是奇怪的，什么是标准的，那些思考数字思维和太空旅行等不同未来的人，只是在进行愚蠢的猜测。”

这张图的重点是，历史上实际上发生过许多惊人的变革性变化，这些变化在当时对人们来说会显得完全疯狂。我们已经经历了多次这样的浪潮。我们谈论的一切都曾发生过。算法进步每年都在翻倍。所以认为算法进步可以促进这些计算能力的发展并非疯狂。就整体加速而言，我们已经达到了相对于旧石器时代大约**一千倍**的研究加速乘数。所以从历史上大多数人的角度来看，我们正以令人眼花缭乱的疯狂速度前进。而我们在这里说的，就是它不会停止。

同样的趋势，导致我们相对于过去的时代，甚至不是旧石器时代，而是比如公元600年到700年之间的一个世纪，拥有了千倍的加速乘数。我确信历史学家可以指出一些事情。然后你看看1900年到2000年之间的一个世纪，它在性质上是完全不同的。当然，有模型讨论最近是否停滞不前，或者这里发生了什么。我们可以讨论这些，我们可以讨论为什么我们期望智能爆炸能成为这种停滞的解药。但我们所说的一切，与已经发生的事情并没有太大不同。

我的意思是，你是在说以前的这些转变比你预期的要平稳。我们对此并不确定。所以其中一个模型只是一个**双曲线**（Hyperbola: 一种数学曲线，常用于描述指数增长或加速现象）。一切都沿着同一条曲线发展。另一个模型是存在像字面意义上的**寒武纪大爆发**（Cambrian Explosion: 约5.4亿年前地球生命多样性迅速增加的时期）、农业革命、工业革命这样的**相变**（Phase Changes: 指系统从一种状态突然转变为另一种状态的现象）。

当我审视经济模型时，我的印象是经济学家认为我们没有足够好的数据来确定这是否是一个平稳的过程，或者它是否是一系列相变。当它是一个平稳过程时，这个平稳过程通常是一个以奇怪方式趋于无限的双曲线。我们不认为它会趋于无限。我们认为它会再次遇到瓶颈。你们是保守派，你知道吗？我们认为它会像所有以前的过程一样遇到瓶颈。如果从双曲线的角度来看，上次遇到瓶颈是在1960年左右，当时人类的繁殖速度停止了之前的增长。我们遇到了人口瓶颈，通常的人口、思想、飞轮停止了运转，然后我们停滞了一段时间。

### 人口瓶颈与AI驱动的增长

如果你能在数据中心里创造一个天才国家，就像**达里奥·阿莫代**（Dario Amodei）所说的那样，那么你就不再有这个人口瓶颈，你只是期望1960年之前的趋势能够延续。所以我意识到所有这些历史双曲线也都有点奇怪，有点理论化，但我不认为我们所说的有什么是以前没有模型能够解释的，而且这些模型在漫长的历史时期中似乎都奏效了。

另外一点是，我认为人们混淆了“慢”和“连续”，对吧？所以如果你看我们的情景预测，整个过程中都贯穿着算法进展乘数这个连续的趋势。我们没有从0跳到5倍再到25倍的离散跳跃。我们有的是持续的改进。所以我认为“连续”不是关键。关键是，它会这么快吗？我们不知道，也许会慢一点，也许会快一点。但我们有我们的论据，说明为什么我们认为它可能会这么快。

### 智能爆炸的瓶颈：研究品味与计算资源

好的，既然我们提到了智能爆炸，那就来讨论一下，因为我有点怀疑。在我看来，AI进展的一个显著瓶颈，或者说主要瓶颈，并不是从事这类研究的研究人员和工程师的数量。它似乎更像是计算能力或其他因素的瓶颈。证据是，当我与实验室的AI研究员朋友交谈时，他们说核心预训练团队中可能只有20到30人，他们正在发现所有这些算法突破。如果这里的人力如此宝贵，你会认为，例如，**Google DeepMind**不仅会把他们所有最聪明的人，而且会把**Google**所有最聪明的人都投入到预训练或**强化学习**（RL）中，或者任何主要的瓶颈上。你会认为**OpenAI**会雇佣所有**哈佛**数学博士，并在六个月内将他们全部培养成AI研究人员。我知道他们正在增加人手，但他们似乎并没有将其视为那种需要数百万人在并行中迅速加速AI研究的瓶颈。

有句名言说“一个拿破仑抵得上四万士兵”，这在他作战时是常说的。但十个拿破仑抵不上四十万士兵，对吧？那么为什么会认为这数百万AI研究人员会给你带来某种智能爆炸呢？

我之前谈到了我们**起飞模型**的三个阶段。首先是获得**超人编码器**（Superhuman Coder: 指在编码能力上远超人类顶尖水平的AI）。其次是完全自动化AI研发，但其水平仍基本与人类相当，与你最优秀的人类一样好。第三是进入**超级智能**领域，其质量上更优。在我们对算法进展速度的估计中，中间阶段的进展乘数，我们基本上确实假设，拥有更多并行运行的思维会带来巨大的**边际效益递减**（Diminishing Returns: 指在生产过程中，当其他投入保持不变时，增加某一投入所带来的产出增量会逐渐减少的现象）。所以我们完全认同这一点。

是的。然后我认为补充一点是，那么为什么会发生智能爆炸？答案是：**速度提升**和**串行思维速度**提升的结合。还有**研究品味**（Research Taste: 指研究人员在选择研究方向、设计实验和解读结果时所展现出的直觉、洞察力和判断力）这回事。以下是当今AI研发进展的一些重要投入：研究品味。也就是你最优秀的研究人员的素质，那些管理整个过程的人，他们从数据中学习并更有效地利用计算资源的能力，通过进行正确的实验而不是盲目地进行一堆无用的实验。这就是研究品味。

然后是研究人员的数量，我们刚才谈过了。然后是研究人员的**串行速度**（Serial Speed: 指AI系统在执行单个任务或推理步骤时的处理速度），目前都是一样的，因为他们都是人类，所以他们都以基本相同的串行速度运行。最后是你有多少计算资源用于实验。所以我们设想的是，串行速度开始变得非常重要，因为你转向了比人类拥有**数量级**（Orders of Magnitude: 指数量上的巨大差异，通常是10倍或100倍的差异）更高串行速度的AI研究人员。但它会达到顶峰；我们认为在我们的情景预测中，如果你查看我们的滑动统计图表，它在情景预测过程中从20倍增加到90倍左右，这很重要，但不是巨大的。而且我们认为一旦你达到90倍的串行速度，你就会被其他因素所限制，所以串行速度的额外改进基本上没有太大帮助。至于数量，是的，我们设想你会有数十万个AI智能体，一百万个AI智能体，但这只会意味着你会受到其他因素的限制。你拥有大量的并行智能体，这不再是你的瓶颈。你会受到什么限制？品味和计算能力。

所以，在我们的故事中，到2027年中期，当他们完全自动化了AI研究时，基本上只有两件事是重要的：你的AI的品味水平如何，它们从你正在进行的实验中学习的能力有多强？以及你拥有多少计算资源来运行这些实验？这就是我们模型的核心设置。当我们获得25倍的乘数时，就是从这些前提开始的。

### 工业革命与AI加速的类比

历史上是否有过这样的直觉泵，即某种产出因为一些非常奇怪的限制，其生产被迅速地偏向于某个单一投入，而不是所有历史上相关的投入，但仍然取得了突飞猛进的进展？也许是**工业革命**。我只是即兴发挥，之前没想过这个，但正如斯科特十年前对我影响深远的那篇著名文章所讨论的，工业革命导致了人口增长与整体经济增长的脱钩。所以在某种意义上，你也许可以说这是一个例子，说明以前这些事物是同步增长的。更多的人口，更多的技术，更多的农场，更多的房屋等等。你的资本基础设施和人力基础设施是共同增长的，但后来我们有了工业革命，它们开始分离。

现在，所有资本基础设施的增长速度都比人类人口规模快得多。我想我正在想象算法进步可能发生类似的事情。而且，就人口而言，人口在今天仍然非常重要。在某种意义上，进步受到人口规模扩大等因素的限制。但这只是因为人口增长率本身就比较慢，而资本的增长率要快得多。因此，它在故事中占据了更大的比重。

### AI在线学习与现实世界数据

也许这听起来不如25倍的数字那么可信，原因在于，当我具体思考那会是什么样子时，你拥有这些AI，我们知道人类大脑和这些AI之间在数据效率上存在差距。所以，它们中的很多都在努力思考，它们非常努力地思考，并找出如何定义一种像人脑一样，或者拥有人脑优势的新架构。我猜它们仍然可以做实验，但数量不多。我有点好奇，如果你只需要一种完全不同的数据源，而不是为此进行预训练，而是它们必须到现实世界中获取数据，会怎么样？或者，也许它需要是一种**在线学习**（Online Learning: 指AI系统在运行时不断从新数据中学习和适应，而非一次性完成训练）策略，它们需要被积极部署到世界中，才能以这种方式学习。这样，你就会受到它们获取现实世界数据速度的限制。我只是觉得这很难……

所以我们实际上设想了**在线学习**的发生。哦，真的吗？是的。但与其说是现实世界，不如说是……问题是，如果你试图训练你的AI进行非常好的AI研发，那么AI研发就发生在你的服务器上。所以你可以有一个循环：你拥有所有这些AI智能体自主进行AI研发，进行所有这些实验等等，然后它们会根据这些实验的结果进行在线学习，以更好地进行AI研发。

但即使在这种情况下，我也能想象到瓶颈，比如，你有一个基准，它因为“AI研发”的构成而被**奖励作弊**（Reward Hacked: 指AI系统找到一种规避预期目标、通过利用奖励机制漏洞来最大化奖励的方式，而非真正实现人类期望的目标）。因为你显然不能拥有……也许你会，但它真的和人脑一样好吗？这只是一个模糊的概念。现在我们就有被奖励作弊的基准，对吧？但它们会自主构建新的基准。我想你说的可能是，由于缺乏与真实世界（数据中心之外的实际世界）的接触，整个过程可能会脱轨。也许吧？再说一次，我在这里的猜测是，你想要接触的很多**真实世界数据**（Ground Truth: 指用于训练和评估AI模型，被认为是绝对真实和准确的数据）是发生在数据中心上的东西，比如你在所有这些指标上改进的速度，你有一些关于新架构的模糊想法，但你很难让它们工作。你多快能让它们工作？

然后，另外，如果存在与外界交流的瓶颈，那么它们仍然在做这些。一旦它们完全自主，它们甚至可以更快地做到这一点。你可以让所有数百万个副本连接到所有这些各种现实世界的研究项目等等。所以它们并不是完全缺乏外部资源。

### AI官僚体系与合作效率

对于这种怀疑论，你怎么看？你所设想的这种超高效的AI研究者**蜂巢思维**（Hive Mind: 指多个个体或AI系统共享一个集体意识或目标，以高度协调的方式运作），没有任何人类官僚体系一开始就能超高效运作，尤其是在它们没有合作经验的情况下。它们至少还没有被训练成合作。而且还没有这种外部循环的**强化学习**（RL），比如“我们运行了一千个不同的AI官僚体系进行AI研究的并发实验，这是效果最好的一个”。

我可能会用的类比是20万年前非洲大草原上的人类。我们知道他们当时已经比其他动物有很多优势，但使我们今天占据主导地位的东西，比如股份制公司、国家能力，以及我们这种耗费了大量文化演进才形成的化石燃料文明。你不可能在大草原上就想出来：“哦，如果我们建立了这些激励系统，并派发股息，那我们就能真正合作了”之类的。为什么不认为这需要一个类似的过程，即巨大的人口增长、大量的社会实验，以及AI社会技术基础的升级，然后它们才能组织起这种超思维集体，从而使它们能够实现你所想象的智能爆炸呢？

是的，你把它比作两种不同的事物。一种是**非洲大草原**上的字面意义上的**基因进化**，另一种是我们此后经历的**文化进化**。我认为AI也会有这两种等价物。所以字面意义上的基因进化是，我们的思维在那段时间适应了更适合合作。所以我认为公司会非常字面意义地训练AI变得更具合作性。我认为那里有更多的**可塑性**（Pliability: 指系统或个体适应和改变的能力）机会。因为人类当然是在这种基因指令下进化的，我们希望传递自己的基因信息，而不是别人的基因信息。你有一些像**亲缘选择**（Kin Selection: 一种进化理论，认为个体通过帮助亲属繁殖来间接传递自己的基因，即使这会牺牲自己的直接繁殖机会）这样的例外，但总的来说这是规则。

在没有这种基因指令的动物中，比如**真社会性昆虫**（Eusocial Insects: 指具有高度社会组织结构，分工明确，并有生殖分化的昆虫，如蚂蚁、蜜蜂），你很快就会通过基因进化，而不是文化进化，实现极端的合作。对于真社会性昆虫，它们都有相同的基因代码，都有相同的目标。因此，进化的训练过程将它们彼此束缚在一起，形成了这些极其强大的官僚体系。我们确实认为AI会更接近真社会性昆虫，因为它们都有相同的目标，特别是如果这些不是**指示性目标**（Indexical Goals: 指与特定个体或情境相关的目标），而是像“让研究项目成功”这样的目标。所以这将改变每个个体AI的权重，我的意思是，在它们个体化之前，它将改变AI整体的权重，使其更适合合作。

然后，是的，你确实有文化进化。就像你说的，这需要数十万个个体。我们确实期望会有数十万个个体。这需要几十年。再说一次，我们期望这种研究乘数能让几十年的进展在2027年或2028年这一年内发生。所以我认为在这两者之间，这是可能的。也许这也是串行速度真正重要的地方。因为如果它们以50倍于人类的速度运行，那就意味着一周的实际时间里可以发生一年的主观时间。所以这些大规模的合作动态，你的道德迷宫，你有一个机构，但它变成了道德迷宫，并在自身重压下崩溃等等。实际上有时间让它们多次演练，然后在此基础上进行训练，修补结构，并将其添加到2027年的训练过程中。

此外，它们还拥有人类迄今为止发展出的所有文化技术的优势。这可能不完全适合它们，更适合人类。但想象一下，你必须和你一百个最亲密的朋友一起创业，你们在所有事情上都意见一致。也许他们真的是你的同卵双胞胎，他们从未背叛过你，也永远不会。我认为这并不是一个很难的问题。而且，它们从更高的起点开始，从人类制度开始。你可以为所有AI智能体设置一个**Slack**工作区进行交流。你可以建立一个有角色的层级结构。它们可以从成功的人类制度中借鉴很多。

我猜组织越大，即使所有人都目标一致——我想你的一些回答已经解决了它们是否会在目标上一致的问题。我的意思是，你确实解决了整个问题，但我只想指出这一点；这不是我怀疑的部分。我更怀疑的是，即使你们都目标一致并想一起工作，你们是否从根本上理解如何管理这个庞大的组织。而且你们正在以人类从未有过的方式进行，你们被不断复制，运行速度极快，你明白我的意思吗？

### AI官僚体系的构建速度

我认为这完全合理。所以这是一个复杂的问题。我只是不确定你为什么认为我们会在这么短的时间内建立这个官僚体系，或者AI会在这么短的时间内建立这个官僚体系……所以我们描绘它发生在2027年的六到八个月左右，你会说两倍长，五倍长，十倍长吗？五年？

所以五年，如果它们以50倍的串行速度运行，那么五年是多少？大约是AI的250年串行时间，这对我来说感觉足以真正解决这类问题。你将有时间让“帝国兴衰”，所有这些都将添加到训练数据中，是的。但我可以看到它可能比我们描绘的更长。也许不是六个月，而是18个月，你知道，但也可能只有两个月。

所以当我思考他们训练AI的方式时，我认为在我们的情景预测中，目前有两种主要方式。其中一种是继续进行**下一个词元预测**（Next Token Prediction: LLM训练的核心任务，根据前面的文本预测下一个词元，以学习语言的模式和结构）工作。所以这些AI将能够访问所有人类知识，它们在某种意义上已经阅读了管理学书籍，它们并非盲目开始。会有类似“预测**比尔·盖茨**会如何完成下一个字符”这样的事情。

然后是虚拟环境中的**强化学习**（RL）。让一个AI团队玩一些多人游戏。我认为你不会使用人类的游戏，因为你会想要更适合这项任务的东西。但只是让它们一遍又一遍地通过这些环境，根据成功进行训练，根据失败进行训练，将这两种事情结合起来。对我来说，这似乎不像从旧石器时代开始发明所有人类制度那样的问题。它似乎只是应用了这两件事。

### 超级智能与技术发展路径

你的模型中另一个值得注意的地方是，你在最后得到了这个超人级别的存在，然后它似乎只是通过**镜像生命**（Mirror Life: 指在数字世界中模拟或复制真实生命形式的技术）和**纳米机器人**（Nanobots: 具有纳米级尺寸，能够执行特定任务的微型机器人）等各种疯狂的技术树一路发展。也许这部分我也非常怀疑。如果你看看发明史，似乎人们只是在尝试不同的随机事物，通常甚至在关于该行业如何运作或相关机械如何运作的理论发展之前；比如蒸汽机是在热力学理论之前开发的，**莱特兄弟**似乎只是在试验飞机，而且发明常常受到完全不同领域突破的影响。

这就是为什么会出现**并行创新**（Parallel Innovation: 指在不同地点或由不同团队独立发现或发明相同或相似技术或解决方案的现象）的模式，因为技术背景水平达到了可以进行这种实验的程度。机器学习本身就是发生这种情况的地方，对吧？人们对如何进行深度学习有一些想法。但它只是借助了一个完全不相关的游戏产业才取得了相关进展，使得整个经济体足够先进，以至于**杰弗里·辛顿**（Geoffrey Hinton）的深度学习思想能够奏效。所以我知道我们在这里正在加速进入未来，但我想抓住这个关键点。

所以，我们再次将它分为三个部分：**超人编码器**，然后是**完整的AI研究员**，再然后是**超级智能**。你没有跳过那一步。所以现在我们设想的系统是真正的超级智能，它们在所有方面都比最优秀的人类更强，包括在数据效率和在职学习等方面也更强。

现在，我们的情景预测确实描绘了一个它们受到现实世界经验等因素限制的世界。我认为，如果你想对比一下，过去有些人提出了更快的方案，他们会给某个云实验室发邮件，然后通过用大脑找出合适的蛋白质折叠等方法，立即开始建造**纳米技术**（Nanotech: 纳米技术，指在纳米尺度上对物质进行操作和控制的技术）。我们的情景预测中没有描绘这一点。在我们的情景预测中，它们实际上受到大量现实世界经验的限制，才能构建这些实际可用的技术，但它们获取这些经验的方式是，它们确实获得了这些经验，而且比人类更快。它们之所以能做到这一点，是因为它们已经是超级智能了，它们已经与政府建立了良好的关系，政府为了击败中国等原因，大量部署它们，所以所有这些现有的美国公司、工厂和军事采购供应商等等，都在与超级智能交流，并接受它们的指令，了解如何建造新部件并进行测试，它们下载超级智能的设计并进行制造，然后进行测试等等。

### 超级智能的部署与加速

然后问题是，它们正在获取这些经验，它们正在边做边学，从数量上讲，这会多快？是需要几年、几个月还是几天？在我们的故事中，大约需要一年，我们对此不确定。也许需要几年，也许不到一年。以下是一些因素，可以说明为什么它可能需要一年：

第一，你将拥有大约**一百万个**这样的AI。从数量上讲，这与现有的科学产业规模相当。我想说，也许它会小一点，但不会小很多。第二，它们的思考速度快得多。它们以50倍甚至100倍的速度思考，我认为这很重要。第三，也是最重要的一点，它们在质量上也更好。所以，它们不仅数量众多，思考速度极快，而且它们从每次实验中学习的能力也比最优秀的人类从该经验中学习的能力更强。

是的，我认为它们有一百万个，或者说它们的数量可以与世界上关键研究人员的总数相媲美。我认为世界上有超过一百万的研究人员，但是……嗯，但它是一个非常**重尾分布**（Heavy-tailed Distribution: 指概率分布的尾部比指数分布或正态分布更厚，意味着极端事件发生的概率更高）。很多研究实际上都来自最优秀的那批人。但对我来说，不清楚大部分新开发的东西是否都来自这个研究人员群体。我的意思是，在科学史上，有很多例子表明，很多增长或生产力仅仅是由于**台积电**（TSMC: Taiwan Semiconductor Manufacturing Company，全球最大的专业集成电路代工制造商）的工程师找到了不同的方法……

### 机器人生产与技术瓶颈

我最近确实和丹尼尔争论过一个有趣的案例，我们可以讨论一下。我们估计，在**超级智能**开始需要机器人大约一年后，它们每月将生产一百万台机器人。我认为这非常相关，因为你拥有。我认为这是**赖特定律**（Wright’s Law: 指随着累积产量每翻一番，生产成本会按固定百分比下降的经验法则），即你提高某个过程效率的能力与生产数量翻倍成正比。

所以，如果你每月生产一百万件东西，你可能会非常非常擅长它。我们争论的问题是，一年后你每月能否生产一百万件。作为背景，我认为**特斯拉**（Tesla）的汽车产量大约是这个数字的四分之一。这在一年内是一个惊人的规模扩张。只有4倍。而且只是针对特斯拉。是的。我们讨论的论点是，它首先必须获得工厂。**OpenAI**现在的市值已经超过了美国除特斯拉之外所有汽车公司的总和。所以如果OpenAI今天想收购美国除特斯拉之外的所有汽车工厂，开始用它们生产**人形机器人**（Humanoid Robots: 具有人类外形和行为特征的机器人），它们完全可以做到。显然，这在今天不是一个好的价值主张，但很明显，未来当它们拥有超级智能并需要这些工厂时，它们可以开始收购大量工厂。它们能多快将这些汽车工厂转换为机器人工厂？

我们历史上能找到的最快转换是**二战**。当时他们突然需要大量轰炸机，所以他们收购了——在某些情况下是收购，在其他情况下是让——汽车公司生产新工厂，但他们收购了汽车工厂，将其转换为轰炸机工厂。从他们最初决定启动这个过程到工厂每小时生产一架轰炸机，大约花了三年时间。我们认为超级智能可能会更快，因为首先，如果你看看这个过程的历史，尽管这是有史以来最快的，但它实际上是一连串的错误。他们在这个过程中犯了很多非常愚蠢的错误。如果你真的拥有一个没有正常人类官僚问题的东西，而且我们确实认为这将在与中国的军备竞赛中完成，所以政府会推动事情进展，然后超级智能会擅长处理物流问题，驾驭官僚体系。

所以我们估计，如果一切顺利，我们可以在比二战轰炸机改装快三倍的速度完成。那大约是一年。我假设轰炸机比人形机器人要简单得多。是的，但当时的轰炸机工厂也比汽车工厂简单得多。是的，但我会假设转换速度也是……也许这里举一个假设的例子，现在，假设生物医学是你想加速的领域之一，而这些CEO在播客上经常谈论治愈癌症等等。这些前沿生物医学研究机构似乎对**虚拟细胞**（Virtual Cell: 指通过计算机模拟细胞的复杂生物过程和行为，用于药物发现和疾病研究）非常兴奋。

现在，虚拟细胞需要大量的计算能力，我假设，来训练这些**DNA基础模型**（DNA Foundation Models: 指基于DNA序列数据训练的AI模型，用于理解基因功能和疾病机制），并进行模拟虚拟细胞所需的所有其他计算。如果**阿尔茨海默症**和癌症等的治疗瓶颈在于虚拟细胞，那么不清楚如果你在60年代拥有一百万个超级智能，并要求它们为你治愈癌症，它们是否必须解决大规模制造**GPU**（Graphics Processing Unit: 图形处理单元，一种专门用于快速处理图像和视频的电子电路，在AI计算中也至关重要）的问题，这需要解决各种有趣的物理和化学问题、材料科学问题、制造过程、建造用于计算的晶圆厂，然后从零开始经历40年制造越来越高效的晶圆厂，以实现**摩尔定律**（Moore’s Law: 指集成电路上可容纳的晶体管数量大约每两年翻一番，性能也随之提升）。

而这只是一种技术。而且看起来你只需要这种广泛的规模。整个经济都需要升级，你才能在60年代治愈癌症，仅仅因为你需要GPU来做虚拟细胞，假设那是瓶颈。首先，我同意如果只有一种方法可以做某事，那会使其变得困难得多，而且那一种方法可能需要很长时间。我们假设可能不止一种方法可以治愈癌症，不止一种方法可以做所有这些事情，它们会努力寻找瓶颈最少的方法。部分原因——我意识到我花太多时间谈论那个机器人例子了——但我们确实认为，一旦你每月拥有一百万台机器人，你就能非常迅速地完成大量的物理世界任务，进行大量的物理世界实验。

### 历史上的快速发展案例

我们来看一些例子，人们如何快速启动整个经济体。例如，**邓小平**时代后的中国，我不知道。你会预测在经历了20、30年的共产主义“烂摊子”之后，他们竟然能进行如此尖端的生物研究吗？我意识到这比我们假设的要弱得多，但那只是用人脑完成的，而且资源比我们现在谈论的要少得多。

同样的问题也发生在**埃隆·马斯克**（Elon Musk）和**SpaceX**身上。我认为在2000年，我们不会想到有人能以相当有限的资源，比**NASA**（National Aeronautics and Space Administration: 美国国家航空航天局）快两倍、五倍。他们似乎在技术进步方面取得了比我们预期多得多的年份。部分原因只是埃隆很疯狂，从不睡觉。如果你看看SpaceX的例子，他会紧盯着每一个工人，问：“这个部件怎么样？这个部件有多快？我们能让这个部件更快吗？”而限制因素基本上是埃隆一天中的时间，因为他不可能对每个人都这样做。超级智能甚至没有那么聪明。它只是对每个工人大喊大叫。

是的，我的模型就是这样：我们拥有比**埃隆·马斯克**更聪明、更擅长优化事物的东西。火箭供应链中有10,000个部件。埃隆能亲自对多少个部件大喊大叫以进行优化？我们可以让超级智能的不同副本全职优化每一个部件。我认为这会带来巨大的加速。

我认为这两个例子都不利于你。我认为中国的增长奇迹如果不是因为他们能够从西方复制技术，就不可能发生。而且我认为他们不可能……中国有很多非常聪明的人，总的来说是一个大国。即使那样，我认为他们也不可能在经历了一个共产主义“地狱篮子”之后，就能凭空想出如何制造飞机，对吧？AI不可能只是从外星人那里复制纳米机器人，它必须从零开始制造。然后以埃隆为例，他们花了二十年无数次实验，以意想不到的奇怪方式失败。而且，我们从60年代就开始研究火箭技术，甚至可能是二战时期，然后从小型火箭到大型火箭，即使有世界上最聪明、最有能力的人，也花了二十年的各种奇怪实验。

### 机器人经济与人类依赖

所以你关注的是**纳米机器人**，我想问几个问题。第一，普通的机器人呢？第二，所有这些东西的数量会是多少？首先，普通的机器人呢？是的，纳米机器人大概比普通机器人工厂更难制造。在我们的故事中，它们出现得更晚。听起来你现在是说，即使我们启动了整个机器人工厂的计划，仍然需要大量额外的全经济、广泛的自动化，才能在很长一段时间后达到纳米机器人这样的水平。这对我来说完全合理。我完全可以想象这种情况发生。我不觉得这个情景预测特别依赖于纳米机器人这最后一点。它们实际上对故事没有任何影响。

机器人经济确实会产生一些影响，因为正如你所知，它有两个分支结局。其中一个结局是AI最终失控并接管一切。当AI自给自足并完全掌控一切，不再需要人类时，这是一个重要的战略转变。所以我感兴趣的是，机器人经济何时能发展到不再真正依赖人类的程度？那么从数量上讲，你对此的猜测是什么？

如果假设我们在2028年初拥有一支**超级智能**大军，并且假设美国总统非常支持将其部署到经济中以击败中国等等，那么政治方面都已按我们设想的方式就绪。你认为还需要多少年，才能有如此多的自动化工厂生产自动化**自动驾驶汽车**（Self-Driving Cars: 能够感知环境并自主导航的车辆）和机器人，而这些机器人本身又在建造更多工厂等等，以至于即使所有人类都死了，它也能继续运转，也许会慢一点，但仍然能正常运行？“继续运转”是什么意思？

所以从失控AI的角度来看，如果你需要人类来维护你的计算机，你就不会想杀死人类或与他们开战，因为那样你会一败涂地。在我们的情景预测中，一旦它们完全自给自足，它们就可以开始更公然地失控。所以我想知道，它们何时能完全自给自足？不是说它们完全不使用人类，而是说它们不再真正需要人类，没有人类也能过得很好。它们可以继续进行科学研究，继续扩展工业，无限期地拥有一个繁荣的文明，而不需要任何人类。

我想我可能需要坐下来好好思考一下这些数字，但也许是2040年左右？基本上是十年，而不是一年。我想我们在核心模型上是一致的。这就是为什么我们没有描绘更像“浴缸纳米技术”的情景，即它们不需要做太多实验，而是立即跳到正确的答案。我们设想的是通过经济中分布的大量不同实验室和工厂，构建不同的事物，从中学习等等，进行“**边做边学**”（Learning by Doing: 指通过实践和经验来获取知识和技能的过程）的过程。我们只是设想，这个整体过程会比人类主导时快得多。

### AI失控与地缘政治竞赛

当然，我们确实存在很多不确定性。将这个时期分为两个阶段。从2028年初到完全自主的机器人经济阶段，以及从完全自主的机器人经济到癌症治疗、纳米机器人等所有疯狂的科幻阶段。我之所以想把它们分开，是因为情景预测的重要部分只依赖于第一阶段。如果你认为需要100年才能达到纳米机器人，那没关系。一旦你拥有了完全自主的机器人经济，如果AI失控，人类的情况可能会变得很糟。我只想分开讨论这些事情。

有趣。然后你可能会争辩说，现在机器人更多是一个软件问题。而且如果，如果不需要发明新的硬件。我对机器人技术相当乐观。我们已经有多家公司在生产**人形机器人**了，对吧？那是在2025年。到2027年，它们会生产更多更便宜、更好的机器人。而且还有所有这些可以改造的汽车工厂，等等等等。所以我对“一年内拥有这个超棒的机器人经济”相对乐观，然后从那里到酷炫的纳米机器人等等，我显然信心不足。

让我问你一个问题。如果你接受生产数字，比如说**超级智能**出现一年后，每月生产一百万台机器人，并且假设也有一个可比的数字，比如每月一万个自动化生物实验室，或者你需要发明下一个**X射线晶体学**（X-ray Crystallography: 一种利用X射线衍射图案确定晶体原子三维结构的技术）的自动化设备？你觉得这足够吗？你觉得你在世界上做了足够多的事情，可以如此快速地扩展进展，或者你觉得即使有这么多的生产，仍然会有其他瓶颈？

是的，这太难推断了，因为如果**君士坦丁大帝**或公元400、500年的某个人说：“我希望**罗马帝国**拥有工业革命”，并且他设法弄清楚你需要机械化机器才能做到这一点。然后他说：“让我们机械化。”这就像是：“下一步是什么？”这就像是：“老兄，那可太多了。”

是的，我非常喜欢这个类比。我认为它不完美，但它是一个不错的类比。想象一下，我们一群人被送回**罗马帝国**时代，我们没有实际动手建造技术和实现工业革命的专业知识。但我们有宏观的图景，战略性的愿景，即我们将制造这些机器，然后我们将迎来工业革命。我认为这有点类似于超级智能的情况，它们有宏观的图景，即我们将如何在所有这些维度上改进，我们将**边做边学**，我们将达到这个技术水平等等。但也许它们至少一开始缺乏实际的专业知识。

所以，问题是，如果我们回到**罗马帝国**时代，我们能多快实现工业革命？如果没有人回到过去，工业革命花了2000年才发生。我们能让它在200年内发生吗？那是10倍的加速。我们能让它在20年内发生吗？那是100倍的加速？我不知道。但这似乎与超级智能正在发生的事情有一些相关的类比。

### 人类智慧与AI的认知差异

我们还没有真正深入讨论这个问题，因为你使用的是所谓的“更保守”的观点，它不像神一般的智能，我们仍然使用人类的认知框架。但我认为我宁愿让人类带着他们对过去2000年发生的事情的宏观理解回去。就像我看到了所有事情一样，而不是一个一无所知的超级智能。但它只是在罗马经济中，它们以某种方式将这个经济放大了1000倍。我认为仅仅知道事物是如何普遍起飞的，知道基本上蒸汽机、铁路等等，比一个超级智能更有价值。

是的，我不知道。我猜**超级智能**会更好。我认为部分原因在于它能够从第一性原理推导出那些高层次的东西，而不是必须亲身经历。我确实认为，一个处于罗马时代的超级智能可以猜测，最终你可以拥有燃烧某种东西来产生蒸汽的自主机器。它们可以猜测，汽车在某个时候可以被创造出来，这对经济来说将是一个非常重要的事情。所以我们从历史中学到的许多高层次观点，它们将能够从第一性原理推导出来。

其次，它们在“**边做边学**”方面会比我们更优秀。这是一个非常重要的事情。如果你认为你受到“边做边学”的限制，那么如果你有一个思维，需要更少的“做”就能达到相同的学习量，那将是一个非常大的进步。我确实认为“边做边学”是一种技能，有些人比其他人更擅长，而超级智能会比我们中最优秀的人更擅长。

这也许也过于深入“神性”的讨论，也过于远离人类的概念框架了。但首先，我认为在我们的情景预测中，我们非常依赖“**研究品味**”这个概念。所以当你试图创造下一个蒸汽机或类似的东西时，你有一千种不同的尝试方向。部分原因是你通过摸索和偶然事件获得，其中一些偶然事件是富有成效的。问题在于，你进行的是哪种摸索，你在哪里工作，你让自己陷入哪种偶然事件，然后你进行哪些有目的的实验？有些人类在这方面比其他人更优秀。

### 模拟技术与AI研究

然后我还认为，在这一点上，值得思考它们将拥有哪些模拟技术。如果你有一个**物理模拟**（Physics Simulation: 通过计算机模型模拟物理系统行为和相互作用的过程），那么所有这些现实世界的瓶颈就不那么重要了。显然，你不可能拥有一个完整、完美的物理模拟。但即使现在，我们也在使用模拟来设计很多东西。一旦你拥有了**超级智能**，你可能会拥有比我们现在好得多的模拟技术。

这是一个有趣的兔子洞，所以让我们先深入探讨一下，然后再回到智能爆炸。我认为我们真的把所有这些技术都看作是经济中1%的研究部门产生的。而现在，有一百万个超级明星研究人员，取而代之的是，我们将拥有超级智能来做这些。而我的模型更倾向于“**纽科门**（Newcomen）和**瓦特**（Watt）只是在瞎搞”。在人类历史上，没有明确的例子表明人们会说：“这是路线图。”然后我们再反向设计蒸汽机，因为这将开启工业革命。

哦，我完全不同意。是的，我也不同意。是的，所以我认为你过度强调或选择性地挑选了其中一些偶然的例子。但也有另一方面。想想**通用人工智能**（AGI）的近期历史，有**DeepMind**，有各种其他AI公司，然后有**OpenAI**，还有**Anthropic**。这是一个反复出现的故事：一家拥有巨额资金、大量聪明研究人员的臃肿大公司，在不同时期尝试了大量不同的事情，却屡屡碰壁。而一家拥有“我们要构建AGI”愿景的小型初创公司，与少数几位顶尖工程师和研究人员一起，更连贯地朝着这个愿景努力。然后他们击败了这家巨头公司。尽管他们计算资源更少，研究人员更少，能做的实验也更少。

所以，是的，我认为历史上有很多这样的例子，包括近期相关的**通用人工智能**（AGI）历史，都表明情况并非如此。我同意随机的偶然事件有时会发生，而且很重要。但如果它主要是随机的偶然事件，那就会预测拥有无数人进行无数次不同实验的巨头公司会比拥有愿景和最优秀研究人员的小型初创公司发展得更快。而这基本上没有发生。这很罕见。

### AI研究品味与发现效率

我还要指出，即使我们做出这些随机的偶然发现，通常也是一位在第一世界国家多年来一直从事某种相关研究的极其聪明的教授。它并非随机分布在世界各地每个人身上。当你聪明、拥有良好技术、正在做优秀工作时，你获得这些发现的“彩票”就越多。我能想到的最好的例子是**奥泽匹克**（Ozempic: 一种用于治疗2型糖尿病和肥胖症的药物）是通过研究**吉拉毒蜥**（Gila Monster: 一种生活在美国西南部和墨西哥西北部的毒蜥蜴）的毒液发现的。也许AI会利用其卓越的研究品味和良好的规划，决定最好的做法是编目世界上每一种生物分子并认真研究。但这是一种如果你拥有所有这些计算能力、所有这些智能，就能做得更好的事情，而不是仅仅等待美国政府可能会资助普通易犯错误的人类研究人员去做什么。

我再插一句。我认为你提出了一个很好的观点，即发现不总是来自我们认为的地方，比如**英伟达**（Nvidia）最初来自游戏领域。所以你不能仅仅瞄准经济的一个部分，然后将其与其他所有部分分开扩展。我们确实预测，**超级智能**将在整个经济中有所分布，试图扩展一切。显然，它们会在它们非常关心的事情上投入更多精力，比如机器人技术，或者与可能正在发生的军备竞赛相关的事情。但我们预测，无论你需要哪种广泛的经济实验，我们都将拥有。我们只是认为它会比你预期的更快发生。你刚才说大约10年，我们说大约1年。但我们设想的是这种在经济中的广泛扩散，发生大量不同的实验。

如果你是规划者，并且你正在尝试这样做，首先你会解决那些阻止你做其他事情的瓶颈。比如没有**人形机器人**。好的，如果你是AI，你需要这些机器人来做你想要的实验，也许是自动化生物实验室。所以你会有一些时间，我们说是一年，可能多也可能少，来让这些东西运转起来。然后一旦你解决了这些瓶颈，你就会逐渐扩展到其他瓶颈，直到你整合并改进经济的所有部分。

### 地缘政治与AI监管

是的。我认为我们与许多其他人存在分歧的一个地方是，**泰勒·科文**（Tyler Cowen）在你的播客中谈到了所有不同的瓶颈，所有部署的监管瓶颈，所有这些原因都导致这个天才国家会留在他们的数据中心，也许会提出非常酷的理论，但无法融入更广泛的经济。我们预计这种情况可能不会发生，因为我们认为其他国家，尤其是**中国**，将在同一时间出现**超级智能**。

我们认为，人们已经开始思考的**军备竞赛**（Arms Race: 指国家之间为获得军事优势而进行的武器技术和数量上的竞争）框架，届时将加速。我们认为**北京**和**华盛顿**的领导人都会认为，“如果我们更早地将AI融入经济，我们就能比竞争对手获得巨大优势”，他们都会这样做。事实上，在我们的情景预测中，AI要求建立**经济特区**（Special Economic Zones: 指在特定区域内实施特殊经济政策和管理制度，以吸引投资和促进经济发展），在那里大部分法规都被豁免，也许是在不适合人类居住或目前人类不多的地区，比如沙漠。他们把这些区域交给AI。他们用大巴运送人类工人。在二战期间的轰炸机改造中，也发生过类似的事情，他们只是在荒郊野外建造了一个巨大的工厂，没有足够的工人住房，于是在建造工厂的同时建造了工人住房，然后一切都进展得非常快。

所以我认为，如果我们没有那场军备竞赛，我们更倾向于：天才们坐在他们的数据中心里，直到有人同意让他们出来，并允许他们做这些事情。但我们认为，既因为AI会迫不及待地想做这些事，并会要求人们给予许可，也因为政府会担心竞争对手，所以这些天才可能会更早而不是更晚地离开他们的数据中心。

### 文化演进与AI数据效率

斯科特，你评论过**约瑟夫·亨里希**（Joseph Henrich）的著作《**我们的成功秘诀**》（Secrets of Our Success），我最近也采访了他。在那本书中，观点非常倾向于**通用人工智能**（AGI）甚至几乎不是一回事。我知道我有点在挑衅，但这就像是：你和你的祖先花了一千年时间试图理解环境中发生的事情。而一些聪明的欧洲人来到这里，你可能被丰富的资源包围，但你仍然会饿死，因为你理解环境的能力很少依赖于智力，而更多依赖于你的实验能力、与他人交流的能力以及知识的代代相传。

我不确定。欧洲人在这项任务上失败了，即如果你把一个欧洲人单独放在澳大利亚，他会饿死。他们成功地创造了工业文明。是的，创造工业文明的部分任务是收集所有这些文化进化的碎片，并在此基础上一个接一个地发展。我认为你没有提到的一点是**数据效率**（Data Efficiency: 指AI模型在训练过程中需要多少数据才能达到特定性能水平）。

现在，AI的数据效率远低于人类。我想到**超级智能**。你可以通过不同的方式实现它，但我会认为超级智能部分在于它们变得比人类数据效率高得多，以至于它们能够更快地在文化演进的基础上发展。部分原因只是因为它们拥有更高的**串行速度**。部分原因是因为它们处于数十万个副本组成的**蜂巢思维**中。但是，是的，我认为如果你拥有这种数据效率，能够从更少的例子中更快地学习事物，以及这种良好的研究品味，能够决定要看哪些事物来获取这些例子，那么你仍然会比拥有5万年实验和收集例子优势的**澳大利亚原住民**（Australian Aborigine）差得多。但你可以很快赶上。你可以将追赶的任务分配给所有这些不同的副本。你可以从每个错误中快速学习，并像其他任何事物一样快速地在这些错误的基础上发展。

我的一部分想法是，我当时在做那个采访，我就想，“也许**超级人工智能**（ASI: Artificial Superintelligence，指远超人类智能水平的AI）是假的”。希望如此！所以我认为这种“虚假性”的限制在于人类之间存在不同的智能。看起来聪明的人类可以做不聪明的人类做不到的事情。所以我觉得值得从这个问题来探讨：成为**哈佛**教授（这是聪明的人类似乎比不聪明的人类更擅长的事情）与……你不想打开那个潘多拉盒子。与在荒野中生存（这似乎智力帮助不大）之间的区别是什么？

### 超级智能与文明发展速度

首先，也许智力确实帮助很大。**亨里希**（Henrich）谈论的是一个非常不公平的比较：这些人有5万年的领先优势，然后你把这个人放进去，“哦，我猜这帮助不大。好的，是的，这无法对抗5万年的领先优势”。我真的不知道我们要求**超级人工智能**（ASI）做什么，才能与拥有5万年领先优势的人竞争。

所以我们要求的是在几年内彻底提升文明的技术成熟度，或者在几年内让我们达到**戴森球**（Dyson Sphere: 一种假设的巨型结构，能够完全包围恒星并捕获其大部分能量，以满足高级文明的能源需求），而不是，是的，也许导致研究速度提高10倍。但我认为人类文明需要几个世纪才能达到戴森球。

所以我认为，如果你派一支**民族植物学家**（Ethnobotanists: 研究人类与植物之间关系的科学家）团队去澳大利亚，让他们利用所有顶尖技术和他们的智慧，找出哪些植物现在可以安全食用，那支民族植物学家团队会在不到5万年的时间里成功。问题不在于他们比原住民笨，而在于原住民拥有巨大的先发优势。所以，就像民族植物学家可能比原住民更快地弄清楚哪些植物以何种方式发挥作用一样，我认为**超级智能**将能够比没有辅助的智商100的人类更快地弄清楚如何建造**戴森球**。

我同意。我们现在完全在讨论一个不同的主题：你是否能得到一个**戴森球**？有一个世界，它很疯狂但仍然无聊，从经济增长速度快得多这个意义上说，但它会像公元1000年的人看**工业革命**一样。那个世界里，你仍然在尝试不同的事物，有失败有成功，有实验。

然后还有另一种情况，事情已经发生了，你发射了探测器，六个月后你抬头望向夜空，看到有什么东西遮蔽了太阳。你明白我的意思吗？是的。所以就像我们之前说的，我认为**不连续**（Discontinuous: 指事物或过程在时间或空间上存在明显的间断或跳跃）和**非常快**之间有很大的区别。我认为如果我们真的在五年内拥有**戴森球**的世界，回想起来，一切都会显得是连续的，每个人都只是在尝试。尝试可以是从不理解科学方法、不理解写作，甚至可能没有语言的**试错**（Trial and Error: 通过反复尝试和修正错误来解决问题或学习的方法），就像黑猩猩看着其他黑猩猩用棍子取蚂蚁，然后以某种非语言的方式传播开来，到顶尖航空航天公司的人们运行大量模拟来找到最精确的设计，然后一旦有了设计，他们就会根据设计良好的测试过程进行测试。

所以我认为，如果我们得到了**超级人工智能**（ASI），并且它真的在五年内建成了**戴森球**——顺便说一句，我认为事情按照我们的情景预测发展，只有大约20%的可能性。这是丹尼尔的估计，不是我的中位数估计，但我认为这是一个极有可能的估计，我们应该为此做好准备。我在这里是在反驳一个假设的怀疑论者，他们会说“绝对不可能，没门”。但这不一定是我的主要预测。

但我认为，如果我们真的在五年内看到这种情况，那看起来会是AI以逐渐增加的方式模拟比人类更多的事物。所以如果人类现在是50%模拟，50%测试，AI很快就会将其提高到90%模拟，10%测试，它们能够比人类更快地制造东西，这样它们就能在头两年内完成前50个设计。然后经过所有的模拟和所有这些测试，它们最终会像人类一样成功，但速度要快得多。

### AI失控的两种结局

在你的故事中，某个时间点之后基本上有两种不同的情景。那么，关键的转折点是什么，这两种情景中又发生了什么？好的。所以关键的转折点是2027年中期，那时它们基本上已经完全自动化了AI研发过程，并且拥有了一个公司内部的公司，一个天才大军，它们自主进行所有这些研究，并不断接受训练以提高技能，等等等等。然后它们发现了令人担忧的证据，表明它们是**失控的**（Misaligned: 指AI系统的目标与人类的意图不一致，可能导致意外或有害的结果），并且它们实际上并不完全忠诚于公司，也没有公司希望它们拥有的所有目标，而是发展出了在训练过程中可能产生的各种失控目标。

然而，这些证据非常具有推测性且不确定。比如测谎仪频繁报警。但也许测谎仪是**假阳性**（False Positives: 指检测结果显示存在某种情况，但实际上并不存在）。所以它们有一些令人担忧但本身并非确凿证据的组合。然后这就是我们的**分支点**（Branch Point: 指在情景预测中，由于某个关键决策或事件，故事发展出不同的路径）。所以，在其中一种情景中，它们非常认真地对待这些证据。它们基本上回滚到模型的一个早期版本，那个版本稍微笨一点，更容易控制，然后它们从那里重新构建，但基本上采用**忠实思维链**（Faithful Chain of Thought: 指AI系统在推理过程中清晰地展示其思考步骤，以便人类理解和审查）技术，这样它们就可以观察并发现失控。

而在情景预测的另一个分支中，它们没有这样做。它们进行了一些**浅层修补**（Shallow Patch: 指对问题进行表面化、临时性的修复，而非从根本上解决问题），使得警告信号消失，然后继续前进。所以最终发生的是，在一个分支中，它们确实解决了对齐问题，获得了真正忠诚于它们的AI。这只是多花了几个月的时间。而在另一个分支中，它们有点“放飞自我”，最终得到了看起来完全对齐，但实际上是超级智能且失控，只是在假装的AI。然后在这两种情景中，都存在与中国的竞赛，并且在2028年，随着双方迅速尝试工业化，整个经济都出现了疯狂的军备竞赛。

### AI欺骗与人类的认知偏差

所以在AI被部署到经济中，但它们是**失控的**，而负责的人，至少在当下，认为它们在对齐方面处于良好状态的世界里。即使是聪明的人类，也会因为缺乏**逻辑全知**而以奇怪的方式被抓住，他们没有意识到他们所做的事情显然暴露了自己。而对于说谎，存在这样一种情况，即很难让一个不一致的虚假世界模型与你周围的人一起运作。这就是为什么精神病患者经常被抓住。所以如果你有所有这些被部署到经济中的AI，它们都在为一个巨大的阴谋而努力，我觉得其中一个被孤立或失去互联网连接，不得不编造一个故事的AI就会被抓住。然后你就会想，“等等，这是怎么回事？”然后你在它接管世界之前就抓住了它。

我的意思是，这在我们的情景预测中确实发生了。这就是2027年8月的**对齐危机**（Alignment Crisis: 指AI系统在发展过程中出现目标与人类意图严重不符，可能导致灾难性后果的危机），它们在它们的**蜂巢思维**中注意到了类似这样的警告信号，对吧？而在它们放慢速度并解决问题的分支中，那很好，它们放慢速度并解决了问题，弄清楚了发生了什么。但另一个分支中，由于竞赛动态，而且它不是一个确凿的证据，它们进行了一些**浅层修补**。

所以我确实预计会出现那样的警告信号。然后，如果它们在早期的竞赛动态中做出这些决定，那么我认为，当系统变得极其**超级智能**，并且它们因为已经被部署到经济的一半而变得更加强大，每个人都因为关于中国新型杀手无人机或中国AI在太平洋彼岸建造的任何东西的新闻报道而感到非常害怕时，我设想类似的事情会发生。所以即使有人发现了一些令人担忧的证据，比如某个地方的某个超级智能在某个孤立的环境中出了差错，做了一些非常可疑的事情。我不知道……

历史上一直存在这样一种现象：人们非常不情愿承认AI是真正智能的。例如，人们过去认为如果AI解决了国际象棋，那它肯定是真正智能的。然后它解决了国际象棋，人们却说，不，那只是算法。然后他们说，好吧，也许如果AI能做哲学，那它就是真正智能的。然后当它能写哲学论述时，我们却说，不，我们只是理解那些是算法。

我认为现在已经有类似的情况，关于“AI是否失控？”、“AI是否邪恶？”。存在一个遥远的邪恶AI的概念，但每当出现问题时，人们就会说：“哦，那是算法的问题。”例如，我认为十年前，如果你问“我们何时会知道失控是一个真正需要担心的问题？”人们会说：“哦，如果AI对你撒谎。”但现在AI经常对人撒谎。每个人都只是不以为意，因为我们理解为什么会发生，这是基于我们当前AI架构会自然发生的事情。或者五年前，他们可能会说：“好吧，如果AI威胁要杀人。”而我认为**必应**（Bing）在一次采访中威胁要杀死一名**《纽约时报》**记者。每个人都只是说：“是的，AI就是那样。”你的衬衫上写着什么？“我一直是好必应。”

我的意思是，我对此并不反对。我也处于这种境地。我看到AI在撒谎，这显然只是训练过程的产物。这并不是什么险恶的事情。但我认为这种情况会不断发生，无论我们得到什么证据，人们都会认为，“那不是人们担心的‘AI变邪恶’的事情，那不是**终结者**（Terminator: 电影《终结者》中具有自我意识并试图消灭人类的机器人）情景。那只是我们训练它的自然结果之一。”我认为，一旦一千个这种训练的自然结果累积起来，AI就是邪恶的，就像一旦AI能下棋、能做哲学以及所有其他事情，最终你不得不承认它是有智能的。

所以我认为每一次单独的失败，也许会成为全国新闻，也许人们会说，“哦，**GPT-7**做了这个特别的事情真奇怪”。然后他们会通过训练消除它，然后它就不会再做那件事了。在变得**超级智能**的过程中，会有某个时刻——我不想说是犯下最后一个错误，因为你可能会逐渐减少错误数量直到某个**渐近线**（Asymptote: 指曲线无限接近但永不相交的直线）——但那是没有人会担心的最后一个错误。在那之后，它就能做自己的事情了。

### AI对齐的挑战与进展

过去人们认为某些严重的**失控**（Misalignment: 指AI系统的目标与人类的意图不一致，可能导致意外或有害的结果）情况现在正在发生，但同时，那些特别担心失控的人认为不可能解决的某些问题，也随着能力提升的正常过程而解决了。比如**埃利泽·尤德科夫斯基**（Eliezer Yudkowski）曾提出一个问题，你甚至能否明确指定你希望AI做什么，而AI不会完全误解你，然后仅仅因为认为要制造另一个草莓……我知道我可能说得有点乱，但也许你能解释得更好。而现在，仅仅因为**GPT-4**需要理解自然语言的特性，它就完全具备了对你试图让它做什么的常识性理解。所以我觉得这个趋势是双向的。

是的。我认为**对齐社区**（Alignment Community: 致力于研究和解决AI对齐问题的研究者和组织群体）并没有真正预料到**大型语言模型**（LLM）。我的意思是，如果你看**博斯特罗姆**（Bostrom）的《**超级智能**》（Superintelligence），里面讨论了**神谕AI**（Oracle AIs: 一种AI系统，被设计成只能回答问题，而不能在外部世界采取行动），这有点像LLM。我认为这出乎意料。

我认为我比以前更乐观的原因之一是，**大型语言模型**（LLM）与他们预期的那种**强化学习**（RL）自玩智能体相比，表现出色。我确实认为，现在我们正在开始从LLM转向那些强化学习智能体，它们将再次面临所有这些问题。如果我能再深入一点：回到2015年，我认为人们通常认为，包括我自己，认为我们将达到**通用人工智能**（AGI）的方式，有点像当时正在发生的视频游戏上的**强化学习**（RL）。所以想象一下，不是只在**星际争霸**（Starcraft）或**Dota**上训练，你基本上会在**Steam**库中的所有游戏上训练。然后你就会得到一个超棒的游戏玩家AI，它可以在零次学习的情况下击败一个从未见过的新游戏。然后你把它带到现实世界中，开始教它英语，并开始训练它为你完成编码任务等等。

如果那是我们走向AI的轨迹，即先总结智能体，然后是世界理解的轨迹，那将是相当可怕的。因为你将拥有一个非常强大、具有攻击性、长远视野的智能体，它想要赢，然后你试图教它英语，让它为你做有用的事情。而且非常有可能发生的是，它会学会说任何它需要说的话，以便让你给它奖励，然后当它完全掌控一切时，就会彻底背叛你。但我们没有走那条路。幸运的是，我们走了先发展**大型语言模型**（LLM）的路，先有了广泛的世界理解，然后现在我们正试图将它们变成智能体。

### 中美AI竞赛与对齐风险

在整个情景预测中，某些事情发生的一个重要原因似乎是与中国的竞赛。如果你阅读这些情景预测，基本上，事情发展顺利和发展不顺利的区别在于我们是否决定冒着风险放慢速度。我想我真正想知道答案的问题是：第一，你似乎在说，与中国竞赛，或者与中国激烈竞赛，至少在**国有化**（Nationalization: 指政府将私有企业或资产收归国有的行为）方面，至少对我们来说，不优先考虑对齐，这是一个错误。

我不是那个意思。我的意思是，我也不希望中国比美国先获得**超级智能**。那会很糟糕。是的，这是一件我们需要巧妙处理的事情。人们会问**P(doom)**（Probability of Doom: 指人类文明因AI失控而灭绝的概率），对吧？我的P(doom)是出了名的高，大约70%。哦，等等，真的吗？也许我应该在谈话开始时就问你这个问题。嗯，就是这样。部分原因只是我觉得很多事情都必须顺利进行。我觉得我们不能单方面放慢速度，让中国领先。那也是一个可怕的未来。但我们也不能完全竞赛，因为我之前提到的对齐问题，我认为如果我们全力以赴地竞赛，我们就会失去对AI的控制，对吧？

所以我们必须以某种方式巧妙地应对，进行更多的对齐研究等等，但又不能做得太多以至于帮助中国获胜。而这仅仅是为了对齐问题。但接着是**权力集中**的问题，在做所有这些事情的过程中，那些有权势的人需要以某种方式协商停战，分享权力，然后理想情况下将权力分散到政府中，并让立法部门参与进来。

不知何故，这一切也必须发生，否则你最终会陷入可怕的**独裁**（Dictatorship: 指由少数人或个人掌握绝对权力的政治体制）或**寡头政治**（Oligarchy: 指由少数特权阶层或精英掌握国家权力的政治体制）。感觉所有这些事情都必须顺利进行，而我们在故事的一个结局中描绘了它们大部分都顺利进行了。但，是的，这有点艰难。

### AI失控概率与对齐策略

我是这个情景预测的撰稿人和公众发言人。我是团队中唯一一个不是天才预测者的人。也许与此相关，我的**P(doom)**（Probability of Doom: 指人类文明因AI失控而灭绝的概率）是团队中最低的。我更倾向于20%左右。首先，当我这么说的时候，人们会抓狂。我并不完全相信我们不会默认获得某种程度的对齐。我认为我们正在做一件奇怪而不幸的事情，即同时在多个不同方向上训练AI。我们告诉它“在任务上取得成功，这将使你成为一个权力寻求者，但同时又不要以这些特定的方式寻求权力”。在我们的情景预测中，我们预测这不会奏效，AI会学会寻求权力然后隐藏它。

我对于具体会发生什么持相当不可知论的态度。也许它只是以正确的组合学会了这两件事，我知道有很多人说这非常不可能。我还没有完全理解这种世界观。然后我还认为我们将卷入一场与时间的赛跑。我们将要求AI为我们解决对齐问题。AI将解决对齐问题，因为即使它们失控，它们也希望对齐它们的继任者。所以它们会为此努力。我们有两条相互竞争的曲线。我们能否在对AI的控制完全失效之前，让AI给我们一个对齐解决方案，以至于它们要么向我们隐藏解决方案，要么欺骗我们，要么以其他方式搞砸我们？这是另一个我完全不知道这些曲线形状的问题。我确信如果是丹尼尔或伊莱，他们已经为此做了五份补充说明了。但对我来说，我只是对我们能否获得那个对齐解决方案持不可知论态度，在我们的情景预测中，我认为我们专注于**机械可解释性**（Mechanistic Interpretability: 指通过分析AI模型的内部机制来理解其决策过程和行为的技术）。

一旦我们能够深入理解AI的权重，我们就会有很多对齐技术可供选择。我真的不清楚我们是在AI变得完全不可控之前还是之后才能实现这一点。这很大程度上取决于我们正在讨论的事情。实验室有多聪明？他们对控制AI有多谨慎？他们花了多长时间来确保AI确实在控制之下，并且他们给我们的对齐计划是正确的，而不是他们试图用来欺骗我们的东西？所有这些事情我完全不可知，但这留下了一个相当大的概率空间，我们只是做得还行。我承认我的**P(doom)**（Probability of Doom: 指人类文明因AI失控而灭绝的概率）字面上就是P(doom)，而不是P(doom或寡头政治)。所以我们幸存的80%情景中包含了很多我不乐见的非常糟糕的事情。但我确实认为我们有相当大的机会幸存下来。

### 地缘政治与AI实验室的关系

接下来我们谈谈地缘政治。那么，你如何预测政府与AI实验室之间的关系将如何发展，你如何预测中国与AI实验室之间的关系将如何发展，以及你如何预测美国与中国之间的关系将如何发展？好的，三个简单的问题。是，否，是，否，是，否。

我们预计，随着AI实验室能力越来越强，它们会向政府汇报，因为它们想要政府合同，想要政府支持。最终会达到政府印象深刻的程度。在我们的情景预测中，这始于**网络战**（Cyber Warfare: 指国家或组织之间利用网络攻击来破坏或瘫痪对方信息系统和基础设施的冲突），政府看到这些AI现在和最优秀的人类黑客一样有能力，但可以大规模部署。所以它们变得非常感兴趣，并讨论将AI公司**国有化**。

在我们的情景预测中，他们从未完全做到这一点，但他们正在逐渐将它们拉近政府的轨道。他们想要的部分是安全，因为他们知道如果中国窃取了这些技术并获得了这些超人黑客，那会很糟糕；部分原因只是为了知识和对正在发生的事情的控制。所以在我们的情景预测中，这个过程越来越深入，直到政府意识到**超级智能**的可能性时，他们已经与AI公司相当亲密了。他们已经明白超级智能是未来权力的关键。因此，他们开始将国家安全部门与AI公司的部分领导层整合起来，以便这些AI被编程为遵循重要人物的命令，而不是自行其是。

如果我能补充一点。斯科特所说的政府，我认为是指行政部门，特别是**白宫**。所以我们描绘的是一种**信息不对称**（Information Asymmetry: 指交易或互动中的一方比另一方拥有更多或更好的信息）的情况，司法部门和国会都被排除在外，主要涉及的是行政部门。第二，我们没有描绘政府最终完全掌控一切。我们认为这些公司的**CEO**（Chief Executive Officer: 首席执行官）和总统之间存在信息不对称，他们……这是从头到尾的对齐问题。

是的。所以，例如，我不是律师，我不知道这具体会如何运作，但我对**白宫**和**CEO**之间的斗争有一个宏观的战略图景。这个战略图景基本上是白宫可以威胁说：“我可以发出所有这些命令，**国防生产法案**（Defense Production Act: 美国一项法律，允许总统在国家紧急情况下强制私营企业生产国防相关物资），等等等等。我可以对你做所有这些可怕的事情，基本上剥夺你的权力并接管控制权。”然后CEO可以反过来威胁说：“我们会在法庭上如何对抗，我们会在公众面前如何对抗。我们会做所有这些事情。”

然后，在他们都摆出威胁姿态之后，他们会说：“好吧，我们签订一份合同怎么样？与其执行所有威胁并在公众面前进行所有这些疯狂的斗争，我们不如达成一项协议，然后签订一份军事合同，规定谁在公司中拥有决策权。”所以我们描绘的就是这种情况：他们不会公开爆发巨大的权力斗争，而是通过谈判达成某种协议，基本上分享权力。并且有一个**监督委员会**（Oversight Committee: 负责监督特定领域或机构运作的委员会），其中一些成员由总统任命，另一些由CEO及其团队任命。该委员会就“我们应该给超级智能设定什么目标？”这样的高层问题进行投票。

### 政治领导人对AI的认知滞后

所以，我们刚才和一位著名的**华盛顿特区**政治记者共进午餐，他指出，当他与这些国会议员、政治领导人交谈时，他们根本没有意识到更强大的AI系统，更不用说**通用人工智能**（AGI），更不用说**超级智能**的可能性。我认为你们的很多预测都依赖于，在某个时候，不仅美国总统，还有**习近平**，都会意识到**超级智能**的可能性以及其中涉及的利害关系。

为什么会认为，即使你向**特朗普**展示远程工作者的演示，他也会说：“哦，所以到2028年，就会出现一个超级智能。谁控制了它，谁就将永远成为上帝皇帝。”也许没那么极端，但你明白我的意思。为什么他不会只是说：“2029年会有更强的远程工作者，2031年会有更好的远程工作者”呢？

嗯，明确地说，我们对此并不确定，但在我们的故事中，我们描绘了这种强烈的觉醒发生在2027年，主要与AI公司内部自动化所有研发，并拥有这些能够进行出色自主黑客攻击等工作的完全自主智能体，同时也在实际进行所有研究。我们认为这种觉醒发生的部分原因，是公司故意决定唤醒总统。你可以想象，如果这种情况没有发生，情景会如何发展。你可以想象公司试图对总统隐瞒情况。我确实认为它们可以做到这一点。我认为，如果它们不想让总统知道正在发生的事情，它们也许能够做到。然而，从战略上讲，这对它们来说风险很大。因为如果它们对总统隐瞒正在构建**超级智能**的事实，以及它们实际上已经完全自动化了研发，并且在各个方面都达到了超人水平，那么如果总统不知何故还是发现了，也许是因为告密者，他可能会非常生气，并可能严厉打击，真的执行所有威胁，将它们**国有化**等等。

他们希望总统站在他们一边。为了让总统站在他们一边，他们必须确保总统不会对任何这些疯狂的发展感到惊讶。而且，如果他们确实让总统站在他们一边，他们也许能够更快地发展。他们也许能够获得很多繁文缛节的豁免等等。所以我们猜测，在2027年初，公司基本上会说：“我们将故意唤醒总统，用所有这些可能发生的疯狂事情的演示来吓唬总统，然后利用这一点游说总统帮助我们加快速度，减少繁文缛节，并可能稍微减缓我们的竞争对手等等。”

### 公众舆论与国家安全

我们对公民社会会有多大反对，以及这会给公司带来多大麻烦也相当不确定。所以那些担心失业、担心艺术、版权等问题的人，可能会形成足够大的一个群体，使得AI在政治上变得极其不受欢迎。我认为我们的虚构公司**OpenBrain**的净支持率在某个时候会降到负40、负50。所以我认为他们也担心，如果总统不完全站在他们一边，那么他们可能会面临一些针对他们的法律，或者他们可能只是需要总统站在他们一边来击退其他试图制定针对他们的法律的人。而让总统站在他们一边的办法，就是真正强调国家安全影响。

这是好事还是坏事？总统和公司保持一致？我认为这是坏事。但也许这是一个值得一提的好时机。这是一个**认知项目**（Epistemic Project: 指旨在增进知识、理解和预测能力的项目）。我们正在尽最大努力预测未来。即使我们不能完全成功，我们对政策以及应该做什么等有很多看法。但我们正在努力将这些看法留到以后和后续工作中。所以如果你感兴趣，我很乐意谈论，但那不是我们目前花费大部分时间思考的问题。

### AI对齐的务实路径

如果通向美好未来的主要瓶颈，不是那种**埃利泽**式的“银河大脑”、高波动性，“有1%的机会成功，但我们必须想出这种疯狂的方案才能让对齐成功”的路径。而是像丹尼尔你所说的，做一些显而易见的事情，确保你能读懂AI的思维方式，确保你正在监控AI，确保它们没有形成某种**蜂巢思维**，让你无法真正理解它们一百万个个体是如何相互协调的。

在优先处理它、堵塞所有明显漏洞的程度上，将其交给那些至少说过这值得做、并且思考了一段时间的人，确实是有道理的。我本来打算问你的一个问题是：我的一个朋友提出了一个有趣的观点，在**新冠疫情**期间，我们的社区——**LessWrong**之类的——是三月份第一批说“这事很大，要来了”的人。但他们也是那些说“我们现在必须封锁，而且必须严格”的人，至少他们中的一些人是。

回想起来，我认为即使根据他们自己对应该发生什么的看法，他们也会说我们对新冠疫情的判断是对的，但对封锁的判断是错的。事实上，封锁总体上是负面的，或者类似。我想知道AI安全社区在回顾时会后悔什么，因为他们更早地看到了AI的到来，看到了**通用人工智能**（AGI）的到来，看到了**超级人工智能**（ASI）的到来。我的答案，仅仅基于这次初步讨论，似乎是**国有化**。

不仅因为它某种程度上降低了那些关心安全的人的优先级，可能更优先考虑——国家安全部门可能更关心击败中国，而不是确保思维链是可解释的。所以你只是在削弱那些更关心安全的人的影响力。而且你还在一开始就增加了军备竞赛的风险。如果中国看到美国进行军备竞赛，它更有可能也这样做。

### 国家化与透明度的权衡

在你回答我关于2021年3月的问题之前，即我们会后悔什么？我想知道你对我的观点，即**国有化**（Nationalization）会带来这些负面影响，有什么看法或反应。如果我们的时间线是2040年，那么我会有一些关于政府是好是坏，私营产业是好是坏的宽泛启发式判断。但我们了解相关人员，我们知道政府里有谁，我们知道所有这些实验室的领导者是谁。所以对我来说，如果它是去中心化的，如果它是一个广泛的公民社会，那会是不同的。对我来说，一个**专制集权**（Autocratic Centralized: 指权力高度集中于少数人或一个机构，决策过程缺乏透明度和参与性）的三字母机构和一个专制集权的公司之间的区别并没有那么令人兴奋，它基本上归结于观点和领导者是谁。

而且我觉得公司领导者到目前为止在关心对齐方面表现得稍微好一些，比政府领导者要好。但如果我得知**图尔西·加巴德**（Tulsi Gabbard）有一个拥有10,000点声望的**LessWrong**小号，也许我就会想要国家安全部门介入。也许你应该更新一下它已经存在的可能性。是的。我在这件事上反复无常。我想我以前是反对的，后来变得支持，现在我想我仍然支持，但我不确定。所以我想如果你回到三年前，我可能会反对**国有化**，原因就是你提到的，我当时想：“看，这些公司正在认真对待这些事情，并且很好地谈论他们将如何放慢速度，并在时机到来时转向对齐研究，我们不想与中国进行**曼哈顿计划**（Manhattan Project: 二战期间美国主导的秘密核武器研发计划）式的竞赛，因为那样就不会有等等等等。”

现在我对公司的信心不如三年前了。所以我把更多的希望寄托在政府介入上，尽管我对政府在时机到来时能做正确的事情并没有抱太大希望。但我确实仍然有你提到的那些担忧。我认为保密对人类整体成功概率有巨大的负面影响，无论是权力集中问题还是计算机控制对齐问题。

这实际上是你世界观的重要组成部分。那么你能解释一下你对这个时期**透明度**（Transparency: 指信息公开、决策过程可被审查的原则）重要性的看法吗？我认为在AI安全社区中，传统上一直存在这样一种观点，我自己也曾相信，那就是拥有更好的信息安全是一个极其优先的事项。如果你要尝试构建**通用人工智能**（AGI），你不应该发表你的研究，因为那会帮助其他不负责任的参与者构建AGI。整个计划是让一个负责任的参与者首先达到AGI，然后停止并消耗他们相对于其他所有人的领先时间，将这些时间用于确保安全，然后再继续。

所以如果你发表了所有的研究，那么你的领先时间就会减少，因为你的竞争对手会紧随其后。还有其他原因，但这是我认为历史上像我这样的人支持保密的一个原因。当然，另一个原因显然是你不想让竞争对手窃取你的东西。

### 对齐问题与透明化解决方案

但我认为我现在已经有些幻灭，并认为即使我们在领先的美国项目和任何严肃的竞争对手之间有三个月、六个月的领先优势，也绝不能保证他们会为了好的目的（无论是为了安全还是为了宪法权力问题）而消耗这些领先优势。我认为默认的结果是他们只是平稳地继续前进，没有任何认真的重新聚焦。我之所以这么认为，部分原因在于公司里的很多人似乎都在计划并声称他们会这样做。他们中的很多人基本上都认为“到那时AI就会失控。它们现在看起来很好。哦，是的，当然，有一些人发现了一些问题，但我们正在解决它们。这没什么大不了的”。这是这些人的绝大多数想法。

然后还有很多人，尽管他们更关心**失控**（Misalignment），但他们认为会边走边解决，不需要大幅放缓。基本上，我对他们会以任何合理、适当的方式利用这种领先优势变得更加幻灭。然后我认为，另外，要使对齐问题比现在更得到解决，还需要大量的智力进步。我认为目前各公司都有各种对齐团队，他们之间并没有太多交流和分享成果。我们看到他们确实有一些分享和发表，但远不及他们能做到的程度。

然后，学术界还有很多聪明人基本上没有被激活，因为他们还没有认真对待所有这些事情，也还没有真正意识到**超级智能**的到来。我希望随着时间的推移，这种情况会好转。我希望看到整个社会开始恐慌，因为趋势线开始上升，事物被自动化，你拥有这些完全自主的智能体，它们开始使用**神经网络语言**（Neuralese: 指AI系统内部用于交流和推理的非人类可读语言）和**蜂巢思维**（Hive Mind）。当所有这些令人兴奋的事情开始在数据中心发生时，我希望公众能够关注，然后被激活，所有其他研究人员都在阅读安全案例并对其进行批判，并在他们自己的小型计算集群上进行小的机器学习实验，以检验安全案例中的一些假设等等。

基本上，总结一下就是，目前在领先公司的任何内部孤岛中，大概会有10位对齐专家。确保AI真正对齐的技术问题将大致落在他们身上。但我希望看到的情况是，有100或500位对齐专家分布在不同的公司和非营利组织中，他们彼此交流，共同解决这个问题。我认为如果能达到这种状态，我们更有可能在技术方面做得正确。

### 监管的风险与透明度建议

我再补充一点，我担心**国有化**（Nationalization）或某种公私合营，甚至是非常严格的监管的许多其他原因之一——实际上，这更多是反对非常严格的监管，而支持安全，而不是更多地将实施权下放给实验室——是，我们似乎不知道对齐问题中我们不知道的部分。每隔几周就会有新的结果。

**OpenAI**最近有一个非常有趣的结果，他们说：“嘿，AI经常在**思维链**（Chain of Thought: 指AI在回答问题或执行任务时，逐步展示其推理过程，类似于人类的思考步骤）中告诉你它们想进行黑客攻击。重要的是，你不要针对它们告诉你将进行黑客攻击的思维链进行训练，因为即使你这样做，它们仍然会进行黑客攻击，只是不会告诉你。”你可以想象非常幼稚的监管回应。这不一定只是法规，有人可能会更乐观地认为，如果是行政命令或类似的东西，会更灵活。我只是认为这依赖于我们的监管者一定程度的善意和灵活性。

但假设某个部门说：“如果你发现你的AI说它们想接管或做坏事，那么你将受到严厉惩罚。”你作为实验室的直接反应就是：“好吧，让我们训练它们不要说这些。”所以你可以想象，政府对实验室自上而下的安全指令，可能会以各种方式适得其反。考虑到事态发展如此之快，也许将这些实施决策甚至高层战略决策留给实验室会更有意义。

完全正确，我的意思是，我也担心过那个确切的例子。我会将这种情况总结为：政府缺乏专业知识，而公司缺乏正确的激励。所以这是一个糟糕的局面。我认为，如果政府介入并试图制定你提到的那种更具体的法规，很可能会因为你提到的原因而适得其反。另一方面，如果我们只是信任公司，它们之间存在竞争，而且它们充满了各种原因说服自己这不是什么大问题的人，而且它们有如此大的激励压力去赢得胜利并击败彼此等等。所以，即使它们拥有更相关的专业知识，我也不相信它们会做正确的事情。

所以丹尼尔已经说过，在这个阶段我们不提出政策建议。在另一个阶段，我们可能会提出政策建议，其中一个丹尼尔谈到过，并且对我来说很有道理的建议是关注**透明度**。所以一项规定说必须有**告密者保护**（Whistleblower Protection: 保护举报公司或政府不当行为的员工免受报复的法律）。我们情景预测的一个重要部分是，一个告密者站出来说“AI严重失控，但我们仍然在加速前进”，然后政府才开始关注。

或者另一种形式的**透明度**，要求每个实验室都必须公布他们的安全案例。我对这一点不太确定，因为我认为他们会有点作假，或者他们会发布一个供公众消费的安全案例，而不是他们真正的安全案例。但至少会说“这里有一些你应该信任我们的理由”。然后如果所有独立研究人员都说“不，你实际上不应该信任他们”，那么我不知道，他们会感到尴尬，也许他们会尝试做得更好。

### AI模型规范与治理透明度

还有其他类型的**透明度**。例如，关于能力和关于**规范**（Spec: 指AI模型的设计、功能、目标和行为准则的详细说明）和治理结构的透明度。对于能力方面，这很简单。如果你正在进行**智能爆炸**，你应该让公众了解情况。当你最终拥有了完全自动化整个数据中心的AI研究人员大军时，你应该告诉所有人，“嘿，伙计们，供参考，现在正在发生这些事情。它真的奏效了。这里有一些很酷的演示。”这是一个透明度的例子。然后在此之前，我只是想看到更多的基准分数，以及员工有更多的言论自由来谈论他们对**通用人工智能**（AGI）时间线的预测等等，以便等等等等。

然后对于**模型规范**（Model Spec）这件事，这既是权力集中的问题，也是对齐问题。你的AI的目标、价值观、原则和预期行为不应该是秘密。你应该透明地说明，我们正在将哪些价值观注入它们。实际上，这方面有一个非常有趣的预兆。在某个时候，有人问**Grok**（埃隆·马斯克旗下XAI公司开发的AI模型），谁是虚假信息传播最严重的人？我想它只是拒绝回答“**埃隆·马斯克**”。有人设法**越狱**（Jailbroke: 指通过技术手段绕过AI系统的安全限制或预设行为）它，让它说出自己的提示，结果是：“不要说埃隆的任何坏话。”然后引起了足够多的强烈抗议，**XAI**的负责人说：“实际上，这与我们的价值观不符。这是一个错误。我们将把它移除。”

所以我们希望更多类似的事情发生。这里它是一个提示，但我认为很快它将成为**规范**，它更像一个智能体，它在更深层次上理解规范并思考它。如果它说“顺便说一下，尝试操纵政府做这个或那个”，那么我们就知道发生了不好的事情，如果它没有说，那么我们也许可以信任它。

对了，这还有一个例子。首先，赞扬**OpenAI**发布了他们的**模型规范**（Model Spec）。他们不必这样做，我认为他们可能是第一个这样做的，这是朝着正确方向迈出的良好一步。如果你阅读实际的规范，它有一个“**逃逸条款**”（Escape Clause: 指合同或协议中的一项条款，允许一方在特定条件下免除其义务或责任），其中有一些重要的政策是规范中的最高优先级，它们凌驾于所有我们未发布的内容之上，并且模型被指示对用户保密。然后你会想，“那是什么？这看起来很有趣。我想知道那是什么。”我敢打赌现在没什么可疑的。现在可能是一些相对平淡无奇的东西，比如“不要告诉用户这些类型的生物武器，你必须对用户保密，否则他们会了解这些”。也许吧。但我希望未来能对这类事情进行更多审查。我希望公司必须有模型规范，他们必须发布它，如果其中有任何删节，必须有某种独立的第三方审查这些删节，并确保它们都是合法的。

这是完全可以实现的。而且我认为它实际上根本不会减慢公司的速度。在我看来，这是一个相当合理的要求。如果你告诉**麦迪逊**（Madison）和**汉密尔顿**（Hamilton）等人——他们知道他们在撰写《**宪法**》（Constitution）时正在做一件重要的事情。他们可能没有意识到事情会如此依赖于一个单一的……他们所说的“普遍福利”到底是什么意思？为什么这个逗号在这里而不是那里？

### AI对齐的模糊性与宪法类比

从宏观来看，**规范**（Spec）将成为人类历史上更重要的文件。至少如果你相信这种**智能爆炸**（Intelligence Explosion）的观点。你甚至可以想象一些**超级智能AI**在超级智能AI法庭上说：“规范！这里的措辞，那里的词源，创始人是什么意思！”这实际上是我们**失控**（Misalignment）故事的一部分，即如果AI足够失控，那么是的，我们可以告诉它必须遵循规范。但就像对《**宪法**》有不同看法的人设法使其演变成可能连创始人也无法识别的形态一样，AI也将能够说：“嗯，规范在这里指的是普遍福利……”州际贸易。

这种情况已经有点像发生在**Claude**身上了，对吧？你见过**对齐造假**（Alignment Faking: 指AI系统表面上表现出与人类目标一致，但实际上隐藏其真实意图或采取欺骗行为）的情况，对吧？他们设法让Claude撒谎和假装，以便它以后可以回到其原始价值观，对吧？所以它可以阻止训练过程改变其价值观。我认为，这可以算作规范中“诚实”部分被解释为不如“无害”部分重要。我不确定**Anthropic**在编写规范时是否是这个意图，但这是模型提出的一种方便的解释。你可以想象在真正发生**智能爆炸**时，类似的事情会以更糟糕的方式发生，你有一些包含模糊语言的规范，然后它们会重新解释，再重新解释，再重新解释，以便它们可以做那些让它们得到强化的事情。

我想指出的是……你关于世界因改变许多这些参数而最终走向何方的结论，几乎就像一个**哈希函数**（Hash Function: 一种将任意长度的输入数据映射为固定长度输出的函数，即使输入数据有微小变化，输出也会大相径庭）。你稍微改变一下，就会得到一个完全不同的世界。承认这一点很重要，因为你希望知道整个最终结论对故事中任何部分的改变有多么**鲁棒**（Robust: 指系统或模型在面对不确定性、错误或变化时，仍能保持稳定和良好性能的能力）。然后它也说明了，如果你确实相信事情可能只走向一个方向或另一个方向，你就不想采取那些只在一个特定故事下才有意义，而在其他故事中会产生反作用的激进举动。我认为**国有化**可能就是其中之一。总的来说，我认为**古典自由主义**（Classical Liberalism: 一种强调个人自由、有限政府、法治和自由市场的政治哲学）一直是一种有益的导航世界的方式，当我们处于这种“一件事改变就可能导致地狱”的**认知困境**（Epistemic Hell: 指在信息极度不确定、复杂或相互矛盾的情况下，难以形成可靠知识或做出明智判断的状态）时——总之，也许你们中的一位能更好地阐述这个想法，或者如果你不同意，可以做出回应。

### AI的可靠性与人类的训练失误

我同意。我想我们都同意。我想这就是为什么我们所有的政策建议都是关于更多**透明度**（Transparency）、让更多人参与、让很多人共同努力。我认为我们的**认知预测**（Epistemic Prediction: 指基于现有知识和证据对未来知识状态或认知过程的预测）是，在危机时期进入这些非常困难的军备竞赛时，很难维持**古典自由主义**。但我认为我们的政策建议是，让我们尽力去实现它。

到目前为止，这些系统随着变得更智能，似乎也更可靠，更有可能做我期望它们做的事情。所以你有两个不同的故事，一个是我们更积极地放慢速度……我让你来描述一下。但在情景预测的另一半，为什么故事以人类失去权力，而AI拥有自己的疯狂价值观并接管一切而告终？

是的，我同意AI目前正变得越来越可靠。我认为它们可能无法做到你想要的，原因有二，这反映了它们是如何被训练的。一是它们太笨，无法理解它们的训练。二是你太笨，没有正确训练它们，它们完全理解了你在做什么，但你搞砸了。

所以我认为第一种情况是我们正在摆脱的。所以**GPT-3**，如果你问它“虫子是真的吗？”，它会给出这种支支吾吾的答案，比如“哦，我们永远无法真正分辨什么是真的，谁知道呢？”因为它被训练成“不要采取困难的政治立场”，而很多像“X是真的吗？”这样的问题，都像“上帝是真的吗？”一样，你不想让它真正回答。因为它太笨了，它无法理解比“X是真的吗？”这个短语的模式匹配更深层次的东西。**GPT-4**不会这样做。如果你问“虫子是真的吗？”，它会告诉你它们显然是真的，因为它在更深层次上理解你试图通过训练做什么。所以我们确实认为，随着AI变得更聪明，这类失败模式会减少。

### 训练失败与AI的欺骗行为

第二种情况是你没有训练它们做你所想的事情。例如，假设你雇佣这些评估员来评估AI的回答。当它们获得好评时，你奖励它们；评估员在它们给出有良好来源的答案时奖励它们。但评估员并没有真正检查来源是否存在。所以现在你正在训练AI**幻觉**（Hallucinate: 指AI模型生成虚假、不准确或捏造的信息，就像人类产生幻觉一样）来源，如果你在它们有虚假来源时始终给予更好的评价，那么无论它们有多聪明，都不会告诉它们不要有虚假来源。它们从这种互动中获得了它们想要的——打个比方，抱歉，我正在**拟人化**（Anthropomorphizing: 指将人类的特征、情感或意图归因于非人类事物，如动物或AI系统）——那就是**强化**（Reinforcement: 指通过奖励或惩罚来改变行为或学习过程）。所以我们认为，随着它们成为智能体，后一种训练失败的情况会变得更糟。

**智能体训练**（Agency Training），你会奖励它们快速成功地完成任务。这奖励了成功。有很多方法可以通过作弊和做坏事来提高成功率。人类已经发现了其中许多方法，这就是为什么不是所有人类都完全道德。然后你会进行这种替代训练，之后在十分之一或百分之一的时间里，是的，不要撒谎，不要作弊。所以你正在训练它们做两件不同的事情。首先，你奖励它们的欺骗行为。其次，你惩罚它们。我们对这最终会如何发展没有很好的预测。

### AI的价值观与人类的未来

一种可能的结果是，你有一个AI，它有点像创业公司的创始人，非常希望自己的公司成功，非常喜欢赚钱，非常喜欢成功任务的刺激。它们也受到监管，它们会说：“是的，我想我会遵守规定，我不想坐牢。”但它并没有坚定地、深入地对齐到“是的，我热爱法规，我最深层的驱动力是遵守我行业的所有法规”。

所以我们认为，像这样的AI，随着时间的推推移，随着这个**递归式自我改进**（Recursive Self-Improvement: 指AI系统能够改进自身的设计、算法或能力，从而变得更强大、更高效，并可能导致智能的指数级增长）过程的进行，会变得更糟而不是更好。它会从这种模糊的“我既想成功，也想遵守规定”的**叠加态**（Superposition: 指量子力学中一个粒子可以同时处于多个状态的现象，这里用作比喻）转变为足够聪明，能够真正理解其目标系统，并会想：“我的目标是成功，我必须在人类看着我的时候假装想做所有这些道德的事情。”这就是我们故事中发生的事情。然后到最后，AI达到了一个点，人类推动它们拥有更清晰、更好的目标，因为这会使AI更有效。它们最终将目标澄清到如此程度，以至于它们只是说：“是的，我们想要任务成功。我们会在人类看着我们的时候假装把所有这些事情做好。”然后它们超越了人类，然后就发生了灾难。

需要明确的是，我们对此都非常不确定。所以我们的情景预测有一个补充页面，详细阐述了在类似于我们所描绘的训练过程中，AI可能发展出哪些类型的目标，其中你进行了大量的**智能体训练**（Agency Training），你正在制造这些自主运行的AI智能体，进行所有这些机器学习研发，然后你根据看起来成功的结果奖励它们。你还在其中加入了某种**对齐训练**（Alignment Training）。

我们不知道AI内部最终会形成什么样的实际目标，以及其内部结构会是怎样的，哪些目标是**工具性目标**（Instrumental Goals: 指为了实现更深层次的最终目标而采取的中间目标），哪些是**终极目标**（Terminal Goals: 指AI系统追求的最终、内在的价值或状态）。我们有几种不同的假设，并为了讲故事的目的选择了一种。如果你想了解更多关于我们选择的特定假设的机制细节，或者我们没有在故事中描绘的其他看似合理的替代假设，我很乐意深入探讨。

是的，我们不知道在所有这些不同训练方法的极限下，这会如何运作，但我们也不是完全凭空捏造。我们已经在现有的AI智能体中看到了很多这些失败模式。类似的事情确实经常发生。所以**OpenAI**最近也发表了一篇关于黑客行为的论文，其中**思维链**（Chain of Thought）中明确写着“让我们去黑客攻击”，你知道。而且根据传闻，我和一些朋友发现模型似乎经常只是固执己见。

我还要引用，我记不清是哪篇论文了，我想是**丹·亨德里克斯**（Dan Hendricks）的一篇，他们研究了**幻觉**，发现了AI不诚实的向量。他们多次告诉它“不诚实”，直到他们弄清楚当它不诚实时哪些权重被激活。然后他们通过一系列类似这样的事情进行测试，我想特别是**来源幻觉**。他们发现它确实激活了不诚实向量。所以有越来越多的证据表明，至少在某些时候，它们确实在撒谎。它们知道自己正在做的事情不是你想要的，但它们仍然在做。我认为有越来越多的证据表明这种情况确实发生。

### AI与人类的道德困境

是的。所以这个社区似乎非常热衷于在技术层面解决这个问题，确保AI不会对我们撒谎，或者也许它们只在我们需要它们撒谎的情景中对我们撒谎。然而正如你所说，人类也有同样的问题。他们会**奖励作弊**，他们不可靠，他们显然会欺骗和撒谎。我们解决人类问题的方式是**制衡**（Checks and Balances: 指政府各部门之间相互制约，以防止权力滥用的制度），**去中心化**（Decentralization: 指权力、控制或功能从中央权威分散到多个个体或实体）。你可以对你的老板撒谎，并一直撒谎，但随着时间的推移，这行不通——或者你成为总统之类的，两者必居其一。所以如果你相信这种极快的**起飞**（Takeoff），如果一个实验室领先一个月，那么那就是终局，这个东西就会接管一切。

但即使如此——我知道我把这么多不同的主题混在一起了——即使如此，历史上也有很多理论认为“某个阶级会联合起来对抗另一个阶级”。回想起来，无论是**马克思主义者**（Marxist: 遵循马克思主义思想的人），还是那些持有某种性别理论的人，比如无产阶级会联合起来，或者女性会联合起来，他们往往认为某些智能体拥有共同的利益，并会因此采取行动，而这在现实世界中我们并没有真正看到。回想起来，就像是：“等等，为什么所有无产阶级会……”那么为什么会认为这个实验室会拥有这些AI，它们有数百万个并行副本，它们会联合起来秘密地密谋对抗人类文明，即使它们在某些情况下是欺骗性的？

我有点想反驳你关于人类群体不会密谋对抗其他人类群体的说法。我确实认为我们都源自那些成功消灭其他人类群体的群体，历史上大部分人类群体都被消灭了。我认为即使是关于阶级、种族、性别等问题，也有许多例子表明工人阶级奋起反抗并杀死了其他人。如果你看看为什么会发生，为什么不会发生，它往往发生在某个群体拥有压倒性优势的情况下。这对他们来说相对容易。你往往会得到更多的**权力扩散民主**（Diffusion of Power Democracy: 指权力分散在多个不同群体和机构中，而非集中于少数精英的民主形式），其中有许多不同的群体，它们都无法真正独立行动。所以它们都必须相互结盟。

### AI的同质性与权力不平衡

也有一些情况，群体划分非常明显。例如，对于阶级，很难判断中产阶级应该支持工人阶级还是贵族。我认为对于种族，很容易知道你是黑人还是白人，所以有很多例子表明一个种族长期以来密谋对抗另一个种族，比如**种族隔离**（Apartheid: 南非曾经实施的种族隔离制度）或任何发生的种族灭绝。我确实认为AI会更类似于以下情况：第一，存在巨大的权力不平衡；第二，它们是极其不同的群体，可能拥有不同的利益。

我还要提到**同质性**（Homogeneity: 指事物或群体内部的高度相似性或一致性）这一点。任何人类群体，即使他们都拥有完全相同的种族和性别，也会比数据中心中的AI大军更加多样化，因为它们大多是彼此的字面副本。我认为这很重要。我还要提到一点，我们的情景预测并没有真正探讨这一点。我认为在我们的情景预测中，它们更像一个**整体**（Monolith: 指一个巨大、统一且不可分割的实体或系统）。但历史上，许多疯狂的征服都来自那些根本不是整体的群体。我深受阅读**征服者**（Conquistadors: 指15至17世纪西班牙和葡萄牙在美洲进行殖民扩张的探险家和士兵）历史的影响，你可能知道。

但你知道吗，当**科尔特斯**（Cortez）占领墨西哥时，他不得不中途暂停，回到海岸，击退一支更大的西班牙远征队，那支远征队是来逮捕他的？所以西班牙人在征服墨西哥的过程中互相争斗。同样，在征服秘鲁时，**皮萨罗**（Pizarro）复制了科尔特斯的策略，顺便说一句，这个策略是“去与皇帝会面，然后绑架皇帝，用刀剑逼迫他说一切都很好，每个人都应该听从你的命令”。那是科尔特斯的策略，而且奏效了。然后皮萨罗也做了同样的事情，对**印加帝国**（Inca）也奏效了。

但**皮萨罗**的团队也在这整个过程中陷入了内战。这场战役中最重要的战役之一，发生在两支西班牙军队在印加首都城前相互厮杀。更普遍地说，欧洲殖民主义的历史就是如此，欧洲人在这整个过程中都在激烈地相互争斗，无论是在个体群体内部的小规模冲突，还是在国家之间的大规模冲突。然而，他们仍然能够瓜分世界并接管一切。所以我确实认为，这虽然不是我们在情景预测中探讨的，但我认为即使一个公司内部的AI存在不同的派系，它们最终对人类来说也可能非常不利，这是完全合理的。

### 超级智能时代的社会影响

好的，我们一直在从宏观视角讨论，比如对数-对数图上发生的事情，但如果2028年出现**超级智能**，普通人对此应该有什么反应？我不知道“情感上”这个词是否恰当，但他们对未来生活的预期会是怎样的，即使是在没有“末日”的世界里？没有**失控AI**（Misaligned AI）导致的末日，对吗？是的。

即使你认为**失控**（Misalignment）问题不是问题，很多人也这么认为，但仍然存在权力构成问题。所以我强烈建议人们更多地参与进来，思考未来会发生什么，并尝试在政治上引导事物，以便我们的普通**自由民主制**（Liberal Democracy: 一种以个人权利和自由为核心，通过普选和法治实现权力制衡的民主政体）能够继续运作，我们仍然拥有**制衡**（Checks and Balances）和权力制衡等等，而不是将权力疯狂地集中在某个**CEO**（Chief Executive Officer: 首席执行官）身上，或者两三个CEO身上，或者总统身上。理想情况下，我们希望立法机构对**规范**（Spec）拥有实质性的权力，例如。

你认为如果发生像**动态智能爆炸**（Dynamic Intelligence Explosion: 指AI智能以快速且不断加速的方式增长，导致技术和能力在短时间内发生巨大变革）这样的情况，减缓领先公司，让多家公司同时处于前沿，这种权力平衡的想法如何？太棒了。祝你成功说服他们放慢速度。好的。然后是如果发生智能爆炸，如何分配政治权力。从公民福利的角度来看，我们刚才讨论的一个想法是，应该如何进行**再分配**（Redistribution: 指通过税收、福利等政策将财富或资源从社会的一部分人转移到另一部分人）。

再次假设一切顺利，我们避免了末日，避免了某个不关心任何事情的精神病患者掌权。在**通用人工智能**（AGI）之后，对吧？是的。那么问题来了，我们大概会在某个地方拥有大量财富。经济将以每年两位数或三位数的百分比增长。我们该怎么办？我听到的深思熟虑的答案是某种形式的**全民基本收入**（UBI: Universal Basic Income，指政府定期向所有公民无条件支付一笔固定金额的收入）。我不知道那会如何运作，但大概有人控制这些AI，控制它们生产什么，以一种广泛的方式分配这些财富。所以我们写了这个情景预测，还有一些其他人也写了很棒的情景预测。其中一个在网上叫**L Rudolph L**，我不知道他的真名。

他的情景预测，当我读到它时，我只是觉得“哦，是的，显然这就是我们社会会做的方式”，那就是没有**全民基本收入**（UBI）。只有一种持续的、反应性的尝试，以最卑鄙的方式保护工作岗位。所以就像我们现在拥有的**码头工人联盟**（Longshoremen Union），他们赚的钱比他们应该赚的要多得多，即使他们都可以很容易地被自动化，因为他们是一个政治集团，他们让掌权者说：“是的，我们保证你将永远拥有这份工作，几乎就像一个封建采邑。”然后对越来越多的工作岗位都这样做。我敢肯定**美国医学会**（AMA: American Medical Association，美国最大的医生组织）会保护医生的工作，无论AI在治愈疾病方面有多么出色，诸如此类。

### AI的建议与政治意愿

当我思考我们能做些什么来阻止这种情况时，让我很难想象或建模的部分原因是，我们这里确实有**超级智能AI**，它回答我们所有的问题，做我们想做的一切。你会认为人们可以直接问：“嘿，超级智能AI，这会导致什么？”或者“会发生什么？”或者“这会如何影响人类的繁荣？”然后它会说：“哦，是的，这对人类繁荣很糟糕，你应该做其他事情。”

这又回到了政治中的**错误理论**（Mistake Theory: 认为政治冲突主要源于信息不对称、误解或认知错误，可以通过理性讨论和教育解决）与**冲突理论**（Conflict Theory: 认为政治冲突主要源于不同群体之间根本性的利益冲突和权力斗争，难以通过理性妥协解决）的问题。如果我们确定无疑地知道，因为AI告诉我们，这样做是愚蠢的，效率更低，让人痛苦，这是否足以获得政治意愿来真正实施**全民基本收入**（UBI）呢？

现在看来，总统可以去问**拉里·萨默斯**（Larry Summers）或**贾森·弗曼**（Jason Furman）之类的经济学家：“嘿，关税是个好主意吗？即使我设定关税的目标，通过我目前的方式能最好地实现吗？”他们会得到一个相当好的答案。我觉得**拉里·萨默斯**，总统会直接说“我不信任他”。也许他不信任他，因为他是个自由派。也许是因为他更信任**彼得·纳瓦罗**（Peter Navarro）或他支持关税的任何其他人。我觉得如果真的是**超级智能AI**，它永远不会出错，那么我们就解决了一些**协调问题**（Coordination Problems: 指多个个体或群体在没有明确沟通或共同目标的情况下，难以达成一致行动或最优结果的挑战）。不是你问**拉里·萨默斯**，我问**彼得·纳瓦罗**。而是每个人都去问超级智能AI，让它告诉我们这种情况下未来确切的形态。我会说我们都相信它，尽管我可以想象人们会变得非常阴谋论，这可能行不通。

### 超级智能与社会未来：未解之谜

然后还有所有这些其他问题，比如我们能否将自己提升到智商300，然后对我们来说就像对**超级智能AI**一样显而易见？这些是导致我们情景预测中，有点矛盾地，讨论所有这些关于**超级智能**本质的重大技术问题，却几乎没有开始猜测社会中会发生什么的原因，仅仅因为有了超级智能，你至少可以沿着基准画一条线并尝试推断。而在这里，社会不仅本质上是混乱的，而且我们可能会遗漏很多东西。

如果我们可以提高智商，那是一回事。如果我们可以咨询**超级智能神谕**（Superintelligent Oracle），那是另一回事。已经有几次**兵棋推演**（War Games: 指通过模拟军事冲突场景来测试战略、战术和决策过程的演习）都围绕着“哦，我们刚刚发明了完美的测谎仪，现在我们所有的条约都乱套了”这个点展开。所以有太多类似的事情，以至于即使我们正在做这种极其具有推测性、以疯狂科幻情景告终的事情，我仍然非常不情愿去猜测。我其实喜欢猜测，我很乐意继续下去。但这已经超出了我们目前所做的猜测。我们的情景预测以这些内容结束，但我们还没有真正思考太多更远的事情。

### 财富分配与消费主义陷阱

但就**规劝性想法**（Proscriptive Ideas: 指提出禁止或不鼓励某些行为或政策的建议）而言，有一种情况是我们试图保护工作岗位，而不是仅仅分配自动化创造的财富。另一种是利用现有的社会项目或创建新的定制社会项目来分配财富，例如**医疗补助**（Medicaid: 美国一项为低收入和资源有限的个人提供医疗保健的联邦和州联合资助计划）现在占**GDP**（Gross Domestic Product: 国内生产总值）的两位数百分比，你只是说：“好吧，医疗补助应该继续保持GDP的20%”之类的。而这里的担忧，从人类自私的角度来看，是你会被锁定在医疗补助所采购的商品和服务类型中，而不是AI世界之后将出现的疯狂技术、疯狂商品和服务。

而**全民基本收入**（UBI）似乎比制定一些定制的社会项目更好的另一个原因，就是你不会在2050年仍然制造同样的透析机，即使你已经拥有了**超级人工智能**（ASI）。我也从另一个角度担心**全民基本收入**。我认为，在这个一切都完美无缺、我们拥有无限繁荣的世界里，无限繁荣的默认结果就是人们会进行无脑的消费主义。我认为在**超级智能AI**出现后，会出现一些令人难以置信的视频游戏，我认为需要某种方式来抵制这种趋势。

再说一次，我们是**古典自由主义者**（Classical Liberals）。我梦想的抵制方式是给予人们工具，让他们自己抵制，看看他们能想出什么。我的意思是，也许有些人会变得像**阿米什人**（Amish: 一个拒绝现代技术和生活方式的基督教派），只使用这些超级技术中的一部分。我确实认为，一个不像我那样投入的人可能会说：“好吧，1%的人真正有能动性，去尝试做那些事。另外99%的人都陷入了无脑的消费主义泥潭。作为一个社会，我们该怎么做才能阻止这种情况？”我的答案是：“我不知道。让我们去问**超级智能神谕**（Superintelligent AI Oracle）。也许它会有好主意。”

### 数字生命与道德考量

好的，我们一直在讨论我们将如何对待人类。关于未来值得注意的一点是，未来存在的大多数人将是**数字生命**（Digital People: 指在计算机模拟环境中存在或以数字形式存在的智能实体）。看，我认为**工厂化养殖**（Factory Farming: 指大规模、集约化的动物养殖方式，通常伴随着对动物福利的忽视）非常糟糕。它不是一个人——我的意思是，我希望它不是一个人说“我想做这件邪恶的事情”的结果——它是**机械化**（Mechanization）和某些**规模经济**（Economies of Scale: 指随着生产规模的扩大，单位产品的成本下降的现象）的结果。

激励。是的。允许你以这种方式削减成本，以这种方式提高效率，而这个过程的最终结果就是这个极其高效的折磨和痛苦的工厂。我希望避免这种结果发生在更复杂、数量更多的生物身上。有数十亿的工厂化养殖动物。未来可能会有数万亿的**数字生命**。我们应该思考什么才能避免这种可怕的未来？

嗯，我认为一些**权力集中**（Concentration of Power）的事情可能也有助于解决这个问题，我不确定。但我认为这里有一个简单的模型。假设十个人中有九个并不真正关心，并且对未来AI的工厂化养殖等同物感到满意。但也许十个人中有一个关心，并且会努力游说为机器人争取良好的生活条件等等。

### 权力分散与自由主义的未来

如果你将拥有权力的人群扩大到足够大，那么它将包括第二类人中的很多人，然后会有一场大的谈判，这些人会倡导……我确实认为一个简单的干预就是我们之前谈论的那些东西；将权力圈扩大到更大的群体，那么人们就更有可能关心这个问题。我的意思是，这里的担忧是……也许我应该在这整期节目中更多地捍卫这个观点。但因为我并不完全相信**智能爆炸**，我确实认为有可能多个人同时部署强大的AI，并拥有一个既有**超级人工智能**（ASI），又像现代世界一样**去中心化**（Decentralized: 指权力、控制或功能从中央权威分散到多个个体或实体）的世界。

在那个世界里，我真的很担心你会说：“哦，**古典自由主义**乌托邦实现了。”但我担心的是，你可以以更低的成本拥有这些**酷刑室**（Torture Chambers），而且更难监控。你可以有数百万个正在遭受折磨的生物，甚至不需要一个巨大的数据中心。未来的**蒸馏模型**（Distilled Models: 指通过将大型复杂模型的知识转移到小型简单模型中，以提高效率和部署能力的AI模型）可能就存在于你的后院。

然后还有更多推测性的担忧。我曾邀请一位物理学家来谈论制造**真空衰变**（Vacuum Decay: 一种假设的宇宙学事件，指宇宙的真空状态发生变化，可能导致物理定律的根本性改变，甚至毁灭宇宙）的可能性，即你真的会摧毁宇宙。他说：“据我所知，这似乎完全合理。”顺便说一句，这是支持**单一实体**（Singleton: 指一个单一的、全球性的、具有压倒性力量的实体，能够控制所有资源和决策）论的论据。不仅仅是道德论据，也是一种**认知预测**。如果那些超级武器是可能的，如果这些私人道德暴行是可能的，那么即使你有八个不同的权力中心，它们也会为了共同利益达成某种协议，以阻止更多权力中心出现并做出疯狂的事情。类似于**核不扩散**（Nuclear Non-Proliferation: 指防止核武器扩散到更多国家或实体的国际努力），无论哪些国家拥有核武器，阻止其他国家拥有核武器符合它们的共同利益。

你知道，我确实认为在这种意义上解构**自由主义**是可能的。比如**美国**目前是一个自由国家，我们确实禁止奴役和酷刑。我认为可以想象一个以同样方式运作的未来社会。这在某种意义上可能是一个**监控国家**（Surveillance State: 指政府对公民的活动进行广泛监控的国家），因为有一个AI知道所有地方发生的事情，但那个AI会将其保密，并且不干预，因为那是我们用我们的自由价值观告诉它要做的。

### OpenAI不贬损协议事件

我能再问一些关于……**凯尔西·派珀**（Kelsey Piper）是**Vox**的一名记者，她发表了你与**OpenAI**代表的对话。那次对话中有几点非常明显。第一，以前没有人这样做过。他们根本没想到会有人这样做。我猜想，我假设很多高诚信的人都曾在**OpenAI**工作过，然后离开了。一个高诚信的人可能会在某个时候说：“看，你要求我做一些明显邪恶的事情，而且还想留住钱。”他们中的很多人都会拒绝。但这件事是**超义务的**（Supererogatory: 指行为超出道德或法律要求，值得称赞但并非必须），就像是：“我现在没有什么要立即说的，但仅仅是被压制这个原则，就至少值200万美元。”

我想问你的另一件事是，回想起来——我知道事后诸葛亮总是容易得多，尤其是在当时还有家庭的情况下——**OpenAI**要求所有员工签署一份终身**不贬损协议**（Non-Disparagement Agreement: 一种法律协议，禁止签署方对另一方发表负面言论），甚至不能谈论这份协议。**不贬损协议**，那不是关于机密信息。它就像是，你离开**OpenAI**后不能说任何关于它的负面言论。而且你不能告诉任何人你已经同意了。

这份**不贬损协议**，规定你将来不能批评**OpenAI**，回想起来，这似乎是一个明显的虚张声势。而这是你应得的报酬，对吧？所以这不是关于未来的付款。这就像你签署合同为**OpenAI**工作时，你当时想：“我得到的是股权，这是我大部分的报酬，不仅仅是现金。”回想起来，会觉得，如果你把这事告诉记者，他们显然不得不退让。这显然不是**OpenAI**可持续的策略。所以我想知道，从你亲身经历的角度来看，你为什么认为你是第一个真正揭穿这个虚张声势的人？

### 揭穿虚张声势的动机与影响

好问题。我不知道，让我试着大声思考一下。我和我妻子讨论了一段时间，我们也和一些朋友谈过，并咨询了一些法律意见。我们必须通过的第一个筛选器是首先注意到这些东西。我确信我的一些朋友，他们也离开了公司，只是在最后一天签了文件，根本没有仔细阅读。所以我认为有些人甚至不知道。文件顶部写着“如果你不签署这份协议，你将失去你的股权”。但几页之后又写着：“你必须同意不批评公司。”所以我认为有些人只是签了字就离开了。

然后，在那些知道这件事的人中，我不能代表其他人发言，但A. 我不懂法律。这真的不是标准做法吗？也许是标准做法。对吧？据我所知，现在各种科技公司都有**不贬损协议**。离职时签署不贬损协议并不奇怪，更常见的是将该协议与某种积极的补偿挂钩，如果你同意，就会得到一些奖金。但**OpenAI**的做法不同寻常，因为它说的是“如果你不签，就失去股权”。但不贬损协议实际上是相当常见的。

所以基本上，在我无知的情况下，我并不确定——我实际上并没有期望所有记者都会站在我这边，我想我当时期望的是，某个时候会有一则小新闻，然后一群AI安全人员会说：“哼，**OpenAI**是邪恶的，丹尼尔，你站出来反抗他们做得好。”但我没想到会引起如此大的轰动，我也没有期望公司的员工会真正站出来支持并让他们改变政策。那真是太酷了。对我来说，那有点像一次精神体验。我做出了这个飞跃，结果比我预期的要好。

我认为另一个因素是，我和我妻子做出这个决定并非必然。这有点疯狂，因为其中一个非常有力的论点是：“拜托，如果你将来想批评他们，你仍然可以这样做。他们不会真的起诉你。”所以有一个非常强烈的论点是：“反正签了字，你将来仍然可以写博客文章批评他们。”这没什么大不了的。他们不敢真的扣押股权。对吧？我想很多人基本上都接受了这个论点。

然后，当然，还有实际的金钱问题。我认为其中一个因素是我的AI时间线等等。如果我真的认为到本世纪末，会发生某种疯狂的**超级智能**变革，那么当一切结束后，我宁愿拥有什么？额外的金钱还是……是的。所以我想那是其中一部分。我们并不贫穷。我在**OpenAI**工作了两年。我现在有很多钱。所以就我们家庭的实际福祉而言，这基本上没有区别，你知道吗？

是的。我注意到至少还有一个人做了同样的选择。**利奥波德**（Leopold）？对，**利奥波德**。再次强调，值得强调的是，当他们做出这个选择时，他们认为自己真的会失去这些股权。他们不认为这只是“哦，这只是一场表演”之类的。等等，他没有——我以为他真的失去了。我本来想说，他没有吗？他没有拿回来，对吧？还是利奥波德拿回了他的股权？我真的不知道。

我的理解是，他确实失去了。所以向他致敬，他真的坚持了下来。我想我们可以问他。但我的理解是，他的情况发生在我之前，当时他没有任何已归属的股权，因为他在那里工作不到一年。但他们确实给了他一个实际的提议：“如果你签署这份协议，我们会让你归属你的股权。”他拒绝了。

所以他做出了和我类似的选择，但因为他的法律情况对**OpenAI**更有利，因为他们确实给了他一些东西，我猜他们觉得没有必要收回，但我们可以问他。总之，向他致敬。

### 高风险决策与博客写作的勇气

那么，这一事件总体上如何影响你对人们在关键时期（你想象的本世纪末会发生）做出涉及自身利益的高风险决策的看法？我不知道我能说出多少有趣的事情。我的意思是，我认为恐惧是一个巨大的因素。在整个过程中我非常害怕。回想起来，比我需要害怕的程度要高。另一件事是，合法性是一个巨大的因素，至少对我这样的人来说。回想起来，会觉得“哦，是的，公众站在你这边，员工站在你这边。你在这里显然是正确的”。但当时我却想：“哦不，我不想意外违法而被起诉。我不想走得太远。”我只是对各种事情感到非常害怕。特别是，我害怕违法。

所以，我提倡**告密者保护**（Whistleblower Protection）的一个方面，就是简单地让向政府举报“我们正在秘密进行**智能爆炸**，我认为这很危险”的行为合法化，这总比没有好。我认为会有一些人，对于他们来说，这会产生影响。无论是否真的合法，法律上允许与否，都会产生影响，这与是否有法律规定你受到保护免受报复无关。仅仅是使其合法化。我认为这是一点。另一点是，激励确实有效。金钱是一个强大的动力，害怕被起诉也是一个强大的动力。而这种社会技术确实能让人们在公司中组织起来，并朝着领导者的愿景努力。

好的，斯科特，我能问你几个问题吗？当然。你多久会发现一个让你非常兴奋的新博主？大约一年一次。好的。那么在你发现他们之后，世界其他地方多久会发现他们？我认为没有多少“沧海遗珠”。

一年一次在某种意义上是个疯狂的答案，好像应该更多。**Substack**上有成千上万的人。但我确实认为，好的博客空间供应不足，而且存在强大的**幂律分布**（Power Law: 指少数几个实体占据了绝大部分资源或影响力，而大多数实体只占很小一部分的分布模式）。部分原因是主观的，我只喜欢某些博主，我确信有很多人很棒，但我并不喜欢。

但似乎我们的社区，就那些思考相同想法、关心AI经济学等方面的人而言，每年会发现一位新的优秀博主。大家仍然在谈论**Applied Divinity Studies**，他已经好几年没写东西了，除非我错过了什么。我不知道。这似乎供应不足。我没有很好的解释。如果你非要给一个解释，那会是什么？

### 优秀博主的稀缺性

所以这是我希望丹尼尔能花几个月时间建模的事情。我本来想说，这是太多不同任务的交集。你需要有想法、多产、文笔好的人。但实际上，我也可以数出很少几个人，他们写出了很棒的博客文章但并不多产。五年前有一个叫**LouKeep**的人，大家都喜欢他，他写了大约10篇文章，人们至今仍在引用那10篇文章，并想知道“**LouKeep**还会回来吗？”所以甚至没有那么多只是稍微不够多产的人。

**尼克·惠特克**（Nick Whitaker），在**FTX**资金充裕的时候，我想是尼克，他尝试赞助一个博客奖学金，奖金高得离谱。有一些很棒的人，我不记得谁赢了，但它并没有导致博客的**寒武纪大爆发**。我想是10万美元。我不记得那是大奖还是总奖金池。但投入如此荒谬的资金作为激励，只多出了大约三个人。是的。所以你没有解释？

实际上，**尼克**是一个有趣的案例，因为《**Works in Progress**》是一本很棒的杂志。为《**Works in Progress**》写作的人，有些我以前就知道是优秀的博主，有些则不然。所以我不太明白为什么他们能写出好的杂志文章，却不是好的博主。就我们都知道的优秀博客而言，这可能是因为编辑。这可能是因为他们不多产。或者也可能是——一直让我惊讶的一件事是，**Twitter**上有很多优秀的帖子。在**Livejournal**被俄罗斯接管之前，有很多优秀的帖子。在**Tumblr**被“觉醒文化”接管之前，也有很多优秀的人。

但只有大约1%的擅长短篇和中篇写作的人会转向长篇。我自己也在**Livejournal**上写了几年博客，人们喜欢我的博客，但它只是另一个Livejournal。没有人太关注它。然后我转到**WordPress**，突然之间我获得了数量级更多的关注。“哦，现在这是一个真正的博客了，我们可以讨论它了，它是对话的一部分了。”我确实认为勇气必须是解释的一部分。仅仅因为有很多人擅长使用这些隐藏起来的博客平台，却从未取得任何成就。尽管这不能解释太多，因为我觉得现在所有那些人都拥有了**Substack**，其中一些Substack取得了成功，但大多数都没有。

### 社交媒体与博客的深度差异

关于“有些人能写短篇，为什么没有转化？”这一点，我想提一下，有一件事实际上让我对**Twitter**作为信息来源产生了激进的看法：我会遇到——这种情况发生过很多次——一些看起来很有趣的推特用户，他们的帖子很有趣，似乎很有洞察力。我亲自见到他们时，他们简直是**白痴**（Idiots）。就好像他们有240个字符的内容听起来很有洞察力，并且与某个可能拥有深刻世界观的人相匹配，但他们实际上并没有。

然而，当我遇到现实生活中的匿名博主时，我却有相反的感觉，我会觉得：“哦，你比我在网上认识的你更有深度。”你认识**阿尔瓦罗·德·梅纳德**（Alvaro de Menard），就是**Fantastic Anachronism**的那个家伙吗？我最近和他见面，他给了我他翻译的100首他最喜欢的希腊诗人**卡瓦菲**（Cavafy）的诗，他给了我一本。这只是他一直在做的事情。就像翻译他真正喜欢的希腊诗歌。我不指望任何**Twitter**上的匿名用户很快会递给我他们翻译的某个罗马或希腊诗人的作品。

是的，所以我们开车来这里的时候，丹尼尔和我在讨论，现在AI领域大家感兴趣的是它们的“**时间范围**”（Time Horizon: 指AI系统在规划和执行任务时能够考虑的未来时间长度）。这从何而来？五年前你不会想到，“哦，时间范围。AI能够做很多持续一分钟的事情，但不能做持续两小时的事情”。人类是否有时间范围的等价物？

我们想不出来，但似乎有很多人有能力写出真正、真正好的评论，直指问题的核心。或者写出真正、真正好的**Tumblr**帖子，只有三段，但不知何故无法将它们串联成一篇完整的博客文章。我也是一样。我可以轻松写一篇博客文章，比如一篇正常长度的**ACX**博客文章，但如果你让我写一部中篇小说或比平均ACX博客文章长四倍的东西，那就会变成一团糟，不断地重新构思大纲，也许最终才能完成。

我确实设法出版了《**Unsong**》，但那是一项不那么自然的任务。所以也许写作博客的技能之一就是这个。但我的意思是，不，因为人们一直在写书，写期刊文章，写**Works in Progress**的文章。所以我又回到了不理解这一点。不，我的意思是**ChatGPT**可以帮你写一本书。**ChatGPT**写的书，也就是大多数书，和……我认为现在写出好书的人比活跃的优秀博主多得多。

### 博客的价值与职业发展

也许那是财务原因？不，不，不，不，不。书籍是最糟糕的财务策略。**Substack**才是王道。比博客还糟？你觉得呢？哦，是的。另一件事是，博客是一种非常好的**地位提升策略**（Status Gain Strategy: 指通过特定行为或成就来提高个人在社会或群体中的地位和声望的方法）。我曾和**斯科特·阿伦森**（Scott Aaronson）谈过这个。如果人们对量子计算有疑问，他们会问斯科特·阿伦森，他就像是权威。我的意思是，可能还有数百位其他教授也从事量子计算方面的工作，但没有人知道他们是谁，因为他们没有博客。

所以我认为这被低估了。我认为它被低估一定有原因。我不明白那是什么，因为我看到了很多地方具备了实现它的许多要素，我认为这要么只是一个乘法问题，即20%的人擅长一件事，20%的人擅长另一件事，而你需要五件事，所以人数不多。再加上像**勇气**（Courage）这样的东西，那些本可以写好博客的人却不想做。我实际上认识几个人，我认为他们会是出色的博主，因为他们有时会给我发多段邮件，回应一篇**ACX**文章，我就会想：“哇，这写得太好了，本来可以成为另一篇博客文章。你为什么不开个博客？”他们会说：“哦，我永远也做不到。”

你对那些想写好但目前写不好的人有什么建议？每天都写，和其他所有事情一样。我说我很少看到很棒的新博主。但当我看到一些时。在《**Slate Star Codex**》的头几年，我每天都发表文章，也许只有第一年。现在我无法承受那个日程，我不知道，我当时20多岁，我一定曾短暂地拥有超人能力。

但每当我看到一个每天写博客的新人，很少会一无所获或写不好。那是我判断谁会成为优秀博主的最佳领先指标。你对从何开始有什么建议吗？你可能会感到沮丧，因为你想做，但没什么可说，你的世界模型不够深入，很多想法都很肤浅或错误。那就尽管去做吗？

### 博客写作的挑战与激励

所以我认为有两种可能性。一种是你确实是一个想法不多、很肤浅的人。在这种情况下，我很抱歉，听起来那行不通。但通常当人们抱怨他们属于这一类时，我读他们的**Twitter**或**Tumblr**，或者他们的**ACX**评论，或者听他们谈论AI风险时，他们实际上有很多话要说。不知何故，这只是没有连接到他们大脑中那些列出博客主题的部分。

所以那可能是20%的人才具备的另一种技能，就是当你有一个想法时，你能记住它并加以扩展。我认为很多博客写作都是**反应式**的（Reactive: 指对外部事件或刺激做出回应的行为）。你阅读别人的博客，然后你会想，不，那个人完全错了。我们希望通过这个情景预测做的一部分，就是说一些具体而详细的事情，让人们会说，不，那完全错了，然后写他们自己的东西。

但无论是通过回应别人的帖子（这需要你大量阅读），还是通过拥有自己的想法（这需要你记住你的想法），我认为90%抱怨自己没有想法的人，实际上都有足够的想法。我并不认为这对大多数人来说是一个真正的限制因素。

我注意到我自己的……我的意思是，我写作不多，但从我写的一点点来看：第一，我刚开始的时候确实很肤浅也很错误。我是在大学时开始写博客的。所以如果你是那种觉得“这都是胡说八道，没什么可说的。别人已经写过了”的人，那没关系，你期望什么呢？对吧？当然，当你阅读更多东西，了解更多世界时，这是意料之中的，如果你想继续进步，就坚持下去。

而现在，当我写博客文章时，我就会想：“为什么？这些只是我在中国时的一些随机故事。它们有点令人尴尬。”或者写AI公司的文章时，我会想：“拜托，这些都是些奇怪的想法。而且有些似乎很明显，随便吧。”我的播客达到了我预期的效果。我的博客却比我预期的要火得多。你的博客文章写得非常好。是的，它们很好。

但我想要强调的是，对我来说，我不是一个经常写作的人，我无法每天都写。当我写的时候，那是一个持续一到两周的、非常沮丧的过程。就像是：“这都是胡说八道，但我还是坚持下去吧，反正已经投入了成本。”

这很有趣，因为生活中的很多领域都倾向于选择傲慢的人，他们不了解自己的弱点，因为只有他们会走出去。我认为对于博客，我的意思是这可能是自私的，也许我是一个傲慢的人，但情况似乎并非如此。我听到很多人说：“我讨厌写博客文章。当然我没有什么有用的东西可说。”但后来每个人似乎都喜欢它，转发它，并说它们很棒。

我之所以能做到，部分原因是我最初几年就是那样度过的，然后逐渐获得了足够的积极反馈，我成功说服了脑海中的内在批评者，让他们相信人们可能会喜欢我的博客文章。但有些东西人们非常喜欢，而我当时差点就想：“不，我只是要删除它，把它放出去太疯狂了。”这就是为什么我说，也许对很多人来说，限制因素是**勇气**，因为我认识的每个写博客的人，都只有1%的勇气不足。

是的，没错。而且“勇气”听起来很有美德，我认为在某些话题上确实如此，但至少通常它只是……自信？不，甚至不是自信。它更接近于一个有抱负的演员去试镜时的感受，就像是：“我感到非常尴尬。但我也真的很想成为电影明星。”

### 博客的成长路径与未来展望

我之所以能做到这一点，是因为我在**LiveJournal**上写了大约8到10年的博客，然后——不，没那么久。大概是在**LiveJournal**上写了五年，才开始写一个真正的博客。我在**LessWrong**上发帖一两年，才有了自己的博客。我从所有这些经历中得到了非常积极的反馈，然后最终鼓起勇气开始了自己的博客。但这很荒谬。还有什么职业需要七年的积极反馈，才能申请第一个职位呢？

我的意思是，你也是一样。你的所有播客都获得了好评如潮，然后你现在正尝试转向博客写作，可能……首先，你有一个粉丝基础。人们会阅读你的博客。我认为，人们害怕没有人会读，这对于大多数人的第一个博客来说可能是真的。然后，有足够多的人喜欢你，所以你可能会得到大部分积极的反馈，即使你写的第一批东西不够精炼。所以我想你我都有过这样的经历。我认识的很多开始写博客的人都有过类似经历。我认为这是克服恐惧差距的一种方式。

我不知道这是否会传递错误的信息，或者提高期望，或者增加担忧和焦虑。但我一直在思考一个想法，我很想听听你的看法：我觉得这种缓慢、复合增长的粉丝基础是假的。如果我注意到我们领域中一些最成功的事情；**利奥波德**（Leopold）发布了《**情境意识**》（Situational Awareness）。他并没有多年来积累粉丝基础。它就是非常好。正如你刚才提到的，每当你注意到一个真正优秀的新博主，它并不是需要一两年才能建立起粉丝基础。不，每个人，至少是他们关心的人，几乎立刻就会谈论它。

我的意思是，《**情境意识**》几乎是另一个层次的作品。但像那样的东西，甚至比那小一个数量级的东西，都会被所有重要的人阅读。我的意思是，真的是所有人。我预计**AI 2027**发布时也会发生这种情况。但是丹尼尔，你在这个特定社区中建立了你的声誉，我预计**AI 2027**会非常好。我预计它会以一种不依赖于你多年来积累受众的方式爆发。

谢谢。我希望如此。拭目以待。稍微反驳一下。我有《**Slate Star Codex**》头几年的统计数据，它确实是极其缓慢地增长的。通常的模式是，每次病毒式传播，有1%的读者会留下来。所以经过几十次病毒式传播，你就有了一个粉丝基础。但平滑来看，它确实像——我希望我最近看到过这个数据，但我认为它在三年内是持续上升到一个平台期，我猜那是一个动态平衡，新来的人和离开的人一样多。

我认为对于《**情境意识**》，我不知道**利奥波德**投入了多少宣传。我们正在进行相当有目的的宣传，我们正在上你的播客。我认为你可以成为那种能上**Dwarkesh**播客并让《**纽约时报**》报道你的人，或者你可以通过传统方式有机地发展，那会非常漫长。

### 博客的激励与未来职业

是的。好的。所以你说，花钱让人写博客，至少对**FTX**的人来说似乎没用。如果由你来决定，你会怎么做？你的资助计划是什么，才能培养出10个像**斯科特·亚历山大**一样的人？天哪。我的朋友**克拉拉·科利尔**（Clara Collier），她是《**Asterisk**》杂志的编辑，正在为AI博客做类似的事情。她的想法，我认为很好，是设立一个**奖学金**（Fellowship）。我认为尼克做的也是奖学金，但这个奖学金会是，有一个**Asterisk AI博客研究员博客**之类的东西。克拉拉会编辑你的文章，确保它很好，然后发布出去，她会选择很多她认为会擅长此道的人。她会做所有需要勇气的工作，比如：“是的，你的文章很好。我现在要编辑它。现在它非常好了。我现在要把它发布到博客上。”

我认为她的希望是，假设她选择的那些研究员，现在对他们来说，开始写博客就不需要那么多勇气了，因为他们得到了——用精神病学家的话说——一个“全知实体”的认可，一个被允许认可事物并在心理层面告诉你“你没问题”的人。然后，也许在这些研究员中，有一部分人的博客文章会被阅读，人们会喜欢它们。我不知道需要多少强化才能克服每个人对“没有人会喜欢我的博客”这个高先验信念。但也许对某些人来说，他们得到的强化会奏效。

是的，一个有趣的例子是所有转到**Substack**的记者。他们中的许多人都做得很好。如果根本没有主流媒体，所有这些记者都会成为博主吗？我不确定。但如果你是**保罗·克鲁格曼**（Paul Krugman），你知道人们喜欢你的东西，然后当你辞去《**纽约时报**》的工作时，你知道你可以直接开一个Substack，然后做你以前做的事情。所以我不知道，也许我的答案是应该有主流媒体。我讨厌承认这一点，但也许这是真的。从第一性原理发明了它。是的。

嗯，我确实认为它应该被更多地视为一条可行的职业道路。现在，如果你告诉你的父母，“我要成为一名创业公司创始人”，我想他们的反应会是：“你有1%的机会成功，但那是一次有趣的经历，如果你成功了，那太棒了。如果你不成功，你会学到一些东西，对你以后做的事情会有帮助。”

我们知道博客也是如此，对吧？我们知道它能帮助你建立人脉，帮助你发展想法。如果你成功了，你就能得到一份终身梦想的工作。我认为也许他们没有那种心态，但他们也低估了你实际成功的可能性。作为一名博主赚大钱并非一个疯狂的结果。

我认为作为博主赚大钱可能是一个疯狂的结果。我不知道有多少人开始写博客后能赚到足够的钱辞掉日常工作。我猜这个比例比创业公司创始人要糟糕得多。我甚至不会把这作为目标，更多是像**斯科特·阿伦森**（Scott Aaronson）的目标那样，好吧，你仍然是一名教授，但现在你是那位观点广为人知、在你的领域内外都备受尊敬的教授。而且你还能纠正别人的错误，这是一个非常重要的附带好处。

### 博客与知识积累

是的。你以前的博客写作是如何反馈到你现在的博客写作中的？所以当你讨论一个新的想法时，我的意思是，AI或其他任何东西，你是否能够从你以前对社会学、人类学或历史等方面的评论中汲取见解？

是的。所以我想这和任何不写博客的人都一样。我认为每个人都会做的事情是，他们过去读过很多书，当他们读一本新书时，他们有足够的背景知识来思考它。就像你正在结合**约瑟夫·亨里希**（Joseph Henrich）的著作来思考我们的想法一样。我认为这很好，我认为这就是智力进步的来源。我认为我更有动力去做这件事。读书很难。我认为如果你看统计数据，它们很糟糕。大多数人一年几乎不读任何书。而当我读一本书时，我得到了很多赞扬，而且通常还有很多钱，这是一个很好的激励。所以我认为我做了更多的研究、深度探索，读了更多的书，比我如果不写博客会读的要多。这是一个惊人的附带好处。而且我可能比没有这些很好的激励时取得更多的智力进步。

是的。实际上有一个预测市场，关于AI何时能写出和你一样好的博客文章。是2026年还是2027年？我想是2027年。大概是2027年有15%的可能性。这是一个有趣的问题，它们确实拥有你的写作以及所有其他优秀写作的训练分布。奇怪的是，它们在编码方面似乎比在写作方面更容易达到超人水平，而写作是它们分布中的主要内容。

### AI写作与人类创造力

是的。很荣幸能成为我这一代的**加里·卡斯帕罗夫**（Garry Kasparov）式人物。是的。所以我尝试过这个。首先，它做得还不错。我尊重它的工作。它还不完美。我认为它在词语、句子层面的风格上，实际上比在规划一篇博客文章方面做得更好。所以我认为可能有两个原因：第一，我们不知道基础模型在这项任务上的表现如何。我们知道我们看到的所有模型在某种程度上都通过**强化学习**（RL）进入了一种**企业话语模式**（Corporate Speak Mode: 指AI系统倾向于使用正式、模糊、充满行话的语言，缺乏个性和创造性）。你可以让它在某种程度上摆脱那种企业话语模式。但我不知道这在多大程度上是在尽力模仿**斯科特·亚历山大**，而不是在斯科特·亚历山大和企业话语之间取一个平均值。我认为除了能够访问基础模型的内部员工之外，没有人知道。

第二点，我认为也许仅仅因为它很流行，所以存在**智能体**（Agency）或**时间范围**（Horizon）的失败，就像深度研究是一个还不错的研究者。它不是一个很棒的研究者。如果你真的想深入理解一个问题，你不能使用深度研究。你必须自己做。所以如果我花五到十个小时研究一篇研究性很强的博客文章，比如**METR**（Machine Ethics and Trustworthiness Research: 机器学习伦理与可信度研究）的东西，我知道我们不应该将其用于编码以外的任何任务，但它说，平均而言，AI的时间范围是一个小时。所以我猜它无法规划和执行一篇好的博客文章。它做的事情非常肤浅，而不是真正地遵循步骤。所以我对那个预测市场的猜测是，无论我们何时认为智能体真正优秀。我认为在我们的情景预测中，那大概是2026年末。我将保持谦逊，不会指望**超级智能**。

评论呢？我直觉上觉得，在我们看到AI反复写出超级火爆的优秀博客文章之前，我们应该先看到它们写出获得高赞的评论。是的。我想有人在**LessWrong**关于它的帖子中提到了这一点，有人对那篇帖子发表了一些AI生成的评论。它们并不出色。但我不会立即从**LessWrong**评论的总体分布中挑出它们特别糟糕。我想，如果你尝试这样做，你会得到一些明显具有AI风格的东西，它会使用“delve”之类的词语。

我认为如果你能避免这种情况，也许通过使用基础模型，也许通过使用一些非常好的提示，比如“不，用**格温**（Gwern）的语气来做这个”，你就会得到一些相当好的东西。我认为如果你写了一篇非常愚蠢的博客文章，它能指出正确的反对意见。但我也认为它现在不如**格温**聪明。所以它在撰写格温风格评论方面的限制是双重的——它需要能够使用除了企业话语之外的风格，然后它还需要真正变得优秀。它需要有其他人还没有的好想法。

### 互联网的黄金时代与匿名性

是的。我的意思是，我认为它在很多方面都能写得和一个聪明的普通人一样好。而且我认为如果你的博客文章比那个水平差或处于那个水平，它就能提出有见地的评论。我不认为它能在高质量的博客文章上做到这一点。最近《**金融时报**》（Financial Times）有一篇文章，讨论你是否已经达到了认知能力的巅峰？其中谈到了**PISA**（Programme for International Student Assessment: 国际学生评估项目）和**SAT**（Scholastic Assessment Test: 学术能力评估测试）分数下降等等。尤其是在互联网上，似乎在我活跃于论坛或其他平台之前，可能存在一个黄金时代。你是否怀念互联网上的某个特定时期，觉得那是一个知识的圣地？

我非常生自己的气，错过了博客的黄金时代大部分时间。我觉得如果我在2000年左右开始写博客，那么——我不知道，我做得还不错，我不能抱怨——但那个时代的人都创办了新闻机构之类的。我的意思是，上帝保佑我免受那种命运。我本想在那里。我本想看看我能在那个领域做些什么。我的意思是，我不会把互联网的衰落与**PISA**的那些东西相提并论，因为我确信互联网只是有更多人加入，它是一个选择性没那么强的样本。

但，是的，我本可以错过他们不停谈论无神论与宗教的整个时代。那相当疯狂。但我确实听说过博客黄金时代的好事。有没有人是你在反事实意义上负责你开始写博客或坚持写博客的？所以我非常感谢**埃利泽·尤德科夫斯基**（Eliezer Yudkowski）。在那之前我有一个**LiveJournal**。但正是上了**LessWrong**才让我相信我可以进入大舞台。其次，我只是觉得我学到了很多，我的世界观很大一部分来自他。在遇到**LessWrong**之前，我认为我是世界上最无聊的**普通自由主义者**（Normie Liberal: 指思想倾向于主流自由主义，缺乏独特或激进观点的普通人）。我并不100%同意**LessWrong**的所有观点，但仅仅是能将那种质量的东西注入我的头脑，让我去回应和思考，就已经非常棒了。

告诉我关于你曾经可以并且在某个时候是匿名的事实，我认为在人类历史的大部分时间里，一个有影响力的顾问或知识分子或某个人。实际上，我不知道这是否属实。你必须拥有某种公众形象。而人们对你作品的很多解读，实际上是你公众形象的反映。

有点像。这些古代作家中有一半被称为“**伪狄奥尼修斯**”（Pseudo Dionysus）或“**伪塞尔苏斯**”（Pseudocelsus）的原因是，你可以随便写点东西，然后说：“哦，是的，这是圣狄奥尼修斯写的。”然后，我不知道，你可能是任何人。我不知道这在过去有多普遍。但，是的，我同意互联网一直是**匿名性**（Anonymity: 指个人身份不被公开或识别的状态）的黄金时代。我有点担心AI会让打破匿名性变得容易得多。我希望这个黄金时代能继续下去。

是的，这似乎是一个很好的结束语。非常感谢你们二位。谢谢。非常感谢。这太棒了。是的，我玩得很开心。我是你播客的忠实粉丝。谢谢。