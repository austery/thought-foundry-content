---
area: "society-thinking"
category: technology
companies_orgs:
- DeepSeek
- OpenAI
- Mistral
- Anthropic
date: '2025-05-03'
draft: true
guest: ''
insight: ''
layout: post.njk
products_models:
- Claude
project: []
series: ''
source: https://www.youtube.com/watch?v=s266BzGNKKc
speaker: Hung-yi Lee
status: evergreen
summary: 本课程深入探讨了大型语言模型（LLM）推理能力的评估方法。文章首先介绍了通过数学和编程问题进行粗暴评估的现状，并揭示了模型可能存在记忆而非真正推理的问题。随后，讨论了GSM8K和ARC-AGI等基准测试的局限性，以及Chatbot
  Arena这种人类评测机制如何因风格偏好而被“攻破”。最终，文章引用古德哈特定律，指出任何被设定为目标的指标都将失去其有效性，强调了评估系统对模型发展的异化影响。
tags:
- human
- law
- llm
- philosophy
- technology
title: 大型语言模型评估的挑战与古德哈特定律
---
### 大型语言模型推理能力的评估现状

本堂课将探讨大型语言模型（LLM）的评估问题。在讨论了许多关于模型推理能力的内容后，我们如何判断这些模型是否真的具备良好的推理能力呢？

目前，评估模型推理能力的方法通常简单粗暴：直接考察模型解决数学问题的能力。如果模型答对了，就被认为具备良好的推理能力；如果答错了，则被认为不具备。例如，在**DeepSeek**（一家人工智能公司）的技术报告首页，就展示了其模型与**OpenAI o1系列**（OpenAI开发的一系列大型语言模型）模型在数学和编程问题上的正确率。其中，**AIME**（American Invitational Mathematics Examination: 美国数学邀请赛）是一个知名的数学竞赛，图表纵轴代表答题正确率。数据显示，DeepSeek模型在AIME上的正确率略高于o1模型。

当前，评估模型推理能力的方式普遍如此：能解决数学问题就被视为有推理能力，反之则无。不仅DeepSeek的论文采用这种衡量方式，OpenAI的o系列模型在发布时也常通过数学问题来展示其推理能力。

### 记忆与推理的边界：基准测试的局限性

然而，能解决数学问题就真的代表模型具备推理能力吗？有没有可能模型只是恰好“看过”这道数学题，记住了答案，然后“装模作样”地进行推理，最终输出已知答案，让人误以为它很会推理？这种情况并非不可能。

一篇发表于去年10月的论文探讨了有多少答案可能是模型记忆而非推理得出的。在衡量模型数学能力时，有一个著名的**GSM8K**（Grade School Math 8K: 一个包含小学数学应用题的数据集）。目前，大多数模型都能正确回答GSM8K中的问题。这篇论文尝试替换GSM8K应用题中的一些符号或词汇，以观察模型表现。例如，GSM8K中常出现的人名，如“Sophia”，与解题本身毫无关系。将人名或亲戚关系（如“侄子”）替换，或者替换数字（在不影响问题难度的情况下），是否会影响模型的正确率呢？

论文将原始GSM8K题目中的词汇和数字进行修改，在不改变问题难度的前提下，重新测试了市面上的多数模型。结果显示，在修改后的GSM8K测试中，大多数模型的正确率都有所下降。特别是**Mistral**（一家人工智能公司）和**Gemma**（Google开发的一系列轻量级大型语言模型）等模型，其正确率受到了相当大的影响。这表明这些模型确实可能记忆了一些问题的答案，否则为何替换了数字或人名后，它们就答错了呢？

不过，从结果来看，一些真正具备推理能力的模型依然表现出色。例如，**o1-mini**（OpenAI o1系列中的一个小型模型），在去年10月该论文发表时，其受到的影响非常小。这篇论文还有更多内容，例如，他们通过在题目中添加完全不相关的句子，成功地使o1模型的正确率暴跌。然而，这并不能完全说明o1模型没有推理能力或记忆了题目，因为如果人类面对这种干扰性句子，也可能会受到影响。例如，题目告诉你“有三个苹果”，后面括号又说“其中有几个烂掉了”，人类可能会疑惑烂掉的苹果是否与解题有关，从而想太多导致答错。因此，对于这种“修改题目后o1表现变差”的说法，可能需要更严谨的测试，例如让人类也来做这些题目，如果人类不受影响而模型受影响，才能更好地说明模型可能记忆了答案。

类似的发现还有很多。另一篇论文尝试让几个模型解决GSM8K问题，他们设计了两种题目：灰色代表原始GSM8K题目，蓝色代表将句子顺序对调但不影响题意的问题。他们发现，仅仅是改变句子的顺序，这些模型的解题正确率竟然都下降了。这表明模型可能学到了一些不该学的东西。

有人可能会问，如何避免模型学到不该学的东西呢？考虑到这些模型都在互联网上学习了海量数据，它们“偷看”过GSM8K中的题目是非常有可能的。一个可能的解决方案是在训练时，将训练数据中与GSM8K测试数据相似的题目移除，以避免模型提前看到测试题目。然而，这种方法并不能完全解决问题。因为GSM8K的题目可能被翻译成蒙古文等其他语言并发布在网上，模型的爬虫可能抓取到这些数据，并用于训练。由于模型具备跨语言学习能力，它仍然可能看到相同的题目，只是以不同语言呈现，而我们很难检测出来。你不可能将GSM8K英文版翻译成所有语言进行检查。因此，我们很难完全确定模型是否“偷看”过这些测试问题，导致测试结果往往不可靠。

### ARC-AGI：一个更具挑战性的智力测试

在进行模型推理能力测试时，有一个目前常被讨论的基准测试集叫做**ARC-AGI**（Abstract Reasoning Corpus - Abstract General Intelligence: 一种旨在测试模型抽象推理能力的基准测试）。在o3模型发布时，OpenAI也特别宣传了其在ARC-AGI上的表现。

ARC-AGI的题目都是图形式的智力测验，例如：给出几对输入-输出图，然后给出一个新的输入图，要求模型推断出正确的输出图。一个相对简单的例子是：如果规则是找出原图中出现频率最高的形状并输出，那么如果“加号”出现最频繁，正确答案就是输出一个加号。ARC-AGI的优势在于，模型在回答时不能依赖其在网上学到的现有知识，而是需要凭借真正的推理能力来解决问题。

在o3发布的演示视频中，他们展示了一个ARC-AGI的示范题目：给出三对输入-输出图，然后要求模型推断第四个输入图的输出。这道题的规则是：外部彩色框的厚度等于内部彩色点的数量。因此，如果内部有1个点，外部就框1圈；有2个点，框2圈；有5个点，框5圈。这其实也是ARC-AGI中相对简单的题目，其官网上有许多范例题目，其中不乏人类都需要花费大量时间思考甚至无法解决的难题。

那么，语言模型是如何解决这类图形问题的呢？难道也要考验语言模型的看图能力吗？并非如此。ARC-AGI在提供给语言模型时，题目是以文字形式呈现的。它使用文字来描述图案内，并用数字代表不同的颜色（例如0代表无色，12345代表不同颜色）。模型接收到的输入是“这是输入，这是输出；这是输入，这是输出；这是输入，接下来你要输出什么？”这样的文字描述。

ARC-AGI自2019年问世以来，一直被认为是一个相当困难的问题，且至今未取得重大突破。顺带一提，ARC-AGI的作者也是**Keras**（一个高级神经网络API，用于快速实验）的作者。一张来自ARC-AGI官网的图表显示，纵轴代表正确率，横轴代表基准测试集发布的时间。大多数基准测试集在发布两三年内就会被模型“攻破”，达到接近100%的正确率。然而，ARC-AGI在发布五年内，正确率的进步非常小。图表中的一个峰值是由于举办了**ARC Prize**（ARC-AGI的比赛），吸引了更多人参与，从而提升了正确率。

然而，o3模型目前在ARC-AGI上似乎取得了不错的成绩。图表纵轴是ARC-AGI上的正确率，横轴代表模型回答一个问题所需的成本（以美元计）。图表比较了o系列模型（**o1-mini**、**o1-preview**、**o1 (Low)**、**o1 (Medium)**、**o1 (High)**，这些指其推理过程的长度）和人类的表现。其中一个点代表平均人类表现，另一个点代表**STEM**（Science, Technology, Engineering, Mathematics: 科学、技术、工程、数学领域）毕业生的表现。结果显示，o3最强的模型介于一般人类和STEM领域学生的能力之间，甚至比一般人类更强。然而，它付出了巨大的代价：每回答一道题需要耗费相当于1000美元的算力。

尽管ARC-AGI的初衷是避免模型记忆网上已有的内容，并相信其问题在网上找不到一模一样的，且有一个未公开的测试集用于最终评比，但要“攻破”这个比赛也并非完全不可能。比赛会发布一些范例问题，因此完全可以根据这些范例问题，自己创造出更多题目。例如，有人可能创造了几千万道题目，然后模型什么都不做，只专注于刷ARC-AGI，也可能取得好成绩。因此，即使是像ARC-AGI这种被认为是智力测验的题目，虽然可以防止模型记忆答案，但要“hack”这种比赛也并非完全不可能。

### 人类偏好与评估陷阱：Chatbot Arena的启示

那么，如何才能公平地衡量模型呢？有人认为，只要有固定的出题方向，就有可能被“猜题”猜中。如果一个机构负责出题，它可能会有固定的出题倾向，一旦这种倾向被掌握，基准测试就很容易被“玩坏”。那么，能否让全世界的人都来测试，从而避免固定的出题倾向呢？

有一个平台叫做**Chatbot Arena**（一个用于评估大型语言模型的众包平台）。在Chatbot Arena上，每次登录时，系统会随机分配两个模型（模型A和模型B）。用户向这两个模型提出相同的问题，例如“如何评估模型的推理能力”，然后根据两个模型的答案，决定哪一个模型更好。根据这些比赛结果，每个模型都会获得一个分数，并形成排行榜。

有人会认为，Chatbot Arena这种评估方式应该很难被“hack”吧？因为全世界的人都可以提问，每个人都是主考官，你根本不知道人们会问什么样的问题，也无法针对特定题型训练模型。但事实真的如此吗？

传说Chatbot Arena也是有办法被“hack”的，因为人类有其固有的偏好。例如，一个传说是，如果你的模型输出较多的表情符号（emoji），就能在Chatbot Arena上击败其他模型。或者，如果你的模型更善于生成带有粗体字或项目符号（bullet point）的回答，就更容易获得青睐。再或者，如果你的模型喜欢长篇大论，也更容易受到欢迎。

因此，有人认为Chatbot Arena的评估结果并不那么准确，因为大多数人在评估模型时，并不会仔细审视其内容，而通常是看其输出风格。输出风格反而对结果影响更大。当前的模型都非常强大，一般人的知识储备也有限，很难判断模型是否在“胡说八道”。这就像你要评价一个比你聪明的人一样，你根本无法判断他说的对不对，只能听他声音好不好听。所以，Chatbot Arena也并非完美。

Chatbot Arena的官方也曾尝试解决这个问题。他们采用**Elo score**（Elo等级分系统: 一种衡量棋类或其他两人对弈运动选手相对技能水平的方法）作为评比机制，这也是许多竞赛会使用的评分方式。其算法是：假设有K个模型（M1到MK），每个模型都有一个代表其“战力”的分数（用β表示）。当随机匹配到两个模型（第j个模型和第i个模型）进行评比时，第i个模型战胜第j个模型的概率取决于两者战力的差距。这个概率可以通过一个**sigmoid函数**（Sigmoid Function: 一种常用的S形曲线函数，在机器学习中常用于将输入值映射到0到1之间）计算得出。如果i的战力远大于j，胜率将趋近于1；反之，则趋近于0。

在实际比赛中，我们无法直接得知这些战力值，但可以根据比赛结果统计出模型之间的胜率，然后通过胜率反推出每个模型的战力值β。然而，Chatbot Arena的官方认为，有太多与模型本身实力无关的因素会干扰评比结果。因此，在计算正确率时，除了βi和βj，还需要加上一项β0，代表模型实力以外的因素。在棋类比赛中，有时也会加上β0来考虑先手优势。

在Chatbot Arena中，许多因素都可能影响人类对模型的评价，而这些因素与模型实力无关。例如，模型回答越长，人类可能越喜欢。因此，β0中应该包含长度差异乘以一个常数γ1。或者，表情符号的数量可能影响模型的受喜爱程度和胜率，表情符号越多，人类可能越喜欢。因此，应该计算两个模型在本次评比中表情符号数量的差异，然后乘以γ2。总之，你可以找出所有你认为与模型实力无关但会影响评比结果的因素，并为每个因素乘以一个γ值，代表其影响程度，然后将它们全部加起来形成β0。

然后，你可以根据实际统计出的胜率，反推出β值和γ值。如果计算出的γ是正值，则表示长度差异会产生影响，即i的答案比j的答案更长时，i的胜率会更高。如果γ趋近于零，则表示该变量没有显著影响。如果小于零，则表示负相关，即长度越短越好。

是否考虑这些与实力无关的因素（例如书写风格）对评比结果具有决定性影响。Chatbot Arena官方博客的一篇文章指出，是否考虑这些风格相关因素会影响模型的排名。左图是根据对战成绩直接计算出的模型战力，不考虑任何与实力无关的因素；右图是考虑了风格影响后的评比结果。他们列举了一些可能产生影响的变量，例如模型回答的长度、表情符号的使用数量、回答的积极程度、是否使用标题等各种风格因素。他们发现，在考虑了这些风格因素并重新排名后，模型的排名发生了大幅变化。

有些模型原本排名靠前，但在考虑风格后排名下降；有些模型原本排名靠后，但考虑风格后排名上升。其中上升幅度最大的就是**Claude**（Anthropic公司开发的一系列大型语言模型）系列模型。Claude系列模型一直被认为是相当强大的模型，但在Chatbot Arena上其评分却不高，一个可能的原因是Claude模型的表达方式比较“无聊”，很少输出表情符号。但这并不妨碍它是一个非常聪明的模型，例如它擅长编程和网页制作。因此，Claude虽然聪明，却吃亏在“不善言辞”，不太会说让人开心的话。如果只看人类觉得哪个模型“说话好听”，Claude并不占优势。但如果剔除书写风格造成的影响，Claude的名次就会大幅提前。

这个故事告诉我们，即使是Chatbot Arena也有可能被“hack”。有人可能会训练一个专门针对Chatbot Arena的模型，使其特别喜欢输出表情符号，喜欢说有趣的话，喜欢说讨人喜欢的话，从而取悦人类。这样，即使模型本身不特别聪明，也可能在Chatbot Arena中占据优势。

### 古德哈特定律：当指标成为目标

到底什么样的指标才是好的评估指标呢？也许结论是：没有好的评估指标。这就是**古德哈特定律**（Goodhart's Law: 一旦一项指标被设定为目标，它就不再是一个好的指标）。

在谈到古德哈特定律时，常会提到一个故事，即**眼镜蛇现象**（Cobra Effect: 指为了解决某个问题而采取的措施，反而使问题变得更糟的非预期后果）。这个故事发生在英国殖民印度时期，当时蛇患猖獗。英国政府为了解决问题，悬赏捕蛇，每捕到一条蛇奖励一英镑。结果，印度人为了赚钱，开始在家中大量养蛇，反而导致蛇的数量更多了。这就是古德哈特定律的典型体现。

如果为本堂课下一个结语，那就是：你这么在意这个评分系统干什么呢？它会把模型的努力给异化掉的。