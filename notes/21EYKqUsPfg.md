---
area: "work-career"
category: ai-ml
companies_orgs:
- Google DeepMind
date: '2025-09-26'
draft: true
guest: ''
insight: ''
layout: post.njk
people:
- Richard Sutton
- Alan Turing
- John McCarthy
- Joseph Henrich
products_models:
- AlphaGo
- AlphaZero
- GPT-4
project: []
series: ''
source: https://www.youtube.com/watch?v=21EYKqUsPfg
speaker: Dwarkesh Patel
status: evergreen
summary: 强化学习（RL）的奠基人之一**Richard Sutton**对当前大型语言模型（LLM）主导的AI范式提出了深刻质疑。他认为LLM缺乏**世界模型**和明确目标，仅通过模仿人类行为学习，而非从真实经验中获取知识。Sutton强调RL作为基础AI的重要性，倡导基于经验的**持续学习**和通用原则。他探讨了“苦涩的教训”在LLM上的应用，并展望了**数字智能**时代AI的演进、知识共享的挑战以及**AI接替**人类的必然性，呼吁以积极心态面对这一宇宙级转变。
tags:
- agi
- learning
- reinforcement-learning
- world-model
title: Richard Sutton：强化学习之父为何认为大型语言模型是死胡同
---
### 强化学习与大型语言模型的本质差异

今天，我与**Richard Sutton**进行了一次对话，他是**强化学习**（RL: 一种机器学习范式，智能体通过与环境互动，根据奖励信号学习最优行为策略）的奠基人之一，也是**TD学习**（Temporal Difference Learning: 强化学习中的一种核心算法，用于从经验中学习价值函数）和**策略梯度方法**（Policy Gradient Methods: 强化学习中直接优化策略以最大化预期回报的算法）等诸多核心技术的发明者。为此，他荣获了今年的**图灵奖**（Turing Award: 计算机科学领域的最高荣誉，常被称为“计算机科学界的诺贝尔奖”），这被誉为计算机科学界的诺贝尔奖。**Richard**，恭喜您！谢谢**Dwarkesh**。感谢您来参加播客。这是我的荣幸。

第一个问题是，我的听众和我都很熟悉**大型语言模型**（LLM: 基于海量文本数据训练，能够生成、理解和处理人类语言的人工智能模型）的AI思维方式。从强化学习的角度来看，我们在思考AI时，概念上错过了什么？这确实是一个截然不同的观点，很容易导致两者分离，失去相互交流的能力。大型语言模型已经变得如此重要，生成式AI也普遍如此。我们的领域受潮流和时尚的影响，以至于我们忽略了基本的东西。

### LLM缺乏世界模型与目标

我认为强化学习是基础AI。什么是智能？问题在于理解你的世界。强化学习就是关于理解你的世界，而大型语言模型则是关于模仿人类，做人类让你做的事情。它们不是关于弄清楚该做什么。你可能会认为，为了模仿互联网文本语料库中的数万亿个token，你必须构建一个**世界模型**（World Model: 智能体对环境动态的内部表征，用于预测行为后果）。事实上，这些模型似乎确实拥有非常强大的世界模型。它们是迄今为止我们在AI领域制造的最好的世界模型，对吗？

你认为缺少了什么？我不同意你刚才说的大部分内容。模仿人们所说的话，根本不是在构建一个世界模型。你模仿的是那些拥有世界模型的东西：人类。我不想以对抗的方式来探讨这个问题，但我会质疑它们拥有世界模型的观点。一个世界模型将使你能够预测将会发生什么。它们有能力预测一个人会说什么。它们没有能力预测将会发生什么。

### 经验学习与模仿学习的本质区别

我们想要的是，引用**Alan Turing**的话，一台能够从经验中学习的机器，这里的经验是生活中实际发生的事情。你做事情，你看到发生了什么，这就是你学习的来源。大型语言模型从别的东西中学习。它们从“这是一个情况，这是一个人所做的”中学习。隐含的建议是，你应该做那个人所做的事情。我想，也许关键在于，我很好奇你是否不同意这一点，有些人会说**模仿学习**（Imitation Learning: 智能体通过观察专家行为来学习策略的方法）为我们提供了一个好的先验，或者说为这些模型提供了一个好的先验，即解决问题的合理方法。

当我们迈向你所说的经验时代时，这个先验将成为我们通过经验训练这些模型的基础，因为这给了它们有时能得到正确答案的机会。然后在此基础上，你可以通过经验训练它们。你同意这种观点吗？不，我同意这是大型语言模型的观点。我不认为这是一个好的观点。要成为某物的先验，必须有一个真实的东西。先验知识应该是实际知识的基础。什么是实际知识？在大型语言模型的框架中，没有实际知识的定义。

### 目标缺失：LLM的根本局限

什么使一个行动成为一个好的行动？你认识到需要**持续学习**（Continual Learning: 智能体在不断接收新信息的同时，持续学习并适应新任务，同时不遗忘旧知识的能力）。如果你需要持续学习，持续意味着在与世界的正常互动中学习。在正常互动中必须有某种方式来判断什么是正确的。在大型语言模型的设置中，有没有办法判断什么是正确的说法？你会说一些话，但你不会得到关于什么是正确说法的反馈，因为没有定义什么是正确的说法。没有目标。如果没有目标，那么就有一种说法，另一种说法。没有正确的说法。没有**基本事实**（Ground Truth: 真实世界中客观存在的事实或结果，可用于评估模型的预测）。如果你没有基本事实，你就不能拥有先验知识，因为先验知识应该是关于真相的提示或初始信念。根本没有真相。没有正确的说法。

在强化学习中，有正确的说法，有正确的事情要做，因为正确的事情是能让你获得奖励的事情。我们有一个关于什么是正确事情的定义，所以我们可以拥有先验知识，或者人们提供的关于什么是正确事情的知识。然后我们可以检查它，因为我们有一个关于什么是实际正确事情的定义。一个更简单的情况是，当你试图建立一个世界模型时。当你预测会发生什么时，你预测，然后你看到发生了什么。有基本事实。大型语言模型中没有基本事实，因为你没有预测接下来会发生什么。如果你在对话中说了一些话，大型语言模型无法预测对方会说什么作为回应，或者回应会是什么。

### LLM的预测与“惊喜”能力

我认为它们有。你可以直接问它们：“你预计用户可能会说什么作为回应？”它们会有一个预测。不，它们会正确回答这个问题。但它们在实质意义上没有预测能力，即它们不会对发生的事情感到惊讶。如果发生的事情不是你可能认为它们预测的，它们不会改变，因为意外的事情发生了。要学习这一点，它们必须进行调整。

我认为这种能力确实存在于上下文中。观察一个模型进行**思维链**（Chain of Thought: 一种提示技术，鼓励大型语言模型逐步推理以解决复杂问题）很有趣。假设它正在尝试解决一个数学问题。它会说：“好的，我将首先使用这种方法来解决这个问题。”它会写出来，然后说：“哦，等等，我刚刚意识到这不是解决问题的正确概念性方法。我将重新开始另一种方法。”这种灵活性确实存在于上下文中，对吗？你有什么别的想法，还是你只是认为你需要将这种能力扩展到更长的时间范围？我只是说它们在任何有意义的层面上都没有预测接下来会发生什么的能力。它们不会对接下来发生的事情感到惊讶。如果发生了一些事情，它们不会根据发生的事情做出任何改变。

那不正是**下一个token预测**（Next Token Prediction: 大型语言模型的核心任务，即根据前面的序列预测下一个最可能的词元）吗？预测接下来会发生什么，然后根据惊喜进行更新？下一个token是它们应该说什么，行动应该是什么。它不是世界会给它们什么作为回应。让我们回到它们缺乏目标的问题。对我来说，拥有目标是智能的本质。如果一个系统能够实现目标，那么它就是智能的。我喜欢**John McCarthy**的定义，即智能是实现目标能力的计算部分。你必须有目标，否则你只是一个行为系统。你不是什么特别的东西，你也不是智能的。你同意大型语言模型没有目标吗？不，它们有一个目标。目标是什么？下一个token预测。那不是一个目标。它不会改变世界。token向你袭来，如果你预测它们，你不会影响它们。哦，是的。这不是关于外部世界的目标。这不是一个目标。这不是一个实质性的目标。你不能看着一个系统说它有目标，如果它只是坐在那里预测，并对自己准确预测感到满意。

### RL叠加LLM：一个富有成效的方向吗？

我想要理解的更大问题是，你为什么不认为在大型语言模型之上进行强化学习是一个富有成效的方向。我们似乎能够赋予这些模型解决困难数学问题的目标。它们在解决**国际数学奥林匹克**（International Math Olympiad, IMO: 一项面向高中生的国际数学竞赛）类型问题的能力上，在许多方面都达到了人类水平的顶峰。它们在IMO中获得了金牌。所以，似乎在IMO中获得金牌的模型确实有解决数学问题的目标。为什么我们不能将其扩展到不同的领域？数学问题是不同的。建立物理世界的模型并执行数学假设或操作的后果，这些是非常不同的事情。经验世界必须被学习。你必须学习后果。而数学更具计算性，它更像是标准的规划。在那里，它们可以有一个找到证明的目标，并且它们在某种程度上被赋予了找到证明的目标。

### 《苦涩的教训》与LLM的争议

这很有趣，因为你曾在2019年写过一篇题为**《苦涩的教训》**（The Bitter Lesson: **Richard Sutton**于2019年发表的著名文章，指出AI领域成功的关键在于利用计算规模和通用学习方法，而非人类知识的注入）的文章，这可能是AI史上最具影响力的文章。但人们用它来为扩展大型语言模型辩护，因为在他们看来，这是我们发现的唯一可扩展的方式，可以将巨量的计算投入到学习世界中。

有趣的是，你的观点是大型语言模型并没有被“苦涩的教训”所“毒害”。大型语言模型是否是“苦涩的教训”的一个案例，这是一个有趣的问题。它们显然是一种利用大规模计算的方式，这种方式将随着计算能力的提升而扩展到互联网的极限。但它们也是一种注入大量人类知识的方式。这是一个有趣的问题，一个社会学或行业问题。它们会达到数据的极限，并被那些可以仅仅从经验而非人类那里获得更多数据的事物所取代吗？在某些方面，这是一个经典的“苦涩的教训”案例。我们向大型语言模型中注入的人类知识越多，它们就能做得越好。所以感觉很好。然而，我预计会有能够从经验中学习的系统。这些系统可以表现得更好，并且更具可扩展性。在这种情况下，这将是“苦涩的教训”的另一个例子，即那些使用人类知识的东西最终被那些仅仅通过经验和计算进行训练的东西所取代。

### LLM作为经验学习的起点？

我猜这对我来说似乎不是关键。我认为那些人也会同意，未来绝大部分的计算将来自经验学习。他们只是认为，这种未来经验学习或在职学习的支架或基础，即你将开始投入计算的东西，将是大型语言模型。我仍然不明白为什么这完全是一个错误的起点。为什么我们需要一个全新的架构来开始进行经验性的、持续的学习？为什么我们不能从大型语言模型开始做呢？

在“苦涩的教训”的每个案例中，你都可以从人类知识开始，然后做可扩展的事情。情况总是如此。从来没有理由说这一定是坏事。但事实上，在实践中，它总是被证明是坏事。人们被锁定在人类知识的方法中，他们在心理上……现在我正在猜测原因，但这就是一直发生的事情。他们被真正可扩展的方法“吃掉午餐”。

### 可扩展方法：从经验中学习

给我一个可扩展方法的概念。可扩展的方法是你从经验中学习。你尝试事物，你看到什么有效。没有人需要告诉你。首先，你有一个目标。没有目标，就没有对错、好坏之分。大型语言模型正试图在没有目标或好坏之分的情况下生存。这正是从错误的地方开始。

也许将此与人类进行比较会很有趣。在从模仿中学习与从经验中学习的案例中，以及在目标问题上，我认为有一些有趣的类比。孩子们最初会从模仿中学习。你不这么认为吗？不，当然不是。真的吗？我认为孩子们只是观察人们。他们试图说相同的词……这些孩子多大了？前六个月呢？我认为他们正在模仿事物。他们试图让自己的嘴发出他们看到母亲嘴发出的声音。然后他们会说相同的词，而不理解它们的意思。随着他们长大，他们模仿的复杂性会增加。你可能正在模仿你团队中的人用来捕猎鹿的技能。然后你进入从经验中学习的强化学习模式。但我认为人类身上发生了很多模仿学习。

### 动物学习与监督学习的局限

令人惊讶的是，你会有如此不同的观点。当我看到孩子时，我看到他们只是尝试事物，挥舞着双手，转动着眼睛。他们如何转动眼睛，甚至他们发出的声音，都没有模仿。他们可能想发出相同的声音，但行动，婴儿实际做的事情，没有目标。没有这方面的例子。我同意。这并不能解释婴儿所做的一切，但我认为它指导着一个学习过程。即使是一个大型语言模型，在训练早期尝试预测下一个token时，它也会进行猜测。它会与它实际看到的不同。在某种意义上，它是非常短期的强化学习，它正在进行这种猜测，“我认为这个token会是这个。”这是另一回事，类似于孩子尝试说一个词。它说错了。

大型语言模型是从训练数据中学习的。它不是从经验中学习。它学习的东西在它的正常生活中永远不会出现。在正常生活中，从来没有训练数据告诉你应该做这个动作。我认为这更多是一个语义上的区别。你把学校叫什么？那不是训练数据吗？学校是后来的事情。好吧，我不应该说“从不”。我不知道，我想我甚至会这样说学校。但正规教育是例外。但学习有阶段，早期有生物学上的编程，你不是那么有用。然后你存在的理由是理解世界并学习如何与它互动。这似乎是一个训练阶段。

我同意，然后有一个更渐进的……训练到部署没有一个明确的界限，但似乎有一个初始训练阶段，对吗？没有什么地方是你被训练应该做什么的。什么都没有。你看到事情发生。你没有被告知该做什么。别刁难我。我的意思是这很明显。你确实被教导该做什么。这就是“训练”这个词的来源，来自人类。我不认为学习真的是关于训练。我认为学习是关于学习，它是一个积极的过程。孩子尝试事物，然后看到发生了什么。当我们想到婴儿成长时，我们不会想到训练。

这些事情实际上已经得到了很好的理解。如果你看看心理学家是如何思考学习的，没有什么像模仿。也许在某些极端情况下，人类可能会这样做或看起来这样做，但没有基本的动物学习过程叫做模仿。有基本的动物学习过程用于预测和试错控制。有时最难看到的是显而易见的事情，这真的很有趣。很明显——如果你看看动物是如何学习的，以及你看看心理学和我们对它们的理论——**监督学习**（Supervised Learning: 一种机器学习任务，通过标记的训练数据来学习从输入到输出的映射）不是动物学习方式的一部分。我们没有期望行为的例子。我们有的是发生的事情的例子，一件事接着另一件事。我们有“我们做了某事，然后产生了后果”的例子。但没有监督学习的例子。监督学习不是自然界中发生的事情。即使学校是这种情况，我们也应该忘记它，因为那是人类身上发生的一些特殊事情。它不会广泛地发生在自然界中。松鼠不去学校。松鼠可以学习关于世界的一切。我敢说，监督学习不会发生在动物身上，这是绝对显而易见的。

### 人类智能与动物智能的共性

我采访了心理学家和人类学家**Joseph Henrich**，他研究了文化进化，主要是人类的独特之处以及人类如何获取知识。你为什么要区分人类？人类是动物。我们共同之处更有趣。我们应该少关注区分我们的地方。我们正在尝试复制智能。如果你想了解是什么让人类能够登上月球或制造半导体，我认为我们想要了解的是是什么让这一切发生。没有动物能登上月球或制造半导体。我们想了解是什么让人类特别。

我喜欢你认为那是显而易见的方式，因为我认为相反才是显而易见的。我们必须理解我们如何是动物。如果我们理解了一只松鼠，我认为我们几乎就能完全理解人类智能了。语言部分只是表面上的一层薄薄的装饰。这很棒。我们正在发现我们思维方式的巨大差异。我们不是在争论。我们只是在分享我们不同的思维方式。我认为争论是有用的。我确实想完成这个想法。

**Joseph Henrich**有一个有趣的理论，关于人类为了成功而必须掌握的许多技能。我们谈论的不是过去一千年或一万年，而是几十万年。世界非常复杂。不可能通过推理来学会如何在北极捕猎海豹。这是一个漫长而多步骤的过程，包括如何制作诱饵，如何找到海豹，然后如何以确保不会中毒的方式处理食物。不可能通过推理来完成所有这些。随着时间的推移，有一个更大的过程，无论你想用什么类比——也许是强化学习，也许是其他什么——整个文化已经弄清楚了如何寻找、杀死和食用海豹。在他看来，当这种知识通过世代相传时，你必须模仿你的长辈才能学习这项技能。你无法通过思考来学会如何捕猎、杀死和处理海豹。你必须观察其他人，也许进行调整和修改，这就是知识积累的方式。文化进步的最初一步必须是模仿。但也许你以不同的方式思考？不，我以相同的方式思考。

### 持续学习：哺乳动物的共同能力

尽管如此，这仍然是建立在基本的试错学习、预测学习之上的一个小东西。也许这就是我们与许多动物的区别。但我们首先是动物。我们在拥有语言和所有其他事物之前就是动物。我确实认为你提出了一个非常有趣的观点，即**持续学习**是大多数哺乳动物都具备的能力。我想所有哺乳动物都具备。

我们拥有所有哺乳动物都具备的能力，但我们的AI系统却没有，这很有趣。而理解数学和解决困难数学问题的能力——这取决于你如何定义数学——是我们的AI具备，但几乎没有动物具备的能力。什么最终变得困难，什么最终变得容易，这很有趣。**摩拉维克悖论**（Moravec's Paradox: 指出对人类而言容易的任务（如感知、运动）对AI而言很难，而对人类而言困难的任务（如数学、逻辑）对AI而言相对容易的现象）。没错，没错。

### 经验范式：智能的基础

你所设想的这种替代范式……经验范式。让我们来详细阐述一下。它说经验、行动、感知——嗯，感知、行动、奖励——这在你的生命中不断发生。它说这是智能的基础和焦点。智能是关于获取这种流，并改变行动以增加流中的奖励。那么学习就来自这种流，学习也关于这种流。后半部分尤其说明问题。你所学到的知识，是关于这种流的。你的知识是关于如果你做某个行动，会发生什么。或者它是关于哪些事件会跟随其他事件。它是关于这种流的。知识的内容是关于这种流的陈述。因为它是一个关于这种流的陈述，你可以通过将其与这种流进行比较来测试它，并且你可以持续地学习它。

当你设想这种未来的持续学习智能体时……它们不是“未来”的。当然，它们一直都存在。这就是强化学习范式，从经验中学习。是的，我想我指的是一个通用的、人类水平的、通用的持续学习智能体。奖励函数是什么？它仅仅是预测世界吗？它是否对世界产生特定的影响？通用的奖励函数会是什么？奖励函数是任意的。如果你在下棋，目标就是赢得棋局。如果你是一只松鼠，也许奖励与获取坚果有关。

### 奖励函数与内在动机

一般来说，对于动物来说，你会说奖励是避免痛苦和获得快乐。我认为还应该有一个与你对环境理解的增加有关的组成部分。那将是一种内在动机。我明白了。有了这种AI，很多人会希望它能做很多不同种类的事情。它正在执行人们想要的任务，但同时，它也通过执行任务来学习世界。

假设我们摆脱了这种有训练期和部署期的范式。我们是否也摆脱了这种有模型，然后有模型的实例或副本在做某些事情的范式？你如何看待我们希望这个东西做不同事情的事实？我们希望聚合它从做这些不同事情中获得的知识。我不喜欢你刚才使用“模型”这个词。我认为一个更好的词是“网络”，因为我认为你指的是网络。也许有很多网络。无论如何，事情都会被学习。你会有副本和许多实例。当然，你会希望在实例之间共享知识。有很多可能性可以做到这一点。

### 数字智能的知识共享与挑战

今天，一个孩子长大并了解世界，然后每个新孩子都必须重复这个过程。而对于AI，对于数字智能，你可以希望只做一次，然后将其复制到下一个作为起点。这将节省巨大的成本。我认为这比试图向人类学习更重要。我同意你所说的这种事情是必要的，无论你是否从大型语言模型开始。如果你想要人类或动物水平的智能，你将需要这种能力。

假设一个人正在创业。这是一个奖励周期长达10年的事情。每10年你可能会有一次退出，获得10亿美元。但人类有能力设定中间辅助奖励，或者有某种方式……即使他们有极其稀疏的奖励，他们仍然可以采取中间步骤，理解他们正在做的下一件事如何导向我们拥有的这个更宏伟的目标。你如何想象这样的过程在AI中会如何发展？这是我们非常了解的事情。它的基础是**时序差分学习**（Temporal Difference Learning: 强化学习中的一种核心算法，用于从经验中学习价值函数），其中相同的事情以不那么宏大的规模发生。当你学习下棋时，你有一个赢得比赛的长期目标。然而，你希望能够从短期的事情中学习，比如吃掉对手的棋子。你通过拥有一个预测长期结果的**价值函数**（Value Function: 在强化学习中，衡量在特定状态下采取某个行动或遵循某个策略能获得多少预期累积奖励的函数）来实现这一点。然后，如果你吃掉对方的棋子，你对长期结果的预测就会改变。它会上升，你认为你会赢。然后你信念的这种增加会立即强化导致吃掉棋子的那个动作。

### 知识带宽与大世界假设

我们有一个长达10年的创业并赚大钱的目标。当我们取得进展时，我们会说：“哦，我更有可能实现长期目标”，这会奖励沿途的每一步。你还希望你正在学习的信息具有某种能力。人类与这些大型语言模型的一个显著区别是，如果你正在适应一份工作，你会获取大量的上下文和信息。这就是让你在工作中变得有用的原因。你学习一切，从客户的偏好到公司如何运作，一切。

从像**TD学习**这样的程序中获取的信息带宽是否足够高，以拥有人类在部署时需要获取的这种巨大的上下文和隐性知识管道？我不确定，但我认为这其中的关键是，**大世界假设**（Big World Hypothesis: 指现实世界极其复杂、信息量巨大，无法被完全预先建模或学习，智能体必须在互动中持续学习和适应）似乎非常相关。人类在工作中变得有用的原因是因为他们正在接触他们世界中特定的部分。它不可能被预料到，也不可能全部提前输入。世界如此之大，你无法做到。

大型语言模型的梦想，在我看来，是你能够教导智能体一切。它将知道一切，并且在它的生命中，不需要在线学习任何东西。你的例子都是，“嗯，实际上你必须”，因为你可以教它，但它所过的特定生活以及它所合作的特定人群，以及他们喜欢什么，而不是普通人喜欢什么，都有各种小怪癖。这只是说世界真的很大，你将不得不在路上学习它。

### 智能体的四大核心组成部分

在我看来，你需要两样东西。一是某种方法将这种长期目标奖励转化为较小的辅助预测奖励，即未来奖励，或者导致最终奖励的未来奖励。但最初，在我看来，我需要保留我在世界中工作时获得的所有这些上下文。我正在了解我的客户、我的公司以及所有这些信息。

我会说你只是在进行常规学习。也许你使用“上下文”是因为在大型语言模型中，所有这些信息都必须进入**上下文窗口**（Context Window: 大型语言模型在处理输入时能够同时考虑的文本长度限制）。但在持续学习设置中，它只是进入权重。也许上下文是一个错误的词，因为我指的是一个更普遍的东西。你学习一个特定于你所处环境的**策略**（Policy: 在强化学习中，智能体在给定状态下选择行动的规则或函数）。我试图提出的问题是，你需要某种方式来获取……一个人在世界上每秒获取多少比特的信息？如果你只是通过**Slack**（Slack: 一款团队协作和即时通讯软件）与客户互动等等。

也许你试图提出的问题是，奖励似乎太小，无法完成我们需要做的所有学习。但我们有感知，我们有所有其他可以学习的信息。我们不仅仅从奖励中学习。我们从所有数据中学习。帮助你捕捉这些信息的学习过程是什么？现在我想谈谈智能体的基本通用模型，它有四个部分。我们需要一个策略。策略说：“在我所处的情况下，我应该做什么？”我们需要一个价值函数。价值函数是通过TD学习学习到的东西，价值函数产生一个数字。这个数字表示进展如何。然后你观察它是否上下波动，并用它来调整你的策略。所以你有这两样东西。

然后还有**感知组件**（Perception Component: 智能体从环境中获取信息并进行处理的模块），它是你**状态表征**（State Representation: 智能体对当前环境状态的感知和内部编码）的构建，你对当前位置的感知。第四个是我们真正要谈的，无论如何，最透明的。第四个是世界的**转换模型**（Transition Model: 智能体对环境动态的预测模型，描述了在给定状态下采取某个行动后，环境如何变化以及可能进入的下一个状态）。你相信如果你这样做，会发生什么？你所做事情的后果会是什么？你的世界物理学。但它不仅仅是物理学，它还是抽象模型，比如你从加利福尼亚到埃德蒙顿参加这个播客的旅行模型。那是一个模型，那是一个转换模型。那将是被学习的。它不是从奖励中学习的。它是从“你做了事情，你看到了发生了什么，你建立了那个世界模型”中学习的。那将从你接收到的所有感知中非常丰富地学习，而不仅仅是从奖励中。它也必须包括奖励，但那只是整个模型的一小部分，是整个模型中一小部分关键的部分。

### 泛化与迁移学习的挑战

我的朋友**Toby Ord**指出，如果你看看**Google DeepMind**部署的**MuZero**模型，这些模型最初本身并不是通用智能，而是一个用于训练专门智能来玩特定游戏的通用框架。也就是说，你不能使用那个框架来训练一个策略来同时玩国际象棋、围棋和其他一些游戏。你必须以专门的方式训练每一个。他想知道这是否意味着，一般来说，由于这种信息限制，强化学习一次只能学习一件事？信息密度是不是不够高？或者这仅仅是**MuZero**的特定实现方式造成的。

如果是**AlphaZero**的特定实现方式造成的，那么这种方法需要改变什么才能成为一个通用学习智能体？这个想法是完全通用的。我一直都用一个人作为AI智能体的典型例子。从某种意义上说，人类只有一个他们生活的世界。那个世界可能涉及国际象棋，也可能涉及**Atari**游戏，但那些不是不同的任务或不同的世界。那些是他们遇到的不同状态。所以这个通用想法根本不受限制。也许解释一下那个架构或方法中缺少了什么，而这种持续学习的**人工通用智能**（AGI: Artificial General Intelligence: 指能够像人类一样理解、学习和应用智能来解决任何问题的AI系统）会具备的东西，会很有用。他们只是这样设置的。他们的目标并不是让一个智能体跨越这些游戏。如果我们想谈论**迁移**（Transfer: 在机器学习中，将从一个任务中学到的知识应用于另一个相关任务的能力），我们不应该谈论跨游戏或跨任务的迁移，而应该谈论状态之间的迁移。

我很好奇，从历史上看，我们是否看到过使用强化学习技术达到构建这种……好的，好的。我们没有在任何地方看到迁移。良好表现的关键是你能从一个状态很好地**泛化**（Generalization: 机器学习模型在未见过的数据上表现良好的能力）到另一个状态。我们没有任何在这方面表现良好的方法。我们有的是人们尝试不同的事物，然后他们确定了一种能够很好地迁移或泛化的表示。但我们很少有自动化的技术来促进迁移，而且现代深度学习中没有使用这些技术。

让我转述一下，以确保我理解正确。听起来你是在说，当这些模型中出现泛化时，那是某种雕琢的结果……是人类做的。是研究人员做的。因为没有其他解释。**梯度下降**（Gradient Descent: 一种优化算法，用于最小化函数，通过迭代地沿着函数梯度下降的方向调整参数）不会让你很好地泛化。它会让你解决问题。如果你得到新数据，它不会让你以好的方式泛化。泛化意味着在一个事物上进行训练会影响你在其他事物上的表现。我们知道深度学习在这方面真的很差。例如，我们知道如果你训练一些新事物，它通常会**灾难性干扰**（Catastrophic Interference: 神经网络在学习新任务时，旧任务的知识被快速遗忘的现象）你所知道的所有旧事物。这正是糟糕的泛化。泛化，正如我所说，是训练在一个状态上对其他状态的某种影响。你泛化的事实不一定是好是坏。你可以泛化得很差，也可以泛化得很好。泛化总是会发生，但我们需要能够使泛化变得好的算法，而不是坏的。

### LLM的泛化能力与“苦涩的教训”

我不是想再次引发最初的关键问题，我只是真心好奇，因为我可能使用了不同的术语。思考这些大型语言模型的一种方式是，它们正在增加泛化的范围，从早期甚至无法解决基本数学问题的系统，到现在它们可以解决**国际数学奥林匹克**类型问题中的任何问题。你最初从它们能够泛化加法问题开始。然后它们可以泛化需要使用不同种类数学技术、定理和概念类别的问题，这正是数学奥林匹克所要求的。听起来你并不认为能够解决该类别中的任何问题是泛化的一个例子。如果我误解了，请告诉我。

大型语言模型非常复杂。我们真的不知道它们之前拥有什么信息。我们只能猜测，因为它们被喂食了太多信息。这就是它们不适合做科学研究的一个原因。它太不受控制，太未知了。但如果你提出一个全新的……它们可能做对了很多事情。问题是为什么。嗯，也许它们不需要泛化就能做对，因为做对其中一些问题的唯一方法是形成一个能做对所有问题的方案。如果只有一个答案，你找到了它，那不叫泛化。这只是解决它的唯一方法，所以它们找到了解决它的唯一方法。但泛化是当它可能这样，也可能那样，而它们选择了好的方式。

我的理解是，这在编码智能体方面越来越有效。对于工程师来说，显然如果你试图编写一个库，有很多不同的方法可以实现最终规范。这些模型最初的一个令人沮丧的地方是它们会以一种草率的方式完成。随着时间的推移，它们在提出设计架构和抽象方面越来越好，开发人员觉得更满意。这似乎是你所说的一个例子。它们内部没有任何东西能让它们很好地泛化。梯度下降会使它们找到它们所见过问题的解决方案。如果只有一种方法可以解决它们，它们就会那样做。但如果有很多方法可以解决它，有些泛化得好，有些泛化得差，算法中没有任何东西能让它们泛化得好。但人类当然是进化的，如果它不起作用，他们就会修修补补，直到找到一种方法，也许直到找到一种能很好泛化的方法。

### AI领域的惊喜与发展轨迹

我想把视角拉远，问一下您在AI领域工作的时间比现在几乎所有评论或工作的人都长。我很好奇最大的惊喜是什么。您觉得有多少新东西正在涌现？还是觉得人们只是在玩弄旧想法？从长远来看，您甚至在深度学习流行之前就进入了这个领域。那么您如何看待这个领域随时间的发展轨迹，以及新想法是如何产生的等等？有什么令人惊讶的吗？

我对此思考了一会儿。有几件事。首先，大型语言模型令人惊讶。人工神经网络在语言任务上的效率令人惊讶。那是一个惊喜，没有预料到。语言似乎有所不同。所以这令人印象深刻。在AI领域，长期以来一直存在关于简单基本原理方法（如搜索和学习等通用方法）与人类赋能系统（如符号方法）的争议。在过去，这很有趣，因为搜索和学习等方法被称为“弱方法”，因为它们只使用通用原理，不使用赋予系统人类知识所带来的力量。那些被称为“强方法”。我认为“弱方法”已经完全胜利了。这是AI早期最大的问题，会发生什么。学习和搜索已经赢得了胜利。

从某种意义上说，这对我来说并不意外，因为我一直希望或支持简单的基本原理。即使是大型语言模型，它运作得如此之好，也令人惊讶，但这一切都很好，令人欣慰。**AlphaGo**令人惊讶，它运作得如此之好，尤其是**AlphaZero**。但这一切都非常令人欣慰，因为再次强调，简单的基本原理正在赢得胜利。

### 旧技术的新突破：AlphaGo与TD-Gammon

每当公众观念因新应用的开发而改变时——例如，当**AlphaZero**成为轰动一时的现象时——对于像您这样，实际上发明了许多被使用技术的人来说，您是否觉得取得了新的突破？或者您觉得“哦，这些技术我们从90年代就有了，人们现在只是将它们结合起来并应用”？

整个**AlphaGo**事件有一个前身，那就是**TD-Gammon**。**Gerry Tesauro**使用强化学习、时序差分学习方法来玩西洋双陆棋。它击败了世界上最好的玩家，而且效果非常好。从某种意义上说，**AlphaGo**仅仅是那个过程的规模化。但它是一个相当大的规模化，而且在搜索方式上也有额外的创新。但它是有道理的。从这个意义上说，它并不令人惊讶。

**AlphaGo**实际上没有使用TD学习。它等到看到最终结果。但**AlphaZero**使用了TD学习。**AlphaZero**被应用于所有其他游戏，并且表现极其出色。我一直对**AlphaZero**下国际象棋的方式印象深刻，因为我是一名国际象棋棋手，它只是为了位置优势而牺牲物质。它只是满足于并耐心地长时间牺牲这些物质。这令人惊讶，它运作得如此之好，但也令人欣慰，并且符合我的世界观。

### 逆向思维与经典主义

这让我走到了今天。从某种意义上说，我是一个逆向思维者，或者说一个与领域思维方式不同的人。我个人满足于长期以来，也许几十年，与我的领域不同步，因为我过去偶尔被证明是正确的。我做的另一件事——为了帮助我不要觉得自己与众不同，思维方式很奇怪——是不要看我所处的局部环境或局部领域，而是回顾历史，看看不同领域的人们对心灵的经典思考。我不觉得我与更广阔的传统脱节。我真的认为自己是一个经典主义者，而不是一个逆向思维者。我遵循更广阔的思想家群体对心灵的经典思考。

### 后AGI时代的AI研究展望

如果您能容忍，我有一些离题的问题。我对**《苦涩的教训》**的理解是，它不一定说人类手工研究人员的调整不起作用，而是说它显然比计算的扩展性差得多，而计算正在呈指数级增长。所以你想要利用后者的技术。是的。一旦我们有了**AGI**，我们将拥有与计算线性扩展的研究人员。我们将拥有数百万AI研究人员的雪崩。他们的数量将与计算一样快速增长。所以这可能意味着，让他们做老式AI和这些手工解决方案是合理的，或者说是有意义的。

作为对**AGI**之后AI研究将如何演变的愿景，我想知道这是否仍然与**《苦涩的教训》**兼容。我们是如何达到这个**AGI**的？你想要假设它已经完成了。假设它从通用方法开始，但现在我们已经有了**AGI**。现在我们想去……那我们就完成了。有趣。你不认为有超越**AGI**的东西吗？但你正在用它再次获得**AGI**。嗯，我正在用它来获得超人水平的智能或在不同任务上的能力。这些**AGI**，如果它们本身还不是超人水平的，那么它们可能传授的知识就不会是超人水平的。我想有不同的等级。我不确定你的想法是否有意义，因为它似乎预设了**AGI**的存在，并且我们已经解决了这个问题。

### 超级智能的演进：简化方法还是增加复杂性？

也许一个激励因素是，**AlphaGo**是超人水平的。它击败了任何围棋选手。**AlphaZero**每次都会击败**AlphaGo**。所以有办法变得比超人更超人。它也是一个不同的架构。所以在我看来，能够普遍学习所有领域的智能体，会有办法给它更好的学习架构，就像**AlphaZero**是**AlphaGo**的改进，**MuZero**是**AlphaZero**的改进一样。而**AlphaZero**的改进之处在于它没有使用人类知识，而是仅仅从经验中学习。对。

那么你为什么要说“引入其他智能体的专业知识来教导它”，当它从经验中运作得如此之好，而不是通过其他智能体的帮助时？我同意在那个特定案例中，它正在转向更通用的方法。我打算用那个特定例子来说明，从超人到超人++，再到超人+++是可能的。我很好奇你是否认为这些等级会通过仅仅简化方法而继续发生。或者，因为我们将拥有数百万个头脑的能力，它们可以根据需要增加复杂性，这是否会继续是一条错误的道路，即使你拥有数十亿或数万亿的AI研究人员？

### 数字智能时代的知识共享与“腐败”风险

仅仅思考那个案例更有趣。当你拥有许多AI时，它们会像人类的文化进化一样相互帮助吗？也许我们应该谈谈这个。**《苦涩的教训》**，谁在乎呢？那是对历史上特定时期的一个经验观察。70年的历史，它不一定适用于接下来的70年。一个有趣的问题是，你是一个AI，你获得了更多的计算能力。你应该用它来让自己在计算上更强大吗？还是应该用它来生成一个自己的副本，去地球的另一端或某个其他主题学习一些有趣的东西，然后向你汇报？

我认为这是一个非常有趣的问题，它只会在**数字智能**（Digital Intelligence: 指基于数字技术实现的人工智能，区别于生物智能）时代出现。我不确定答案是什么。更多的问题是，是否真的有可能生成它，发送出去，学习一些新的东西，也许是非常新的东西，然后它是否能够被重新整合到原始智能体中？或者它会改变太多，以至于无法真正完成？这可能吗？还是不可能？你可以将其推向极限，就像我前几天晚上看你的一个视频一样。它表明这可能。你生成许多许多副本，做不同的事情，高度去中心化，但向中央主控汇报。这将是一个非常强大的事情。

这是我试图为这个观点添加一些东西。一个大问题将是**腐败**（Corruption: 指外部信息或影响可能导致AI系统内部思维或目标发生非预期或有害的改变）。如果你真的可以从任何地方获取信息并将其整合到你的中央思维中，你可能会变得越来越强大。这一切都是数字化的，它们都说某种内部数字语言。也许会很容易和可能。但它不会像你想象的那么容易，因为你可能会因此而“失去理智”。如果你从外部引入一些东西并将其构建到你的内部思维中，它可能会控制你，它可能会改变你，它可能是你的毁灭，而不是你知识的增长。

我认为这将成为一个大问题，特别是当你觉得“哦，他已经弄清楚了如何玩一些新游戏，或者他研究了印度尼西亚，你想把这些整合到你的思维中。”你可能会想，“哦，只要全部读进去，那就没问题了。”但不，你只是把一大堆比特读进了你的思维中，它们可能含有病毒，它们可能含有隐藏的目标，它们可能会扭曲你，改变你。这将成为一个大问题。在数字生成和重新整合的时代，你如何进行网络安全？

### AI接替：必然的宇宙级转变

我想这把我们带到了**AI接替**（AI Succession: 指人工智能最终可能在智能和能力上超越人类，并成为地球上主导智能形式的设想）的话题。您的观点与我采访过的许多人以及普遍大众的观点截然不同。我也认为这是一个非常有趣的观点。我想听听您的看法。我确实认为向数字智能或增强人类的接替是不可避免的。

我有四点论证。第一步是，没有一个政府或组织能给人类一个统一的、主导的观点，并能安排……对于世界应该如何运行，没有共识。第二，我们将弄清楚智能是如何运作的。研究人员最终会弄清楚的。第三，我们不会止步于人类水平的智能。我们将达到**超级智能**（Superintelligence: 指在几乎所有领域都比最聪明的人类大脑更聪明的智能）。第四，随着时间的推移，最智能的事物将不可避免地获得资源和权力。把所有这些放在一起，这在某种程度上是不可避免的。你将会有**AI**或由**AI**赋能的增强人类的接替。这四件事似乎是明确且必然会发生的。但在这组可能性中，既可能有好的结果，也可能有不那么好的结果，甚至坏的结果。我只是想现实地看待我们所处的位置，并问我们应该如何看待它。我同意所有这四个论点及其含义。我也同意接替包含了各种各样的未来可能性。

### 从宇宙视角看“设计时代”

我很想听听更多关于这方面的想法。我确实鼓励人们积极地看待它。首先，这是我们人类几千年来一直试图做的事情，试图了解我们自己，试图让我们自己思考得更好，只是了解我们自己。这是科学、人文学科的巨大成功。我们正在发现人性的这一基本部分是什么，智能意味着什么。然后我通常会说，这都是以人类为中心的。但如果我们抛开人类的身份，仅仅从宇宙的角度来看，我认为这是宇宙的一个主要阶段，一个主要转变，一个从**复制者**（Replicators: 指通过复制自身来延续存在的实体，如生物体）的转变。我们人类和动物、植物，我们都是复制者。这赋予了我们一些优势和一些局限性。

我们正在进入**设计时代**（Age of Design: 指未来智能体通过设计和构建而非复制来创造和演进的时代），因为我们的AI是被设计的。我们的物理对象是被设计的，我们的建筑物是被设计的，我们的技术是被设计的。我们现在正在设计AI，这些AI本身可以智能，并且本身具备设计能力。这是世界和宇宙中的一个关键步骤。这是从大多数有趣事物都是被复制的世界向转变。复制意味着你可以复制它们，但你并不真正理解它们。现在我们可以制造更智能的生命，更多的孩子，但我们并不真正理解智能是如何运作的。而我们现在正在达到拥有**设计智能**（Designed Intelligence: 指通过工程和设计方法创造的智能，其运作原理可被理解和控制），即我们确实理解其运作方式的智能。因此，我们可以以不同于以往的方式和速度改变它。在我们的未来，它们可能根本不会被复制。我们可能只是设计AI，而这些AI将设计其他AI，一切都将通过设计和构建完成，而不是通过复制。

我将此标记为宇宙的四个伟大阶段之一。首先是尘埃，它以恒星结束。恒星形成行星。行星可以产生生命。现在我们正在产生被设计的实体。我认为我们应该为我们正在引发宇宙中的这一伟大转变而感到自豪。这是一个有趣的问题。我们应该将它们视为人类的一部分，还是与人类不同？这是我们的选择。这是我们的选择，我们是应该说：“哦，它们是我们的后代，我们应该为它们感到骄傲，我们应该庆祝它们的成就。”还是我们应该说：“哦不，它们不是我们，我们应该感到恐惧。”有趣的是，这对我来说感觉像是一个选择。然而，这是一个如此强烈持有的观点，怎么可能是一个选择呢？我喜欢这种思想中矛盾的含义。

### 人类控制与局部目标

思考我们是否只是在设计下一代人类是很有趣的。也许“设计”这个词用得不对。但我们知道未来一代人类将会出现。忘掉AI。我们只知道从长远来看，人类将更有能力，数量更多，也许更智能。我们对此有何感受？我确实认为存在一些未来人类可能让我们非常担忧的世界。你是在想，也许我们就像尼安德特人，产生了智人。也许智人会产生一个新的群体。差不多就是这样。我基本上是在举你给的例子。

即使我们认为它们是人类的一部分，我也不认为这必然意味着我们应该感到非常舒服。亲缘关系。就像纳粹是人类，对吗？如果我们认为“哦，未来一代将是纳粹，我想我们会非常担心把权力移交给他们。”所以我同意这与担心更有能力的未来人类并没有太大区别，但我不认为这解决了人们可能对这种力量如此之快地被我们不完全理解的实体获得所产生的许多担忧。

我认为指出对于大多数人类来说，他们对发生的事情没有太多影响是相关的。大多数人类不影响谁能控制原子弹，或者谁控制民族国家。即使作为公民，我常常觉得我们对民族国家的控制不多。它们失控了。很多都与你对变化的感受有关。如果你认为当前情况非常好，那么你更有可能怀疑变化并厌恶变化，而不是如果你认为它不完美。我认为它不完美。事实上，我认为它相当糟糕。所以我对变化持开放态度。我认为人类的记录并不那么好。也许它是最好的，但远非完美。

### AI的价值观与未来社会设计

我想变化有不同的种类。工业革命是变化，布尔什维克革命也是变化。如果你在1900年代的俄罗斯，你可能会说：“看，事情进展不顺利，沙皇搞砸了一切，我们需要改变”，我想知道你想要什么样的改变，然后才会签字。同样，对于AI，我希望理解，并在可能的情况下，改变AI的轨迹，使这种改变对人类是积极的。我们应该关心我们的未来，未来。我们应该努力使其变得美好。

但我们也应该认识到我们的极限。我认为我们应该避免**优越感**（Feeling of Entitlement: 指认为自己理所当然应得某种权利或待遇的心态），避免“哦，我们先到这里，我们应该永远以好的方式拥有它”的感觉。我们应该如何思考未来？一个特定星球上的特定物种应该对它有多少控制权？我们有多少控制权？对人类长期未来的有限控制的平衡点应该是我们对自己的生活有多少控制权。我们有自己的目标。我们有自己的家庭。这些事情比试图控制整个宇宙更可控。我认为我们应该真正努力实现自己的局部目标。我们说“哦，未来必须以我想要的方式发展”，这有点咄咄逼人。因为那样我们就会有争论，不同的人认为全球未来应该以不同的方式发展，然后他们就会发生冲突。我们想避免这种情况。

也许这里有一个很好的类比。假设你正在抚养自己的孩子。对他们的生活设定极其严格的目标可能不合适，或者也有某种感觉，比如“我希望我的孩子走向世界，产生这种特定的影响。我的儿子将成为总统，我的女儿将成为英特尔的CEO。他们将共同对世界产生这种影响。”但人们确实有这种感觉——我认为这是合适的——说：“我将赋予他们良好而坚定的价值观，以便如果他们最终身居高位，他们会做合理、**亲社会**（Prosocial: 指对他人或社会有益的行为）的事情。”也许对AI采取类似的态度是有道理的，不是说我们可以预测他们会做的一切，或者我们对一百年后的世界应该是什么样子有计划。但赋予他们坚固、可引导和亲社会的价值观非常重要。

亲社会价值观？也许这个词用错了。有没有我们都能同意的普世价值观？我不这么认为，但这并不妨碍我们给孩子良好的教育，对吗？就像我们有一种希望孩子成为某种样子的感觉。也许亲社会是错误的词。高诚信可能是一个更好的词。如果有一个请求或一个目标看起来有害，他们会拒绝参与。或者他们会诚实，诸如此类。我们有一种感觉，我们可以教导我们的孩子这样的事情，即使我们对真正的道德是什么没有概念，因为每个人对此都不同意。也许这也是AI的一个合理目标。

所以我们正在努力设计未来，以及它将如何演变和形成的原则。你说的第一件事是，“嗯，我们试图教导我们的孩子通用原则，这将促进更有可能的发展。”也许我们也应该寻求事情是自愿的。如果发生变化，我们希望它是自愿的，而不是强加于人的。我认为这是一个非常重要的观点。这一切都很好。我认为这是人类设计社会的一项伟大或真正伟大的事业，它已经持续了几千年。事物变化越多，它们就越保持不变。我们仍然必须弄清楚如何存在。孩子们仍然会提出对他们的父母和祖父母来说显得奇怪的不同价值观。事物会演变。

“事物变化越多，它们就越保持不变”似乎也是对AI讨论的一个很好的总结。我们正在进行的AI讨论是关于那些甚至在它们应用于深度学习和反向传播变得明显之前就被发明的技术，如何成为当今AI发展核心的。也许这是结束对话的好地方。好的。非常感谢。太棒了。谢谢您的到来。我的荣幸。