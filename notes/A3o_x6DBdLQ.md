---
area: society-systems
category: technology
companies_orgs:
- OpenAI
- Anthropic
- XAI
- Meta
- Microsoft
date: '2025-08-27'
draft: true
guest: ''
insight: ''
layout: post.njk
media_books:
- The New Yorker
- Deep Work
- Digital Minimalism
people:
- Dario Amodei
- Sam Altman
- Elon Musk
- Gary Marcus
products_models:
- ChatGPT
- GPT-3
- GPT-4
- GPT-4o
- GPT-5
- AlphaGo
project:
- ai-impact-analysis
- systems-thinking
series: ''
source: https://www.youtube.com/watch?v=A3o_x6DBdLQ
speaker: 北美王路飞
status: evergreen
summary: 本文深入探讨了OpenAI最新模型GPT-5未能达到预期突破的原因，揭示了支撑AI发展的“缩放定律”已告失效。麻省理工计算机博士凯尔·纽波特作为“AI现实主义者”，批判了硅谷对通用人工智能（AGI）的盲目狂热，指出大语言模型（LLM）的本质局限在于其静态架构和模式匹配机制。文章呼吁回归理性，强调AI的未来在于多模块、专业化的神经符号系统，而非单一的“万能之神”，并提醒普通人无需为AI焦虑，应专注于提升自身核心技能。
tags:
- llm
- scaling-law
title: GPT-5“翻车”：AI缩放定律失效与硅谷狂热的终结
---

### 引言：ChatGPT的现状与AI现实主义者的警示

大家好，欢迎来到我的频道。今天我们来聊聊ChatGPT。我的频道制作了许多与AI相关的内容，说实话，如果没有ChatGPT模型，我不可能制作这么多视频。很多工作，比如收集信息、整理数据，以及处理和校对字幕，都可以交给AI完成。如果没有这种效率提升的工具，我不可能在这么短的时间内制作这么多视频。

然而，今天这期节目我们恰恰要聊一下刚刚推出的ChatGPT-5。我个人使用下来，并没有那种眼前一亮的感觉。与之前推出的GPT-4o和ChatGPT-4相比，提升并不那么明显。这与ChatGPT-4和GPT-4o刚推出时给我的感觉完全不同，因为当时的模型真的让我眼前一亮，非常兴奋。现在，这个模型似乎已经撞到了一堵墙上。

今天这期节目，我们将探讨一位**AI现实主义者**（AI Realist: 强调AI技术应基于实际能力而非过度炒作的观点）的预测。本节目基于他写的一篇文章以及他最近的一些采访。这位麻省理工学院（MIT）毕业的计算机科学教授，名叫凯尔·纽波特（Kyle Newport），他在《纽约客》（The New Yorker）上发表了一篇文章，标题是《万一这就是AI能够达到的最好水平了呢？》。在文章中，他引用了一位厉害的AI怀疑论者的话说，现在最新的GPT是“姗姗来迟，过度吹嘘，平平无奇”。纽波特本人对此深表赞同。这很有意思：一边是硅谷的大佬们天天吹嘘“AI革命改变世界”，一边是顶级的计算机科学家告诉你，它有可能撞墙了。这到底是怎么回事呢？今天我们就好好地看一下他这篇文章到底写了哪些内容。

### 卡尔·纽波特：一位清醒的技术反思者

首先，给大家介绍一下这篇文章的主角，敢于与整个硅谷唱反调的卡尔·纽波特。他不是一般的网红或评论家，而是正儿八经科班出身的麻省理工学院计算机科学博士，现在是乔治城大学的终身教授。所以你看，人家是懂技术的，不是瞎咋呼。

而且，他之前就写了像《深度工作》（Deep Work）这样的畅销书，提出了**数字极简主义**（Digital Minimalism: 一种有意识地减少数字设备和应用使用，以专注于更有价值活动的哲学）。这本书我也做过一期节目，当时频道流量不大，没什么人看，但我会把链接放在视频描述栏，其实对大家还是有所帮助的。说白了，他其实一直在提醒我们，别被技术给绑架了，技术应该是工具，而不是我们的主人。所以你看，他对于技术，尤其是对于科技公司的宣传，一直保持着一种审慎甚至批判的态度。

这很有意思，当所有人都为AI狂欢的时候，一个懂技术又对技术有深刻反思的人站出来说：“等一等，这事有点不对劲。”那么他的话，我们就得好好听听了，对不对？他有点像童话故事里那个说皇帝没穿衣服的小孩。我们来看看他这篇文章，到底描述了哪些他看到了但我们没有看到的东西。

### GPT-3的横空出世与“缩放定律”的狂热

首先，时间倒回到2020年，这股AI的狂风到底是怎么刮起来的呢？那一年，有一个叫OpenAI的小公司，当时还不是特别出名，他们发布了一个东西，叫做**GPT-3**（Generative Pre-trained Transformer 3: 生成式预训练变换器3，OpenAI开发的大型语言模型）。这玩意一出来，直接就把AI圈给整不会了。你想想，在GPT-3之前，语言模型都是什么水平？就是写一点通顺的句子，稍微长一点就前言不搭后语，逻辑混乱，就跟学前班小孩似的。

但GPT-3完全不是一回事，你让他给你写一段文字，他能写出一篇完整的文章，而且文笔流畅，逻辑清晰。当时的研究人员一看都惊呆了，说这玩意的语言能力跟以前的模型比简直就是博士生吊打小学生，这是一个巨大的飞跃。整个AI研究圈就非常兴奋，大家觉得语言模型也可以做到这种程度。但在那个时候，普通人还不太知道这件事，因为它用起来还比较麻烦，有一点像一匹没有驯服的野马。你让他写诗，他可能会写得很漂亮，但是突然就跑偏了，开始胡说八道，甚至说一些很不**政治正确**（Politically Correct: 符合社会主流价值观，避免冒犯特定群体）的话。所以当时它还只是研究圈子里的一个神兽。

可在那时候，一个足以引爆硅谷的惊天大发现，就已经在OpenAI的内部悄悄酝酿了。这个惊天大发现是什么呢？说白了，就是一种简单粗暴的“大力出奇迹”。当时，OpenAI有个研究员叫杰瑞德·凯普兰（Jared Kaplan），他领着一帮人，包括后来成为Anthropic公司CEO的达里奥·阿莫代伊（Dario Amodei），他们做了一个现在看起来很明显，但当时很反直觉的实验。他们想知道一个问题：就是我们把这个GPT模型，也就是它的神经网络做得更大更大，给它喂更多更多的数据，让它训练更长更长的时间，会发生什么呢？

你想一下，这其实是当时违反机器学习领域几十年传统智慧的。老前辈都说，模型不能做得太大，太大了就会**过拟合**（Overfitting: 模型在训练数据上表现良好，但在新数据上表现差的现象），只会死记硬背训练数据，一旦遇到新的问题就抓瞎。得找到一个**甜点区**（Sweet Spot: 最佳平衡点），大小刚刚好才行。但是Kaplan他们就不相信，他说我们试试看，把它往大里整会怎么样。结果他们发现，这玩意不仅没有变糟，反而变得好了很多，而且性能飙升，蹭蹭往上涨。他们把数据画成一条曲线，发现这条线非常陡峭，甚至可以预测。这就是后来大名鼎鼎的**缩放定律**（Scaling Law: 指模型性能随参数量、数据量和计算量增加而提升的规律）。

这发现的意义是什么呢？太重大了！它等于告诉全世界，我们好像找到了一条通往超强AI的高速公路。我们不需要再苦思冥想什么新的绝妙的算法了，我们只需要干一件事情，就是把模型做大，再大，更大！只要有钱有算力，就能沿着这条曲线一路冲向神坛。GPT-3就是他们用这个理论搞出来的第一个巨无霸，相比于之前的**GPT-2**（Generative Pre-trained Transformer 2: OpenAI开发的早期大型语言模型）大了15倍。结果它的性能完美落在那条预测曲线上。这一下就彻底点燃了硅谷，这相当于你又获得了一个验证的数据，证明你这个模型还是有效的。

### 硅谷的“弥赛亚情结”与AGI的幻想

这个缩放定律一被验证，整个硅谷就疯了。你想想，那帮人本来就迷信技术能够解决一切，忽然有人告诉他们，通往**人工智能天堂**（AI Heaven: 指实现高度发达、近乎完美的AI状态）的电梯已经找到了，只要不停地往里面砸钱就行了。他们能不疯吗？当时OpenAI的老大萨姆·奥特曼（Sam Altman）激动得不行，马上就写了一篇著名的文章，叫做《万物的**摩尔定律**》（Moore's Law of Everything: 摩尔定律: 集成电路上可容纳的晶体管数量大约每两年翻一番，此处指AI能力将以类似摩尔定律的速度指数级增长）。

他里面说什么呢？他说AI很快就能够自动化经济中的大部分工作了，那时候会产生巨大的财富不平等，我们必须对那些拥有AI的公司征收股权税，然后搞**全民基本收入**（UBI: Universal Basic Income: 无条件向所有公民定期支付基本生活费的社会福利制度）来分钱，不然社会就要造反了。你听听，这是不是有点“天下大乱，我主沉浮”的这种感觉了？他为什么这么说呢？因为他手里攥着那条缩放定律曲线，他往上一推，我的天，GPT-5、GPT-6再搞个几代，那不就是**通用人工智能**（AGI: Artificial General Intelligence: 能够像人类一样执行任何智力任务的AI）了吗？这东西能够干所有人类能干的活，离我们可能只有五年的时间了。这个想法在2018年的时候还没人敢想，都觉得AGI遥遥无期，现在突然发现了一条直达的快车道，这帮人的脑袋都快炸了。

然后，为了让全世界看到这条光明的未来，他把GPT-3驯化了一下，让它变得更听话，更安全，不会胡说八道。然后打包成了一个普通人也能够使用的产品，这就是我们后来所知道的ChatGPT。ChatGPT一发布，全世界都看到了AI的魔力。四个月后，他们又砸了重金，让微软给他们专门建了新的数据中心，甚至开发了新的空调技术来散热，搞出了一个比GPT-3又大十倍的**GPT-4**（Generative Pre-trained Transformer 4: OpenAI开发的第四代大型语言模型）。结果，ChatGPT-4的性能再一次分毫不差地跳到了那条预测曲线更高点上。这下硅谷彻底进入了**弥赛亚时刻**（Messiah Moment: 指人们对某事抱有极高期望，认为它将带来根本性变革的时刻）。他们觉得神马上就要降临了，我们离AGI只差两代模型，所有的钱都应该投给我们，谁赢了这场比赛，谁就控制了未来的世界经济。大家都翘首以盼，等待下一个更大的奇迹：GPT-5。

### GPT-5的“翻车”：缩放定律的失效

但接下来发生的事情，让所有人都傻眼了。就在所有人都觉得马上要见证历史的时候，一条神奇的缩放定律曲线，它突然就断了。OpenAI信心满满地启动了**GPT-5**（Generative Pre-trained Transformer 5: OpenAI开发的第五代大型语言模型）的研发项目，代号**猎户座项目**（Orion Project: OpenAI内部GPT-5研发项目的代号）。他们建了更大的数据中心，搞了更大的模型，投入了前所未有的资源。奥特曼到处放风，说这玩意会把GPT-4炸得粉碎，甚至说这东西有点吓到我了，好像马上就要打开新世界大门了。你看，这个奥特曼也是个戏精。

结果呢，2024年的夏天，这个猎户座项目训练完成了，大家兴冲冲去测试，结果发现它只是比GPT-4好了那么一点点。这就尴尬了，说好的巨大飞跃呢？说好的指数级增长呢？怎么我们花了那么多钱，搞了那么大阵仗，结果性能就提升那么一丢丢？这神奇的曲线，它不灵了。

而且，发现这个问题的还不止OpenAI一家。埃隆·马斯克（Elon Musk）的XAI公司，他们也搞了一个巨无霸，叫**Grok-3**（Grok-3: XAI公司开发的第三代大型语言模型）。他们用了一个叫做**巨象数据中心**（Grok-3 Data Center: XAI公司为Grok-3模型训练建设的超大规模数据中心），里面有10万块**H100 GPU**（NVIDIA H100 GPU: 英伟达推出的一款高性能图形处理器，主要用于AI计算）。这算力简直是丧心病狂，结果训练出来一看，也就比之前好那么一点点。Meta那边也一样，他们搞了一个巨兽的模型，也是用了他们有史以来最大的数据中心，结果做完了发现和之前的模型比，没有质的提升。所以呢，干脆就雪藏了，说这玩意发不出去，不够牛逼。

到了2024年的秋天，整个AI的核心圈子都意识到一个残酷的现实：那条支撑了所有人信念的缩放定律，到GPT-4这个点上就已经失效了。简单粗暴的“做大做强”这条路，好像已经走到头了。

### 大语言模型的本质缺陷：凯美瑞的局限

那么问题来了，为什么会这样呢？为什么这条路走不通了呢？纽波特在这里就用了一个非常形象的比喻。他说，现在的AI模型就像一辆丰田凯美瑞，你不停地给它花钱，给它改装，给它加涡轮，给它改轮胎装尾翼，你把它**爆改**（Soup Up: 指对汽车进行大幅度改装以提升性能），它可能也就会在某些方面的性能变得更好一些，跑得更快一些。但是你再怎么改，凯美瑞也不会变成法拉利。

这什么意思呢？他说我们得回到最基础的层面，去理解这些所谓的大语言模型，它到底是个什么东西。它底层架构从头到尾就是一个猜词的机器。它的训练过程呢，就是你给它一句话的前半部分，比如说“the quick brown”，它来猜下一个最有可能的词是“fox”。它存在的意义就是玩这个猜词游戏。我们之前之所以这么训练它，不是因为这是通往人工智能最佳途径，而是因为只有这种方法我们才有海量的、几乎无限的训练数据。你想想，互联网上所有文字、所有书、所有文章，都可以拿来当教材。你只需要随便往那边截断，让他猜下一个词，然后告诉他正确答案，再根据对错去调整他的内部参数就行了。这是一个非常取巧，但是非常有效率的训练方法。所以说，这个模型的基因，它的底盘就是一辆凯美瑞，它天生就是为猜词这个任务而设计的。

### “涌现”的幻象与模式匹配的真相

当然了，肯定会有人反驳，说你这个说法不对。在GPT-3和GPT-4成功之后，当时最流行的理论就是**涌现**（Emergence: 复杂系统在简单规则下，自发产生高级行为或属性的现象）。这个理论说什么呢？就是说，当这个猜词机器做得足够大，喂它的数据足够多的时候，为了能够把猜词这个游戏玩到极致，它会在内部涌现一些我们意想不到的高级能力。你想想这个逻辑啊，比如，为了准确猜出某个逻辑谜题的答案那个词，它是不是就得在内部学会怎样解逻辑题呢？为了能够猜出数学方程式等号后面那个数字，它是不是就会学会怎么做数学题呢？为了写一段幽默的情景喜剧对话，它是不是就得理解什么是笑话，什么是人物性格呢？这个想法特别有诱惑力，对不对？它意味着我们只需要“大力出奇迹”，把模型做大，真正的智能就会像魔法一样，从这个简单的猜词任务中涌现出来。当时很多人都相信，这个凯美瑞的引擎里头，正在自发地生长出一个法拉利的核心。

但是呢，最近越来越多的研究发现，这个所谓的涌现可能只是我们的一厢情愿，甚至是一种海市蜃楼。我们来看证据：之前很多人觉得GPT-4很厉害的一点，就是它好像学会推理了。OpenAI也专门训练它，要求它在回答问题时要“show your work”，展示解题步骤，一步步地写出来，看起来像是一个逻辑大师。这以前我们在解数学题的时候，老师都要求，你不写这个解题步骤直接写个答案，我们是不会给你分的。OpenAI也是这么要求这个GPT的。

但是呢，去年夏天，好几个顶级实验室的论文都出来打脸了。他们发现，这玩意根本不是在推理，它是在做一种非常高级的**模式匹配**（Pattern Matching: 通过识别和模仿已知模式来解决问题或生成输出）。这什么意思呢？就是说，它看过太多太多类似的题目和解法了，它只是在模仿它见过的模式。你给他一个题库里有的题型，它会给你标准答案。但是如果你把这个题目规模再稍微搞大一点，超出它见过的范围，它马上就崩溃了，开始胡说八道。更有意思的是，有一个实验，研究人员直接把解题的逻辑和方法都告诉它了，你就按照这个方法来就能够解开这个谜题。结果呢，它还是会搞错。这说明了什么呢？这说明它脑子里头根本没有形成那个抽象的逻辑回路，它不是真正的理解了“因为A所以B”，它只是记住了“看到A这个模式就输出B那种模式”。这完全是两码事。

一个更打脸的例子就是下棋。你有可能觉得AI下棋不是很牛吗？像什么**AlphaGo**（AlphaGo: Google DeepMind开发的人工智能围棋程序）把人类冠军都干翻了。没错，那是专门为了下棋设计的AI。你让ChatGPT这种大语言模型去下国际象棋，哪怕是最新最强的GPT-5，它都会走出**非法移动**（Illegal Move: 在棋类游戏中，不符合规则的走法）。什么是非法移动呢？就是说让这个棋子走出了不属于它应该走的这样一个规则的走法。这说明什么呢？这说明在它那庞大无比的神经网络里头，连一个最最基本的国际象棋规则模型都没有建立起来。它下棋靠的是什么呢？还是靠模式匹配，它看过无数的棋谱，它只是在模仿在某个局面下人类高手最可能走哪一步。它的模型是一团乱麻，是模糊的，是不准确的。它偶尔会灵感迸发，走出一步连规则都不符的棋。你想想，一个连最基础的游戏规则都无法在内部精准建模的系统，你怎么能够指望它涌现出对我们这个复杂的物理世界深刻的理解呢？

### 静态架构的桎梏：AI为何无法真正思考

说到底，这个问题的根源其实都指向大语言模型一个娘胎里带出来的根本性的缺陷。纽波特在一年前的文章里头就直说这一点：它的架构是**静态的**（Static Architecture: 指模型一旦训练完成，其内部参数和知识即被固定，不会随交互而更新）。这什么意思呢？就是说，一个模型，比如说GPT-5，一旦训练完成了，它内部所有参数，那个代表着它全部知识的数字，就被烧录了进去，永远不会再变了。你跟它说的每一句话，它身上的每一个词，都是用一套固定不变的参数。它不会因为跟你聊聊天就学到了新的东西，更新了它的世界观。它没有状态这个概念，没有记忆，不会循环思考，更不会做向前看的规划。

你想想，我们人类是怎么思考的？我们不断学习新的知识，更新我们的认识，我们会根据目标来规划未来，模拟各种可能性，对不对？但这些能力，目前的大语言模型架构从根本上就不支持。所以啊，大家看明白了吗？这条缩放定律之所以会失效，很可能就是因为这个纯粹靠猜词和模式匹配的凯美瑞架构，它的潜力已经被压榨到极致了。你现在只想通过堆料，把它变成自我进化、能够规划未来的法拉利，那是在做梦。

### 从“造车”到“改装车”：AI行业的策略转变

好了，真相现在就清晰了。大概在2024年秋天开始，当OpenAI和其他公司发现了缩放定律这条路走不通之后，他们开始怎么办呢？总不能跟投资人说：“哎呀，我们搞砸了，通往AGI的高速公路断了吧？”那股价不得崩盘啊！于是呢，他们悄悄把研究重心从这个“做大模型”这种“造车”的活，转移到了“改装车”上面。这就是纽波特说的**后训练技术**（Post-training Techniques: 指在模型预训练完成后，通过微调、指令调整等方法提升模型在特定任务上的表现）。

说白了，就是拿一个已经训练好的模型，比如说GPT-4这样级别的凯美瑞，然后开始花里胡哨地搞各种技术去调教它，让它在某些特定任务上表现得更好。比如说，让它学会了展示解题步骤，显得很会推理；或者呢，让它在某个特定的编程语言测试中得分更高；再或者呢，让它的回答更加安全，更有礼貌。这些改装工作呢，确实能够让模型增加一些新的功能，在一些测评指标上看起来更漂亮。但跟之前那种模型的核心能力巨幅提升，完全是两码事。这只是在优化和修补，不是在进化。

而所谓的GPT-5发布，说白了更像是一场营销活动。它把过去一年零零散散做出的各种改装和优化，用一些让人搞不懂的名字发布出来的这个模型，再重新打包在一起，取名叫GPT-5，告诉你们：“我们又取得了重大进展！”从用户角度来说呢，这当然是好事了，车更好开了嘛。但对于那些期待看到法拉利的人来说，这就是巨大的失望。因为它清清楚楚地告诉大家：“我没有造出新车，我们只是把去年的凯美瑞又改了改。”那条通往AGI的高速公路，至少在目前呢，是彻底堵死了。

### 通往AGI的真正路径：多模块的神经符号AI

那么既然“一条路走到黑”的“大力出奇迹”、“暴力美学”走不通了，那通往人工智能更强的道路应该是怎么样的呢？纽波特认为，我们应该去看那些真正解决了复杂问题的AI系统，而不是只盯着大语言模型。他提到了一个特别牛的系统，叫做**西赛罗**（Cicero: Meta公司开发的一款能够在复杂外交游戏中与人类玩家协作和谈判的AI系统）。这是Meta公司几年前搞出来的一个AI，它能够玩外交游戏。这个游戏呢，极其复杂，它不光是像围棋那样的策略游戏，更重要的是，它充满了人与人之间的谈判、欺骗、结盟和背叛。每一轮之前，你都要和其他玩家进行一对一的私下密谈。

你想想，要玩好这个游戏，这个AI需要什么能力呢？它得理解人类的自然语言对话，能够从中分析出对方的真正意图。它得有规划能力，能够模拟“如果我这么走，或者我选择和A结盟，背叛B，未来会发生什么”。它要有价值判断能力，能够评估哪种未来对他最有利。西赛罗是怎么做到这一点的呢？它就不是一个单一的大模型，它是**多模块系统**（Multi-module System: 指由多个独立且功能专一的模块协同工作以完成复杂任务的系统）。它里面有一个语言模型，专门负责和人聊天理解对话；但它还有一个未来模拟器，这是一个传统的基于规则的程序，用来推演各种可能性；它有一个价值评估的神经网络，专门来判断局面的好坏；最后呢，有一个总的控制程序，把所有的模块的意见结合起来，做出最终的决策。你看，这才是真正强大的智能系统应该有的样子。它不再是一个混沌的大脑，而是一个分工明确、各司其职的精密系统。

像西赛罗这种系统，其实代表AI研究领域一个由来已久的思想，就是被纽波特引用的那个暴躁的怀疑论者加里·马库斯（Gary Marcus）从上世纪90年代就一直开始鼓吹的这个思想。他管这个叫做**神经符号AI**（Neuro-symbolic AI: 结合神经网络的模式识别能力和传统符号AI的逻辑推理能力的人工智能方法）。说白了，就是把神经网络这种擅长模式识别的神经方法，和传统的基于逻辑和规则的符号结合起来。

### AI的未来：专家工具箱而非万能之神

所以呢，马库斯和纽伯特这样的人，他们到底在怀疑什么呢？他们不是在怀疑人工智能本身，他们是怀疑只靠超大型语言模型这一条路就能够实现通用人工智能这个想法。他们是“大语言模型万能论”的怀疑者。马库斯甚至非常乐观，他认为如果我们走神经符号这条路，在2030年代，也就是十年内，我们就能够拥有AGI。但是呢，这条路有个特点，它和之前硅谷幻想的完全不一样。它不会生成一个万能的AI上帝，它创造出来的是一个高度定制化的、专门解决特定问题的专家系统。比如说呢，专门玩外交的AI，一个专门玩德州扑克的AI，纽波特也提到另一个系统，就是专门打德州扑克的一个专门帮你处理公司法务合同的AI。这些系统呢，会非常强大，甚至在他们的领域里头远超人类。但他们是分散的，是术业有专攻的。你不可能把这些系统用一个USB连线起来，就变成一个无所不能的“天网”。所以呢，真相就是，AI的未来有可能是一个由无数专家组成的工具箱，而不是一个唯一的神。这个未来呢，会以一种更渐进、更正常，也更不容易失控的方式到来。

### 祛魅与理性：对AI狂热的冷静反思

把整个故事捋清之后，我们就能明白纽伯特到底想说什么了。在我看来，他这篇文章就是给整个狂热的AI行业，特别是那些被冲昏头脑的硅谷大佬们，进行一场及时的**祛魅**（Disenchantment: 指破除神秘感和幻想，回归理性现实）。我们想一想，过去一两年，硅谷那帮人什么心态？就是弥赛亚情结大爆发。他们不只是在做技术，他们是在造神。他们真心相信自己手里掌握着开启人类新纪元的钥匙，相信这个缩放定律就是神谕。这种信念呢，已经脱离了科学和商业的范畴，变成了一种近乎宗教的狂热。

这个狂热带来什么呢？是万亿级别的疯狂投资，也是带来对于普通人巨大焦虑的贩卖。他一边说AI要毁灭世界了，我们必须小心；一边又用恐惧来倒逼政府和市场，给他们更多资源和权力。而纽波特呢，作为一个AI现实主义者，他做的事情就是捅破这个神话。他用严谨的技术史考据和清晰逻辑告诉你：“对不起，你们信奉的那个神谕，它已经失灵了。你们以为的高速公路其实是条死胡同。你们以为的造神，其实只是在改装一辆家用车。”在我看来，这就特别重要，因为它把AI从神坛上拉回到现实世界。AI不是神，也不是魔鬼，它只是一项技术。它有自己的优势，也有它根本的局限性。它会发展，但是也会遵循正常的技术发展路线，有突破有瓶颈，有激动人心的时刻，也有漫长而乏味的平台期。所以呢，纽伯特不是在唱衰AI，他只是在呼吁一种理性和现实。他告诉我们，别再听那些什么末世预言家或者是救世主讲故事了，就像对待任何一个新技术一样，我们应该去研究它真实能力，找到它真正的应用场景，而不是活在一个对虚幻的AI上帝崇拜或者恐惧之中。我觉得确实如此，因为这个**生成式AI**（Generative AI: 能够生成文本、图像、音频等新内容的AI模型）在我用下来，现在这个性能的提升已经非常有限了。就是跟之前一个模型的版本，虽然是非常好的工具，但是它已经不会再出现那种几何数量级别的增长了。

### 对普通人的启示：无需焦虑，聚焦核心技能

最后，这个故事对我们普通人有什么启发呢？我觉得最大启发就是我们真的没有必要那么焦虑。纽波特在访谈里头说了一句我特别认同的话，他说呢，那种“你必须赶紧去学习怎么使用AI，否则你就会被淘汰”的论调，是21世纪以来科技公司最成功的一次**营销PUA**（Marketing PUA: 指通过营销手段，利用用户的焦虑和不安全感，使其产生购买或学习的冲动）。他说凭什么一个产品好不好用，有没有价值，应该是由产品公司需要向消费者证明的事情，怎么反而变成了消费者的义务呢？

你想想，当以前Email出来的时候，有人需要教你怎么用吗？不需要啊，因为它实实在在解决了传真和电话留言的痛点。电子表格出来的时候，需要人去劝会计师去学吗？不需要，它显而易见比手写账本高效无数倍。真正有价值的工具，它的用处一定是不证自明的。所以呢，对于我们普通人来说，那些通用聊天机器人，它有可能在某些方面确实能帮上忙，但它不会从根本上颠覆我们的工作。未来真正能够颠覆我们行业的，一定是为我们行业深度定制的、融合了多种技术的专家系统。而这种工具出现时呢，你根本不用担心学不会，因为它价值和用法会非常清晰。

所以说呢，与其花时间去研究怎么跟一个聊天机器人说“黑话”，不如把时间花在打磨我们真正的、不可替代的核心技能上。就像纽波特说的，AI的未来是一个个分散的专业工具，而这些工具终究是要为人类服务的。一个只会猜词的机器，它再强大，也没有办法取代一个真正会思考、会创造、会与人深度共情的人。这可能就是卡尔·纽伯特，这一个AI现实主义者，真正想告诉我们的吧。

好的，今天这期节目就录到这里了，非常感谢大家观看，我们下期节目再见。