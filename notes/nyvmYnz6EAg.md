---
area: "work-career"
category: ai-ml
companies_orgs:
- Anthropic
- OpenAI
date: '2025-08-01'
draft: true
guest: ''
insight: ''
layout: post.njk
people:
- Sholto Douglas
- Trenton Bricken
products_models:
- GPT-4
- Claude Code
- Gemini 2.5
project: []
series: ''
source: https://www.youtube.com/watch?v=nyvmYnz6EAg
speaker: Dwarkesh Patel
status: evergreen
summary: 本文探讨了当前大型语言模型（LLMs）在实现通用人工智能（AGI）方面面临的核心瓶颈——缺乏持续学习能力。演讲者认为，尽管LLMs能力强大，但其无法像人类一样通过实践不断改进和适应，这阻碍了它们在经济领域的广泛应用和自主代理的实现。因此，作者对近期AGI的到来持谨慎态度，但对AI的长期发展前景表示乐观。
tags:
- ai-agent
- learning
- llm
title: AI 发展瓶颈：持续学习的挑战与 AGI 的时间线预测
---
### AGI 时间线讨论

在我主持的播客中，我们经常深入探讨达到**通用人工智能（AGI: Artificial General Intelligence，指拥有与人类相当或超越人类的认知能力，能够理解、学习和应用知识于广泛任务的智能系统）**的时间线。一些嘉宾认为还需要二十年，另一些则预测只需两年。截至 2025 年 7 月，我的看法是这样的。

### 当前 AI 的局限性

有人认为，即使所有 AI 进展完全停止，今天的系统仍比互联网具有更经济的变革潜力。我并不同意这种观点。我认为，当今的**大型语言模型（LLMs: Large Language Models，指在海量文本数据上训练的深度学习模型，能够理解和生成人类语言，如GPT系列、Claude系列等）**确实非常神奇。但 **Fortune 500**（财富世界 500 强企业）的公司未能利用它们彻底改变工作流程，并非因为管理层过于守旧。

### 核心瓶颈：难以获得类人劳动

我更倾向于认为，从这些 LLMs 中获得正常、类人的劳动输出，实际上非常困难。这涉及到这些模型在一些基本能力上的欠缺。作为 **Dwarkesh Podcast** 的主持人，我自诩是“AI 前瞻者”。我曾花费超过一百小时的时间，尝试为我的后期制作流程构建小型 LLM 工具。而试图让它们变得有用的经历，反而延长了我对 AI 发展时间线的预期。

### 实践中的 LLM 挑战

我曾尝试让 LLMs 为我重写自动生成的字幕，以人类的方式优化可读性；或者让它们从我输入的讲稿中识别出精彩片段；有时我还会尝试让它们与我逐段共同撰写文章。这些都是简单、独立、短视界、输入语言输出语言的任务——本应是 LLMs 的核心能力范围。然而，它们在这些任务上的表现大约只能达到 5/10。这并非否定其能力，而是指出根本问题：LLMs **不会像人类一样随着时间推移而进步**。

### 持续学习的缺失

这种**缺乏持续学习的能力**是一个巨大的瓶颈。在许多任务上，LLMs 的基线能力可能高于普通人，但我们无法向模型提供高层次的反馈。你只能依赖模型开箱即用的能力。你可以不断调整系统提示词（system prompt），但在实践中，这根本无法产生人类员工所经历的那种学习和改进。

### 人类价值的真正来源

人类之所以如此有价值，主要并非依靠原始智力，而是他们构建上下文、审视自身失败、并在实践任务中不断吸收微小改进和提高效率的能力。这就好比教一个孩子吹萨克斯风：你会让他们尝试吹奏，听听声音，然后进行调整。

### 学习模式的根本差异

现在想象一下，教授萨克斯风的唯一方式是：学生尝试一次。一旦犯错，你就让他们离开，然后写下详细的错误说明。接着叫下一个学生进来。这个学生阅读你的笔记，然后尝试“冷启动”演奏查理·帕克（Charlie Parker）的曲子。他们失败后，你再完善说明，并邀请下一位学生。这种方式根本行不通。无论你的提示词多么精炼，没有孩子能仅仅通过阅读说明学会吹萨克斯风。

然而，这恰恰是我们“教授” LLMs 的唯一模式。是的，存在**强化学习（RL: Reinforcement Learning，一种机器学习范式，通过试错和奖励机制来训练智能体，使其在特定环境中做出最优决策）**的微调。但它并非像人类学习那样，是一个深思熟虑、适应性强的过程。我的编辑们已经变得非常出色，如果他们必须为工作中涉及的各种子任务构建定制化的 RL 环境，他们不可能达到今天的水平。他们是通过自己注意到大量细微之处，并深入思考什么能引起观众共鸣、我喜欢什么类型的内容，以及如何改进日常工作流程而进步的。

### 对未来 AI 学习模式的设想

当然，我们可以设想更智能的模型能够为自己构建一个专用的 RL 循环，从外部看感觉非常自然。我提供一些高层次的反馈，模型就能生成大量可验证的练习问题来进行 RL 训练——甚至可能是一个完整的环境，让它排练自己认为缺乏的技能。但这听起来非常困难，而且我不知道这些技术在不同任务和反馈类型上能推广到何种程度。

### 短期内持续学习的挑战

最终，模型将能够像人类一样，在工作中进行有机学习。但鉴于目前没有明显的方法能将持续学习融入 LLMs 的模型架构中，我很难想象这能在未来几年内实现。LLMs 在一次会话中确实会变得更聪明、更有用。例如，我有时会与 LLM 共同撰写文章。我会给它一个大纲，让它逐段起草。但它写到第四段之前的所有建议都可能很糟糕。我会从头重写每一段，并告诉它：“看，你写得太差了。这是我写的。”此时，它才会开始为下一段提供好的建议。但这种对我的偏好和风格的细微理解，在会话结束时就会完全丢失。

### 上下文窗口与记忆的局限性

也许有一个简单的解决方案，比如一个长滚动上下文窗口，类似于 **Claude Code** 已有的功能，它每 30 分钟将对话记忆压缩成摘要。但我认为，将所有这些丰富的隐性经验提炼成文本摘要，在软件工程（一种高度基于文本的领域，本身就有代码库作为外部记忆支架）以外的领域会非常脆弱。再次想想，仅凭文本来教孩子吹萨克斯风会是怎样一番景象。即使是 Claude Code，在我点击 `/compact` 之前，也常常会撤销我们共同努力实现的、来之不易的优化——因为其背后的原因没有被包含在摘要中。

### 对专家预测的异议

这就是为什么我不同意 **Anthropic** 研究员 **Sholto Douglas** 和 **Trenton Bricken** 在我播客上的一些观点。引用 Trenton 的话：“即使 AI 进展完全停滞（你认为模型非常‘尖锐’，缺乏通用智能），它在经济上仍然如此有价值，并且收集关于各种白领工作任务的数据也足够容易，以至于按照 Sholto 的观点，我们应该预计在未来五年内看到它们被自动化。”

如果 AI 进展今天就完全停止，我认为不到 25% 的白领就业会消失。诚然，许多任务会被自动化。**Claude 4 Opus** 确实可以帮我重写自动生成的字幕。但由于我无法让它随着时间推移而改进并学习我的偏好，我仍然会雇佣人类来完成这项工作。因此，即使我们获得更多数据，如果没有在持续学习方面的进展，我认为我们在其他各类白领工作中也将处于大致相似的境地。是的，技术上 AI 可以相当令人满意地完成许多子任务，但它们无法建立上下文的能力，将使其无法作为实际员工在你的公司运营。

### 长期乐观与智能爆炸

虽然这让我对未来几年颠覆性 AI 的发展持悲观态度，但我对未来几十年的 AI 发展则非常乐观。当我们解决持续学习的问题时，我们将看到这些模型价值的巨大飞跃。即使没有软件领域的“奇点”（singularity，指 AI 能够快速构建更智能的后继系统），我们也可能看到一种广泛部署的“智能爆炸”（intelligence explosion）。AI 将被广泛部署到经济的各个领域，执行不同的工作并边做边学，就像人类一样。然而，与人类不同的是，这些模型可以整合它们所有副本的学习成果。因此，一个 AI 实际上是在学习如何胜任经济中的每一项工作。

### 持续学习的实现方式

一个能够进行这种在线学习的 AI，即使没有进一步的算法进步，也可能迅速成为超智能（superintelligence）。但我并不指望看到 **OpenAI** 的直播，宣布持续学习已被完全解决。由于研究机构有快速发布创新的激励机制，我们可能会先看到一个不完善的早期版本的持续学习（或测试时训练，或其他称呼），而不是真正像人类一样学习的东西。我预计在这个重大瓶颈完全解决之前，会有很多预警信号。

### 对 AI 代理能力的预测

当我采访 **Anthropic** 研究员 **Sholto Douglas** 和 **Trenton Bricken** 时，他们表示预计明年年底就能拥有可靠的计算机使用代理（computer use agents）。我们现在已经有了计算机使用代理，但它们相当糟糕。他们设想的是完全不同的东西。他们的预测是，到明年年底，你应该能够告诉 AI：“去处理我的税务。”它会浏览你的所有邮件、**Amazon** 订单和 Slack 消息，与所有你需要获取发票的人来回发送邮件，汇总所有收据，判断哪些是实际的业务开销，并在边缘情况下征求你的批准，然后向 **IRS**（美国国税局）提交 **Form 1040**。

### 对 AI 代理预测的质疑

我对此表示怀疑。我不是 AI 研究员，所以我不便在技术细节上反驳他们。但根据我有限的了解，以下是我不看好这一预测的原因：

一、随着任务时长的增加，执行过程必须变得更长。AI 需要完成两小时的代理计算机使用任务，我们才能看到它是否做得对。更不用说计算机使用需要处理图像和视频，这本身就更耗费计算资源，即使不考虑更长的执行时间。这似乎会减缓进展。

二、我们缺乏大规模的、多模态的计算机使用预训练语料库。我喜欢 **Mechanize** 关于自动化软件工程的文章中的这句话：“过去十年，我们一直被海量免费的互联网数据所‘宠坏’。这足以攻克自然语言处理，但不足以让模型成为可靠、称职的代理。想象一下，用 1980 年所有可用的文本数据来训练 GPT-4——即使你有必要的计算能力，数据也远远不够。”

我并非在实验室工作。也许纯文本训练已经能让你对不同用户界面（UI）的工作方式以及不同组件之间的关系有一个很好的先验认知。也许 RL 微调的样本效率非常高，以至于你不需要那么多数据。但我没有看到任何公开证据表明这些模型突然变得不那么“饥渴”了，尤其是在它们实践较少的领域。或者，也许这些模型是如此优秀的**前端编码器**，以至于它们可以为自己生成数百万个玩具 UI 来练习。

三、即使是那些回想起来似乎很简单算法创新，也花了很长时间才完善。**DeepSeek** 在其 R1 论文中解释的 RL 程序，从高层次看似乎很简单。但从 **GPT-4** 的开发和发布到 o1 的发布，却花了两年时间。当然，我知道说 R1/o1 很容易是极其傲慢的——它需要大量的工程、调试和对替代想法的筛选才能得出这个解决方案。但这恰恰是我的观点！看到实现“我们应该训练一个模型来解决可验证的数学和编码问题”这一想法花了多长时间，就让我觉得我们低估了解决更棘手的计算机使用问题的难度，因为那是在一个完全不同的模态下，数据量少得多。

### 对 AI 能力的肯定与展望

好了，泼完冷水。我不会像那些 **Hackernews** 上被宠坏的孩子一样，即使得到一只会下金蛋的鹅，也只会抱怨它叫声太响。你读过 **o3** 或 **Gemini 2.5** 的推理轨迹吗？它确实在推理！它在分解问题，思考用户的需求，对自己的内部思考做出反应，并在意识到自己走错了方向时纠正自己。我们怎么能轻易地说：“哦，是的，机器当然会思考，会产生很多想法，然后给我一个聪明的答案。机器就是这么做的。”

有些人过于悲观的原因之一是，他们没有在模型最擅长的领域与最智能的模型进行过互动。给 **Claude Code** 一个模糊的需求，然后坐等十分钟看它零样本（zero-shot）地完成一个可用的应用程序，这是一种惊人的体验。它是怎么做到的？你可以谈论电路、训练分布、RL 或其他任何东西，但最直接、最简洁、最准确的解释是，它是由一个“婴儿通用智能”（baby general intelligence）驱动的。此时，你的一部分会想：“它真的在起作用。我们正在制造智能机器。”

### 我的 50/50 赌注时间线

我的概率分布非常宽泛。我想强调，我确实相信概率分布。这意味着为应对一个可能在 2028 年出现的、不匹配的**超人工智能（ASI: Artificial Superintelligence）**所做的准备仍然非常有意义。我认为这是完全可能的结果。但以下是我会下 50/50 赌注的时间线：

一个 AI 能够像一个称职的总经理那样，在一周内为我的小企业端到端地处理税务：包括在不同网站上追踪所有收据，找到缺失的部分，与任何需要催要发票的人来回发送邮件，填写表格，并提交给 IRS。我估计这会在 **2028 年**实现。我认为我们在计算机使用方面还处于 **GPT-2** 时代。我们没有预训练语料库，模型正在为一个更稀疏的奖励进行优化，跨越一个更长的时间跨度，使用它们不熟悉的动作原语（action primitives）。话虽如此，基础模型已经相当智能，并且可能对计算机使用任务有很好的先验认知，加上世界上有更多的计算资源和 AI 研究人员，这些因素可能会相互抵消。为小企业准备税务，对于计算机使用来说，就像 GPT-4 对于语言一样。从 GPT-2 到 GPT-4 花了四年时间。

需要澄清的是，我并不是说 2026 年和 2027 年不会出现非常酷的计算机使用演示。GPT-3 非常酷，但实用性并不强。我只是说，这些模型将无法端到端地处理一个涉及计算机使用的、为期一周且相当复杂的项目。

### 人类式学习的实现时间

那么，关于 AI 何时能像人类一样，轻松、有机、无缝、快速地在任何白领工作中“边做边学”的预测。例如，如果我雇佣了一个 AI 视频编辑，六个月后，它将对我的偏好、我们的频道、对观众有效的内容有与人类同等深入的理解。我估计这将在 **2032 年**实现。虽然我没有看到明显的方法能将持续在线学习融入 LLMs 当前的模型架构中，但七年是非常长的时间！七年前，GPT-1 才刚刚问世。我认为，在接下来的七年里，我们找到某种方法让这些模型真正实现在岗学习，并非不可能。

### AGI 时间线的对数正态分布

此时你可能会想：“等等，你之前大肆宣扬持续学习是多么大的障碍，但你的预测却是我们距离一个至少看起来像是广泛部署的智能爆炸还有七年？”是的，你说得对。我预测在相对较短的时间内，世界将发生翻天覆地的变化。AGI 的时间线呈非常**对数正态分布（lognormal distribution）**。要么在本世纪末实现，要么就“泡汤”（this decade or bust）。（当然，并非如此绝对，更像是每年实现概率的边际递减——但这听起来没那么吸引人。）

### 计算与算法的未来

过去十年，AI 的进步主要得益于在最前沿系统上扩展训练计算量。每年增长超过 4 倍。这种趋势在本世纪末之后将无法持续，无论你考虑芯片、电力，还是用于训练的 GDP 占有率。2030 年之后，AI 的进步必须主要来自算法进步。但即使在那时，至少在深度学习范式下，所有触手可及的“低垂的果实”都将被摘取。因此，AGI 的年化实现概率将急剧下降。这意味着，如果最终落入我 50/50 赌注的较长一端，我们可能要到 2030 年代甚至 2040 年代才能看到一个相对正常的世界。但在其他所有可能性中，即使我们对当前 AI 的局限性保持清醒认识，我们也必须预期一些真正疯狂的结果。

### 结语与订阅

这最初是我在我的网站 dwarkesh.com 上发表的一篇博文。它显然受到了我在播客中与 Sholto 和 Trenton 的讨论的启发，尽管我在时间线上与他们意见相左，但之后我花了几个星期思考，梳理清楚了分歧所在以及我为何持有更长的时间线。我也为其他播客节目做类似的事情。我写下了我对 **Stephen Kotkin** 关于**斯大林（Stalin）**的数千页著作的一些思考，显然我们无法在一期两小时的采访中详尽覆盖。

所以，如果你想看到这些因播客而产生、以及为准备播客而撰写的额外内容和文章，你应该订阅我的博客和新闻通讯。你可以在 dwarkesh.com 上完成订阅。否则，下周我将与一位真正的嘉宾带来完整的一期节目。