---
author: Dwarkesh Patel
date: '2025-12-23'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=_zgnSbu5GqE
speaker: Dwarkesh Patel
tags:
  - llm
  - reinforcement-learning
  - agi
title: AI 与 AGI 的未来之路：强化学习的规模扩大
summary: 探讨 AI 与 AGI 的未来发展，强化学习的规模扩大及其在实现类似人类学习者方面的作用。
insight: ''
draft: true
series: ''
category: ai-ml
area: tech-work
project:
  - ai-impact-analysis
people:
  - Baron Millig
companies_orgs:
  - ''
products_models:
  - Gemini
  - GPT-3
media_books: []
status: evergreen
---
### 我们到底在扩展什么？AI 与 AGI 的未来之路

为什么一些人对强化学习的规模扩大持乐观态度，但却又拥有极短的时间线？如果我们在接近一种类似人类的学习者，那么基于可验证结果训练的整套方法注定失败。

目前实验室正在通过中间训练阶段，将一系列技能“烘焙”进这些模型中。有一整条供应链上的公司都在构建强化学习环境，教模型如何在网页浏览器中导航或使用 Excel 来建立财务模型。

然而问题在于，要么这些模型最终能以自我驱动的方式在实际工作中学习，使得所有目前的“烘焙技能”变得无用；要么它们无法做到这一点，那意味着 AGI 并不会很快到来。人类不需要在入职前反复练习每一款可能要用到的软件。

巴伦·米利格最近在他的博客中指出，当我们在看到前沿模型在各种基准测试中的改进时，不应该只关注规模的扩大和机器学习研究的新想法，还应意识到数十亿美元被支付给博士、医生和其他专家来编写问题并提供示例答案和推理，以针对这些特定能力。

这一点在机器人学中尤为明显。从某种基本意义上讲，机器人问题是一个算法问题而非硬件或数据的问题。人类在经过极少量训练后就能学会控制当前的硬件以完成有用的工作。所以如果真的存在一种类似人类的学习者，机器人学就会在很大程度上成为已解决的问题。但我们没有这种学习者，因此必须进入成千上万个不同的家庭练习数百万次如何拿盘子或叠衣服。

一些人辩称，如果我们能在未来五年内实现突破性的起飞，就必须依赖繁琐的强化学习来构建超级人类级别的 AI 研究者。然后，这些自动化的伊利亚（Ilia）可以去解决从经验中学习的稳健而高效的方法。但这让我想起一个老笑话：“我们在每一笔销售上亏钱，但我们会在销量上弥补回来。”这种自动化的研究者似乎能解决困扰人类半个世纪的 AGI 算法问题，而他们却不具备儿童级的基础学习能力。

<details>
<summary>Original English</summary>

I'm confused why some people have super short timelines yet at the same time are bullish on scaling up reinforcement learning a top LLMs. If we're actually close to a humanlike learner, then this whole approach of training on verifiable outcomes is doomed.

Now currently the labs are trying to bake in a bunch of skills into these models through mid-training. There's an entire supply chain of companies that are building RL environments which teach the model how to navigate a web browser or use Excel to build financial models.

Now either these models will soon learn on the job in a self-directed way which will make all this freebaking pointless or they won't which means that AGI is not imminent. Humans don't have to go through the special training phase where they need to rehearse every single piece of software that they might ever need to use on the job.

Baron Millig made an interesting point about this in a recent blog post he wrote. He writes, quote:

"When we see frontier models improving at various benchmarks, we should think not just about the increased scale and the clever ML research ideas, but the billions of dollars that are paid to PhDs, MDs, and other experts to write questions and provide example answers and reasoning targeting these precise capabilities."

You can see this tension most vividly in robotics. In some fundamental sense, robotics is an algorithms problem, not a hardware or a data problem. With very little training, a human can learn how to tell or operate current hardware to do useful work. So if you actually had a humanlike learner, robotics would be in large part a solved problem.

But the fact that we don't have such a learner makes it necessary to go out into a thousand different homes and practice a million times on how to pick up dishes or fold laundry.

Now one counter argument I've heard from the people who think we're going to have a takeoff within the next 5 years is that we have to do all this cludgy RL in service of building a superhuman AI researcher. And then the million copies of this automated Ilia can go figure out how to solve robust and efficient learning from experience. This just gives me the vibes of that old joke: we're losing money on every sale, but we'll make it up in volume.

Somehow this automated researcher is going to figure out the algorithm for AGI—which humans have been banging their head against—for better half of a century—while not having the basic learning capabilities that children have.

I find it super implausible. Besides even if that's what you believe, it doesn't describe how the labs are approaching reinforcement learning from verifiable reward. You don't need to pre-bake in a consultant skill at crafting PowerPoint slides in order to automate Ilia.

So clearly the lab's actions hint at a worldview where these models will continue to fare poorly at generalization and on-the-job learning thus making it necessary to build in the skills that we hope will be economically useful beforehand into these models.

</details>