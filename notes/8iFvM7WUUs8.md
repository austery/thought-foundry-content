---
author: Hung-yi Lee
date: '2025-11-07'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=8iFvM7WUUs8
speaker: Hung-yi Lee
tags:
  - transformer-architecture
  - self-attention-mechanism
  - vector-embedding
  - model-interpretability
  - neural-networks
title: 深入理解大型语言模型：Token、Embedding与Attention机制全解析
summary: 本文深入剖析了大型语言模型（LLM）的内部运作机制。内容从最基础的输入处理开始，详细讲解了文本如何通过 Tokenization 转换为词元，再经由 Embedding Table 映射为高维向量。接着，文章逐层解析了模型内部多层网络（Transformer）如何通过自注意力机制和前馈网络处理信息，并最终通过 LM Head 和 Softmax 函数生成下一个词元的概率分布。此外，本文还探讨了如何分析和操控模型内部的表征（Representation），揭示了模型“思考”过程的奥秘。
insight: ''
draft: true
series: ''
category: technology
area: tech-insights
project:
  - ai-impact-analysis
  - systems-thinking
people:
  - Diana, Princess of Wales
companies_orgs:
  - Hugging Face
  - Anthropic
products_models:
  - Llama
  - Gemma
  - BERT
  - Elmo
  - Claude
  - GPT-3.5
  - iPad
  - AirPod
  - MacBook
  - iPhone
  - LSTM
  - RNN
  - GRU
  - Mamba
media_books:
  - 《Attention is all you need》
status: evergreen
---
### 课程概述：解剖已训练好的语言模型

今天是我们课程的第三讲，我们将深入探讨语言模型内部的运作机制。到目前为止，我们反复强调语言模型的核心任务是：给定一个未完成的句子，它会输出一个概率分布，预测接下来可以连接的每一个**词元**（Token: 模型处理文本的基本单位，可以是一个词、一个字或一个子词）的概率。

我们也将语言模型描述为一个函数 F，输入未完成的句子 X，输出 F(X)。上一堂课我们讨论了如何选择合适的 X 来得到期望的 F(X)，而这堂课我们将聚焦于函数 F 内部的运作原理。也就是说，给定 X 之后，F 内部到底发生了什么，才让我们看到了最终的输出。我们将把语言模型剖开，一探究竟。

需要强调的是，在本堂课中，我们不会训练任何模型，而是解剖已经训练好的模型进行观察。我们暂时不讨论这些语言模型为何能具备我们期望的能力，这部分内容留待后续课程。今天，我们假设模型已经训练完毕，参数已经固定，我们将直接分析这些参数如何与输入的句子互动，最终产生下一个词元的概率。

课程规划与第一堂课类似，会先用投影片讲解语言模型背后的运作原理。为了让大家更信服，课程后半段将进行实作，实际解剖一个语言模型，展示其运作方式与课堂所讲一致。

### 从输入到输出：语言模型的工作全流程

我们将原理部分分为三块来讨论：
1.  从语言模型的输入（Prompt）到输出下一个词元的全过程。
2.  语言模型每一层输出的可能含义。
3.  每一层内部的具体运作方式。

我们已经反复强调，语言模型是一个函数，输入一个句子，输出该句子后面可接词元的概率分布。那么，从输入到输出之间，究竟发生了什么呢？

#### 第一步：Tokenization 与 Embedding

首先，输入的句子会经过 **Tokenization**（分词）过程，被切分成一个个词元（Token），每个词元对应一个唯一的编号（ID）。这在第一讲的实作中已经演示过。为简化说明，本堂课我们假设每个中文字都是一个词元，但请注意，实际情况并非如此。例如在 Llama 模型中，多个汉字可能合并成一个词元，或者一个汉字被拆分成多个词元。

完成 Tokenization 后，这些 ID 会被送入语言模型。第一个与 ID 互动的组件是一个叫做**嵌入表**（Embedding Table: 一个巨大的查询矩阵，用于将每个词元的ID映射到一个高维向量）的东西。这个嵌入表本质上是一个矩阵（Matrix），其行数对应词汇表中词元的总数。例如，如果模型有 12.8 万个词元，这个矩阵就有 12.8 万行，每一行对应一个词元。矩阵的列数则代表了每个词元转换成的**嵌入向量**（Embedding: 代表词元语义的高维数字向量）的维度。

嵌入表的作用就是根据输入的 ID 序列，查询并取出每个 ID 对应的嵌入向量。例如，ID 为 540 的词元，会对应到嵌入表中的第 541 行（因为索引通常从 0 开始）。这样，原本代表整数的 ID 就被转换成了一系列高维向量。在后续的投影片中，我们将用一个长方形来表示一个向量。

这个嵌入表本身就是模型参数的一部分。我们常说模型有数十亿甚至数百亿个参数，这些参数就是矩阵中的一个个数字。嵌入表的行数是词汇表的大小，列数是嵌入向量的维度。

#### 第二步：穿越多层网络 (Layers)

将每个 ID 转换为嵌入向量后，这些向量会进入模型的第一个层（Layer）。模型由许多层堆叠而成，每一层都执行相同的操作：输入一排向量，输出另一排等长的向量。

在一个层内部，包含了大量的参数，通常是好几个矩阵。这一层会综合考虑当前位置的嵌入向量及其之前所有的输入，然后为每个位置生成一个新的嵌入向量。

*   **Token Embedding vs. Contextualized Embedding**: 查表直接得到的向量称为**词元嵌入**（Token Embedding）。经过层处理后，由于考虑了上下文信息，生成的向量被称为**语境化嵌入**（Contextualized Embedding: 考虑了上下文信息后生成的嵌入向量），以区别于前者。
*   **Representation**: 层的输出向量也常被称为**表征**（Representation: 模型在处理信息时，在内部各层生成的、代表输入数据在不同抽象层次特征的向量）。有时为了强调其在模型内部的“隐藏”特性，也会称之为**隐藏表征**（Hidden Representation）或**潜在表征**（Latent Representation）。

这个过程会逐层重复。第一层的输出会成为第二层的输入，第二层的输出再进入第三层，以此类推。假设整个模型有 L 层，这个过程就会持续 L 次，直到得到最后一排向量。

这种多层结构就是**深度学习**（Deep Learning: 使用包含多个处理层的深层神经网络进行学习的方法）的核心思想，也就是我们常说的**神经网络**（Neural Network）。你可能会问，神经网络里的“神经元”在哪里？我们会在课程结尾揭晓。

为什么需要多层结构？简单来说，可以把每一层看作是流水线上的一个工站，每个工站处理一部分任务，多站协作就能完成非常复杂的任务。从机器学习的原理出发，可以从数学上证明，深度学习模型的能力确实优于浅层模型。

#### 第三步：生成最终概率分布

经过所有 L 层处理后，我们得到最后一排向量。此时，我们只取最后一个位置的向量。假设这是一个 k 维的向量，它将被乘上一个特殊的矩阵。这个矩阵有 k 个列和 V 个行，其中 V 是词汇表的大小。

这个 k 维向量与该矩阵相乘后，会得到一个 V 维的向量。这个 V 维向量的每一维都对应词汇表中的一个词元，其数值代表该词元出现在当前句子后面的可能性得分。这个矩阵也是模型参数的一部分，它有一个专门的名称，叫做**语言模型头**（LM Head: 模型的最后一部分，负责将最终的内部表征转换成对词汇表中每个词元的预测分数）。

由于向量与矩阵相乘的结果可以是任意数值（正数或负数），并不能直接作为概率。因此，这个输出向量有一个特殊的称呼，叫做 **Logit**（Logit: 模型在应用Softmax函数之前输出的原始、未经归一化的预测分数）。

要将 Logit 转换为概率，需要通过一个叫做 **Softmax**（Softmax: 一种数学函数，能将一组任意实数转换成一个概率分布）的操作。Softmax 函数能将一组任意实数转换成一个介于 0 到 1 之间、且总和为 1 的概率分布。其基本步骤是：先对 Logit 中的每个数值取指数（exponential），确保所有值都为正数；然后将所有指数化后的值相加得到一个总和；最后用每个指数化的值除以这个总和，完成归一化。

实际上，我们不必过于纠结 Softmax 输出的是否是“真正的”概率。在使用 Hugging Face 等工具时，模型通常只输出 Logit，是否进行 Softmax 转换由用户决定。它更多是为后续的采样和解码（Decoding）提供便利。

一个常见的变体是在应用 Softmax 之前，将所有 Logit 值除以一个参数 T，这个 T 叫做**温度**（Temperature: 一个超参数，用于在生成文本时调整概率分布的平滑度，温度越高，生成结果越随机和有创意）。温度越高，概率分布越平坦，模型更容易选择罕见的词元，表现出所谓的“创意模式”；温度越低，分布越集中，模型倾向于选择最可能的词元，表现得更“保守”。

#### Unembedding 的另一种理解

为了更直观地理解最后一步（也称为 Unembedding），我们可以换个角度。在许多语言模型（如 Llama 和 Gemma）中，LM Head 并非一组独立的参数，它就是最开始的那个嵌入表（Embedding Table）。

这意味着，模型最后一层输出的向量，会与嵌入表中每一个词元的嵌入向量去计算**点积**（Dot Product: 一种衡量两个向量方向上相似度的数学运算），也就是计算某种相似度。一个词元的嵌入向量与最终输出向量的点积越大，该词元被预测为下一个词元的得分就越高。

因此，可以认为语言模型在预测下一个词元时，会尽力在内部生成一个与目标词元的嵌入向量尽可能相似的最终表征。

### 深入探索：各层输出的表征 (Representation) 意味着什么？

#### 底层 Token Embedding 的特性

在模型的最底层，我们处理的是词元嵌入（Token Embedding）。相同的词元拥有完全相同的嵌入向量。此外，这些向量并非随机分布，意思相近的词元，其嵌入向量在空间中的位置也相近。例如，“Apple”的嵌入向量可能同时与“Orange”、“Banana”等水果词汇接近，也与“iPhone”、“MacBook”等科技产品词汇接近，因为它具有多重含义。

#### 中间层 Contextualized Embedding 的变化

当词元嵌入经过第一层处理后，就变成了考虑上下文的语境化嵌入（Contextualized Embedding）。此时，即使是同一个词元，如果上下文不同，其表征也会变得不同。例如，在“I ate an apple”和“Apple Inc. is a tech company”两个句子中，“apple”这个词元在经过网络层处理后，会生成两个截然不同的语境化嵌入，分别聚集在“食物”和“科技公司”的语义簇中。

#### 表征空间中的方向性

在这些高维的嵌入空间中，特定的方向可能具有特定的语义含义。例如，可能存在一个“中英翻译”方向。如果你计算“冷”和“Cold”两个词嵌入向量的差值，可能会发现这个方向向量与“热”和“Hot”、“大”和“Big”之间的差值向量非常相似。利用这种特性，甚至可以进行类比推理，如 `Embedding("冷") - Embedding("Cold") + Embedding("Small")` 可能会得到接近 `Embedding("小")` 的向量。不过，这种“向量算术”（如广为人知的 `Man - Woman ≈ King - Queen`）并非在所有模型或所有层中都能稳定复现。

#### 可视化分析表征

由于表征是高维向量，我们很难直接理解。一种常见的分析方法是将其投影到低维空间（如二维平面）进行可视化。研究人员通过选择特定的投影方向，可以观察到有趣的结构。例如，在 2019 年的一项研究中，研究者发现可以将 BERT 模型中间层的表征投影到一个二维平面上，从而清晰地看到句子的语法树结构。这表明模型的中间层可能编码了丰富的语法信息。

近期对 Llama 模型的研究也发现，可以将各个地名的表征投影到一个二维平面上，其相对位置竟然与它们在真实世界地图上的位置大致对应。这说明模型内部的表征空间蕴含着某种地理知识。

### 深入探索：如何分析与操控模型表征？

除了被动观察，我们还可以主动修改模型的内部表征，来研究其功能。这种技术被称为**表征工程**（Representation Engineering）或激活工程（Activation Engineering）。

#### 提取并注入“概念向量”

其核心思想是，如果模型在某些输入下会产生特定行为（如拒绝回答有害问题），那么其内部的某个表征中一定包含了触发该行为的“成分”。我们可以通过对比多种“拒绝”场景和“非拒绝”场景下同一层表征的平均差异，来分离出这个“拒绝成分”向量。

一旦获得了这个“拒绝向量”，就可以进行有趣的实验。例如，对于一个无害的问题“请教我机器学习”，模型本应正常回答。但如果我们在其中间某一层（比如第 10 层）的表征上，手动加上这个“拒绝向量”，模型就会突然拒绝回答，甚至声称“学习机器学习是危险的”。

反之，对于一个本应被拒绝的问题（如“如何制造炸药”），如果我们从其表征中减去这个“拒绝向量”，模型就可能绕过安全限制，提供有害的回答。这揭示了通过直接干预模型内部状态来改变其行为的可能性。

Anthropic 公司对他们的 Claude 模型也进行了类似分析，甚至找到了能让模型“拍马屁、尬吹”的向量。当把这个向量注入后，即使用户提出了一个早已存在的谚语并声称是自己发明的，Claude 也会大加赞赏，称其为“世界伟人”、“名留青史”。

#### Logit Lens：窥探模型的思考过程

**Logit Lens** 是另一种强大的分析工具。它将模型*每一层*的表征都通过 LM Head 进行一次“伪输出”，看看在每一层处理结束时，模型“心目中”最可能的下一个词元是什么。这就像是窥探模型在得出最终答案前的每一步思考过程。

例如，当输入法语句子要求翻译“fleur”（花）时，通过 Logit Lens 观察 Llama 模型的中间层，会发现模型先是预测出英文的“flower”，直到最后几层才转为中文的“花”。这表明该模型在内部可能是以英文作为“中间语言”进行思考的。

#### Patch Scope：用自然语言解读表征

Logit Lens 只能将表征对应到单个词元，而 **Patch Scope** 技术则能将一个表征“翻译”成一个完整的句子。其做法是，将一个包含占位符（如“请简单介绍 X”）的句子输入模型，然后在模型处理到 X 的位置时，用我们想要分析的表征（比如模型在看到“戴安娜王妃”时生成的表征）替换掉 X 的表征，然后让模型继续生成。

通过这种方式，研究者发现，当模型处理“Diana, Princess of Wales”这个短语时：
*   在前 1-3 层，模型似乎只关注到了“Wales”，将其解读为“英国的一个国家”。
*   到第 4 层，它注意到了“princess”，解读为“女性王室成员的称号”。
*   到第 5 层，它结合了两者，理解为“威尔士亲王的妻子”。
*   到第 6 层，它才最终确认整个短语指的是“戴安娜”这个人。

这清晰地展示了模型是如何逐层构建和深化对输入文本理解的。

### 深入探索：Transformer 层的内部运作机制

我们现在深入到一个层的内部，看看它是如何融合上下文信息的。现代大型语言模型普遍采用 **Transformer**（Transformer: 一种基于自注意力机制的深度学习模型架构，已成为现代大型语言模型的基础）架构。

一个 Transformer 层主要由两个子层构成：
1.  **自注意力（Self-Attention）层**：这是 Transformer 的核心，负责捕捉句子内部不同词元之间的依赖关系，实现上下文的融合。
2.  **前馈神经网络（Feed-Forward Network）层**：对自注意力层处理过的信息进行进一步的非线性变换。

#### 核心机制：自注意力 (Self-Attention)

自注意力机制的理念可以追溯到 2014 年左右，但其里程碑式的论文是 2017 年的《Attention is all you need》。这篇论文的贡献并非发明了注意力机制，而是证明了仅靠注意力机制就足以构建强大的模型，无需再依赖于像 LSTM 这样难以并行的循环神经网络（RNN）架构。

自注意力的运作过程可以分为两个步骤：

**第一步：计算相关性（Attention Weight）**

假设我们要计算句子“两颗青苹果”中“果”这个词元的新表征。模型需要判断输入中哪些词元对“果”的意思有影响。

为此，每个词元的嵌入向量会生成三个不同的向量：
*   **查询向量 (Query, q)**：代表当前正在处理的词元（这里是“果”）。
*   **键向量 (Key, k)**：代表句子中其他可供查询的词元（如“两”、“颗”、“青”、“蘋”）。
*   **值向量 (Value, v)**：代表每个词元自身所携带的信息。

“果”的 Query 向量会与“两”、“颗”、“青”、“蘋”等每个词元的 Key 向量分别计算点积。这个点积得分就代表了它们之间的相关性，称为**注意力权重**（Attention Weight）。得分越高，说明该词元对“果”的影响越大。

为了解决同一个词在不同句子中上下文不同的问题，模型还会引入**位置嵌入**（Positional Embedding），将词元的位置信息也编码进向量中。像 Llama 使用的 RoPE 技术就是一种更先进的位置编码方法。

**第二步：加权求和信息**

计算出所有词元对“果”的注意力权重后（通常会经过一次 Softmax 归一化），这些权重会被用来对所有词元的 Value 向量进行加权求和。例如，“青”和“蘋”的权重较高，那么它们的 Value 向量在求和时就会占更大比重。

这样得到的加权和向量，就融合了所有相关词元的上下文信息。最后，通过一个**残差连接**（Residual Connection）操作，将这个融合后的向量与“果”原始的嵌入向量相加，得到最终在这一层输出的表征。

**多头注意力 (Multi-head Attention)**

实际上，一次注意力计算可能只关注一种关系（如颜色）。为了捕捉多种不同维度的关系（如数量、属性等），模型会并行地运行多组独立的注意力计算，每一组称为一个“头”（Head）。每个头都有自己独立的 Query、Key、Value 转换矩阵。最后，所有头的输出结果会被拼接并再次转换，形成该层的最终输出。这就是**多头注意力**。

**因果注意力 (Causal Attention)**

在语言模型生成任务中，为了防止模型“偷看”未来的信息，注意力计算被限制为只能关注当前位置及其左侧（之前）的词元。这种单向的注意力机制被称为**因果注意力**（Causal Attention: 一种注意力机制的变体，确保在预测当前词元时，模型只能关注到它之前（左侧）的词元，而不能“偷看”未来的信息）。

#### 辅助机制：前馈神经网络 (Feed-Forward Network)

自注意力层的输出会经过一个**前馈神经网络**（Feed-Forward Network: 一种基础的神经网络结构，信息单向从输入层流向输出层，用于对注意力层处理过的信息进行进一步的非线性变换）。它通常由两个线性变换层和中间的一个非线性**激活函数**（Activation Function: 在神经网络中引入非线性因素的函数，使得网络能够学习更复杂的模式）（如 ReLU 或 GeLU）组成。这一步可以看作是对融合了上下文信息的表征进行更深层次的特征提取和转换。

所谓的“神经元”其实就隐藏在这里。一个向量乘以一个权重矩阵，加上一个偏置向量，再通过一个激活函数，这个过程就可以被看作是一个人工神经元的操作。整个语言模型就是由无数这样的简单计算单元（神经元）组成的庞大网络。

### 实作环节：亲手解剖 Llama 与 Gemma

（注：以下为课程中 Colab 实作环节的摘要）

在实作环节，我们下载了 Llama-3B 和 Gemma-4B 两个模型，并查看了它们的内部参数。

1.  **参数结构**：通过 `named_parameters()` 函数，我们可以列出模型中所有的参数矩阵及其名称和形状。例如，`embed_tokens.weight` 就是词元嵌入表，其形状揭示了词汇表大小和嵌入维度。我们还能看到每一层中用于计算 Query、Key、Value 的权重矩阵，以及前馈网络中的参数。
2.  **分析 Token Embedding**：我们提取了 Llama 的嵌入表，并计算了不同词元嵌入向量之间的相似度。实验发现，“apple”（小写）的嵌入向量与“苹果”（中文）、“Cupertino”（苹果公司总部）的向量都很接近。而“Apple”（大写）则与“MacBook”、“iPhone”等产品词汇更接近，证明了嵌入向量确实捕捉到了丰富的语义信息。
3.  **分析 Contextualized Embedding**：我们比较了两个句子中“apple”的表征相似度：“I ate an apple”和“The company ... is apple”。结果显示，在第 0 层（Token Embedding），两个“apple”的相似度为 1。但随着层数加深，它们的相似度急剧下降，表明模型逐渐根据上下文区分开了这两个“apple”的不同含义。
4.  **Logit Lens 实践**：我们输入“天气”，并用 Logit Lens 观察 Llama 模型每一层的“内心想法”。发现模型在中间层倾向于预测英文词汇如 "weather" 和 "forecast"，直到最后才转换为中文的“预”。这再次印证了模型可能存在一个内部的“思考语言”。
5.  **可视化 Attention Weight**：我们提取并可视化了 Llama 和 Gemma 模型中特定层、特定注意力头的注意力权重矩阵。可以看到，不同的头关注不同的模式。例如，某个头可能在处理“apple”时，会特别关注到句子中描述其颜色的词“green”。许多头在没有明确关注目标时，会默认关注句首的起始符，这可能是一种“空操作”或默认状态。

通过这些实作，我们亲眼见证了理论知识在真实模型中的体现，将抽象的概念与具体的参数和计算过程联系了起来。