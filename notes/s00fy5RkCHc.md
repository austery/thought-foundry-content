---
area: tech-engineering
category: ai-ml
companies_orgs: []
date: '2025-08-19'
draft: true
guest: ''
insight: ''
layout: post.njk
products_models: []
project: []
series: ''
source: https://www.youtube.com/watch?v=s00fy5RkCHc
speaker: Best Partners TV
status: evergreen
summary: 谷歌DeepMind的丹尼·周在斯坦福大学的讲座系统梳理了AI推理能力的发展。本文深入解析大语言模型推理的本质、思维链、自洽性、监督微调与自我进化范式，以及检索增强等核心技术，并探讨了当前面临的挑战。
tags:
- llm
- self
title: 丹尼·周解读大语言模型推理的本质、技术演进与未来挑战
---
### 引言：AI推理能力的巨大反差与核心谜题

许多接触**大语言模型**（Large Language Models, LLM: 能够理解和生成人类语言的AI模型）的用户都曾有过这样的经历：当向AI提出一个复杂问题时，它不仅能给出答案，还能提供详细、一步一步且逻辑严密的解题过程，让人感觉屏幕对面的不再是冰冷的代码，而是一个真正能够“推理”的智能体。然而，有时当用户提出一个同样复杂但略有不同的问题时，AI又会给出离谱的错误答案，让人觉得它只是一个更高级的“复读机”。这种体验上的巨大反差，正是当前AI领域最核心的谜题之一：大语言模型展现出的“推理能力”，究竟是一种真正的智能涌现，还是一种基于海量数据训练出的高级“模式匹配”？它是在进行逻辑推导，还是在模仿网上看过的无数解题步骤？

关于这个问题，学术界和工业界争论不休。但争论的意义远不如去搞清楚这种所谓的“推理能力”到底是怎么来的，以及如何才能稳定、可靠地驾驭它。要解答这个问题，**谷歌 DeepMind**（Google DeepMind: 谷歌旗下的AI研究公司）的**丹尼·周**（Denny Zhou: 谷歌DeepMind研究员，在大语言模型推理领域做出开创性贡献）是一个绕不开的名字。他和他的团队奠定了我们今天理解和使用大语言模型推理能力的基石，开创性地提出了像**思维链提示**（Chain-of-Thought Prompting, CoT Prompting: 一种通过提供包含中间推理步骤的示例来引导模型生成详细解题过程的提示工程技术）和**自洽性**（Self-Consistency: 一种通过多次采样模型输出并选择出现次数最多的最终答案来提高推理可靠性的方法）这样的关键技术，并深度参与了**谷歌 Gemini 模型**（Google Gemini Model: 谷歌开发的多模态大语言模型）推理能力的构建。

最近，丹尼·周在**斯坦福大学**（Stanford University: 位于美国加利福尼亚州斯坦福市的私立研究型大学）做了一场讲座，系统性地梳理了从他创立谷歌大脑的推理团队开始，到今天所看到的强大AI，这条技术路线是如何一步步演进的。这场讲座信息量巨大，不仅揭示了AI推理能力的本质，更是对过去几年中所有相关技术的一次“祛魅”。本文将以丹尼·周的这场讲座为蓝本，从最基础的概念出发，层层递进，彻底搞懂大语言模型“思考”的秘密，揭示那些看似神奇的技术背后所遵循的简单而深刻的原理。

### 定义“推理”：中间步骤的重要性

在深入探讨之前，必须先明确一点：当谈论大语言模型的“推理”时，到底在谈论什么？丹尼·周在讲座一开始就抛出了一个非常清晰且可操作的定义，这个定义也成为了整个领域的共识。他表示，关于模型到底会不会推理的哲学辩论，他从不参加，因为没有一个明确的定义，大家都是在自说自话。而在他的团队里，“**推理**”（Reasoning: 在模型的输入和最终输出之间生成的所有“中间步骤”）有一个非常具体的含义，那就是在模型的输入（即你的问题）和最终输出（即答案）之间，生成的所有“中间步骤”（intermediate tokens）。

这个定义非常关键，它把一个模糊的、哲学层面的“思考”概念，转化成了一个具体的、工程上可以实现和优化的目标。为了更好地理解这一点，丹尼·周设计了一个巧妙的任务，叫做“**末尾字母拼接**”（Last Letter Concatenation: 丹尼·周设计的一个任务，用于具体展示模型生成中间步骤的推理过程）。例如，如果模型被要求拼接“artificial intelligence”这两个单词的末尾字母，如果我们直接让模型输出答案，它可能会凭借着语言的惯性，直接猜一个答案，比如“LE”。此时它只是在预测下一个最可能的字符，而不是在执行一个多步骤的逻辑操作。但是，如果我们引导模型先生成“中间步骤”，它的输出就会变成这样：“artificial的最后一个字母是l，intelligence的最后一个字母是e，将l和e拼接起来，得到le。”这就是丹尼·周所定义的“推理”。它把一个复杂任务分解成了一系列简单的、可执行的子任务，最终导出了正确的答案。

这似乎与人类解决问题的方式——先思考，再作答——异曲同工。然而，丹尼·周提醒研究者，必须时刻记住大语言模型不是人类，它们只是概率模型，将其拟人化虽然有助于理解，但也容易走入歧途。一个有趣的故事是，丹尼·周最初尝试的是“首字母拼接”，但他发现当时所有的模型都能做得很好。原因在于互联网上有大量的缩写词（initiatives），模型在预训练阶段已经“背”会了如何拼接首字母。于是他换成了“末尾字母拼接”，结果当时所有的模型都失败了。这恰恰说明模型并没有真正“理解”拼接这个动作，而只是记住了某种常见的模式。

### 为什么中间步骤至关重要：理论依据

那么，为什么要如此执着于生成这些“中间步骤”呢？仅仅是为了模仿人类吗？当然不是。这背后有非常坚实的理论依据。丹尼·周提到了他们和斯坦福大学教授滕尚华团队合作的一项理论研究，该研究得出了一个非常强大的结论：对于任何一个可以被大小为T的**布尔电路**（Boolean Circuit: 一种由逻辑门组成的数学模型，用于表示和执行逻辑运算，可以分解任何复杂的计算任务）解决的问题，一个常数大小的**Transformer 模型**（Transformer Model: 一种基于自注意力机制的深度学习模型架构，广泛应用于自然语言处理领域）可以通过生成O(T)长度的中间步骤来解决它。

这句话听起来有些技术性，可以将其翻译为：布尔电路可以被看作是执行逻辑运算的基本单元，任何复杂的计算任务，例如运行一个大型软件，本质上都可以被分解成一个巨大规模的布尔电路。这里的“大小T”就代表了问题的计算复杂度。这个理论告诉我们，哪怕是一个相对简单的Transformer模型，只要允许它生成足够长的“思考过程”（即中间步骤），它就有潜力解决几乎任何可计算的问题。反过来说，如果强制模型直接蹦出最终答案，就相当于要求这个模型的网络深度本身要能够模拟整个复杂的计算过程，这要么需要一个巨大到不切实际的深度，要么就根本无法解决问题。

因此，让模型“思考”并生成中间步骤，不是一个可有可无的选项，而是在计算原理上解锁模型解决复杂问题能力的一把“金钥匙”。这彻底改变了我们训练和使用大语言模型的范式，从单纯地追求“答案”，转向了追求“过程”。

### 如何生成“过程”：解码的奥秘

既然“推理过程”如此重要，那么下一个问题自然就是如何让模型来生成这个过程。丹尼·周在这里提出了一个颠覆当时很多人认知的观点：当时普遍认为，一个普通的、只经过预训练的大语言模型是不会推理的，必须通过像思维链提示（CoT prompting）这样的高级技巧，或者进行专门的微调，才能教会它推理。但是丹尼·周认为这个观点是错误的，而且大错特错。他认为预训练模型早就已经准备好进行推理了，我们所需要做的仅仅是改变“**解码过程**”（Decoding Process: 大语言模型根据其内部概率分布生成文本序列的过程）。

这又是一个非常深刻的洞察。为了证明这一点，他举了一个经典的数学应用题：“我有3个苹果，我的爸爸比我多2个苹果，我们总共有多少个苹果？”如果把这个问题直接输入给一个原始的预训练模型，比如早期的GPT-3或LLaMA，然后使用默认的“**贪婪解码**”（Greedy Decoding: 一种简单的解码策略，在生成每个词时总是选择当前概率最高的词）方式，模型很可能会直接输出一个看似合理但却是错误的答案，比如“5个苹果”。因为它看到了“3个”和“多2个”，就直接联想到了“5”。这是模型的一种直觉反应，或者说是一种系统思维。

但是，模型的强大之处在于，它的输出概率分布中并不仅仅只有这一个选项。在生成第一个词的时候，“5个”可能是概率最高的，但是还有第二、第三、第四高的选项。如果我们不那么“贪婪”，而是去探索一下那些概率稍低一些的“岔路”，奇迹就会发生。丹尼·周展示了这些隐藏的“候选答案”：
*   候选二可能以“我”字开头，模型会生成：“我有3个苹果，我爸爸比我多2个，所以他有5个苹果。3 + 5 = 8。所以我们总共有8个苹果。”这是一个完美的推理链，答案也正确。
*   候选三可能以“我们”开头，模型会生成：“我们总共有8个苹果。”虽然没有过程，但是答案也对了。
*   候选四可能以“你”字开头，模型会生成：“你有3个苹果，你爸爸有 3 + 2 = 5 个苹果。你们总共有 3 + 5 = 8 个苹果。”这同样是一个清晰的推理链。

正确的推理路径其实一直都存在于模型的输出空间里，它们就像是隐藏在主干道旁边的小路，默认的“贪婪解码”因为只看了眼前最宽的路，所以错过了它们。这个发现被称为“**思维链解码**”（Chain-of-Thought Decoding: 一种观察，即正确的推理路径在模型输出空间中以较低概率的候选形式存在，通过探索这些路径可以找到正确答案）。它告诉我们，推理能力不是被“注入”到模型里的，而是模型在学习海量文本中蕴含的逻辑关系后自然“涌现”出来的。于是，我们的任务从“教会”模型推理，变成了如何引导模型把它已经知道的东西以正确的形式表达出来。

### 优化解码：答案置信度与提示工程

那么，在这么多候选的输出里，我们怎么知道哪一个是最好的呢？一个简单的想法是看长度，带有思考过程的回答通常更长。但是丹尼·周的团队发现了一个更可靠的指标，那就是“**答案置信度**”（Answer Confidence: 模型在生成某个特定词，尤其是最终答案时给出的内部概率，通常高置信度与正确答案相关）。他们观察到一个惊人的现象：对于那些包含了正确思维链的回答，模型在生成最终答案那个词（比如数字“8”）的时候，其内部的置信度，也就是概率，会异常地高。在这个苹果的例子里，模型预测“8”这个词的概率可能高达98%。这是一个非常强的信号，因为对于一个拥有巨大词汇表的模型来说，通常每个词的概率都接近于零。这就像一个人在经过深思熟虑后，对自己得出的结论会非常笃定一样。

所以，“思维链解码”的核心就两步：一是超越贪婪解码，生成并检查更多的候选输出；二是选择那个对“最终答案”置信度最高的候选。这个方法虽然简单有效，但还是需要写一些代码，对于普通用户来说不够友好。于是，研究者们开始思考，能不能用更自然的方式，比如自然语言，来“重塑”模型的输出概率分布，让那些带有思考过程的优秀答案能够“自动”排到第一名，这样我们用最简单的贪婪解码就能直接得到它呢？这就引出了后来耳熟能详的一系列提示工程技术。

首先最著名的就是思维链提示。它的做法非常直观：在你提出你的问题之前，先给模型看一两个类似的、从问题到思考过程再到答案的例子。比如，你想让模型解决前面那个苹果问题，你可以先给它一个例子：
*   问题：一个农民有5个香蕉，他又买了6个，后来吃了2个，还剩几个？
*   答案：农民开始有5个香蕉。买了6个后，他有 5+6=11个。然后他吃了2个，所以他剩下 11-2=9个。答案是9。

然后，你再提出你的问题：“我有3个苹果，我爸爸比我多2个苹果，我们总共有多少个苹果？”神奇的事情发生了，模型会“模仿”你给出的例子的风格，自动地开始一步步分析，生成详细的解题步骤，最后给出正确答案。从概率分布的角度看，你给出的例子极大地提升了模型生成类似“思考过程”的句式的概率，从而把原本隐藏在后面的正确推理路径推到了最前面。

但是这种方法有个问题：你需要为不同类型的任务手动编写高质量的示例，这很麻烦。而且如果你自己都知道怎么解决一个类似的问题，那你为什么还要问AI呢？于是，一个更加“神奇”的提示出现了，它就是“让我们一步步思考”（Let's think step-by-step）。丹尼·周坦言，这篇论文刚出来的时候，他以为这是个玩笑，怎么可能在问题后面加上这么一句简单的话，模型就会自动开始思考了呢？他当时就在谷歌内部的PaLM模型上做了测试，他非常清楚PaLM的训练数据里绝对没有针对这个“咒语”做过任何的优化。结果，他震惊地发现，它真的有效！模型真的开始输出一步步的解题过程了。

这个发现极大地启发了他。尽管“Let's think step-by-step”这种**零样本提示**（Zero-Shot Prompting: 不提供任何示例，仅通过指令引导模型执行任务的提示方法）效果通常比不过提供具体示例的**少样本思维链提示**（Few-Shot Chain-of-Thought Prompting: 通过提供少量问题-思考过程-答案的示例来引导模型进行推理的提示方法），但是它证明了我们可以用非常通用的方式来激发模型的推理潜能。

然而，无论是哪种提示方法，都感觉有点“奇怪”。想象一下，你问一个聪明人问题，还必须得在后面加上一句“请一步步思考”，否则他就不会思考了。这显然不符合我们对一个真正智能体的期望。所以，我们需要一种更稳定、更内化的方法，让推理能力成为模型固有的一部分，而不是需要外部“咒语”来触发。这就把我们带到了下一个阶段——微调。

### 从监督微调到自我进化：新范式的崛起

首先是“**监督微调**”（Supervised Fine-Tuning, SFT: 一种机器学习技术，通过在带有标签（例如问题和人类手写的解题步骤）的数据集上训练预训练模型，使其适应特定任务）。它的思路非常直接：我们不就是希望模型能生成从问题到思考过程再到答案这样的数据吗？那我们就雇佣一批人，针对大量的问题，手写出高质量的、一步一步的解题方案。然后，我们把这些“标准答案”喂给模型，让模型去学习。这个方法在机器学习里叫“**最大似然估计**”（Maximum Likelihood Estimation: 一种统计方法，用于估计模型参数，使得观察到现有数据的概率最大化），简单来说，就是让模型生成的序列跟人类专家写的序列尽可能地一模一样。

这个想法其实很早就有了。丹尼·周提到，早在2017年DeepMind的一篇论文就在做类似的事情，他们收集了一批数学应用题和人类手写的解题步骤来训练一个序列模型。后来在2021年，OpenAI更进一步，构建了一个更大、更著名的数据集，也就是**GSM8K**：一个包含8000多个小学水平数学应用题及其详细解法的公开数据集，用于微调大语言模型，用来微调GPT-3模型。这种方法训练出来的模型，在你给它一个新问题的时候，确实能够生成不错的解题步骤，看起来问题似乎解决了，一旦模型训练好，就可以随处部署，不再需要复杂的提示了。

然而，在2021年夏天，丹尼·周的团队发现了一个严重的问题：SFT训练出来的模型**泛化能力**（Generalization Ability: 模型在未见过的新数据上表现良好的能力）很差。它在那些和训练数据很相似的问题上表现很好，但是一旦遇到一个新的、类型稍微不同的问题，就很容易失败。他们尝试了“大力出奇迹”的方法，扩大数据规模，找更多的人，标注更多的数据。可惜结果却是，无论如何扩大规模，这个问题始终存在。丹尼·周在这里给出了一个重要的教训：不要盲目地扩大规模，当你的范式本身是错误的时候，再多的数据也无济于事。

那么，SFT的范式错在哪了呢？问题又出在流程里的哪一步呢？丹尼·周给出的答案可能会让你大吃一惊，他说错误出在“人”身上。这个转折点来自于**自我提升**（Self-Improve）或**STaR**（Self-Taught Reasoner: 一种新的训练范式，模型通过自己生成多样化的解题步骤，并利用验证器筛选出正确的结果作为训练数据，从而进行自我强化学习）的方法。当他第一次听到“机器生成的训练数据可能比人类专家写的还好”这个想法的时候，他自己也感到非常惊讶。

这个新范式的流程是这样的：首先我们仍然从一批问题开始，但是我们不再找人类去写解题步骤。我们让一个已经比较强大的大语言模型自己去针对这些问题，生成大量的、多样的解题步骤。最关键的一步是，我们用一个“**验证器**”（Verifier: 一个能够自动检查模型生成结果，例如数学题的答案是否正确的组件）去检查模型生成的这些解题步骤，看哪个最终得出了正确的答案。比如对于数学题，我们知道标准答案就可以直接判断。于是，我们只保留下来那些“过程多样但结果正确”的生成结果，把它们当作新的、高质量的训练数据。然后用这些由模型自己生成、并且经过验证的“好数据”，再去微调模型自己。这个过程可以不断地迭代，一个微调后变得更强的模型又可以去生成质量更高、更复杂的解题步骤，用来进一步训练自己。这就形成了一个“自我进化”的闭环。丹尼·周提到，一篇在2024年1月发表的、来自字节跳动的论文《Reasoning with Reinforced Fine-Tuning》，是他学术界看到的、最早公开阐述类似思想的出版物之一。他相信，在OpenAI等多个机构内部，大家可能都独立地发现了这个简单而又极其有效的思想。

### 自我进化的威力：机器学习的第一性原理

现在，我们必须回答那个核心问题：为什么模型自己生成的数据会比人类专家手写的数据在训练上效果更好呢？这背后其实蕴含着机器学习的一个第一性原理，那就是直接优化你想要的东西。在SFT的范式里，我们优化的目标是让模型的输出模仿人类的解题步骤，我们假设人类的思维过程就是最优的。但实际上，人类的思维方式千差万别，充满了跳跃和不一致，而且人类专家写的“标准答案”，对于模型来说，可能并不是最容易学习和泛化的路径。

而在新的范式里，我们的目标变了，我们不再关心模型的解题过程是否和人类一模一样，我们只关心一件事：它最终的答案是否正确。我们用“最终答案的正确性”这个指标，相当于**强化学习**（Reinforcement Learning: 一种机器学习范式，智能体通过与环境互动，根据奖励信号学习最优行为策略）里的奖励信号，来指导模型的学习。这在数学上就等同于我们要求解一个**策略梯度**（Policy Gradient: 强化学习中一类直接优化策略函数，即智能体行为方式的算法）问题，模型需要调整自己的参数，使得生成能够获得高奖励的序列的概率最大化。丹尼·周强调，我们不需要用“激励模型去思考”这种拟人化的、神秘的语言来描述这个过程，本质上就是三件事：定义你的目标（Metric），计算梯度（Gradient），然后**反向传播**（Back Propagation: 一种在神经网络中计算损失函数梯度的方法，用于更新模型权重）。这就是机器学习的全部。

通过这种方式，模型会自己去探索什么样的“思考过程”能够最稳定、最泛化地导向正确答案。这些过程可能看起来跟人类的思维不完全一样，但是它们更符合模型自身内部结构的学习路径。这个范式的转变，威力是巨大的。它也让我们明白，在整个“自我进化”的循环中，最最关键的环节不是什么花哨的强化学习算法，而是那个“验证器”。一个可靠的、能够自动判断答案好坏的验证器，是整个新范式的基石。这让丹尼·周想起了加拿大计算机科学家、强化学习之父**理查德·萨顿**（Richard Sutton: 加拿大计算机科学家，被誉为“强化学习之父”）在2001年写的一篇文章标题——《验证是通往人工智能的关键（Verification is the Key to AI）》。二十多年前的洞见，在今天的大语言模型时代得到了完美的印证。

### AI智慧的本质：学习与启发式推理

通过这种“自我进化”的方式训练出来的模型，推理能力达到了一个前所未有的高度。它所展现出的“智慧”，与经典的人工智能有着本质的不同。丹尼·周在这里引用了一句名言，来自国际象棋大师加里·卡斯帕罗夫在1997年输给IBM的“**深蓝**”（Deep Blue: IBM开发的一台国际象棋超级计算机，于1997年击败了世界冠军加里·卡斯帕罗夫）后说的话。他说，深蓝的智能就像你给闹钟编程让它准时响起一样，是程序化的智能。卡斯帕罗夫说得没错，深蓝的强大来自于**穷举式搜索**（Exhaustive Search: 一种解决问题的方法，通过系统地检查所有可能的解决方案来找到正确答案），它会暴力计算未来几步甚至几十步棋的所有可能性，然后选择最优解。这是经典AI的核心思想。

但是，大语言模型的推理完全不同，它是一种类人的、**启发式推理**（Heuristic Reasoning: 一种利用经验法则、直觉或近似方法来寻找解决方案的推理过程，通常在计算资源有限或问题过于复杂时使用）过程，是从海量的语言数据中“涌现”出来的，而不是依赖于任何显式的、暴力的搜索。为了展示这一点，丹尼·周分享了一个令人拍案叫绝的例子，这个例子来自谷歌内部的一个模型。问题是这样的：“请使用数字1到10，每个数字只能用一次，通过加法和乘法运算，得到结果2025。”这是一个非常难的组合优化问题，如果用传统方法，你需要写一个程序去进行暴力搜索，尝试各种组合。但是让我们看看这个Gemini模型是怎么“思考”的。

丹尼·周展示了模型在生成最终答案之前内部的思考过程。模型首先判断2025是一个相对较大的数字，这表明乘法将在其中扮演重要的角色。这是一个非常像人类的启发式判断。然后，模型突然冒出了一个惊人的洞察：“值得注意的是，2025是45的平方。”丹尼·周坦言，他自己出这道题的时候都完全没有意识到这一点，这给解决问题提供了一个巨大的线索。接下来模型的思考继续深入：“目标很大，我们应该考虑如何得到较大的中间乘积。我们的目标是构建一些乘积，让它接近2025的平方，也就是45。”在经过一长串类似这样的自我对话和推理之后，模型最终给出了答案，并且它的答案完美地遵循了自己的思考路径。它将1-10的数字分成了两组，每组都通过运算得到了45：第一部分：10x4+5=45；第二部分：9x3+8+7+2+1=45。最后模型将两个45相乘，得到了2025。整个过程没有任何穷举搜索，模型就像一个顶尖的数学家，通过洞察、启发式思考和目标分解，一步步逼近了答案。

这个例子有力地回应了理查德·萨顿在他著名的文章《**苦涩的教训**》（The Bitter Lesson: 理查德·萨顿提出的人工智能发展经验总结，指出最终能够规模化成功的只有“学习”和“搜索”两种方法）中提出的观点。萨顿在看到**AlphaGo**（AlphaGo: 谷歌DeepMind开发的人工智能围棋程序，于2016年击败了世界围棋冠军李世石）的成功后总结道，人工智能领域几十年的研究表明，最终能够规模化（scale）并且取得成功的，只有两种方法：学习（Learning）和搜索（Search）。但是丹尼·周在这里对这个“苦涩教训”提出了一个更进一步的看法：也许，我们只需要学习就足够了。一个通过大规模学习训练出来的模型，它的内部涌现出的推理能力本身就可以完成过去需要依赖搜索才能完成的任务。当然，这并不是说搜索完全没用，搜索可以作为一种外部工具被模型调用，就像我们使用计算器一样，但是在构建模型的核心推理能力时，重点应该放在“学习”上。

### 提升推理可靠性：聚合与检索

通过强化学习微调训练出来的模型已经非常强大，但这还不是终点。丹尼·周接着介绍了两种在“推理时”（inference time）进一步压榨模型性能、提升结果可靠性的前沿技术。

**1. 聚合与自洽性**

我们首先要回到一个根本性的问题上：大语言模型在生成答案时，它的数学本质是什么？我们前面提到，默认的“贪婪解码”是选择思考过程+答案整个序列联合概率最高的那一个。但是，我们作为用户，真的关心它的推理过程是不是最优美、最高概率的吗？不，我们只关心一件事：哪个“最终答案”本身是正确的。显然，这两个数学目标是完全不一样的。后一个目标需要把所有可能导向这个答案的“推理过程”的概率全部加起来。这个过程在数学上叫做“**边际化**”（Marginalization: 在概率论中，通过对联合概率分布中的部分变量求和（或积分）来得到边缘概率分布的过程）。直接计算这个值非常困难，因为可能的推理路径是无穷的。但是我们可以用一个非常简单的方法来近似它，这个方法就是“自洽性”。

“自洽性”的操作极其简单，那就是我们不再使用确定性的贪婪解码，而是开启“**随机采样**”（Random Sampling: 一种解码策略，模型在生成每个词时根据概率分布随机选择，而不是总是选择概率最高的），让模型针对同一个问题，像掷骰子一样生成许多个不同的、多样的、从推理过程到答案的序列。你会看到，模型可能会因为一些微小的随机扰动，走上完全不同的推理路径，得出不同的答案。比如对于一个数学题，它可能在30次生成中得出答案是18，在20次中得出答案是26，其他答案则五花八门。最后一步，我们完全忽略掉所有的推理过程，只看最终的答案。我们进行“投票”，哪个答案出现的次数最多，我们就认为哪个是最终的正确答案。在这个例子里，18出现了30次，我们就选择18。

这个简单的“投票”过程，在经验上就是对“边际化”的一个很好的近似。它背后的直觉是：如果一个答案是正确的，那么通往这个答案的“道路”应该有很多条；即使模型在某条路上犯了小错误，它在另一条路上可能就走对了。正确的答案会在多次尝试中反复、稳定地出现。这个看似简单的技巧带来的性能提升却是惊人的。丹尼·周用GSM8K这个基准测试举例：经过微调的GPT-3模型，准确率大约是33%。OpenAI使用一个额外的“验证器”模型来筛选，可以提升到55%。谷歌的PaLM模型加上思维链提示，达到了58%，已经非常接近验证器的水平。然而，当在这个基础上再用上“自洽性”技术后，准确率直接飙升到了75%，相对提升接近50%。后来在更强的PaLM 2上，这个数字甚至达到了92%。这充分说明模型的单一输出可能存在偶然性，但是它多次输出的“共识”，则具有高度的可靠性。

丹尼·周还回答了两个关于自洽性的常见问题：首先是，如果模型不生成中间步骤，直接输出答案，用自洽性还有用吗？答案是没用，而且没必要。因为在这种情况下，我们直接就可以看到每个答案的概率，选择概率最高的那个就行了。自洽性是专门为“推理”这种包含隐藏变量的场景设计的。其次是，我能不能不进行多次采样，而是让模型一次性生成多个不同的答案呢？答案是这样做没有意义，因为这违背了自洽性背后的数学原理，即通过独立采样来近似概率分布。当然，自洽性也有局限，它要求答案的形式是唯一的，比如一个数字。对于那些答案形式不唯一的开放性问题，比如“请列出亚洲最大的三个国家”，模型可能会生成“中国、印度、日本”或者“印度、中国、日本”，顺序不同但是内容一样。为了解决这个问题，他们还提出了“**通用自洽性**”（Universal Self-Consistency: 一种扩展的自洽性方法，用于处理答案形式不唯一或需要模型自行判断一致性的开放性问题）的方法，让模型自己去判断哪个回答是与其他回答“最一致”的。

**2. 检索**

关于大语言模型，另一个永恒的争论是：它到底是在“推理”，还是在“**检索**”（Retrieval: 从外部知识库或记忆库中获取相关信息的过程）？也就是说，它是在进行逻辑推导，还是仅仅从它庞大的记忆库里找到了一个最相似的已知答案呢？丹尼·周对此的态度非常务实，他说：“作为从业者，我只关心性能。为什么要在两者之间做选择呢？把检索和推理结合起来，效果就是更好。”他用“**类比推理**”（Analogical Reasoning: 一种通过将当前问题与已知相似问题进行比较来解决问题的方法）的例子来说明。比如这样一个几何问题：“求四个顶点坐标分别为(-2, 2), (2, -2), (-2, -6)和(-6, -2)的正方形的面积。”当他直接把这个问题扔给当时的GPT-3.5等模型时，模型失败了。但是，他仅仅在问题前面加了一句提示：“请先回忆一个相关的问题，然后再解决这个问题。”神奇的事情再次发生，模型在解决问题前，先自己生成了一段话：“一个相关的问题是，如何在坐标平面上计算两点之间的距离。距离公式是，巴拉巴拉……”然后，它利用这个自己“检索”出来的知识，先计算出正方形的边长，再计算出面积，最终成功解决了问题。

另一个例子叫“**退一步思考**”（Step-Back Prompting: 一种提示策略，引导模型在解决具体问题前，先思考所需的基本原理或抽象概念）。在解决一个具体的、复杂的物理问题前，先提示模型：“退一步，思考一下解决这类问题所需的基本物理原理是什么？”模型会先总结出相关的定律和公式，然后再用这些“检索”到的原理来指导具体的解题过程。这些方法其实就是现在非常火热的“**检索增强生成**”（Retrieval-Augmented Generation, RAG: 一种将大语言模型的生成能力与外部信息检索系统相结合的技术，以提高回答的准确性和知识时效性）技术的思想雏形，都是将大模型的推理能力与外部强大的信息检索能力结合起来。所以，不必再纠结于推理和检索的二元对立，一个强大的推理系统必然是一个开放的、懂得如何利用外部知识的系统。

### 总结与展望：黄金法则与未来挑战

讲座的最后，丹尼·周对整个大语言模型推理的技术演进做了一个精炼的总结，可以把它看作是四条经过实践检验的黄金法则：
1.  **有推理优于无推理**：生成中间步骤是解锁复杂问题解决能力的基础。
2.  **强化学习微调优于SFT**：让模型在“正确答案”的引导下自我进化，远比单纯模仿人类更有效。
3.  **聚合多个答案优于单次生成**：利用自洽性等方法，汇集模型的“集体智慧”，可以大幅提升可靠性。
4.  **检索+推理优于纯推理**：将模型的内部推理与外部知识库相结合是未来的方向。

这四条法则清晰地勾勒出了从一个原始的预训练模型，到今天看到的像Gemini这样强大的推理系统的完整技术路径。在展望未来的时候，丹尼·周也指出了当前面临的巨大挑战：我们今天讨论的所有技术，尤其是强化学习微调和自洽性，都严重依赖于一个前提——任务的答案是可以被自动验证的（automatically verifiable）。比如数学题有唯一答案，代码题可以通过单元测试。但是在现实世界中，大量更有价值的任务并没有这样的“验证器”。例如创意写作，如何判断一首诗写得好不好？代码设计，我们不只关心代码能不能运行，更关心它的架构是否优雅、可读性是否高、是否易于维护。以及战略规划，如何判断一份商业计划书是否可行？在这些没有唯一正确答案、充满主观性和复杂权衡的领域，我们该如何定义“奖励”？又该如何构建“验证器”呢？这可能是下一代人工智能需要突破的最大瓶颈。

同时，他也呼吁我们应该把更多的精力从在基准测试上“刷分”，转移到构建真正能够解决实际问题的应用上来，因为所有的基准测试都很快会在模型的能力提升下达到“饱和”。最后，丹尼·周引用了他非常欣赏的物理学家**理查德·费曼**（Richard Feynman: 著名的美国理论物理学家，诺贝尔物理学奖得主）的一句话来结束了演讲，那就是：“真相最终总是比你想象的要简单（The truth always turns out to be simpler than you thought）”。回看整个历程，无论是思维链、自洽性还是强化学习微调，它们背后的核心思想都惊人地简单，甚至可以说是回归了机器学习最本源的原理。这或许就是科研最大的魅力所在：我们穿过层层迷雾，最终发现的往往不是一个无比复杂的屠龙之术，而是一个简单、深刻、足以改变一切的真理。