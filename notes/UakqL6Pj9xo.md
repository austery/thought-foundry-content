---
area: tech-work
category: ai-ml
companies_orgs:
- Google
- Zapier
- OpenAI
- Amazon Mechanical Turk
- GitHub
- Kaggle
- Epoch AI
- Scale
date: '2024-06-11'
draft: true
guest: ''
insight: ''
layout: post.njk
media_books:
- 《On the Measure of Intelligence》
people:
- François Chollet
- Mike Knoop
- Nat Friedman
- Luke Farritor
- Jack Cole
- Dan Hendrycks
- Collin Burns
- Trenton Bricken
- Jean Piaget
products_models:
- Keras
- GPT-4
- GPT-5
- Gemini 1.5
- GPT-3
- Claude 3 Opus
- GPT-4o
- Mistral
- Llama
- NVIDIA Tesla P100
- H100
- ChatGPT
project:
- ai-impact-analysis
series: ''
source: https://www.youtube.com/watch?v=UakqL6Pj9xo
speaker: Dwarkesh Patel
status: evergreen
summary: 本期播客深入探讨了大型语言模型（LLMs）在解决复杂问题上的局限性，特别是其在**ARC基准测试**（Abstraction and Reasoning
  Corpus）中的表现。**François Chollet**（Keras创建者）与**Mike Knoop**（Zapier联合创始人）共同发起了百万美元的**ARC挑战赛**，旨在推动AI研究超越单纯的记忆和模式匹配，实现真正的通用智能（AGI）。讨论强调了LLMs依赖“记忆”而非“即时适应新颖性”的根本区别，并提出了结合深度学习与离散程序合成的混合系统作为通向AGI的潜在路径。
tags:
- agi
- canada
- memorization
title: 大型AI模型为何难解简单谜题？——ARC基准与通用智能的挑战
---

### LLM与AGI的争议
**LLM** (Large Language Model: 大型语言模型) 非常擅长记忆静态程序。如果你扩大数据库的规模，你并没有增加系统一丝一毫的智能。我觉得你用“记忆”这样的词，我们绝不会用在人类儿童身上。如果他们能解决任何任意的代数问题，你不会说他们“记忆”了代数，你会说他们“学会”了代数。我们设立了一个百万美元的奖金池，其中**50万美元**将奖励给第一个达到**85%**基准的团队。如果**ARC** (Abstraction and Reasoning Corpus: 抽象推理语料库) 在三个月内通过考验，我们会增加奖金。**OpenAI**基本上将**AGI** (Artificial General Intelligence: 通用人工智能) 的进展推迟了**五到十年**。他们导致了前沿研究出版的完全封闭，现在LLMs基本上“吸干了房间里的氧气”，好像每个人都在做LLMs。

### ARC基准介绍
今天我很荣幸能与**François Chollet**（**Google**的AI研究员，**Keras**的创建者）对话。他正与**Zapier**的联合创始人**Mike Knoop**合作发起一项奖金，我们稍后也会与Mike Knoop交谈。这是一项百万美元的奖金，旨在解决他创建的**ARC基准测试**。第一个问题是，ARC基准到底是什么？为什么需要这项奖金？为什么我们一年内最大的LLM无法直接达到饱和？

### ARC：抵抗记忆的智能测试
**ARC**旨在成为一种机器智能的**智商测试**。它与大多数LLM基准的不同之处在于，它被设计成**抵抗记忆化**。LLMs的工作方式基本上是一个巨大的插值记忆库。你提升其能力的方式是尽可能多地将知识和模式塞入其中。相比之下，ARC根本不需要大量知识，它只要求所谓的**核心知识**。这是关于基本物理、物体性、计数等方面的基础知识，是任何四五岁儿童都具备的知识。有趣的是，ARC中的每个谜题都是新颖的，即使你记忆了整个互联网，你也可能从未遇到过。这就是ARC对LLMs构成挑战的原因。到目前为止，LLMs在ARC上的表现并不理想。事实上，目前表现良好的方法更倾向于**离散程序搜索**和**程序合成**。

### LLM达到AGI的条件
首先，我很高兴作为LLM的怀疑者，你自己提出了一个基准。如果我们一年内最大的模型能够在这个测试中达到80%，那么你的观点是否会认为我们正通过LLMs走向AGI？你会如何看待这个问题？我对此非常怀疑，认为一年内LLM能达到80%的可能性不大。即便如此，如果真的达到了，你也需要审视其实现方式。如果你只是用数百万或数十亿个与ARC相似的谜题来训练模型，那仍然是依赖训练任务与测试任务之间的重叠，本质上仍是**记忆化**。也许这种方法能奏效，希望ARC能足够好以抵抗这种**暴力破解**尝试，但你永远不知道。我不是说它不会发生，ARC也不是一个完美的基准，它可能有缺陷，也可能被这种方式“破解”。

### 衡量AGI的关键标准
那么，**GPT-5**需要做到什么程度，才能让你确信它正走在通往AGI的道路上？这将是改变我对LLMs看法的关键。我需要看到大量案例，模型能够处理它从未见过的新任务——从其训练数据的角度来看是真正新颖的任务——并且能够**即时适应**。这不仅适用于LLMs，也适用于任何AI技术。如果我能看到模型具备**即时适应新颖性**并高效学习新技能的能力，我就会非常感兴趣，并认为这正走在通往AGI的道路上。

### 智能与记忆的本质区别
LLMs的优势在于它们“看到了”所有东西。我可能会质疑它们对此的依赖程度，但显然它们比人类更依赖。它们拥有如此多的分布内数据，以至于我们很难区分一个例子是否在分布内。如果它们拥有所有分布内的数据，那么它们就能做我们能做的一切。也许对我们来说是分布外的，但对它们来说为什么必须是分布外的呢？为什么我们不能仅仅利用它们能看到一切的事实？你基本上是在问，**真正的智能**——即适应未曾准备之事的能力——与**纯粹的记忆化**（如背诵所见内容）之间有什么区别。这不仅仅是语义上的差异。巨大的区别在于，你永远不可能预训练所有你在测试时可能看到的东西，因为世界总是在变化。这不仅仅是因为可能任务的空间是无限的，即使你训练了数百万个任务，也只看到了总空间的零百分比。更重要的是，世界每天都在变化。

### 人类智能的演化根源
这就是我们人类物种最初发展出智能的原因。如果世界、宇宙、我们的生活存在一个静态的分布，那么我们根本不需要智能。事实上，许多生物，例如昆虫，就没有智能。它们在它们的**连接组**（connectomes: 大脑中神经元连接的完整图谱）和基因中拥有硬编码的程序，这些行为程序将某些刺激映射到适当的反应。它们实际上可以在不学习任何东西的情况下，以一种非常适应进化的方式驾驭自己的生活和环境。如果我们的环境足够静态和可预测，那么进化就会找到完美的行为程序：一个硬编码的、静态的行为程序。它会将其写入我们的基因中。我们将拥有一个硬编码的大脑连接组，并以此运行。但这并没有发生。相反，我们拥有**通用智能**。我们出生时对世界的了解极少，但我们生来就具备高效学习和适应前所未见事物的能力。这使我们独一无二，也是在机器中重现的真正挑战。

### ARC谜题示例与核心知识
在我们深入探讨之前，我将为YouTube观众展示一些类似ARC挑战的例子。对于收听音频的听众，你能描述一下一个典型的ARC挑战会是什么样子吗？一个ARC谜题看起来有点像智商测试题。你会有一些**输入-输出对**的演示。一个对由两个网格组成，一个网格显示输入，第二个网格显示你应该作为对该输入的回应而产生的内容。你会得到几个这样的对来演示任务的性质以及你对输入应该做什么。然后你会得到一个新的测试输入，你的任务是产生相应的测试输出。你通过观察演示对来弄清楚应该做什么，并在新的测试对上展示你已经理解了它。重要的是，你解决这些挑战所需的知识基础只是**核心知识**。它包括诸如构成一个物体、计数、几何、拓扑、对称性等基本概念。这是极其基础的知识，LLMs肯定拥有这些知识，任何孩子也拥有这些知识。真正有趣的是，每个谜题都是新的，你不会在互联网上的其他地方找到它。无论是人类还是机器，你都必须从零开始解决每个谜题，并进行推理。你不能仅仅从记忆中提取答案。

### 多模态模型与空间推理
这里有一个争议点：我们现在才开始拥有经过训练能够进行空间推理的**多模态模型**，这得益于它们所训练的数据。然而，不仅是人类，我们的祖先也经过数十亿年的进化才学会理解抽象的物理和空间属性并识别其中的模式。一种观点认为，在未来一年内，随着我们获得原生多模态能力而非附加功能的新模型，它们将能够理解这些模式，因为这是我们天生就能看到的。目前，ARC看到的是一个`100100`的**JSON字符串**，并被要求识别其中的模式。即使你向人类展示一串这样的数字，他们也很难理解你提出的问题。为什么我们现在正在解锁的多模态模型，一旦出现，不能在ARC类型的空间推理方面表现得更好呢？

### LLM在ARC上表现不佳的原因
这是一个经验性问题，我想我们几个月内就会看到答案。我的回应是，我们的网格只是离散的二维符号网格，而且非常小。例如，如果你将图像扁平化为像素序列，你会得到一个实际上很难解析的东西。但ARC并非如此，因为网格非常小，只有10种可能的符号。它们是很容易扁平化为序列的二维网格。**Transformer**模型（Transformer: 一种基于自注意力机制的神经网络架构，广泛应用于自然语言处理）和LLMs非常擅长处理序列。事实上，你可以通过简单地在任务子集上微调LLM，然后测试这些任务的微小变体，来证明LLMs处理ARC类数据没有问题。你会发现LLM可以很好地编码它以前见过的任务的解决方案程序。它在解析输入或找出程序方面并没有真正的问题。LLMs在ARC上表现不佳的原因，仅仅是**陌生性**。每个新任务都与其他任务不同，你无法提前记忆解决方案程序。你必须为每个新任务即时合成一个新的解决方案程序，这才是LLMs真正 struggling 的地方。

### ARC挑战的吸引力
在我扮演更多“魔鬼代言人”之前，我想退一步解释一下为什么我对这次对话特别感兴趣。显然，有这个百万美元的**ARC奖金**，我自己也很兴奋能尝试一下。**维苏威挑战赛**（Vesuvius Challenge）是**Nat Friedman**设立的奖项，旨在解码被火山掩埋的**赫库兰尼姆图书馆**（Herculaneum library）的卷轴。那次挑战的获胜者是一位22岁的年轻人，名叫**Luke Farritor**，他当时正在收听这个播客。希望有听众能觉得这个挑战很有趣并找到解决方案。最近我采访了很多看好LLMs的人，在采访你之前，我曾与他们讨论如何解释LLMs在ARC上表现不佳的事实。我发现他们的解释有些牵强。我将尝试向你提出他们的一些理由。事实上，一个引人入胜的事实是，这些问题中的一些对人类来说相对简单易懂，但如果直接输入模型，它们就会遇到困难。

### 人类在ARC上的表现
所有这些谜题对人类来说都非常容易。任何聪明的人都应该能在ARC上达到90-95%的正确率。即使是一个知识储备非常少的五岁孩子，也绝对能做到50%以上。我同意聪明的人在这个测试中会表现出色，但普通人可能表现平平。不完全是，我们实际上用普通人做过测试，他们得分大约是**85%**。那是通过**Amazon Mechanical Turk**（亚马逊土耳其机器人: 亚马逊公司提供的一个众包平台，用于完成人类智能任务）的工人进行的，对吗？我老实说不了解Amazon Mechanical Turk工人的**人口统计学特征**。想象他们与亚马逊的远程工作平台互动，我猜那不是全球人类的中位数。更广泛的观点是，我们在人类中看到了能力的谱系，人类显然拥有AGI。但即使在人类内部，你也会看到一个谱系，有些人相对“笨拙”，他们在类似智商的测试中表现更差。例如，有**瑞文渐进矩阵**（Raven's Progressive Matrices: 一种非语言智力测试，用于衡量抽象推理能力）。看看普通人在那上面的表现。如果你看那些“碰运气”的问题——一半人答对，一半人答错——我们可能会觉得它们有点微不足道。

### **Jack Cole**的方法与LLM的局限
人类拥有AGI，但通过相对较小的调整，你可以让一个在这些基本智商测试问题上失误的人，变成一个全部答对的人。我们将讨论人们用这些模型尝试过的一些早期表现。**Jack Cole**用一个2.4亿参数的模型获得了**35%**的成绩。这难道不表明它们正处于人类内部清晰存在的这个谱系上，并且很快就会达到饱和吗？这里有很多有趣的观点。确实，由Jack Cole领导的LLM方法分支表现相当不错，它们实际上是**最先进的**（state-of-the-art: 指在特定领域或任务中表现最好的技术或方法）。但你必须看看那里发生了什么。有两点：第一，要获得这些数字，你需要用数百万个生成的ARC任务来预训练你的LLM。当然，这与一个第一次接触ARC的五岁孩子相比。这个孩子以前从未做过智商测试，也从未见过类似ARC的测试。他们所知道的与测试中必须做的事情之间唯一的重叠是核心知识，比如关于计数、物体、对称性等。他们仍然会表现得非常好，比在数百万个类似任务上训练的LLM表现得好得多。

### 测试时微调的重要性
关于**Jack Cole**的方法，还有第二点需要注意。让模型能够工作的关键之一是**测试时微调**（test time fine-tuning）。顺便说一句，这正是目前LLM方法中真正缺失的东西。大多数时候，当你使用LLM时，它只是进行**静态推理**，模型是冻结的。你只是给它提示并得到一个答案，模型并没有真正即时学习任何东西，它的状态也没有适应手头的任务。Jack Cole实际做的是，对于每个测试问题，它都会为该任务即时微调一个LLM版本。这才是真正释放性能的关键。如果你不这样做，你只会得到**1-2%**的成绩，完全可以忽略不计。如果你进行测试时微调，并在此基础上添加一系列技巧，那么你最终会得到有趣的性能数据。

### 弥补LLM主动推理的不足
这种方法正在试图解决当前LLMs的一个关键局限性：缺乏**主动推理**（active inference: 一种认知理论，认为大脑通过预测和最小化预测误差来理解世界）。它实际上是在LLMs中加入了主动推理，而且效果非常好，这让我着迷。这里有很多有趣的深入探索点。许多**规模最大化主义者**（scale maximalists: 相信通过扩大模型规模可以解决大部分AI问题的人）也认同你的更广泛观点，即需要解锁**自适应/测试时计算**。他们认为，除了规模扩展之外，还需要像自适应计算或某种**强化学习**（RL: Reinforcement Learning: 一种机器学习范式，通过与环境互动学习最优行为）来让**系统2思维**（System 2 thinking: 慢速、有意识、需要努力的思考过程）发挥作用。他们的观点是，这相对简单，可以添加到规模化模型已获得的表示之上。

### 性能衡量与智能本质
这不仅仅是一个技术细节，它不是一件简单的事情，它就是一切，它是最重要的部分。规模最大化主义者提到**缩放定律**（scaling laws: 指模型性能与计算资源、数据量和模型大小之间的经验关系），这是你在训练模型上花费的计算量与你在基准测试中获得的性能之间可以建立的经验关系。当然，这里的关键问题是，你如何衡量性能？通过增加更多的计算和数据，你实际改进的是什么？是**基准性能**。衡量性能的方式不是一个技术细节，也不是事后才考虑的事情，因为它会缩小你正在提出的问题范围。相应地，它也会缩小你正在寻找的答案范围。

### LLM基准的记忆化倾向
如果你看看我们用于LLMs的基准测试，它们都是**基于记忆的基准**。有时它们简直就是基于知识的，就像学校考试一样。即使你查看那些明确关于推理的基准，如果你仔细观察，你会发现要解决它们，只需记忆有限的推理模式集，然后重新应用它们。它们就像静态程序。LLMs非常擅长记忆小的静态程序，它们拥有一个解决方案程序库。当你给它们一个新的谜题时，它们只需提取适当的程序并应用它。这看起来像是推理，但它并没有真正进行任何**即时程序合成**，它所做的只是**程序提取**。

### 技能与智能的混淆
你实际上可以用记忆来解决所有这些基准测试。如果你看这些模型以及你在这里扩展的东西，它们是拟合数据分布的巨大**参数曲线**。它们基本上是这些大型的**插值数据库**、插值记忆。当然，如果你扩大数据库的规模，并塞入更多的知识和模式，你就会提高它在记忆基准测试中衡量的性能。这显而易见。但当你这样做时，你并没有增加系统一丝一毫的智能。你增加的是系统的**技能**，你增加了它的实用性、适用范围，但不是它的智能，因为**技能不是智能**。这是人们遇到的根本性混淆，他们混淆了技能和智能。

### **GSM8K**：小学数学基准的记忆化本质
这里有很多有趣的话题可以讨论：技能、智能、插值。让我们来谈谈它们拟合输入数据的一些**流形**（manifold: 在数学中，指局部具有欧几里得空间性质的拓扑空间）。用还原论的方式来说，人脑不过是轴突相互放电。但我们不关心还原论的解释，我们关心这些东西结合时在宏观层面发生的事情。关于插值，我们来看看一个基准测试。有一个小学数学基准测试，这些问题是聪明的初中生能够解决的，它叫做**GSM8K**。这些模型在这个测试中获得了**95%**的成绩，基本上总是能答对。当然，那是一个记忆化基准。让我们谈谈这意味着什么。这是该基准中的一个问题：“一个班级有30名学生。其中五分之一是12岁，三分之一是13岁，十分之一是11岁。有多少学生不是11、12或13岁？”我同意这不是什么高深的问题，你可以在纸上写下如何解决这个问题。一个聪明的初中生应该能够解决它。关于记忆化，它仍然需要推理分数、整个问题的上下文，然后结合不同的计算来写出最终答案。

### 推理的两种定义
这取决于你如何定义推理。你可以使用两种定义。一种是，我有一套**程序模板**可用，它是谜题的结构，也可以生成其解决方案。我将识别出我记忆中的正确模板，将新值输入模板，运行程序，然后得到解决方案。你可以说这是推理。我说：“是的，当然，可以。”这是推理的另一种定义：当你面对一个谜题，而你记忆中还没有解决它的程序时，你能够基于你已有的现有程序的片段，**即时合成**一个新程序。你必须进行**即时程序合成**。这实际上比仅仅提取正确的记忆程序并重新应用要困难得多。

### 人类学习与模型训练的相似性
也许我们高估了人类的**样本效率**。他们也需要以这种方式进行训练。他们必须通过某些类型的问题来训练这些推理路径。以数学为例，你不能只给一个婴儿看**集合论的公理**，然后他们就懂数学了。在他们成长过程中，你必须教他们多年的**代数预备知识**。然后花一年时间教他们练习，并反复解决代数、几何、**微积分预备知识**和微积分中相同类型的问题。这不也是一样吗？你不能只看一个例子就掌握程序，你实际上必须反复练习。这些模型也必须通过大量的预训练数据进行练习。当然，为了进行即时程序合成，你确实需要构建块作为基础。知识和记忆在这个过程中极其重要。我并不是说记忆与推理是对立的，为了进行有效的推理，你需要记忆。

### 规模化模型与通用智能的界限
但这听起来与你的说法是兼容的。通过看到许多不同类型的例子，这些模型可以在这些例子的上下文中学习推理。我们也可以在越来越大的模型中看到这一点。那是一个高中水平的数学问题。假设一个比**GPT-3**小的模型根本做不到。随着这些模型变得越来越大，它们似乎能够捕捉到越来越大的模式。这并不是一个真正的规模问题，在这种情况下，它更像是一个训练数据问题。嗯，更大的模型可以捕捉到这些类型的电路。较小的模型即使你用这种数据训练它们，也显然做得不好。这难道不意味着随着模型越来越大，它们可以捕捉到越来越大的路径或更一般的推理方式吗？当然。但那不就是智能吗？不，它不是。如果你扩大数据库并不断添加更多的知识和程序模板，那么它当然会变得越来越熟练。你可以将其应用于越来越多的任务。但**通用智能**不是将特定任务的技能扩展到许多技能，因为可能的技能空间是无限的。通用智能是能够处理任何问题、任何技能，并使用极少的数据快速掌握它的能力。这就是让你能够面对你可能遇到的任何事物的原因。这是**通用性**的定义。通用性不是特定性规模化，它是将你的思维应用于任何事物，应用于任意事物的能力。这从根本上要求具备**即时适应**和高效学习的能力。

### **Gemini 1.5**的语言学习能力
我的主张是，通过对越来越大的模型进行预训练，你正在获得非常高效的泛化能力。让我给你举个例子。你自己的公司**Google**，在他们关于**Gemini 1.5**的论文中，有一个非常有趣的例子。他们会在上下文中给模型提供一种活着的说话者少于200人的语言的语法书和词典。这不在预训练数据中。你只需给它词典，它基本上就能说这种语言并进行翻译，包括语言复杂的、有机的结构方式。如果你给我一本英西词典，我无法学会如何组织句子以及如何用西班牙语表达。因为通过这种预训练获得的表示，它现在能够极其高效地学习一门新语言。这难道不能表明这种预训练确实提高了你学习新任务的能力吗？

### ARC对LLM的挑战在于抵抗记忆
如果你是对的，LLMs在ARC谜题上会表现得非常好，因为ARC谜题并不复杂。每个谜题只需要很少的知识，复杂度非常低，你不需要费力思考。它们对人类来说实际上非常明显，甚至儿童都能做到，但LLMs却不能。即使LLMs拥有比你多10万倍的知识，仍然无法做到。唯一让ARC特别的是，它被设计成**抵抗记忆化**。这才是LLM性能的巨大障碍。如果你仔细观察LLMs，很明显它们并没有真正即时合成新程序来解决它们面临的任务。它们更多地是在重新应用存储在记忆中的东西。例如，一个非常引人注目的现象是，LLMs可以解决**凯撒密码**（Caesar cipher: 一种简单的替换密码，通过将字母表中的每个字母替换为沿字母表向下或向上移动固定位数后的字母来加密信息），这是一种非常复杂的算法，但它在互联网上出现得相当频繁。它们基本上是记忆了它。

### LLM对凯撒密码的记忆化处理
真正有趣的是，它们可以处理转置长度为三或五的凯撒密码，因为这些是互联网上提供的例子中非常常见的数字。如果你尝试使用任意数字，比如九，它就会失败。它没有编码算法的**通用形式**，而只是特定案例。它记忆了算法的特定案例。如果它能够即时合成求解算法，那么n的值就根本不重要，因为它不会增加问题的复杂性。我认为人类也是如此。人类当然一直在使用记忆模式匹配，但人类并不局限于记忆模式匹配。他们拥有这种非常独特的能力，可以**即时适应新情况**。这正是让你能够驾驭生活中每一个新日子的原因。

### 国际象棋与**Gemini 1.5**的语言能力
有研究表明，国际象棋大师在特定棋步的语境下表现出色。这是一个很好的例子，因为国际象棋在最高水平上，完全是关于记忆化，即**国际象棋记忆**。你如何解释最初的问题，即**Gemini 1.5**为何能够在上下文中学习一门语言，包括复杂的语法结构？这难道不能表明它们可以获取新知识吗？我猜测它只是从其极其广泛、难以想象的训练数据中挖掘出了所需的模板，然后只是在重复使用。我们知道LLMs在即时合成新的程序模板或甚至适应现有模板方面的能力非常差。它们非常局限于**提取**。

### 程序员的工作与极端泛化
假设**Google**有一位程序员，早上走进办公室。在哪个时刻，他们所做的事情是100%不可能仅仅通过提取某个模板来完成的？假设他们是一个LLM，如果他们只能从程序中提取模板，他们将无法做什么？在哪个时刻，他们必须使用这种所谓的**极端泛化能力**？别提Google的软件开发者了。对于每个人类来说，他们生命中的每一天都充满了他们未曾准备好的新奇事物。你不可能仅仅依靠记忆来驾驭你的生活，那是不可能的。

### LLM的泛化能力与现实世界的挑战
你似乎也同意它们不仅仅是在“记忆”。你似乎在说它们的泛化能力较弱。我只是好奇它们能做到哪种程度的泛化。如果你走进办公室，试图进行这种泛化，你就会在工作中失败。假设你是一名程序员，当你尝试进行那种泛化时，你会在哪个时刻因为无法进行**极端泛化**而失去工作？以这个情况为例。你从未到过这个房间，也许你来过这个城市几次。这里有相当多的新奇事物。你从未采访过我。你生命中每一天的每一个小时都有相当多的新奇事物。总的来说，这实际上比任何LLM能处理的新奇事物都要多。如果你只是把一个LLM放到一个机器人里，它无法完成你今天所做的一切。

### 自动驾驶与LLM的泛化困境
以**自动驾驶汽车**为例。你拿一辆在**湾区**（Bay Area: 指旧金山湾区，美国加利福尼亚州的一个大都市区）运行的自动驾驶汽车，你认为你可以直接把它放到**纽约市**或**伦敦**（那里的人靠左行驶）吗？不，它会失败。它不仅不能泛化到驾驶规则的变化，你甚至不能让它泛化到一个新城市。它需要针对每个特定环境进行训练。我同意自动驾驶汽车不是AGI。但它们是同类型的模型，它们也是**Transformer**模型，架构相同。我不知道。猿猴也有带神经元的脑子，但它们不那么聪明，因为它们更小。我们可以深入探讨这一点。我仍然不理解这个具体的问题。我们也需要训练，这就是教育存在的原因。这就是为什么我们生命的前18年都要进行练习。我们有记忆，但我们不仅仅是记忆。我们不局限于记忆。我否认这些模型必然只做这些事情的前提。

### 软件开发中的新颖性与问题解决
假设你用一个LLM取代了一个远程工作的程序员。你在哪个时刻会意识到这不是人类，而是一个LLM？不如我直接给他们一个ARC谜题，看看他们表现如何？不，我是说作为他们工作的一部分。你必须一直处理新颖性。是否存在这样一个世界：所有的程序员都被取代了，但我们仍然说：“啊，但他们只是在做充满记忆的编程任务。”在那个世界里，他们仍然以代码的形式产生万亿美元的产出吗？软件开发实际上是一个很好的例子，你一直在处理新颖性。如果你不是，我不知道你在做什么。我个人在我的软件开发工作中很少使用**生成式AI**。在LLMs出现之前，我也很少使用**Stack Overflow**（一个面向程序员的问答网站）。有些人可能只是从Stack Overflow复制粘贴代码，或者现在从LLM复制粘贴。就我个人而言，我尝试专注于**问题解决**。语法只是一个技术细节，真正重要的是问题解决。编程的本质是构建你试图解决问题的**心智模型**和心智表示。

### **ChatGPT**与软件工程的未来
有很多人可以自己与这些系统互动。你可以去**ChatGPT**说：“这是我想要的程序规格。”它们会为你构建。只要**GitHub**、**Stack Overflow**等网站上有许多这种程序的例子，它们当然会从记忆中为你提取程序。但你可以改变任意细节。你可以说：“我需要它在另一种服务器上运行。”如果真是那样，今天就不会有软件工程师了。我同意我们还没有达到完全的AGI。这些模型的参数少于一万亿。人脑大约有**10-30万亿**个突触。如果你只是做一些简单的数学计算，你至少**低参数化了10倍**。我同意我们还没有达到，但我困惑的是为什么我们不在这个谱系上。

### 智商测试的局限性
是的，我同意它们有很多种泛化能力无法做到。但它们似乎处于人类内部也能看到的这种平滑谱系上。有些人类在进行ARC类型测试时会遇到困难。我们从**瑞文渐进矩阵**类型的智商测试表现中看到了这一点。我不太喜欢智商测试，因为在大多数情况下，你可以通过训练智商测试来提高成绩。它们非常**基于记忆**。这实际上是ARC试图避免的主要陷阱。

### 远程工作自动化与软件工程师的未来
假设未来五年内所有远程工作都被自动化了。我是指至少那些不需要你提供服务（比如销售员，你需要人类交谈）的远程工作，更像是编程。在那个世界里，你会说那是不可能的吗？因为程序员需要做很多事情，这些事情肯定需要预训练语料库中没有的东西。当然，五年后，软件工程师会比今天更多，而不是更少。我仍然不确定。我学的是计算机科学。如果我大学毕业后成为一个“代码猴子”，我会做什么？我去上班，老板告诉我做些什么？如果我是一个LLM，他什么时候会意识到我不是人类？

### LLM在软件工程中的角色
可能在第一天就会发现。再说一次，如果LLMs真的能够泛化到这类新颖问题——即开发软件来解决它们从未见过的问题——那么你就不再需要软件工程师了。如果我看看今天人们在软件工程工作中如何使用LLMs，他们把它当作**Stack Overflow**的替代品。他们用它来复制粘贴代码片段以执行非常常见的操作。他们实际需要的是一个**代码片段数据库**。他们实际上不需要任何使他们成为软件工程师的能力。

### 创造力与高维插值
让我们回到**插值**的问题。为什么创造力不能只是高维度的插值呢？如果用机器学习的语言来说，一个更大的模型可以学习一个更复杂的**流形**。如果你读一位科学家的传记，他们并不是**零样本学习**（zero-shotting: 指模型在没有见过任何相关示例的情况下，直接解决新任务的能力）新的科学理论。他们是在玩弄现有的想法，试图在脑海中并置它们。在知识传承的谱系中，他们尝试一些略有不同的演化路径。你通过发表论文或类似方式进行实验。这看起来与人类所做的事情类似。存在更高层次的泛化。越来越大的模型似乎正在接近越来越高层次的泛化。**GPT-2**无法解决需要超出其泛化能力的小学数学问题，但**GPT-3**和**GPT-4**可以。

### 技能、泛化与人类思维
不完全是。**GPT-4**拥有更高程度的技能和更广的技能范围，但它的泛化程度是相同的。我不想在这里纠缠于语义。为什么创造力不能只是高维度的插值呢？插值绝对可以是创造性的。就你而言，我确实认为在某种程度上，人类也做了大量的记忆、背诵、模式匹配和插值。这非常像是模式匹配和真正推理之间的一个谱系。人类从来没有真正处于谱系的一端，他们从来没有真正做纯粹的模式匹配或纯粹的推理。他们通常是两者的混合。即使你在做一些看起来非常依赖推理的事情，比如证明一个数学定理，这也是如此。当你这样做时，你在脑海中进行了大量的**离散搜索**和实际的推理。你也被直觉和模式匹配所引导。你被你以前见过的证明形式和你的数学知识所引导。我们所有的思想，我们所做的一切，都是插值记忆式思维、**类型1思维**（Type 1 thinking: 快速、直觉、无意识的思考过程）和**类型2思维**（Type 2 thinking: 慢速、有意识、需要努力的思考过程）的混合。

### 大模型样本效率与泛化局限
为什么更大的模型样本效率更高？因为它们有更多可重用的构建块，可以依靠这些构建块来学习训练数据中的新模式。这种模式会随着模型越来越大而持续下去吗？会的，只要你给模型学习的新模式与它之前学到的东西很好地匹配。如果你呈现一些真正新颖的、不在稳定分布中的东西，比如一个ARC谜题，它就会失败。让我提出这个主张：**程序合成**是一个非常有用的直觉泵。为什么**Transformer**模型中发生的事情不能是这种情况呢？早期的层正在弄清楚如何表示输入的**token**（token: 文本或代码的基本单位）。中间层进行这种程序搜索、程序合成，并将输入组合到模型中的所有电路。它们从低级表示到模型中间的高级表示。它们使用这些程序，结合这些概念。最终输出的是基于那种高级智能的推理。

### LLM解决ARC的真正障碍
可能。为什么不呢？但如果这些模型真的能够合成新颖的程序，无论多么简单，它们都应该能够解决ARC。因为对于任何ARC任务，如果你用**Python**写下解决方案程序，它都不是一个复杂的程序，它极其简单。人类可以解决它，为什么LLMs不能呢？这是一个公平的观点。反过来问你，假设一年后一个多模态模型能够解决ARC，比如说它达到了80%或者普通人类的平均水平。那么我们是否就走在通往AGI的道路上了呢？很有可能，是的。老实说，我希望看到一个LLM类型的模型在ARC上达到80%的成绩，但前提是它只在**核心知识**相关的内容上进行过训练。

### ARC基准的完美与缺陷
但是人类的孩子，我们必然只在基因中拥有的东西上进行训练……让我重新措辞。我希望它只在那些没有明确试图预测ARC测试集中内容的信息上进行训练。ARC的全部意义不就是你无法预测吗？它每次都是一种新型的智力测试吗？是的，这就是重点。如果ARC是一个完美无瑕的基准，那么就不可能预测测试集中的内容。ARC在四年多前发布，到目前为止它一直**抵抗记忆化**。它在一定程度上经受住了时间的考验。但它并不完美。假设你手工制作了数十万个ARC任务，并通过编程生成变体来倍增它们。你最终可能会得到数亿个任务。仅仅通过**暴力破解任务空间**，你所训练的内容和测试集中的内容之间就会有足够的重叠，从而使你能够获得非常高的分数。只要规模足够大，你总能“作弊”。

### 智能的价值与迷雾比喻
如果你能对所有据称需要智能的事物都这样做，那么智能还有什么用呢？显然，你可以通过**暴力破解**来获得智能。如果世界，如果你的生活，是一个静态分布，那么当然，你可以暴力破解可能的行为空间。我喜欢用几个比喻来形容智能。其中之一是，你可以将智能视为**未来情境空间**中的**寻路算法**。我不知道你是否熟悉**RTS游戏**（Real-Time Strategy Game: 即时战略游戏）开发。你有一张地图，一张二维地图，你对它只有部分信息。你的地图上有一些**战争迷雾**（fog of war: 游戏术语，指地图上未探索或不可见的区域）。有些区域你还没有探索过，你一无所知。还有一些区域你探索过，但你只知道它们过去的样子，不知道它们今天是什么样子。

### 智能：应对变化的寻路算法
现在，不要去想二维地图，而是思考你可能遇到的未来情境空间以及它们之间如何相互连接。智能是一种寻路算法。一旦你设定了一个目标，它就会告诉你如何最优地到达那里。当然，它受到你所拥有信息的限制。它无法在你一无所知的区域进行寻路，也无法预测变化。如果你拥有关于地图的完整信息，那么你就可以通过简单地记忆每条可能的路径，从A点到B点的每个映射来解决寻路问题。你可以用纯粹的记忆来解决这个问题。你在现实生活中无法做到这一点的原因是，你实际上不知道未来会发生什么。生活是不断变化的。

### 记忆与学习：人类与LLM的语义差异
我觉得你正在使用“记忆化”这样的词，我们绝不会用在人类儿童身上。如果你的孩子学会了代数，然后又学会了微积分，你不会说他们“记忆”了微积分。如果他们能解决任何任意的代数问题，你也不会说他们“记忆”了代数，你会说他们“学会”了代数。人类从来没有真正做纯粹的记忆化或纯粹的推理。这仅仅是因为你将人类所做的行为语义上标记为“技能”。但当LLM完成完全相同的技能时，它就成了记忆化，正如你可以通过这些基准测试来衡量的那样。你可以插入任何类型的数学问题。

### 学校教育与模型参数化
有时人类所做的事情与LLM所做的事情完全相同。例如，如果你学习加法，你就是在记忆一个算法，你记忆了一个程序，然后你可以重新应用它。你并不是即时合成加法程序。显然，在某个时候，某个人类必须弄清楚如何进行加法。一个孩子并不是从**集合论公理**开始，然后推导出如何进行加法。你在学校学到的东西大多是记忆。我的主张是，这些模型相对于人脑中的**浮点运算**（flops: Floating Point Operations Per Second: 每秒浮点运算次数，衡量计算能力的指标）数量和参数数量而言，**参数化严重不足**。因此，它们无法像最聪明的人类那样提出新定理是合理的。大多数人类也做不到这一点。大多数人类所做的事情听起来与你所说的记忆化类似，即记忆你学到的技能或技术。所以这听起来是兼容的。

### 自动化与智能：静态分布的界限
告诉我这是否错误。在你的世界里，如果所有远程工作者都消失了，但他们所做的技能我们可以潜在地从中生成**合成数据**，这是否兼容？我们记录每个远程工作者的屏幕，我们大致了解他们正在执行的技能。现在我们训练了一个可以做所有这些的模型。所有远程工作者都失业了。我们正在从AI远程工作者那里产生数万亿美元的经济活动。在那个世界里，我们仍然处于记忆化状态吗？当然，通过记忆化，你可以自动化几乎所有事情，只要它是一个**静态分布**，只要你不必处理变化。大多数工作都是这种静态分布的一部分吗？潜在地，有很多事情你可以自动化。LLMs是自动化的绝佳工具。但你必须明白，**自动化与智能不同**。我并不是说LLMs没用，多年来我一直是**深度学习**的坚定支持者。

### 深度学习的规模化与AGI的距离
多年来，我一直在说两件事。我一直在说，如果你继续扩大深度学习的规模，它将继续带来回报。与此同时，我也一直在说，如果你继续扩大深度学习的规模，这并不会导致AGI。我们可以自动化越来越多的事情。是的，这具有经济价值。是的，潜在地，你可以通过这种方式自动化许多工作。那将具有经济价值。但你仍然不会拥有智能。所以你可以问，如果我们能创造所有这些经济价值，那又有什么关系呢？也许我们根本不需要智能。当你必须处理**变化**、**新颖性**和**不确定性**时，你就需要智能。只要你处于一个可以提前精确描述的空间中，你就可以依靠纯粹的记忆化。事实上，你总是可以解决任何问题。只要能够非常非常精确地描述问题及其解决方案，你就可以在任何任务上展示任意水平的技能，而无需利用任何智能。

### 泛化谱系与**Grokking**现象
当它们确实处理新颖性时，你就称之为**插值**。不，插值不足以处理所有类型的新颖性。如果可以，那么LLMs就会是AGI了。我同意它们不是AGI。我只是想弄清楚我们是否走在通往AGI的道路上。这里的关键在于，在我看来，这些事物处于一个谱系上，而我们显然正在用LLMs覆盖这个谱系的最早部分。我想是的。好的，有趣。这里还有一件事我认为可以作为证据：**Grokking**（Grokking: 深度学习模型在训练后期突然从记忆化转向泛化的一种现象）。显然，即使在深度学习内部，记忆化状态和泛化状态之间也存在差异。一开始它们只会记忆数据集。如果你在做**模加法**（modular addition: 一种数学运算，结果是两个数相加后除以某个模数的余数），那就是如何加数字。在某个时候，如果你继续训练，它们就会学会这项技能。

### **Grokking**与最小描述长度原则
这种区分的存在表明，对于深度学习可以学习的泛化电路，如果你有一个**过参数化模型**（overparameterized model: 指模型参数数量远大于训练数据量的模型），就存在一个泛化机制。与我们目前希望这些模型完成的所有任务相比，我们还没有达到那种程度。**Grokking**是一个非常非常古老的现象，我们已经观察了几十年。它基本上是**最小描述长度原则**（Minimum Description Length Principle: 一种信息论原则，认为最好的模型是能够以最短编码描述数据和模型本身的模型）的一个实例。给定一个问题，你可以只记忆一个点对点的输入-输出映射，这完全是**过拟合**的。它根本不泛化，但它解决了训练数据上的问题。从那里开始，你可以继续修剪它，使你的映射更简单、更压缩。在某个时候，它就会开始泛化。这就是所谓的最小描述长度原则。这个想法是，泛化最好的程序是最短的。这并不意味着你除了记忆之外什么都没做，你只是在做**记忆加上正则化**。

### 元学习与流体智能
也就是泛化？是的，那绝对会导致泛化。所以你是在一个技能内做这件事。你在这里看到的**元学习**（meta-learning: 学习如何学习，使模型能够快速适应新任务）模式是，存储一个可以执行多种技能而不是单一技能的程序更高效。这正是我们可能称之为**流体智能**（fluid intelligence: 指在陌生情境下解决新问题、进行抽象推理的能力）。因此，随着模型越来越大，你会期望它在这个泛化层次结构中向上攀升。它泛化到一项技能，然后泛化到多项技能。没错。LLMs不是无限大的，它们只有固定数量的参数。它们必须尽可能地压缩知识。实际上，LLMs主要存储可重用的程序片段，比如**向量程序**。因为它们有这种压缩的需求，每次学习新程序时，它们都会尝试用之前学过的现有程序片段来表达它。这难道不是泛化吗？当然。显然LLMs具有一定程度的泛化能力。这正是原因所在，因为它必须进行压缩。

### LLM的内在局限与程序合成
为什么它天生就受限？在某个时刻，它必须学习更高层次的泛化，然后是更高的层次，最终达到**流体智能**的最高层次。它之所以内在受限，是因为你的模型底层是一个巨大的**参数曲线**。你所能做的只是**局部泛化**。如果你想超越这一点，走向更广泛甚至**极端泛化**，你必须转向不同类型的模型。我选择的范式是**离散程序搜索**和**程序合成**。如果你想理解这一点，你可以将其与深度学习进行比较。在深度学习中，你的模型是一个**可微分参数曲线**。在程序合成中，你的模型是一个**离散的运算符图**。你有一组逻辑运算符，就像一个**领域特定语言**（Domain-Specific Language, DSL: 专门用于特定应用领域的计算机语言）。你选择它的实例，将其构建成一个图，这就是一个程序。这实际上与你可能用**Python**或**C++**等语言编写的程序非常相似。我们在这里做的是机器学习，我们正在尝试自动学习这些模型。

### 深度学习与离散程序搜索的对比
在深度学习中，你的学习引擎是**梯度下降**（gradient descent: 一种优化算法，用于最小化函数）。梯度下降计算效率很高，因为你有一个非常强的、信息丰富的反馈信号，告诉你解决方案在哪里。你可以非常快地找到解决方案，但它的**数据效率**非常低。为了使其工作，你需要对操作空间进行密集采样，你需要对数据分布进行密集采样。然后你只能在那个数据分布内进行泛化。你之所以有这个限制，是因为你的模型是一条曲线。与此同时，如果你看**离散程序搜索**，其学习引擎是**组合搜索**。你只是尝试一堆程序，直到找到一个真正符合你规格的程序。这个过程**数据效率极高**。你只需一个或两个例子就可以学习一个可泛化程序。顺便说一句，这就是它在ARC上表现如此出色的原因。主要的局限性在于它的**计算效率极低**，因为你当然会遇到**组合爆炸**（combinatorial explosion: 指随着问题规模的增加，可能解的数量呈指数级增长的现象）。

### 融合深度学习与程序合成的未来
你可以在这里看到深度学习和离散程序搜索如何拥有非常互补的优势和局限性。深度学习的每一个局限性在程序合成中都有相应的优势，反之亦然。前进的道路将是**将两者融合**。这里还有另一种思考方式。这些用梯度下降训练的参数曲线非常适合所有**系统1类型思维**（System 1-type thinking: 快速、直觉、无意识的思考过程）：模式识别、直觉、记忆等。离散程序搜索非常适合**系统2类型思维**（Type 2 thinking: 慢速、有意识、需要努力的思考过程）：规划、推理。它能快速找出与一两个例子（例如ARC谜题）匹配的可泛化模型。人类从不只做纯粹的系统1或纯粹的系统2，他们总是混合和匹配两者。目前，我们拥有所有系统1的工具，但系统2的工具几乎没有。前进的方向是创建一个**混合系统**。

### 构建混合智能系统
它将采取的形式主要是**系统2**。外部结构将是一个**离散程序搜索系统**。你将利用深度学习来解决离散程序搜索的根本局限性——**组合爆炸**。你将利用深度学习在程序空间中提供指导和直觉，以引导程序搜索。这与你下国际象棋或尝试证明定理时所看到的情况非常相似。它主要是一种推理活动，但你一开始会对解决方案的形状有一些直觉。这正是你可以通过深度学习模型获得的东西。深度学习模型非常像**直觉机器**，它们是**模式匹配机器**。

### 深度学习引导的程序搜索
你从这种解决方案的形状开始，然后进行实际的显式**离散程序搜索**。但你不会通过暴力破解来做，也不会随机尝试。你实际上会向另一个深度学习模型寻求建议。它会告诉你：“这是最可能的下一步。这是你在图中应该去的地方。”你还可以使用另一个深度学习模型来获取反馈，比如：“嗯，这是我目前为止的结果。看起来好吗？我应该回溯并尝试新的东西吗？”离散程序搜索将是关键，但你想通过利用深度学习使其显著改进，效率提高几个数量级。顺便说一句，深度学习还可以用于常识知识和一般知识等方面。你最终会得到一个这样的系统，它拥有一个可以适应新情况的**即时合成引擎**。

### 适应新情境的泛化模型
它适应的方式是，它将从一个模式库中提取模块，这些模块本身可以是曲线、可微分模块，还有一些本质上是算法的模块。它将通过这种**直觉引导的过程**来组装它们。对于你可能面临的每一个新情况，它都会给你一个使用极少量数据合成的**可泛化模型**。像这样的东西就能解决ARC。这实际上是一个非常有趣的提示。这里有一个有趣的症结。我与那些对LLMs极度乐观并期望在未来几年内实现AGI的朋友交谈。在某种意义上，他们也同意规模化并非万能，但其余的进展都以规模化为基础和推动力。你仍然需要在这些模型之上添加**系统2**和**测试时计算**。

### **系统2**与强化学习的结合
他们的观点是，这样做相对简单，因为你已经从预训练中建立了一个**表示库**。这几乎就像只是浏览教科书。你需要一种更深思熟虑的方式来让它与所学材料互动。**上下文学习**（in-context learning: 指模型在不更新参数的情况下，通过输入中的示例学习新任务的能力）的样本效率极高。要真正将其提炼到权重中，你需要模型通过它所看到的事物进行思考，然后将其添加回权重。至于**系统2**，他们谈到添加某种**强化学习**（RL）设置，以便鼓励它沿着最终正确的推理路径前进。他们认为这在未来几年内将是相对简单的事情。

### 智能架构的挑战
这是一个经验性问题，所以我们拭目以待。我猜你的直觉并非如此。我好奇为什么。我的直觉是，整个**系统2架构**才是困难的部分。它是非常困难且不明显的部分。扩展**插值记忆**是容易的部分，它实际上只是一条大曲线。你所需要的只是更多的数据，它是数据集的插值表示。那是容易的部分。困难的部分是**智能的架构**。记忆和智能是独立的组成部分。我们拥有记忆，但我们还没有智能。我同意你的看法，拥有记忆实际上非常有用。如果你只有智能但它没有连接到广泛的记忆，那将不会那么有用，因为它没有足够的材料来工作。

### 智能：分层关联记忆？
前嘉宾**Trenton Bricken**提出了一种替代假设，认为智能只是**分层关联记忆**。当**夏洛克·福尔摩斯**（Sherlock Holmes: 英国作家柯南·道尔笔下的虚构侦探）进入犯罪现场时，他样本效率极高。他只需看几个线索就能找出凶手。他之所以能做到这一点，是因为他学习了更高层次的关联。从某种根本意义上说，这是记忆。这里有一种提问方式。在大脑中，我们据说进行程序合成，但这只是突触相互连接。从物理上讲，肯定是你只是查询正确的电路，对吗？是的，没错。这是一个程度问题。在人类祖先所处的环境中进行训练，意味着你学习了那些电路。如果你训练在人类产生的相同类型的输出上——而复制这些输出需要这些类型的电路——那难道不会训练出人类所拥有的相同东西吗？

### **皮亚杰**论智能与新颖性
这是一个程度问题。如果一个系统有记忆，并且只能从中进行**局部泛化**，那么它的适应性就不会很强。要真正实现通用，你需要记忆加上相当深度的搜索能力，以实现更广泛甚至**极端泛化**。我最喜欢的心理学家之一是**Jean Piaget**（让·皮亚杰: 瑞士著名发展心理学家，认知发展理论的创始人），他是**发展心理学**的创始人。他有一句关于智能的绝佳名言：“**智能是你不知道该做什么时所使用的东西**。”作为一个人，在你的生活中，大多数情况下你已经知道该做什么，因为你以前遇到过这种情况，你已经有了答案。你只有在面对新颖性、面对你意想不到的事情时，才需要使用智能。这是你自己的生活经验或进化历史都未曾准备好的东西。你现在过的这一天，在某些重要方面与你之前过的每一天都不同。它也与你任何祖先所过的任何一天都不同。你仍然能够正常运作。这怎么可能呢？

### 人类智能差异与模型泛化谱系
我并不否认泛化极其重要，是智能的基础。那不是关键。关键在于模型中发生了多少泛化。好的，让我问一个关于人类智能差异的独立问题。也许由于你提到的原因，智力测试并不能很好地衡量它。但显然不同人类之间存在智力差异。你对此有何解释？这与我的说法某种程度上是兼容的。存在一个**泛化谱系**，这些模型正在攀升到人类水平。甚至有些人类也还没有攀升到**爱因斯坦**（Albert Einstein: 著名物理学家）或**François**的水平。

### 智能的遗传基础与架构改进
这是一个很好的问题。有大量证据表明，智力差异主要源于**遗传**。这意味着如果你找一个不太聪明的人，无论你给他多少训练数据，都无法让他成为**爱因斯坦**。这表明你确实需要更好的架构，你需要更好的算法。更多的训练数据实际上并非你所需要的一切。我想我同意这一点。我可能会这样表达：更聪明的人，用机器学习的语言来说，拥有更好的**初始化**。如果你看神经网络的布线，它更高效。也许它们的放电密度更高。故事的一部分是规模化。大脑大小与智能之间存在一定的相关性。在LLMs“规模化”的背景下，人们谈论**架构改进**。像**Gemini 1.5 Flash**这样的模型，其性能与一年前发布的**GPT-4**相当，但输出成本却便宜**57倍**。规模化故事的一部分是，在这些架构改进方面，我们正处于“触手可及的成果”阶段。

### **ARC奖金**的缘起
现在我们回到**Zapier**的联合创始人**Mike Knoop**。你正在资助并与**François**一起运营这项奖金。告诉我这是如何促成的？是什么促使你们发起这项奖金？我对AI好奇了**13年**。我联合创立了**Zapier**，并运营了**13年**。我第一次接触你的工作是在**新冠疫情**期间。我深入研究，当时有很多空闲时间。那是在你发表了论文**《On the Measure of Intelligence》**之后。你提出了AGI的概念，并认为**技能获取效率**是正确的定义，以及ARC谜题。

### **链式思考**论文的启发
我记得当时第一个**Kaggle**（Kaggle: 一个数据科学和机器学习竞赛平台）比赛还没结束，还在进行中。它很有趣，但我只是把这个想法搁置了，因为我在**Zapier**有更重要的事情要做。我们当时正处于一个巨大的转型期，试图推出我们的第二款产品。直到**2022年1月**，**链式思考**（chain-of-thought: 一种提示工程技术，引导大型语言模型逐步推理以解决复杂问题）论文发表，才真正唤醒了我对进展的认识。我甚至给Zapier做了一场关于**GPT-3**论文的完整演示。我感觉我已经充分评估了LLMs能做的一切。那篇论文让我非常震惊，因为它揭示了LLMs拥有我未曾预料到的所有这些**潜在能力**。

### **ARC**：被忽视的AGI评估
我实际上放弃了我的执行团队角色。当时我负责公司一半的业务。我回到了个人贡献者的岗位，与我的联合创始人**Bryan**一起进行AI研究。最终，这让我重新关注**ARC**。我再次研究它。我曾预期会看到**MMLU**（Massive Multitask Language Understanding: 大规模多任务语言理解基准）和**GSM8K**所具有的**饱和效应**。但当我查看过去四年来的分数和进展时，我非常震惊地发现我们在这方面几乎没有取得客观进展。我觉得这是一个非常重要的评估。在过去一年里，我在我的网络和社区中向人们询问此事，很少有人知道它的存在。如果说这是一个真正全球独一无二的AGI评估——并且它不同于其他更狭隘地衡量AI技能的现有评估——那么应该有更多人知道这件事。

### **ARC**知名度低的原因
我也有自己如何击败ARC的想法。我夜以继日地为此工作。今年早些时候我飞去见**François**，向他提问并展示我的想法。最终我问他为什么更多人不知道ARC？你实际上应该回答这个问题。这是一个非常有趣的问题。你认为为什么更多人不知道ARC？在研究社区中获得关注的基准，通常是那些已经相当**易于处理**的基准。其动态是，某个研究小组会取得一些初步突破，然后这会引起其他所有人的注意。你会看到后续论文，人们试图击败第一个团队等等。

### **ARC**：推动新AI思想的催化剂
ARC之所以没有真正发生这种情况，是因为它对现有的AI技术来说确实非常困难。ARC要求你尝试**新想法**。这正是重点。重点不是你应该能够简单地应用现有技术来解决ARC。重点是现有技术已经达到了**平台期**。如果你想超越这一点，并开始能够解决你以前没有记忆或见过的问题，你需要尝试新想法。ARC不仅仅是衡量我们离AGI有多近的一种方式，它也是**灵感的来源**。我希望研究人员看到这些谜题时会想：“嘿，这些谜题如此简单，大多数人类都能很快解决，为什么对现有的AI系统，对LLMs来说却如此困难呢？”

### **ARC**抵抗记忆化的实证与理论
这对于LLMs来说是事实，但ARC实际上是在LLMs真正兴起之前发布的。当时它之所以特别，唯一的原因是它被设计成**抵抗记忆化**。它如此出色地经受住了LLMs和**生成式AI**（GenAI: Generative AI: 能够生成文本、图像或其他媒体的AI模型）的考验，这表明它确实抵抗记忆化。这就是让我着迷的地方。我自己去尝试了一堆谜题，也给我的所有朋友和家人看。他们都说：“哦，这太简单了。你确定AI解决不了吗？”这就是他们的反应，我也是一样。你越深入研究，就越会意识到，不仅有过去四年它未被击败的**经验证据**，背后还有**理论概念**支撑。在这一点上，我完全同意需要新想法才能击败ARC。

### 封闭式研究阻碍AI创新
当前世界有许多趋势实际上正在阻碍这种情况的发生。我们现在产生新想法的可能性实际上更小。其中一个趋势是**前沿研究的封闭化**，对吗？**OpenAI**的**GPT-4**论文没有分享任何技术细节。**Gemini**论文也没有分享任何技术细节，比如那项工作的更长上下文部分。然而，正是这种**开放创新**、进步和分享，才使我们最初有了**Transformer**模型，才有了LLMs。因此，如此多的前沿工作变得封闭，确实令人有些失望。这实际上是在押注这些独立的实验室将是取得突破的主体，而不是整个生态系统。互联网和**开源**已经表明，它是迄今为止，可能也是全世界最强大的创新生态系统。

### **OpenAI**对AGI进展的影响
前沿研究不再公开发表，这确实令人非常遗憾。如果你回顾四年前，一切都是公开分享的。所有**最先进的**成果都已发表。现在情况不再如此。**OpenAI**单枪匹马地改变了游戏规则。OpenAI基本上将AGI的进展推迟了好几年，可能**五到十年**。这有两个原因。一是他们导致了前沿研究出版的完全封闭。但他们也引发了围绕LLMs的最初一波**炒作**。现在LLMs已经“吸干了房间里的氧气”，每个人都在做LLMs。我实际上认为LLMs更像是通往AGI道路上的一个“岔路口”。所有这些新资源实际上都流向了LLMs，而不是它们本可以流向的其他一切。

### 早期AI研究的开放性与多样性
如果你回顾到2015年或2016年，当时从事AI研究的人比现在少了一千倍。然而，进展速度却更高，因为人们探索了更多的方向。世界感觉更加开放。你可以去尝试，你可以有一个很酷的启动想法，尝试一下，然后得到一些有趣的结果。当时有一种能量。现在每个人都在做同一种事情的某种变体。大型实验室也尝试过ARC，但因为结果不佳，他们没有发表任何东西。人们只发表**积极的结果**。

### 前沿模型在ARC上的潜力
我不知道人们投入了多少精力去尝试提示或搭建脚手架，采用某种**Devin**（Devin: Cognition AI公司发布的首个AI软件工程师）类型的方法，让前沿模型在ARC上产生好的解决方案。我是指当今的前沿模型，而不仅仅是一年前的。大量的**后训练**（post-training: 在模型预训练之后进行的进一步训练，通常用于特定任务或微调）已经投入到使它们变得更好。有**Claude 3 Opus**或**GPT-4o**。我希望本期节目能促使人们尝试这项**开放竞赛**。他们必须提交一个开源模型来竞争，但我们也可以弄清楚Claude中是否潜藏着这种能力，看看你是否能展示出来。那会非常有趣。

### **ARC奖金**设置与目标
我们来谈谈奖金。如果你解决了它，你能赢多少钱？假设你在ARC上获得了某个百分比。如果你提交了最好的方案但未能完全破解，你能得到多少钱？我们有**超过一百万美元**的奖金池。我们每年举办一次比赛，从今天开始到**11月中旬**。目标是达到**85%**，这是你们之前谈到的**人类平均水平的下限**。第一个能达到85%基准的团队将获得**50万美元**的奖金。我们不期望今年就能实现。**Zapier**的一位早期统计学家给了我一句话，我一直铭记在心：“时间越长，所需时间越长。”我的预设是ARC需要数年才能解决。我们今年还将设立一个**进步奖**。

### **ARC**进步奖与开放共享
我们设立了**10万美元**的进步奖，将颁发给得分最高的团队。其中**5万美元**将奖励给今年在**Kaggle排行榜**上客观得分最高的团队。我们将在Kaggle上举办比赛。另外**5万美元**将奖励给概念性解释其所获分数的最佳论文。有趣的是，我们还将要求获奖者将解决方案或论文公开发布。通常在比赛中，你会看到很多封闭式分享，人们保持私密和秘密，希望在比赛期间保留自己的“阿尔法”（alpha: 投资领域指超越市场基准的回报，这里引申为独家优势或秘密）。

### 年度竞赛与知识共享机制
由于我们预计这将是一个持续多年的项目，我们希望这里能有一个**互动式游戏**。计划是，在**11月底**，我们将颁发10万美元的进步奖给最高得分者。我们将利用**12月到2月**的空闲时间，分享所有来自高分团队的知识和他们采用的方法。这样，我们将把社区的基线提升到当前的**最先进水平**，然后明年再举办一次比赛。我们将每年都这样做，直到达到85%。

### **ARC**挑战的意外结果
我给大家一些背景信息，解释为什么我认为这个奖项非常有趣。我曾与我的朋友们交谈，他们非常相信当前存在的模型。首先，让我感到好奇的是他们竟然不知道ARC，他们都是经验丰富的**机器学习研究员**。几天前，我们去吃饭，我给他们看了一个示例问题。他们说：“哦，LLMs当然能够解决这样的问题。”我们截了个图，直接放进了我们的**ChatGPT**应用程序，但它没有识别出模式。所以这非常有趣，这是一个值得注意的事实。我在这些问题上一直在扮演你的“魔鬼代言人”，但这确实是一个非常引人入胜的事实。这个奖项极其有趣，因为我们无论如何都会学到一些迷人的东西。

### 基准饱和与**Epoch AI**的观察
关于85%的成绩，除了这个奖项之外，我非常好奇是否有人能复制这个结果。显然，在心理学和其他领域，这种结果似乎与之类似，当你对少量人群进行测试时，结果往往难以复制。我非常好奇，如果你尝试复制这个结果，普通人在ARC上的表现如何？我也好奇破解这个基准的难度以及需要多长时间。思考其他现在已经完全饱和的基准，比如**MMLU**和**MATH**，这非常有趣。**Dan Hendrycks**和**Collin Burns**在创建MMLU和MATH时，他们还是研究生或大学生。几年前他们创建这些基准时的目标是将其作为AGI的测试。当然，它们完全饱和了。我知道你会争辩说这些是记忆力测试。但我们看到了一种模式。事实上，**Epoch AI**有一张非常有趣的图表，你看到一个几乎是**指数级增长的曲线**。随着你在模型中增加计算量，它会从5%、10%、30%、40%开始，然后突然飙升。

### **HumanEval**与ARC的未来趋势
在**GPT-4技术报告**中，他们有一个关于**HumanEval**问题集的有趣图表，其中包含22个编码问题。他们必须将其绘制在**平均对数通过曲线**上。在训练早期，甚至对于较小的模型，它们就能对如何解决这个问题有一个正确的想法。要确保它们始终如一地解决整个问题，需要很高的可靠性。你确实希望提高它们至少在某些时候（也许是1/100或1/1000）答对的信号权重。它们从1/1000上升到1/100和1/10，然后就完全饱和了。这就是所有这些问题最终指向的问题：为什么ARC不会发生同样的事情？人们不得不非常努力地使用更大的模型。现在他们已经找到了像**Jack Cole**那样的方法，仅用一个2.4亿参数的语言模型就能达到35%的成绩。我们难道不应该在所有其他基准测试中看到相同的模式吗？你只是勉强起步，然后一旦掌握了通用思想，就一路飙升到100%？

### **Jack Cole**方法的独特性
这是一个经验性问题，我们将在实践中看到会发生什么。**Jack Cole**所做的事情实际上非常独特。它不仅仅是预训练一个LLM然后进行提示。他实际上正在尝试进行**主动推理**。他正在做测试时微调，对吗？他正在做**测试时微调**。没错，他正在做测试时微调。这实际上是在试图解决LLMs的一个关键局限性。在推理时，它们无法学习任何新东西，也无法即时适应它们所看到的东西。他实际上正在尝试学习。他所做的实际上是一种**程序合成**形式。LLMs包含许多有用的构建块，即编程构建块。通过在测试时对任务进行微调，你正在尝试将这些构建块组装成与任务匹配的正确模式。这正是程序合成的本质。

### 离散程序搜索与LLM方法的对比
我将这种方法与**离散程序搜索**进行对比。在离散程序搜索中，你试图从一组**原语**（primitives: 指构成程序的基本操作或元素）中组装一个程序。你拥有的原语非常少。例如，在ARC上进行离散程序搜索的人倾向于使用具有100到200个原语程序的**DSL**（领域特定语言）。这是一个非常小的DSL，但他们试图将这些原语组合成非常复杂的程序。搜索的深度非常深。另一方面，是**Jack Cole**对LLMs所做的事情。他在LLM中拥有一个由数百万个构建块组成的**向量程序数据库DSL**。这些构建块是通过预训练LLM挖掘出来的，不仅在大量编程问题上，还在数百万个生成的ARC类任务上进行训练。你拥有一个极其庞大的DSL，而微调是对这些原语进行非常浅层的重组。离散程序搜索是对非常小的原语程序集进行非常深度的重组。LLM方法也是如此，但处于这个谱系的完全相反一端。你通过大规模因子扩展记忆化，并进行非常浅层的搜索。它们是同一件事，只是谱系的不同两端。

### 最佳计算价值：记忆与深度搜索的结合
我认为你的计算周期能获得最大价值的地方介于两者之间。你希望利用记忆来建立一个更丰富、更有用的**原语程序库**。你不希望它们像我们看到的典型**RTS**（即时战略游戏）那样是硬编码的。你希望它们能从例子中学习。你还希望进行一定程度的**深度搜索**。只要你只进行非常浅层的搜索，你就局限于**局部泛化**。如果你想进一步、更广泛地泛化，搜索深度将是至关重要的。

### 计算限制与模型规模
我可能会争辩说，他之所以如此严重依赖**合成数据**，是因为他使用了2.4亿参数的模型。当时的**Kaggle竞赛**要求他使用**P100 GPU**（NVIDIA Tesla P100: 英伟达推出的一款高性能计算GPU），其浮点运算能力大约只有**H100**（NVIDIA H100: 英伟达推出的一款更先进的数据中心GPU）的十分之一。对于听众来说，今天的前沿模型实际上比那大一千倍。对于你们的比赛，提交的模型不能进行任何**API调用**，不能上网，并且必须在**NVIDIA Tesla P100**上运行。它的性能显著较低。基本上有**12小时的运行时间限制**，评估中存在一个效率的**强制函数**。

### ARC任务的计算资源与人类大脑
但问题是，你只有100个测试任务。每个任务可用的计算量实际上相当可观，特别是当你将其与每个任务的简单性进行对比时。基本上，每个任务有**7分钟**的时间。那些尝试估算人脑有多少**浮点运算**的人，你可以姑且听之，但作为一个参考点，它基本上是**H100**的浮点运算量。也许你会争辩说，人脑可以在不到7.2分钟内解决这个问题。即使只有十分之一的计算量，你也应该能够在7分钟内完成。显然，我们大脑中的快速访问内存少于**拍字节**（petabytes: 1拍字节等于1000万亿字节），而H100中则有29GB或更多。

### 规模化假说与私有测试轨道
更广泛的观点是，我希望有一种方法也能用某种**脚手架**（scaffolding: 在AI中指为模型提供额外结构或辅助信息以帮助其完成任务）来测试这个奖项，以验证规模化是否是解决ARC的途径。当然。在比赛的背景下，我们希望看到在有限资源下能取得多大进展。但你完全正确，这是一个非常有趣的开放问题：目前最大的模型在ARC上实际能做些什么？我们实际上也想提供一个**私有的一次性测试轨道**，你可以向我们提交一个**虚拟机**（VM: Virtual Machine: 虚拟计算机）。你可以在上面放置任何你想要的模型，你可以选择一个最大的开源模型，进行微调，做任何你想做的事情，然后给我们一个镜像。然后我们会在**H100**上运行它24小时左右。你就可以看到结果。

### 公开与私有测试集的挑战
值得指出的是，有两个不同的测试集。有一个**公开测试集**，位于公共**GitHub**仓库中，任何人都可以用来训练。你可以进行开放的**API调用**，做任何你想做的事情。然后是**私有测试集**，这100个任务实际上衡量的是最先进水平。让人们至少尝试使用公开测试集并进行尝试，这是相当开放和有趣的。现在，任何针对公开测试集报告的分数都会有一个星号，因为它是公开的，它可能已经泄露到训练数据中。这实际上是人们已经在做的事情。你已经可以尝试用公开评估集中的任务来提示最好的模型，比如最新的**Gemini**或最新的**GPT-4**。问题是，这些任务以**JSON文件**的形式在GitHub上可用，而这些模型也是在GitHub上训练的。所以它们实际上是**在这些任务上训练过**的。

### 避免记忆化：创建新颖测试集
这会产生不确定性。如果它们真的能解决一些任务，那是因为它们记住了答案，还是因为其他原因？也许你最好尝试创建自己的私有、类似ARC的**非常新颖的测试集**。不要让任务变得困难，不要让它们复杂。让它们对人类来说非常明显，但要确保尽可能地**原创**。让它们独特、不同，然后看看你的**GPT-4**或**GPT-5**在它们上面的表现如何。

### 基准过拟合与**Scale**的测试
已经有测试探讨这些模型是否在这些基准上**过度训练**。**Scale**最近对**GSM8K**进行了这项测试。他们基本上复制了基准，但使用了不同的问题。一些模型实际上在基准上**严重过拟合**，比如**Mistral**等。而像**Claude**和**GPT**这样的前沿模型，在他们的新颖基准上的表现与在现有公共基准中的特定问题上的表现一样好。

### **API访问**与数据泄露的玩笑
我相对乐观地认为它们只是在**JSON**上进行训练。我曾和**Mike**开玩笑说，你们应该允许**API访问**，但保留一个更私密的ARC问题验证集。这样，你们允许API访问，人们可以使用**GPT-4**的**脚手架**来参加这个比赛。也许之后你们可以在API上运行验证集。如果它的表现比你们最初允许API访问的测试集差，那意味着**OpenAI**正在训练你们的API调用。你们可以公开这一点，然后向他们展示：“天哪，他们泄露了你们的数据。”

### **ARC 2.0**的演进与测试服务器
我们确实希望**ARC数据集**能够演进。这是我们想要实现的目标。**François**提到它并不完美。是的，ARC不是一个完美的基准。我是在四年多前，现在差不多五年了，在LLMs出现之前制作的。从那时起，我们实际上学到了很多关于可能存在的潜在缺陷。任务集中存在一些**冗余**，这当然与基准的目标相悖。实际上，每个任务都应该独一无二，但这并不完全是事实。每个任务也应该非常新颖，但实际上，它们可能不是。它们可能在结构上与你在网上某个地方找到的东西相似。因此，我们希望继续迭代，并在今年晚些时候发布**ARC 2.0版本**。届时，我们将把旧的私有测试集公开。也许我们不会公开发布它，但我们可以创建一个**测试服务器**，你可以在那里查询、获取任务并提交解决方案。当然，你可以在那里使用任何你想要的前沿模型。

### **API**访问确保数据纯净性
因为你实际上必须查询这个**API**，你就能确保没有人会意外地在这个数据上进行训练。这与目前公共的ARC数据不同，后者实际上就在**GitHub**上。模型是否在此数据上进行训练，实际上没有疑问，因为它们在GitHub上进行训练。通过限制访问并要求使用这个API，我们将避免这个问题。对于那些想尝试他们心中任何技术的人，使用他们想要的任何资源，那将是他们获得答案的一种方式。

### **ARC**挑战的潜在结果
我不知道会发生什么。我不确定。一个答案是他们提出了一种全新的AI算法，带有明确的**程序合成**。现在我们走上了一条新轨道。另一个答案是他们对现有模型进行了一些巧妙的“**hacky**”（hacky: 指非正规但有效的解决方案）操作，这种方式实际上是有效的，这揭示了也许智能更多的是将事物引导到分布的正确部分，然后它就能推理。在那个世界里，那将很有趣。也许那会表明你必须对当前模型做一些“hacky”操作。随着它们变得更好，你就不必再做“hacky”操作了。我也非常好奇这些**多模态模型**是否会在ARC类测试中原生表现得更好。

### 奖金升级与新想法的呼唤
如果ARC在三个月内经受住考验，我们将增加奖金。我们即将通过大幅增加奖金池，与现实进行一次非常重要的接触。我们将很快了解到是否存在大量**唾手可得的想法**。再次强调，我认为需要新想法。任何听众可能都有自己的想法，我鼓励每个人都尝试一下。随着时间的推移，这将进一步强化我们进展停滞不前、需要新想法才能击败ARC的论点。

### **Kaggle**竞赛与ARC的韧性
是的，这就是设立奖金的目的。你吸引更多人来尝试解决它。如果有一个简单的方法可以“破解”基准，那就会揭示基准存在缺陷。你会知道的。事实上，这就是2020年最初的**Kaggle** ARC竞赛的目的。我当时举办这个比赛，因为我发布了这个数据集，我想知道它是否可以被“破解”，是否可以作弊。当时有一个小额奖金，大约是**2万美元**。这与**GPT-3**发布的时间差不多。人们当然在公共数据上尝试了GPT-3，它得了**零分**。第一次比赛告诉我们，没有明显的捷径。现在有更多的钱，会有更多人研究它。我们会发现的，我们会看看这个基准是否能经受住考验。

### **ARC**解决方案：软件开发新范式
假设我们最终得到一个解决方案，它不是试图暴力破解可能的ARC任务空间，而是只在**核心知识**上进行训练。我不认为它本身必然就是AGI，但它很可能是通往AGI道路上的一个巨大里程碑。它代表的是**仅从两三个例子中合成问题解决程序的能力**。这本身就是一种新的编程方式，是软件开发的一个全新范式。你可以开始编程潜在的相当复杂的程序，这些程序将很好地泛化。你不再是通过在脑海中构思程序形状然后敲代码来编程，而是直接向计算机展示你想要的输出，让计算机自己去解决。这才是极其强大的。

### **Claude Opus**与代码解释器
我想稍微谈谈这里可能有哪些解决方案，以及哪些你认为会违背ARC的目的，哪些是有效的。这里我提一个例子。我的朋友**Ryan**和**Buck**昨晚熬夜了，因为我告诉了他们这件事。他们说：“哦，LLMs当然能解决这个。”很好，谢谢你传播这个消息。他们尝试用**Claude Opus**来解决这个问题，他们说在公共ARC测试中获得了**25%**的成绩。他们所做的是，提供了其他一些ARC测试的例子，并在上下文中解释了从一个输出到另一个输出的推理过程，然后才给出当前问题。我想他们还以一种对**分词器**（tokenizer: 将文本分解成更小的单元，如单词或子词）更友好的方式表达了**JSON**。另一件事是使用了**代码解释器**。你认为随着这些模型变得更智能，不断改进的代码解释器，就是程序合成本身吗？他们能够通过代码解释器获得单元格的实际输出，即JSON输出，比如“在这里编写能得到正确输出的Python程序”。你认为你所说的程序合成研究，会仅仅看起来像在大型语言模型中使用代码解释器吗？

### 融合深度学习与程序合成的未来
我认为任何能取得好成绩的解决方案，可能都需要利用深度学习模型，特别是LLMs的某些方面。我们已经证明LLMs可以做得相当好，这基本上就是**Jack Cole**的方法。我们也证明了来自小型**DSL**的纯粹**离散程序搜索**表现非常好。在Jack Cole之前，这才是最先进的技术。事实上，它仍然非常接近最先进水平，而且这些模型中根本没有涉及深度学习。我们有两种基本上没有重叠但表现都相当好的方法。它们处于一个谱系的两个极端。一端是数百万个**向量程序**的极其庞大库，但重组非常浅层、简单。另一端是非常简单的DSL，100-200个原语，但搜索非常深入、非常复杂。

### **ARC**竞赛的合法与作弊行为
解决方案将介于两者之间。那些将赢得ARC竞赛并在近期AGI方面取得最大进展的人，将是那些设法将**深度学习范式**和**离散程序搜索范式**以一种优雅的方式融合在一起的人。你问什么是合法的，什么是作弊。如果你想在系统中添加一个**代码解释器**，我认为那很棒，那是合法的。作弊的部分是试图预测测试中可能出现的内容，比如暴力破解可能的任务空间，并在此基础上训练一个记忆系统。你依赖于你正在生成数百万个任务的事实，不可避免地，你生成的内容和测试集中的内容之间会有一些重叠。这违背了基准的目的，因为那样你就可以用它来解决问题，你只需要通过提取一个记忆的解决方案来适应。希望ARC能够抵抗这一点，但没有完美的基准。也许有办法“破解”它。我们很快就会得到答案。

### 核心知识与人类经验
尽管一定程度的微调是有效的，因为他们必须使用**开源语言模型**来竞争，而且它们本身就是语言模型。它们需要能够以ARC类型的方式思考。是的。你希望将**核心知识**，类似ARC的核心知识，输入模型，但你肯定不需要数千万个任务来完成这项工作。核心知识是极其基础的。如果你看一些ARC类型的问题，我确实认为它们有点依赖我一生中见过的事物。例如，一个东西从墙上弹回来，你看到了这种模式。我玩过**街机游戏**，也见过**Pong**（Pong: 史上最早的街机游戏之一，模拟乒乓球）。例如，你看到**弗林效应**（Flynn effect: 指在20世纪，许多国家的人口在智商测试中的平均得分持续上升的现象），以及人们的智力（通过**瑞文渐进矩阵**衡量）在这些类型的问题上有所提高。这可能是一个类似的故事，我们从小到现在，实际上在电视等媒体中看到了这些类型的模式，这些**空间模式**。

### 核心知识的习得与智能
所以我认为这不是核心知识。这实际上也是人类在成长过程中所经历的“微调”的一部分，看到不同类型的空间模式并尝试进行模式匹配。我肯定会将其归类为**核心知识**。核心知识包括基本物理，例如弹跳或轨迹，这些都会被包括在内。但是的，你完全正确。作为人类，你之所以能够快速找出解决方案，是因为你头脑中有一套可以重组的构建块和模式。核心知识是获得智能所必需的吗？对于你拥有的任何算法，核心知识是否必须在某种意义上是硬编码的？或者核心知识甚至可以通过智能来学习？

### 核心知识的早期获取
**核心知识**是可以学习的。对于人类来说，一部分核心知识是与生俱来的。我们出生时就带有一些关于我们将要生活的世界的少量知识，我们并非**白板**（blank slates: 哲学概念，指心灵在出生时是空白的，所有知识都来自经验）。但大多数核心知识是通过经验获得的。核心知识的特点是它不会在学校里学到，而是在你生命的前三四年非常早期就获得了。到四岁时，你就拥有了成年后所需的所有核心知识。

### 开放版本与规模化假说
有趣。关于奖金本身，我非常期待看到开源版本，也许是使用**Llama**（70B）之类的模型，以及人们在比赛中能取得怎样的分数。我也很高兴能专门测试**规模化假说**，并且我非常好奇你是否能在公共版本的ARC上进行提示。你将无法将其提交到这个比赛本身，但我非常好奇是否有人能破解它，让ARC在那里发挥作用。那会更新你对AGI的看法吗？这真的会很有动力。我们将继续举办比赛，直到有人在公共领域发布一个**可复现的开源版本**。即使有人私下击败了ARC评估，我们仍将保留奖金，直到有人能够复现并发布公共可复现版本。

### 加速AGI进展与开放共享
没错。目标是加速AGI的进展。其中一个关键部分是，任何有意义的进展都需要**共享**，需要**公开**，这样每个人都能知道并尝试在此基础上进行迭代。如果没有共享，就没有进展。我特别好奇的是**拆分赌注**。我们能否制作一个开放版本，或者这只能通过规模化实现？我们可以根据公共版本和私有版本来测试这两种情况。

### 计算限制与基准演进
我们也在通过这个与现实接触。我们将学到很多关于计算的实际限制。如果有人出现并说：“嘿，这是一个闭源模型，我用它获得了+50%的成绩，”那可能会更新我们的认知。我们会想：“好吧，也许我们应该增加在私有测试集上提供的计算量以达到平衡。”最初的一些决定有些随意，目的是为了了解人们想要什么，进展看起来是怎样的。我们俩都致力于随着时间的推移不断改进它，使其达到我们所能达到的最好或最接近完美的状态。

### **ARC奖金**：立即参与
太棒了。人们可以去哪里了解更多关于这个奖项的信息，也许可以尝试一下？**Arcprize.org**。它现在已经上线了。今天正式启动。一百万美元的奖金等着大家。谢谢你们来到播客。很高兴能探讨智能的所有关键点，获得不同的视角，并在这里宣布一项奖金。这太棒了。谢谢你们帮助发布这个消息。谢谢你们邀请我们。