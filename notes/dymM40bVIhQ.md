---
area: tech-insights
category: technology
companies_orgs:
- Meta
- Twitter
- MIT
- CMU
- 智源
date: '2025-10-30'
draft: true
guest: ''
insight: ''
layout: post.njk
people:
- 田渊栋
- Ilya Sutskever
- Geoffrey Hinton
- Richard Sutton
products_models:
- GPT-5
- SVM
project:
- ai-impact-analysis
- systems-thinking
- knowledge-pipeline
series: ''
source: https://www.youtube.com/watch?v=dymM40bVIhQ
speaker: 课代表立正
status: evergreen
summary: Meta前研究科学家田渊栋深入探讨AI领域备受关注的“顿悟”（Grokking）现象，揭示模型从记忆到泛化的内在机制。他分享了AI研究中直觉与经验的重要性，并对比了当前主流的“规模法则”与深入理解模型内部机制的两种科研范式。此外，田渊栋还探讨了AI学习中的“优雅”追求、损失函数作为“代理”的角色，以及AI作为研究助手的潜力，为理解AI如何产生新知识提供了独到见解。
tags:
- learning
- llm
- model
- research
title: 田渊栋谈AI的“顿悟”：模型开窍的关键，对优雅的追求与科研范式之争
---

### 离开Meta后的思考与团队贡献

**课代表立正**: 我是在准备访谈的时候才发现您在Meta已经工作10年了。

**田渊栋**: 对，对。那个时候是2015年，Meta还是个很小的公司。2012年上市的。

**课代表立正**: 您加入的时候Meta有多少人？

**田渊栋**: 加入的时候大概有1万多一点。

**课代表立正**: 也不小了。

**田渊栋**: 不是特别小，但对比现在可能几十万。现在我没有访问权限了，所以也算自由了，想干什么就干什么了。

**课代表立正**: 我们沿着原来的访谈大纲聊您的论文，并探讨一些新的发展。

**田渊栋**: 好的，我们可以聊一些论文，我觉得这样比较好，因为我其实不太想说太多关于裁员的事情，我也觉得这不太合适。其实我这边本意是希望我的团队里受影响的几个人能有更好的机会，因为我无所谓，我在家里待着最惨，但是他们很多人身份会有一些问题，如果不能及时找到下一家的话。我想办法帮忙找一找，因为我毕竟认识的人比较多，这也是我的本意。我反正不怕暴露自己被裁，我不在乎，但是希望我的下属那些人能很快找到工作，这也是我的本意。我觉得是这样，不要聊我们在公司的问题，我觉得我已经说够多了，我不想再说了。我一般不太愿意说这些。我觉得我在Twitter上唯一说的是，因为有人跳出来说我们被裁是应该的，因为东西没做出来。那我要至少给我们的团队澄清，对吧？因为我们团队做了很多重要工作，你不能把锅扣到我们头上，所以这个肯定是要讲清楚。但是我现在就比较防御性，如果有人说这是我们的责任，那我会反驳回去。除此之外，我不会说太多公司内部的事情。

**课代表立正**: 那您还有没有什么想澄清的，觉得没有澄清好的？

**田渊栋**: 哈哈，我觉得差不多了，就这样。我觉得我们其实还是做了很多工作，把很多之前的一些问题解决了。比如说，包括**长上下文强化学习**（Long Context Reinforcement Learning: 一种处理序列较长输入数据的强化学习方法）有没有训练得好，还有包括前面的**预训练模型**（Pre-training Model: 在大量数据上预先训练好的模型，作为特定任务的起点），它们的**设计**（design）其实可能有些问题，像有**稀疏注意力机制**（Sparse Attention: 一种优化注意力机制的方法，只关注输入序列中的部分关键信息以提高效率）的问题。这个其实很多是我们团队解决的。我先发现的，问他们这个设计有问题，然后去跟他们讲。但是一开始很难，他们不一定听我们。我当时去的时候，相当于一个研究团队过来的，对方是做大模型的。我这边研究团队过来的话，他们不一定会听，他们可能会觉得这个事情没问题，肯定是对的。我们这边要用各种实验去证明我们之前的那些发现，或者说那些**洞察力**（insights）是对的。后来他们是被说服了，所以他们才会发现这里有问题。这个其实都是我们团队的贡献。还包括，怎么样去让长上下文强化学习更加稳定，包括有很多模型崩溃（blew up）的问题，怎么样去解决。这些东西都是我们这边做的。但这些东西也就属于幕后英雄，毕竟最终我们这个模型也没有真正官方发布。至少我们有一些贡献在里面，这个我得说出来，至少为后面的人添砖加瓦，做一个比较好的基础。就是这样子。

### 研究员的宝贵财富：经验与洞察力

**课代表立正**: 我倒有两个问题了。第一，你们作为一个研究团队，人家不信你，可能是觉得你之前没有训练大型模型的经验，或者怎样，但是你们能很快发现问题，您觉得为什么可以做到？第二个就是，对面那个大模型团队是个什么样的团队？他们自己本身大模型训练经历丰富吗？

**田渊栋**: 他们训练是经验丰富的。那他们有一些之前的实验有**错误**（bug）。这个错误导致他们做出错误的判断。但是我们这边虽然说没有训练模型，但你毕竟是做过大模型的，有些文章，对吧？包括我以前做过稀疏注意力机制。那我当然对注意力结构，我知道什么意思，怎么回事。那我当然一看这个设计，我知道有问题。这个我相信很多人都能看出来，这个并不是说我能看出来，你肯定能看出来。但我并不知道当时这个决定怎么做。但是我愿意说，但是没办法，因为其实也很难说服他们。就是你要花很多时间和精力去跟他们说服这个是有问题。后来他们自己团队发现了也是有这个问题，慢慢就会改变这个想法。虽然说研究员，可能我们当时做研究的时候，并没有直接去接触超大模型训练，但是研究的这些直觉，或者说这些经验其实很有用，对吧？它能够很快能够找到问题，能够发现什么地方是有问题，是有出错的，怎么样去解决。我觉得这个是很重要的。这个是作为一个研究员的宝贵财富。说实在的，你如果是一个完全没有任何洞察力的人，好，我天天就跑实验，然后调参数，这个工作其实你说你能做，别人也能做，对吧？那研究员的优势是说，我能不能根据一些非常稀疏的数据点，能够得到非常重要的结论，这结论能够推广到难题上。这个是研究员的能力。

**课代表立正**: 您说的稀疏的数据点，是不同论文和不同实验的结果吗？

**田渊栋**: 对，就比如说，我如果是个新来的菜鸟，那么对我来说，好，我的任务是调参数，跑程序。比如我跑1万个点，我就得到1万个点的参数的值，然后我就说，我告诉大家这1万个点是我跑的。跑完之后跟大家说，好，跑完了，这是我的结论。但是跑完之后这1万点就在那边，是死的，你也没有什么洞察力，没有什么概念说这1万点其实代表了后面什么意思，有什么样的结构。那这个其实只有那些有经验的人才能看到。有经验的人可能看到20个点，我就知道有什么问题，甚至说看10个点，看到这个训练曲线刚刚训练了一半，哦，我知道不行了，不要跑下去，可能你这里有问题。这个其实是为什么AI的研究还是薪资比较高。我觉得很多时候是这样，你的一个洞察力可以抵，比如说100块计算卡，或者说抵1000块计算卡。我不需要那么多计算卡，但是我还是有洞察力，可以得到一些比较好的结论，这个是重要的。

**课代表立正**: 您刚才用了两个词，一个是经验，一个是洞察力，然后我想深入探讨一下，这个到底是个什么东西？有的人会觉得这是一个品味，有的人会觉得是个直觉，我们有好多词去形容这个东西，那您觉得这个东西到底是什么？

**田渊栋**: 刚刚已经有四个词了，对吧？四个词说的好像都是一件事。那您能不能给大家讲一讲，从您的经验来说，您用多长时间能判断一个人有没有这个？您觉得它有的话，它除了像您刚刚说的，从很小的数据点就能得出来一个更正确的结论，还有什么展现？以及怎么得到这个东西？我觉得是这样，洞察力是一个很难描述的概念。特别是一个有经验的人，比如说在某方面他是个老师傅，那么他怎么做？他要根据很少的数据，然后判断这个现象背后的真正原因是什么，这个是重要的。比如说一个修车师傅，他可能根据蛛丝马迹，会知道你车哪里坏了。

**课代表立正**: 明白。

**田渊栋**: 你还没有反应过来，说这个事情就坏了。或者说一个交易员，我做股票交易的，我说我根据这两个迹象，或者看看财报。所以这种东西是很重要。他讲不清楚到底怎么回事，但他就有种感觉说，这个不行，那个行。有一个**心智模型**（Mental Model: 个人对世界运作方式的内在认知框架），这个心智模型大概率是对的。这个其实很重要。有这些东西之后，其实很快能够发现问题在哪，然后有这问题，我们怎么样去解决这个问题，然后往这个方向去走。这个可能比图形处理器（GPU）还要重要。当然图形处理器也很重要，有图形处理器之后，你会做更多实验，获得更多的洞察力。这两个是相辅相成的，应该这么说。

**课代表立正**: 您能很快判断另外一个人有没有一个好的心智模型吗？

**田渊栋**: 这个其实是有一些办法，就是说你是要跟别人聊，大概聊一下，感觉一下他平时对这问题怎么想的。我觉得这个其实挺重要的。其实我可以举个例子，比如说学校里面有这种**博士资格考试**（PhD Qualifier: 博士生在完成课程后，为评估其研究能力和知识深度而进行的考试）。一个学生坐在自己老师面前，然后老师问他，请问你对这个问题有什么了解？比如说我们讨论一些学术问题，对这个老师来说，他想办法问到底。比如说，你对这个偏微分方程有什么想法？你有什么一二三四五这些经验？然后就是抓住一个点，然后使劲问，你就知道他到底懂不懂了。他到底知道这里面之间什么关系，能用最简单的语言讲清楚。对，然后就能够知道最重要的两个东西的关联是什么。这样的话，就知道他真是懂的，或者说他真的是知道最关键的关联在哪里，可以用这关联去做更多的推广。这个是重要的。像比如做研究的话，比较忌讳的是说，我就只懂书面知识，一二三四五，背出来了，但是他们有什么关系，什么时候他们两个成立，他们俩不能成立，什么时候A蕴含B，什么时候蕴含C，这个并不知道的话，其实是比较难搞的。我觉得这个是一个问题。其实这个很重要，说实在的，是我觉得现在的模型做不到的地方。现在模型可能没有办法用很少的数据，真的去预测将来的结果。

### AI的“顿悟”现象与知识的压缩

**课代表立正**: 那我们就直接到这个话题吧。您的论文是关于**顿悟**（Grokking: 机器学习模型在训练过程中，从记忆训练数据突然转变为泛化理解数据的一种现象）的，但是它是一个底层的，这么一个，在一个时间点，它有了一个学习的这样的东西，顿悟的感觉，是吧？

**田渊栋**: 嗯。

**课代表立正**: 我在看您跟智源的专访，里边您也提到一个点，就是鸽子问题。您当时和Denny Zhou在Twitter上关于**思维链**（Chain of Thought: 大语言模型通过逐步推理来解决复杂问题的一种方法）的一些讨论，就是说确实，理论上也许您的这个逻辑能表述的话，思维链似乎可以解决，但是模型会用无限的数据去试图解决这个问题，但是人似乎一下子就能领会到这个问题。我觉得和您刚说的那个东西有一些联系。但您如果来定义这个能力的话，您会把它定义成**推理**（Reasoning: 从已知信息得出结论的思维过程）吗？还是您把它定义成一个什么顿悟呢？

**田渊栋**: 它是在推理，或者说其他一些任务（task）下面。

**课代表立正**: 下面是什么意思？是更底层的意思还是？

**田渊栋**: 更底层的意思。就是说它是一个**表征学习**（Representation Learning: 机器学习模型自动学习数据有效特征表示的过程）的一些行为。随着这个训练的拓展，你会发现表征会改变。就相当于看金庸小说，对吧？张无忌一开始被他义父谢逊逼说，你把东西全背出来，现在全背出来，背出来之后你不懂没关系，不懂你可以脑子里存着。过了几年之后，突然之间有点会了乾坤大挪移，突然懂了。这个是很有意思的一个机制。比如说你当时教小孩子，可能也是这样，特别是有些小孩说你先把它背出来，读书百遍，其义自现。就是你现在先读，并不知道什么意思，但是过一段时间之后，或者说你跟其他的一些事情能够联系在一起了之后，你就会有一个突然之间你会觉得，这个意思是跟我这个现实世界是有关系的，或者说这两个意思之间是有关联的，我们知道更深的联系。这种其实是应该说是顿悟的一部分。这个机制其实是在思维链之下的。不管你用思维链做推理训练也好，不管你用那个直觉来判断那个答案也好，或者不管你用什么方式来判断答案也好，对吧？这些东西，它的下面有一个共同的机制，就是说我到底用什么样的表示，用什么样的对这个世界的理解，导致了这个思维链。比如说那个小学生做一道题，他可能说，我这道题怎么做？我用穷举法，1加1等于多少，1加2等于多少，1加3等于多少。那么有一些穷举的一个路径，可以把这个事情做了。比如说你要证明一个简单的一道习题，那么小学生会说，我穷举一些答案，看答案差不多了，那可能就对了。但是你这种方式，其实可能很多问题解决不了。等到比如说初中生或者高中生，他们的这个思维其实有一种飞跃。什么叫飞跃呢？就是说我们告诉他，我们可以用**数学归纳法**（Mathematical Induction: 一种数学证明方法，用于证明某个命题对所有自然数成立）来解决这个问题。数学归纳法这个思维，这个层次是高于穷举法的。如果你的数学归纳法能够证明这个事情是对的，那么它就对所有的自然数都成立。那这样的话，我的穷举法穷举无穷长的那个思维链，它其实都比不过数学归纳法的很短的证明。所以这个是一个飞跃。这样的话，你对这个问题的理解，两种方式的思维链，它的后面的理解是不一样的。所以这个理解或者说这个表征，其实就是神经网络学习的一个重要的地方。

**课代表立正**: 我很清楚。然后我想跟您对齐一个认知，然后给您看一个，就是当时我引用的。我们不是教课吗？然后我们教课的时候，我自己发现当时是Ilya Sutskever去MIT，几年前了，2016年的时候他去讲的。总之他当时讲了一个东西，我觉得他说的很深刻，就说为什么这个**反向传播**（Back Propagation: 神经网络训练中用于计算梯度以更新模型参数的算法）会起作用？然后就是这个**理论最优假设空间**（Theoretically Optimal Hypothesis Class: 在给定数据下，理论上能找到最佳解决方案的假设集合）等于**短程序**（Short Programs: 指简洁、高效的代码或模型，通常具有更好的泛化能力）。

**田渊栋**: 对，对。

**课代表立正**: 就听您刚刚那个意思，也是，就是本来我要去走好多条点，走到这，然后突然找到了一个更好的联系，然后我就有一个更好的压缩，然后它就更**可泛化**（generalizable）。

**田渊栋**: 对。这么说，就是说因为压缩，可能也可以说是更通俗的解释，对吧？但是什么时候这事情能压缩，什么事情不能压缩，其实现在不是很清楚。这是为什么你要去研究顿悟这个机制，它给你提供了一个动力学过程，让你知道它怎么从一个不压缩的状态变成压缩的状态。

### 科研范式之争：规模法则与机制理解

**课代表立正**: 我再说一下，就是我接下来问您一个问题的铺垫，就是我会发现，这个和人类理解知识似乎也很接近。人类也是信息连接点，但是这个图是在神经网络之前出现了。

**田渊栋**: 对。

**课代表立正**: 而且很多教育专家他会发现，就我记得在群里面，赵智沉有讲说，推理是一个人类固执的幻觉。然后这个是个教育专家说，最重要的单一因素是先验知识。对，你只有先验知识就行了，你只教先验知识，没有什么聪明不聪明这种说法。反正你只要把这些知识全都连接起来了，似乎就可以。这就是我接下来问题的这个铺垫。那接下来的问题就是，我们不知道我们的这些知识连接是怎么形成的。

**田渊栋**: 对。

**课代表立正**: 我们没有办法去讲清楚。您说在大规模预训练的过程中，似乎大家也不是很清楚。

**田渊栋**: 对。所以这是为什么搞清楚，其实可能孕育着就是下一个模型的一个契机，对吧？如果你搞清楚了之后，你就知道什么地方你要修改，这样的话你模型变得更强。所以这也是一个动力。因为我们现在你看，可以说，我把它搞清楚，我把它当黑盒子。就相当于一个很大的开关，有很多的开关，对吧？像以前那种电脑，那个大型机，非常多的按钮开关，我们就培养操作员坐在上面，然后把按钮开关打开，各种组合，然后看效果怎么样。那这是一种方案。那另外一种方案就是说，我们要把这个大的机器打开，然后理解里面的机制是什么。有这个机制的理解之后，那我以后再去拨这开关的时候，就非常有感觉，我就非常知道哪些开关要开，哪些要关能把这个做出来。我觉得这肯定是一个更好的做法。当然，现在可能主流的思维其实并不是这么说。主流思维是觉得，我们就叫**规模法则**（Scaling Law: 指在机器学习中，模型性能随模型规模、数据量和计算资源的增加而呈现可预测的提升规律），我不需要搞清楚你们在干什么，我就机器很多很多很多，放很多聪明厉害的人进去，然后让他们去拨那些开关。这些开关的某种组合找到了，那我们就能够把这个模型做得很强。两种不同的思维，但哪种是对的呢？现在也不好说，对吧？因为现在确实规模法则有很大的应用，确实那个效果也非常好。至少目前为止看起来，我把它当黑盒子，然后让很多人去拨开关，得到一个更好的解，是一个比较好的方案。另外一方面就是你把那个模型打开，那个时间花的代价其实更大，因为其实并没有多少人真的知道这模型里面在干什么。但是我觉得长远来说，可能后者会有更高的天花板。

**课代表立正**: 我同意您的判断。但这里边有这么一个点，我觉得为什么黑盒子现在它是占主流？因为打开了以后，人类似乎也没有办法真的去判断什么东西是什么，就是这里边有几种学习范式或者怎样？

**田渊栋**: 对，对。现在是这样。所以能不能找到一个比较好的，能够理解整个结构的一个大的框架，是重要的。所以是这样，这是为什么我做论文的原因。比如最近有一篇论文，我们做顿悟，对吧？那么这篇论文为什么要做这个论文？所以我觉得通过这个方式，得到一个对这个问题的一个大的理解框架，可能对以后的模型的改进有很大帮助。这个是我的想法。

### 经验学习、具身性与表征的形成

**课代表立正**: 本来已经有点复杂了，但是我觉得我再多引入一个问题，似乎我们在学习AI怎么学习的过程中，我们会从人类身上，人类的学习过程取得灵感。包括最近很火的Richard Sutton出来说，强化学习是人类学习的方式，大语言模型这种方式不是学习方式，它也不能学习，因为它没有目标。那另外一派呢，Geoffrey Hinton他说经验并不是一个只存在人脑中的，你通过语言可以得到经验。这个争论最后就落到人到底是怎么学习的，然后什么是学习，怎么样才能产生学习，或者产生新的知识，或者连接这些点？就想请您再就这个问题来发表一下您的猜想，甚至都可以。

**田渊栋**: 我觉得通过经验学习，这个是对的。但说这个经验里面，什么样的经验是更有价值的，我觉得这个是一个比较大的问题，对吧？比如说你要说非常直观的经验，就比如说我有一派是这么说，我没有**具身性**（Embodiment: 指智能体通过与物理世界互动来获取经验和知识的能力），我是没有办法去学到真正的感觉。

**课代表立正**: 这是行万里路。

**田渊栋**: 对，对。你要行万里路，或者说你要真正感到痛，感到伤心，喜怒哀乐，你才能真正成为人。就是这样的一种说法。或者说比如说，我只能通过看世界，然后才能知道空间结构，或者说只能通过摸才能看空间结构，对吧？那么还有一种说法，就是说我有一些抽象的概念，我还是能够学会这样一些东西。我觉得这两个东西，其实应该说不是说是互斥的。因为是这样的，其实最终我的目的是要学到一个**表征**（Representation: 数据或概念在模型中的抽象表示形式），学到一个表征学习，对吧？因为如果你学到一个表征的话，有个好的表征，那么对的问题你能够解决。表征怎么学出来的？这个完全取决于你的那个输入有多丰富，结构是什么样子的。不管你是直观的学习也好，还是抽象的学习也好，只要能学到这个表征，就能够最终得到一个比较好的**泛化**（Generalization: 模型将从训练数据中学到的知识应用于未见过的新数据的能力）的效果。我觉得这两个拼起来，应该说是比较好的。那么两边谁能写出表征来？这完全是应该说，有个定量的方式来预测的，而不是说是非此即彼。有可能是说两边都一半都可以学，或者说一边三分之一边三分之二也可以学，都行。所以并不是说一定是黑或者白，或者左或者右，很多时候是混在一起的。然后最终得到个表征，这表征就是能够进行预测，或者说能够操纵你的行动，能够泛化到一个新的，没有见过的情况。

### 从记忆到泛化：顿悟的数学图景

**课代表立正**: 那我觉得顺着这个问的话，就是您刚刚所说的后者的工作，就是不是黑盒子，然后不是所谓的这种规模法则，而是真的去打开它，然后去梳理它，然后怎么样呢？用不同的方式去学习。

**田渊栋**: 对，对。

**课代表立正**: 那它的意义是什么？就是几种，就是要么它学得更有效率，然而似乎现在我们数据已经到了一个瓶颈，效率这件事情不知道它是不是意义那么大。然后可能是在同样的知识里边，能学到新的东西，或它能增加新的数据，就是新的信息集。

**田渊栋**: 嗯，嗯。

**课代表立正**: 它的意义是什么呢？就是那样做的意义。

**田渊栋**: 我觉得首先第一个就是说，数据到瓶颈的话，其实恰恰就需要这个了。因为如果数据到瓶颈的话，那你意味着规模法则不一定有效了。比如说你就这么点数据，比如说你就只有这个几百万亿词元（hundred trillion tokens）这样一个规模。这个词元数目对一些大众的东西已经绝对够了，对吧？但是对一些小众的领域，就是它可能每个小众领域它占的比例就很少。所以这样的话，其实你如果数据不够，再加上你的训练算法比较费数据的话，不管怎么样，你学会的永远是一个**记忆**（Memorization: 模型直接记住训练数据，而非学习其底层规律），或者说是记忆的结构，而不是一个泛化的结构。那这个是一个问题，对吧？那么这种情况下，你怎么样去用规模法则做？你比如说你得去找办法，去做**数据增强**（Data Augmentation: 通过对现有数据进行变换来生成更多训练数据的方法）。也许这是一个办法，对吧？但是如果你对这个问题有理解，对这个模型有更好的理解的话，也许不需要数据增强，也许你需要改变这个训练本身的那个算法，或者说训练的那个架构。那么有这个架构之后，也许这个模型就会做得更好。

**课代表立正**: 您觉得我们现在大语言模型产生出来的**推理**（inference），生成出来的这些新的词元，它是记忆还是泛化？

**田渊栋**: 我觉得这个是，我觉得是混在一起的。任务比较丰富的，你见了很多很多这样情况，可能是泛化。

**课代表立正**: 给它的记忆材料越多，它越有泛化的可能，是这样吗？

**田渊栋**: 你可以这么说，就是给它的材料越多，因为它看到各种组合了之后，它在组合里面可以得到一个比较好的表征。这个表征它能够有预测能力，或者说这个表征对没见过的那个组合，它有一些比较好的结构可以算出来。这个其实就是泛化。我觉得说实在，所谓我们真的懂这东西，我真的理解这东西，往往意识的一个是它的泛化能力很强，对新的情况下，这个表征能够得到正确答案。然后第二个，就是说它能够细化到非常简单的这个逻辑，那么这个逻辑可以应用于所有情况或者应用于很多情况。这两个东西综合起来，就是让你这个学出来的知识能够应用于很多其他的地方。那么这个叫泛化，应该说我们对泛化下个定义的话，这样一个定义。那么如果大语言模型对某个领域看了很多很多数据，它有可能学到更好的表征，然后这边就可以泛化。那这个是一个。然后另外一个，就是说如果它看到的数据很少，那这样的话，有可能就说这个模型本身它没有办法学到很好的那个表征。好，那它就只能把它背出来。它得到的表征就是更偏于背诵的这样的结构。就是它能够至少对付好训练的那个要求，就说我希望这个训练集上的那个错误率还是比较小的。但是它一旦超越了那个训练集的范围之后，你就会发现这个错误率就会提高了。那么这个其实大家就认为这个是**过拟合**（overfitting）了，对吧？或者说是背诵了。所以大概就是这样子。其实我觉得很多时候，你并不能说神经网络是记忆还是泛化，应该说是完全取决于这个数据的分布。如果数据多，那么这个神经网络是泛化多。如果数据少，那么这个神经网络是记忆多。这个是我的观点。

**课代表立正**: 我觉得这里边最引人入胜的一点就是它从记忆到泛化的那一步到底是怎么发生的。

**田渊栋**: 帮我们总结一下，至少从我最近的一篇论文角度上来看，告诉你就是它有很清楚的一个图景告诉你就是这是怎么发生的，内在机制是怎么发生。就是我们现在感觉上是，我从记忆突然间跳到了泛化，好像这个变化非常神秘。但是这篇文章其实告诉你说，其实并不神秘。它有非常清楚的一个数学图景。就是比如说，我们要做优化问题，我们可以构造一个比较复杂的**非凸优化**（Non-convex Optimization: 目标函数或约束条件是非凸的优化问题，通常存在多个局部最优解）的结构。比如说很多山的山峰，然后记忆对应其中一个山峰，那个泛化对应其中另外一个山峰。这两个山峰其实对应着不同的表征。那么这个山峰的这个结构其实完全是取决于数据的分布的。如果你数据不够，你可能就只有记忆的山峰。如果你数据很多的话，某些泛化能力强的山峰就会慢慢变得越来越高，然后记忆的山峰就会变得越来越低。这样的话，你再让神经网络去找到那个好的表征的时候，是相当于是个优化问题。优化这个神经网络这个参数，使得它能够收敛到某个局部的最大点。那么如果你的记忆的山峰缩下去，泛化的山峰提上来，然后泛化的山峰，那么就有很多的那个神经网络，它的参数会收敛到那个泛化的山峰。那么这个模型就泛化了。那么从记忆到泛化，中间为什么会顿悟呢？其实很简单，就说你两个山峰之间的变化此消彼长，对吧？然后在某个情况下，我比你高一点点了，然后突然之间所有人都往那边走，那就是因为它可以泛化，把它给泛化。

**课代表立正**: 因为它能泛化，所以它可以只要多一点点的话，它就全都过去了。

**田渊栋**: 对，对。因为你认为神经网络是一个一直在优化的过程，它会看见，如果这边高，那边低点，那么所有人都涌到那个高的山峰上去，那就突然之间你就懂什么意思。所以我觉得是这样的一个结构。也就是这样，从整个数学框架上，能告诉你这件事情是这么发生的，而不是说是还是非常神秘的这样的一个东西。

**课代表立正**: 那我是不是可以理解为，这个泛化的点一直都在数据里边，只不过我们之前没有找到它，没有搜索到它，或者说搜索到了，但是没有给予足够的注意（pay enough attention）？然后现在，因为它随着越来越多的数据点，凸显了它的价值，然后我们才给予足够的注意？

**田渊栋**: 对，对。你可以。问题是它要存在。

**课代表立正**: 对，它存在。然后它有，你要足够的数据让它显得与众不同。

**田渊栋**: 可以这么想，就是如果数据不够的话，你可以有很多泛化的那个思想，但是就说这些泛化的思想，它的说服力不足以说服记忆这边，就是因为还不如把它记住。规律可能没有那么显然，对吧？

### 奖励函数、隐性偏见与“优雅”的追求

**课代表立正**: 那这又回到了另外一个问题，就是怎么样做评估（evaluation），怎么样做**奖励**（reward）？现在大语言模型还是看你**预测下一个词元**（predict next token）准不准作为奖励吗？还是有其他的方式可以让这个有泛化能力的显得更厉害一些？

**田渊栋**: 应该是这样，就是现在你要看大语言模型，一种是预训练和后训练，对吗？这两个都有。所以你很难讲，你说预训练，我们现在还是用大量预测下一个词元。然后后训练，其实我们可以说有很多办法可以做训练。那么预训练这个结构，或者说这个**损失函数**（Loss Function: 衡量模型预测结果与真实值之间差异的函数，训练目标是最小化损失）其实没有变，因为现在相对来说，这个还是比较好的损失函数。当然现在有一些新的一些方案，比如说**强化训练**（Reinforcement Training: 结合强化学习方法对模型进行训练，以优化特定行为或结果），就是我在训练时候加一段思维链，然后希望这个思维链会导致最后的那个预测是比较准的。这种类型的一些工作，这个就是可能对原来预训练的方式做了一些改变。大概是这样。那后训练它的花样就很多了，对吧？花样就比如说，你可以改强化学习的一些函数，比如说改它的**值函数**（Value Function: 强化学习中评估在特定状态下采取某个行动或遵循某个策略所能获得的预期总奖励），改它的评估，对吧？奖励，对吧？改**评价细则**（Rubric: 评估标准或评分规则）。这些东西都可以改。你这些改了之后，可能就是你其实是希望这个模型往不同的方向走，对吧？然后你往不同的方向走了之后，那么有些方向可能就强化模型的某个能力，某些方向强化模型另外一种能力。那这样的话，你这个模型最后就是百花齐放了。当然就是说很多时候，你要优化它到某个能力的时候，你其实还是希望能够优化得比较，一个是避免**奖励作弊**（Reward Hacking: 智能体找到奖励函数中的漏洞，以非预期方式最大化奖励，而非实现预期目标）。有些时候就是模型还是会最大化你的某个值函数，但是这个最大化的路径是偏的，不想让它这么做，但是它就这么做，这么做有捷径。比如说你答案就只有ABCD四个，然后拿去瞎猜一个25%。我不希望它瞎猜怎么办？那我就希望我的思维链，一个是希望它的每一步经得起考验，每一步逻辑是正确的。你可能需要一个另外的模型去做这个事情，这是比较重要的。一个就是怎么样去做这个事情，那么这样的话，你中间肯定要引入各种评价细则，引入各种东西去把这个模型给调出来。所以其实花样还是挺多的，而且有很多地方是可以有一些人类的那个思维和概念能够放进去。

**课代表立正**: 听到现在用比方去理解它的话，就是大语言模型是个非常非常勤奋，算力非常非常高，就是一天到晚学习的人。读了《唐诗三百首》，结果发现它又找到了《唐诗三万首》，读了300万首唐诗，然后它会作诗了，是因为它穷尽了这里面所有规律，找到了行之有效的方法，且它有一个好的方式去可以帮助它评估它自己的诗作得好不好，它找到了这里边的规律。但前提是这个规律要存在在这里边。

**田渊栋**: 对，对。

**课代表立正**: 然后您刚刚所说的，就是希望用另外一种方式去学习，是说我们不光要让它去背300万首唐诗，我们能不能就是像发现数学公式那种方式去发现一个规律？阿基米德发现浮力定律，它其实是干了两件事。肯定当时在想很多很多的方案，很多很多的可能性，然后它脑子里边找到了这么一个点。但是第二件事是，它马上意识到这个是对的。这两者在机器都挺难做到的，它很难马上意识到这个东西是对的。

**田渊栋**: 我觉得意识到这东西是对的是有可能。比如说你发现一个新的假设，这假设能够解释更多现象，而且它假设更简单，那你会马上意识到这个是对的。就比如说地心说跟日心说，其实说实在的，那个地心说也是对的。地心说你也可以拿来预测，只是在地球上来看，其他行星的运行轨迹非常复杂，本轮均轮这种运行轨迹，就是轮子套轮子，一边这么走，还要换个花样再转再转，就轮子里面套轮子。然后你通过这个方式，你可以预测一个行星的行为。这两个其实都是对的。日心说的时候，你会发现突然之间，所有的轨道都非常漂亮，就是一个椭圆，非常非常简单。这个时候你会马上意识到那个理论，或者说那种解释是更加完美的，或者说更加接近真实，或者更加接近那个更美的，这样的感觉。

**课代表立正**: 原来是这样子一个逻辑。您觉得**优雅**（elegance）这个东西在模型现在训练的奖励函数里吗？

**田渊栋**: 我觉得是这样，它不是奖励函数，但是它在训练的时候，应该有**隐性偏见**（Implicit Bias: 机器学习模型在训练过程中，除了显式损失函数外，还会受到模型架构、优化器等因素影响而产生的倾向性）往这方向走。就比如说那刚才您说Ilya Sutskever说过这个，我希望它压缩，我希望这个模型会自动的找到一个比较优美，或者比较少的，压缩比最高的那个解释。这个我是同意的，这个确实是会发生的。但是这个不是一个损失函数，是说它内建在神经网络的训练过程里面。这训练过程会让这个模型自然地发现更加好的，或者说更加优美的解释。那么这样的话，神经网络它才有这个能力去学会更好的表征，然后才有泛化能力。

**课代表立正**: 在损失函数之上，还有一层更隐含的奖励。

**田渊栋**: 是的，是可以这么说。对，对。这个很重要。因为说实在所有的损失函数都是**代理**（Surrogate: 在优化问题中，用一个更容易处理的函数来近似原始复杂函数）。就比如说预测下一个词元，或者是任何其他，或者什么**对比损失**（contrastive loss），**非对比损失**（non-contrastive loss），或者说**玩家损失**（player loss）。这些东西都是代理。就是它的目的是产生一个**梯度流**（Gradient Flow: 优化算法中梯度下降的方向和路径）。这个梯度流能够让这个表征往正确的方向走，这个是最重要的一个逻辑。至于这个目标函数是什么，其实并不重要，重要的是这个。

**课代表立正**: 哦，我直到今天之前，我一直觉得损失函数是整个学习的目标，现在我才知道了它是代理。这个是共识吗？我为什么到今天才知道这件事？因为它听起来很直观，然后很重要。

**田渊栋**: 我自己毕竟还是做过很多表征学习的工作的，我知道很多表征学习的那个目标函数，你做过些拆解之后，你会发现它们其实就是反向传播梯度的不同形式。你损失函数换了，你的反向传播梯度的结构是不一样的。那么这个结构其实最终能够影响你的表征的学习。但是你这个损失函数其实可以换，你甚至换成一些那个奇怪的东西，你从来没见过，但是你最后得到那个梯度是差不多的，那你求出的表征也差不多。

**课代表立正**: 您对梯度这个词的使用，也让我觉得非常的直观。我心中就是一个一个等高线，这个等高线最后画出来的是我们的一个知识，很本质的东西，可能就是刻画我们世界规律的这么一个。

**田渊栋**: 等高线这个逻辑是经常用的。但是等高线这样的一个思路，其实它忽略了神经网络本身的结构。因为它把整个**景观**（landscape）把它做成一个高维空间中的一个非常复杂的一个山峰。但是这个山峰其实你要知道，山峰其实对应着神经网络的结构。所以这两个是有关系的。应该说，把这个梯度在山峰上的这个指引，去映射到这个神经网络的具体的哪个梯度，对于哪个神经元的，或者每一组神经元的这样一个过程。那么这个时候你能看见，就是它的表征是怎么学出来的。这个是会比较有趣。但这个可能比较细节。大概是这样的一个逻辑。但这个是一家之言。

**课代表立正**: 我们来听的就是一家之言。有教科书的话我们就去学教科书了。

**田渊栋**: 当然每个人都会有自己的想法。这边也是做很多研究，有这样的一个大概的感觉，在上面有很多文章做一些这样的工作，分析这个梯度的结构。我相信就是再往上走，也许那个理解是能够改变这个神经网络的学习的方案。这是我们的最终目的。当然这个方向比较远，这是长期的。当然是希望有很多那短期的东西可以跟它辅佐在一起一起做。

### 科研道路的选择与AI作为研究助手

**课代表立正**: 要回顾一下您的这个科研史，看您的工作，其实我能感觉到它有一个很强的主线。您自己的网站上介绍的时候，我就会发现，您前面一个工作引导到下一个工作，然后再引导到下一个工作，就是每一次，都能在前面非常重要的结果上再往前走。您到底是怎么决定您的科研方向？您怎么样把兴趣、商业和自己的追求结合起来的？

**田渊栋**: 肯定是要结合的，不然的话就是很有可能就很惨。大家都有家庭，大家都希望能够有一些比较高的收入，小孩也有个比较好的环境，社会地位也比较高，大家都希望这都要。成年人说大家都要，不是说小孩子要选一个。所以最终你肯定是要找到一个结合点。因为我从博士开始已经是很多是双线作战了。我可能花9个月时间去想一些不着边际的东西，然后3个月说不行了，我今天要发论文，不然的话老板不爽，对吧？那我可能会跟老板说，你有什么题目，我来帮你做。我花3个月我就把这个事情做了，几篇论文就要对老板有交代。通过这方式至少让我，我是觉得让我会有工作，我能毕业，对吧？然后老板也开心，这个是重要的。工作之后也是一样的。我们当然希望做一些方向，这方向是迎合时代潮流的。不可能说完全脱离时代潮流。比如大家都在做大语言模型，你偏偏不做，比如我就要做**支持向量机**（Support Vector Machine, SVM: 一种监督学习模型，用于分类和回归分析），肯定在公司里面是没有办法活下去的。会想一想，就是说我这边的一些比较偏理论的研究，对于这个问题有更深理解。比如之前我们有一些关于**注意力稀疏性**（attention sparsity），注意力机制如何变得稀疏的这样一些研究。那么这研究本身是比较理论的，但是你就可以拿来做一些比较实用的工作。比如说之前的**注意力汇聚**（Attention Sink: 一种注意力机制优化方法，通过保留关键的注意力权重来提高效率和上下文长度），我们其实没有太多理论，但是我们可以通过观察这个神经网络的稀疏性，我们可能得到新的算法，用这新的算法把上下文扩展到400万以上。这样的话，这个东西有用了，突然之间，你可以拿来做大语言模型的**编码**（coding）解码的这样的一个应用。这应用其实本身也可以放在很多手机上。这样的联系应该说还是比较紧密的，应该容易想到。你的注意力有稀疏性的话，那我就把大部分的注意力分数砍掉，那不就是加速了吗？其实省内存了，对吧？那你有各种办法可以提高这个效率。这两个关系是很大的，你只要稍微想一想，就有一个新的算法，你有新的算法之后，你就有了一个新的思路，那么这个新的思路，你就可以拿来做很多。

**课代表立正**: 最后一个问题，就是直到最近，您的科研您感觉是按照自己的想法走呢，还是要做很多应用的工作？及接下来，可能会吸引您做的事情是什么？是继续您对后一种研究范式继续探索呢还是？

**田渊栋**: 我觉得研究范式探索是很重要的。当然了，我们现在也要与时俱进，对吧？就是了。我不可能我关起门来说，我就用以前的方式来做这研究。也许我们以后要找到一个AI科学家，或者说我自己写一套比如说**智能体框架**（agent framework），然后帮我一起做研究。这也是可以的。就是说我们这篇那个顿悟的论文，就是我和GPT-5进行对话做出来的。其实我觉得很有点像这种**自我对弈**（Self-play: 智能体通过与自身对战来学习和改进策略的训练方法）。我给它一些问题，然后我这边有些想法发给那个GPT-5，让它去思考，给一些比如**公式化**（formulation）。就一开始你这么做，它给你的答案都是非常大路的，非常没什么意思。但是你通过思考之后，关键的一个洞察力给它，它可能会有不一样的输出。这不一样输出可以往下面深挖一层了。但是你还是要找到它的错误，找到它的一些矛盾的地方，它做不出来的地方，然后继续深入，然后一直深入到这个问题的那个理解，或者说这个问题的一个数学的这样描述，已经达到了我想要的这个目的，这部分就成功了。

**课代表立正**: 但是我还有一个点，就是那是个独立作者（solo author）论文，您没有把GPT-5放到共同作者里边。

**田渊栋**: 这篇文章是个会议投稿，会议投稿说大语言模型不能作为作者，所以你没有放，对吧？那我后面写了一段，这段话是说我们广泛地使用大语言模型。我给大语言模型各种想法，让它去公式化，让它证明一个东西，然后发现问题怎么解决，对吧？它基本上所有东西都是错的，但是它有一些比较有意思的洞察力，很多东西可以细化。然后把你的想法从一个想法变成一个具体的过程，这个它很擅长。就相当于它是一个非常勤劳的初级**博士生**（PhD: 哲学博士，指攻读博士学位的学生）。它非常勤劳，我给它一个想法，它马上把它写成一段落，让我能够很快地进入状态。以前你要进入状态，我现在有一个小时的时间，一开始半小时我要进入状态，通过写写公式，看看文章，思考一下，我进入状态了，叫心流，然后才能得到一些结果。这个时间其实比较漫长。有了这个GPT-5之后，进入心流时间很短了。你跟它有一个小想法，然后它给你写一大段，三分钟之内给你写一大段东西。你看完这段东西之后，你马上会进入这个状态，就说我知道我要怎么去想问题，什么地方它做得不好，或者说有什么洞察力可以进来。这个是很大的一个效率的改进。以前你需要几个月的时间做一篇文章，你现在可能几个礼拜，甚至是更短的时间。这个是非常大的效率的提升，如果用得好的话，是很厉害的。当然，现在还是个非常初级的一个自我对弈，对吧？也许说不定以后我们可以做一个更加自动化版的，就很有意思。嗯，那肯定这方面有很多东西可以做，自己也有一些经验了。

### 讲清楚的艺术与模型的未来

**课代表立正**: 就是我跟当时是GPT-4o探讨量子力学的**多世界理论**（Many-Worlds Theory: 量子力学的一种解释，认为每次量子测量都会导致宇宙分裂成多个平行世界），我特别感兴趣。然后我一直觉得它最说得通。但是我们没有对应的哲学，反而那种所谓玄学的哲学和这个多世界理论的哲学是吻合的。就是我如果非要强行地说的话，我就说这个世界的本质就是一个非确定的多世界，然后我们之所以现在共享一个现实，这个是我们的最大概率。当然这个概率可能极大，就是99.99999%，所以说我们就会觉得这个桌子是确定无疑的存在，但是其实它可能并不是真的存在。

**田渊栋**: 嗯。

**课代表立正**: 对，大概是这种感觉。

**田渊栋**: 对，这个是对的。从科学上也是对的。你可以认为它是一堆**波函数**（Wave Function: 量子力学中描述粒子量子态的数学函数）的组合，对吧？然后存在一种可能，是这个桌子突然之间跑到另一堵墙另外一边去了。这概率非常小，但是不是0，这个是存在的。只是因为这个桌子是宏观物体，它的那个**量子态**（Quantum State: 描述量子系统所有可能属性的数学状态）不是那种**相干量子态**（Coherent Quantum State: 具有明确相位关系的量子叠加态，能展现出干涉等量子效应），所以就出现这种概率非常非常小。就是这样子一个东西。

**课代表立正**: 但是我就发现，这个想法我没有办法和它写成一篇文章，因为我自己的水平不行。就是说现在AI能辅助您写出来像您这个顿悟的这样的文章，主要是自己。

**田渊栋**: 最后还是人还是比较重要了。有很多重要的洞察力还是要人给。然后AI现在有很多奇怪问题，比如说它就会卡在一个地方动不了。它会跟你说很多车轱辘话，然后它就说不到本质上。这个很有意思。感觉上就是你去面试一个新来的博士生，然后说一大堆话，它像背诵概念，但它又绕不到，它就找不到那句最重要的本质的话能够说出来。这个其实是一个比较大的问题。但是这个就需要人去总结，然后告诉它，这是我们认为的最本质的东西，然后让它继续往下走。这个是比较重要的。就是说这是一个初级博士生，初级博士生意味着它可能是可以被训练的。

**课代表立正**: 我想到是**多邻国**（Duolingo: 一款语言学习应用）的那个创始人，他是一个计算机教授，我忘了叫什么。他讲了一个故事，就是他去读博士的第一年，他老师是图灵奖的获得者。

**田渊栋**: 对。

**课代表立正**: 然后几个月，他去了以后，他老师就只跟他干一件事，就是你这个东西给我讲讲，我没听懂。下次再来。他第二个月的时候就崩溃了，就这个老师肯定不行，怎么回事？结果后来才发现，就是他自己没有讲清楚。

**田渊栋**: 没讲清楚说明你这个理解不深，对吧？如果理解深的话，讲清楚了，别人会觉得你确实理解深了，你确实懂了，然后你可以做，你可以做研究。所以这个是一个。

**课代表立正**: 对，他叫那个**路易斯·冯·安**（Luis von Ahn）。

**田渊栋**: 对，我想起来。对，对，对。应该是说我当时在**卡内基梅隆大学**（Carnegie Mellon University, CMU: 美国一所顶尖研究型大学）读博的时候，他就在那了。对，他有这么一个故事。所以说，不知道模型是不是也可以这么搞定？

**田渊栋**: 我觉得有希望。

**课代表立正**: 希望可以。

**田渊栋**: 是的，是的。对。当然了，大模型可能会强行地记住怎么样讲能讲清楚，但它自己不懂也是有可能。而且就是说，你怎么样才能获得训练数据，能够让大模型找到最优的讲清楚的这样一个。因为讲清楚这个事情是一个非常主观的东西，很难用这个模型去建模它。在要求大语言模型之前，我们先要求自己，我们先要求自己把一个东西给讲清楚，已经是一个很高的要求了。这个很难。就是说，这部分其实可能就需要人有美感，就是人觉得它的那个讲解是非常有美感的，或者说非常简单扼要，这个才可以。那么这个怎么样去设计一个损失函数是一个问题。

**课代表立正**: 通过这个对话，我也更深层次地理解了这件事多重要，它的上下文是什么，和它其实对人也好，或者对模型来说，其实都有很多共通的地方。我觉得通过讲这篇论文，我们也讲了很多其他的，我觉得挺重要的知识。

**田渊栋**: 对，对，对。

**课代表立正**: 好的，那祝您接下来一切顺利。

**田渊栋**: 谢谢。好，先这样，再见。