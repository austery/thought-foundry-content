---
author: Hung-yi Lee
date: '2025-12-22'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=CbIPjrOj2Tc
speaker: Hung-yi Lee
tags:
  - llm
  - speech-recognition
  - unsupervised-learning
  - self-supervised-learning
  - end-to-end-model
  - reasoning
title: 语音语言模型发展史：从早期探索到智能对话的演进
summary: 本讲座深入探讨了语音语言模型（SLM）的发展历程，从早期的语音识别技术、无监督学习、自监督表示模型，到现代端到端对话式AI的挑战与突破。讲者结合自身团队的研究经验，详细介绍了SLM的定义、不同模式、与文本模型的对比，以及未来在推理、全双工交互等方面的研究方向。
insight: ''
draft: true
series: ''
category: ai-ml
area: tech-work
project:
  - ai-impact-analysis
people:
  - Geoffrey Hinton
  - Abdel-rahman Mohamed
  - 鍾毓安
  - 王育軒
  - 劉廷偉
  - 楊書文
  - 李尚文
  - Shinji Watanabe
  - 陳冠宇
  - 孫帥豪
  - 曾柏翔
  - 李佳軒
  - 張凱為
  - 曾亮軒
  - 陳宜昌
  - 李冠儀
  - 林冠廷
  - 上官思雲
  - Sam Altman
companies_orgs:
  - OpenAI
  - Meta
  - Microsoft
  - ByteDance
  - Kyutai
  - MTK Research
  - CMU
  - NTU
  - 台大圖書館
  - 騰訊
  - Amazon
products_models:
  - ChatGPT Voice Mode
  - Gemini Live
  - AudioGPT
  - Speech Copilot
  - Whisper
  - AlphaGo
  - Restricted Boltzmann Machine
  - Hidden Markov Model
  - Subspace GMM
  - BERT
  - MaskGIT
  - Mockingjay
  - TERA
  - APC
  - wav2vec
  - HuBERT
  - LibriSpeech
  - SUPERB
  - AV-SUPERB
  - ML-SUPERB
  - Dynamic-SUPERB
  - Indic-SUPERB
  - TS-SUPERB
  - wav2vec-U
  - EURO
  - Dual
  - S3PRL
  - Generative Spoken Language Model
  - K-Means Clustering
  - VQ Layer
  - BPE
  - Neural Speech Codec
  - AudioLM
  - Llama 3
  - GPT-4o
  - Moshi
  - Twist
  - Spirit LM
  - GLM-4-Voice
  - CosyVoice
  - TASTE
  - DeepSeek
  - Claude
  - STITCH
  - GLM-4 9B Chat
  - Llama 3.2 1B
  - Audio Reasoner
  - Audio Thinker
media_books:
  - 《Interspeech》
  - 《NeurIPS》
  - 《ICASSP》
  - 《TASLP》
  - 《arXiv》
  - 《饥饿游戏》
  - 《大家说英语》
  - 《托福听力测验》
  - 《生成式人工智慧與機器學習導論2025》
status: evergreen
---
大家好，这是本学期最后一堂课。今天我将跟大家分享**语音语言模型**（Speech Language Model: 一种能处理语音输入并生成语音输出的模型）的发展史。在过去的几堂课里，我们花了很多精力讲解了各式各样的生成式AI技术，其中多数基本观念都已涵盖。剩下的就靠大家自己深入研究了。今天这堂课，我将以第一人称视角，讲述语音语言模型这项技术这些年来的发展历程。

在开始之前，有一个免责声明：本课程并非完整介绍语音语言模型的技术，而是以我及我的团队的第一人称视角，讲述我们所看到的语音语言模型技术发展。所以这不是一个完整的概述。希望大家能通过这堂课，了解一线研究人员是如何看待一个技术的演进的。在某个时间点，受限于概念、资源和想法的局限，我们是如何看待一个问题，以及后来发生了怎样的转变。

### 语音语言模型：定义与模式

在本课程中，多数时候我们讲到语言模型，指的是**文字模型**，即输入文字输出文字。而**语音语言模型**（Speech Language Model），顾名思义，是输入语音输出语音。如果一个模型能做到输入语音输出语音，我们就可以称之为语音语言模型。如今，你可能已经在手机上使用过许多语音语言模型，例如**ChatGPT Voice Mode**（ChatGPT语音模式: OpenAI开发的、支持语音交互的ChatGPT功能）和**Gemini Live**（Gemini Live: Google开发的、支持语音交互的Gemini功能）。除了**ChatGPT**和**Gemini**，还有许多其他知名的语音语言模型。

然而，语音语言模型有多种不同的模式。当你看到文献中提到“语音语言模型”时，需要仔细辨别其具体功能。我大致将其分为两种模式：**对话模式**（Dialog Mode: 语音语言模型将语音输入视为对话，并给出相应语音回应的模式）和**命令模式**（Command Mode: 语音语言模型根据指令处理语音输入并给出特定类型回应的模式）。在对话模式下，你对模型说“How are you”，它会回应“I'm fine”。但在命令模式下，模型除了语音输入，还会有一个**指令**（Instruction）。例如，如果指令是“Identify the emotion”，模型会识别说话人的情绪，回应“happy”或“angry”；如果指令是“Translate the input”，模型会将“How are you”翻译成“你好”。

在今天的课程中，我们主要讨论对话模式。目前，文献中“**Speech Language Model**”这个词汇使用非常混乱，不同论文指涉的技术可能大相径庭。有人尝试用不同名称区分，例如将命令模式的模型称为**Speech Aware Language Model**（语音感知语言模型: 一种能理解语音输入的语言模型）。这种用词的混乱，恰恰说明这是一项非常新的技术，大家仍在摸索阶段。

### 语音语言模型与语音合成（TTS）的区别

需要强调的是，我们讨论的语音语言模型并非**语音合成**（TTS: Text-to-Speech: 文本转语音技术）。当你对一个对话模式的语音语言模型说“How are you”时，它会回答“I'm fine”，而不是重复你的话。而TTS模型是输入文字（如“How are you”），然后将其朗读出来。然而，很多TTS模型也自称是Speech Language Model，因为它们背后也使用**语音Token**（Speech Token: 语音信号经过编码后形成的离散单元）进行**Token接龙**（Token Generation: 模型根据前序Token预测并生成下一个Token的过程），技术上与语音语言模型有相似之处。这进一步加剧了Speech Language Model文献的混乱。

### 端到端与级联方案的优劣

要构建一个语音语言模型，使其能输入语音并给出回应，不一定非要采用**端到端**（End-to-End: 指模型直接从原始输入到最终输出，中间不经过人工干预的模块划分）的方法。今天我们主要探讨端到端方法，即模型直接以声音为输入，以声音为输出。但你也可以轻易想到非端到端（**Cascade Solution**：级联方案: 指将多个独立模型（如语音识别、文本语言模型、语音合成）串联起来完成任务的方法）的做法。例如，将语音识别系统、文本语言模型和语音合成系统串联起来，也能实现语音到语音的交互。

事实上，今天多数号称语音输入语音输出的系统，往往采用**级联方案**。例如，**台大图书馆**门口的数字人，就是将语音识别、文本模型和语音合成串联起来的。另一种思考方式是**Agentic Solution**，即以文本模型为核心，操控各种语音工具（如语音识别、语音合成、语音转换、**Speech Enhancement**语音增强等），让文本模型根据语音输入自行决定如何处理。**AudioGPT**和**Speech Copilot**是这类方案的例子。

非端到端级联方案的巨大优势是**易于实现**。语音合成、语音识别和大型文本语言模型都有现成的API，只需调用即可快速构建语音聊天机器人，无需大量技术背景或模型训练。然而，它也有缺点：首先是**巨大的信息损失**（Information Loss: 指在数据处理过程中，原始信息被丢弃或简化，导致部分信息无法恢复）。当语音信号只转换为文字时，就忽略了文字以外的丰富信息，如语者身份、语调、情绪、环境信息等。其次是**延迟**（Latency: 指从发出请求到接收到响应之间的时间间隔）。串联多个模型通常需要等待前一个模型完成才能传递给下一个，这使得实现实时交互更困难，需要大量工程投入。

相比之下，**端到端模型**的优势在于能够利用语音中所有的信息，且部署时延迟较低，可实现即时响应。但其弱点在于，这仍是**尚待研发的技术**，目前可能没有特别成熟的端到端解决方案能处理所有语音到语音的问题。在2025年这个时间点，通常级联模型才是最强的。因此，若要快速构建语音对话机器人，级联模型能更快给出好结果。但端到端模型具有**更高的能力上限**，我们对AI的最终期望是它能理解声音的所有面向，而非仅限于语音识别。

### 语音识别技术演进：从级联到端到端

你可能会好奇，这种目前表现较差的端到端模型是否缺乏研究价值？我们可以回顾一下历史。今天的语音识别领域，只要接触过的人，都一定听说过**Whisper**（Whisper: OpenAI开发的一种强大的端到端语音识别模型）。它是一个巨大的**Transformer**（Transformer: 一种基于自注意力机制的深度学习模型架构，广泛应用于自然语言处理）神经网络，直接以声音信号为输入，输出文字，非常简洁。

然而，这种端到端解决方案并非一直如此。几年前，在端到端模型尚未统治整个语音领域时，人们更多采用**级联模型**。语音识别被视为一个复杂问题，难以通过一个巨大的神经网络一步到位解决，因此需要拆解成多个阶段，通常包括**声学模型**（Acoustic Model: 语音识别系统中用于将声学特征映射到语音单元（如音素）的模型）、**词典**（Lexicon: 语音识别系统中存储词汇及其对应发音（音素序列）的数据库）和**语言模型**（Language Model: 预测文本序列中下一个词或字符概率的模型）。很长一段时间，人们认为语音识别必须通过级联模型才能解决，端到端方法是不可行的。

很多人想象中，端到端模型的崛起是某位聪明人获得了巨大算力，训练后一飞冲天，击败所有级联模型。但历史并非如此。我记得第一篇真正意义上的端到端语音识别系统发表在**Interspeech 2015**（Interspeech 2015: 国际语音通信协会（ISCA）举办的国际语音领域顶级会议），距今整整十年。在此之前，**NeurIPS 2014**（NeurIPS 2014: 神经信息处理系统大会，机器学习领域的顶级会议）也有一篇端到端模型，但它识别的是**音素**（Phoneme: 语言中最小的、能区分意义的语音单位），而非真正的文字。

我清晰记得，**Interspeech 2015**的那篇论文是一个海报展示。我站在海报前，觉得它“太潮了”，一群人试图用一个巨大的网络，直接从声音信号输出文字，这真的能做到吗？如果单看2015年那篇论文的结果，你会觉得这简直是天方夜谭。例如，他们在**Switchboard**（Switchboard: 一个大型的电话对话语料库，常用于语音识别基准测试）这个**基准语料库**（Benchmark Corpus: 用于评估和比较不同模型性能的标准数据集）上，最好的错误率是**38.8%**。你可能会觉得还不错，但要知道，在2014年，Switchboard上正常的错误率已经可以做到**10.4%**了。所以，近40%的错误率简直是“做坏了”，根本不好意思说系统被成功训练。

然而，这是十年前的情况。当时，端到端系统刚出现时，很难说它有一天能取代级联系统。多数人仍认为级联才是好的语音识别方式。但经过多代研究人员的努力，这些端到端模型逐渐变得越来越强，从“训练失败”到与级联模型仅差几个百分点的错误率，再到最终**主导整个语音社区**。这个过程花费了将近十年。因此，一项新技术刚出现时，它不一定是表现最好的，需要长时间、多团队的努力，才能成为今天我们看到的样子。

### 深度学习在语音识别中的早期应用

我们乘坐时光机回到2015年，如果说2016年**AlphaGo**的出现是**寒武纪**（Cambrian: 此处指AI领域技术大爆发的时代）的话，那我们还在寒武纪之前。现在，让我们再次回溯到更早。在级联时代，2015年，语音识别其实已经**基于深度学习**（Deep Learning Based: 指使用深度神经网络进行模型训练和任务处理的方法），只是级联中的每个模块都是深度学习的。

那么，人们何时开始将深度学习技术应用于语音识别呢？据我所知，最早将深度学习引入语音识别的论文之一，应该是发表在**ICASSP 2010**（ICASSP 2010: 国际声学、语音与信号处理会议，IEEE举办的信号处理领域顶级会议）的这篇。作者是鼎鼎大名的**Hinton**（Geoffrey Hinton: 深度学习领域的先驱之一，被称为“深度学习教父”），第一作者是多伦多大学的学生**Abdel-rahman Mohamed**（Abdel-rahman Mohamed: 深度学习在语音识别领域应用的早期研究者）。他们使用**受限玻尔兹曼机**（Restricted Boltzmann Machine: 一种生成式随机神经网络，是早期深度学习模型之一）进行**音素识别**（Phone Recognition: 语音识别任务中识别出语音对应的音素序列），并未实现真正的大词汇量语音识别。

我当时是一名博士生，参加了ICASSP 2010。很多“农场文”会说Hinton将深度学习应用于语音识别后，所有人都“跪了”，但这并非事实。真实情况是，在ICASSP 2010，这篇论文并未引起太多关注。当时人们认为更关键的技术是**隐马尔可夫模型**（Hidden Markov Model: 一种统计模型，常用于语音识别等序列数据建模）的变体——**子空间高斯混合模型**（Subspace GMM: 隐马尔可夫模型的一种变体，用于语音识别）。

在ICASSP 2010这个“开天辟地”的时代，深度学习在语音识别上的表现并不出色。他们的论文中提到，在**TIMIT**（TIMIT: 一个经典的语音语料库，常用于音素识别和语音识别研究）语料库上，音素识别的错误率是**26.7%**。论文中诚实地引用了其他研究，例如HMM能做到**24.8%**，所以他们并非**SOTA**（State of the Art: 某个领域或任务中当前最佳的技术或模型表现）。而且，他们比较的都是单音素模型，并未采用能大幅降低错误率的**三音素**（Triphone）技术。

因此，在深度学习刚兴起的那段时间，不断有人质疑其效果，认为它没有用。这不是深度学习一出现就立刻被所有人接受，而是一个缓慢演变的过程。人们开始拥护深度学习，也有人认为它不可行，各种争论不断。直到后来，深度学习的论文逐渐增多，才最终压倒了非深度学习的流派。历史的演进并非一项技术一出现就“轰动武林，惊动万教”，通常它以一种**不起眼的方式**出现在世人面前。

### 技术发展的真实曲线与潜力评估

一般人想象中的技术发展曲线是：一项技术（如**隐马尔可夫模型**）发展到瓶颈，聪明人引入新技术（如**深度学习**），然后所有人都转向新方法；接着新方法又遇到瓶颈，再引入更先进的技术（如**端到端**），再次引发变革。然而，真实的**发展往往并非如此**。你之所以在报章杂志上觉得这些方法一出现就统治了世界，是因为你注意到它们时，它们往往已被吹捧。当新技术超越旧技术，并被社会大众开始追捧时，你才意识到它们的存在，从而误以为它们是“一鸣惊人”。

但对于站在第一线的研究人员而言，我在前两波技术变革中看到的都是：这些技术一开始都以**非常不起眼的状态存在**。它们看起来与**SOTA**相去甚远，让人怀疑是否有希望。然后，经过多代人的努力，才逐渐发展成今天的样子。因此，回顾历史，我的感想是，除了关注一项技术当前的表现，我们更应关注它的**上限**。一个现在表现不佳的方法，不代表它未来没有希望；一个现在表现出色的方法，也不代表它能一直发展下去。关键在于其上限。一个表现差但上限很高的方法，也许是潜力股；一个现在看似主流的方法，也许其发展已走到极限，人们所做的只是“奇技淫巧”，不再值得深入研究。

这让我想起人类的演化。哺乳类并非在恐龙灭绝后才出现，它们在中生代就与恐龙共存，只是当时它们像老鼠一样，例如人类祖先**摩根齿兽**。这些小动物在恐龙的阴影下生存，但它们拥有恐龙没有的优势，如恒温和哺乳。最终，它们挺过了白垩纪末大灭绝，演化成了现代智人。许多技术也是如此，有些现在看似平平无奇，但可能蕴藏着他人没有的潜力，最终能独霸一方。

### 早期语音内容检索研究

接下来，我们来讲述语音语言模型的故事，从**序章**开始，讲述在语音语言模型概念出现之前，我们为之做了哪些准备。一开始，我并不知道语音语言模型会问世。这要从我的博士论文讲起。我于**2012年**获得博士学位，博士论文研究的是**Spoken Content Retrieval**（语音内容检索: 在大量语音数据中搜索特定内容的技术）。你可以想象，YouTube上有海量语音信号，我们能否直接搜索这些声音信号的内容？

毕业后，我先后在中研院和**MIT**各待了一年，于**2014年**回到**台大**任教。与多数新老师一样，我刚任教时，研究题目往往是博士论文的延续，因为这是最容易继续进行的研究方向。我当时也继续研究**Spoken Content Retrieval**。当时我思考的问题是：除了用文字搜索语音，能否直接用语音搜索语音？例如，你对机器说一个词汇，比如“ICASSP”，即使语音识别系统无法正确识别，甚至该词不在词典中，我们能否直接在**声音信号层面**，将输入的语音问题与数据库中的声音信号进行比对，找出关键词出现的地方？

在当时的框架下，遇到的一个难点是，在信号层面比对两段声音信号的相似度非常耗时。那时有一个算法叫**DTW**（Dynamic Time Warping: 动态时间规整，一种衡量两个时间序列相似度的方法，常用于语音匹配）。DTW是一个耗费算力和时间的算法，当数据库非常大时，很难实现实时搜索。因此，我们需要更好的语音搜索方法。

### 无监督语音分割与表示：Audio Word Vector

当时我有一个想法：我们先将数据库中的声音信号切成一个个小段，例如一个词汇切成一段。然后，每一小段声音信号通过**Audio Word Vector**（语音词向量: 将一段语音信号编码成固定维度的向量表示）的方式，变成一个固定维度的向量。这样，一段原本复杂且长短不一的声音信号，就被简化为一个向量。当有人输入一个**Spoken Query**时，这个Query也通过Audio Word Vector技术转换为一个向量。接下来，我们只需比较这个Query向量与数据库中每小段声音信号向量之间的相似度，即可得到搜索结果。向量之间的相似度比对速度非常快，有多种加速方法，无论数据集多大，都能在**Constant Time**内通过**Approximate**方式找出最近的向量。因此，将一段声音信号转换为向量具有巨大优势。

那么，如何将一段声音信号转换为向量呢？当时我构思了一个名为**Audio Word Vector**的技术。实现这个网络的同学是专题生**鍾毓安**（鍾毓安: 语音领域研究者）。当时还没有**Transformer**，我们使用的是**LSTM**（Long Short-Term Memory: 长短期记忆网络，一种循环神经网络，擅长处理序列数据）。一个单层LSTM将一段声音信号读入后，输出一个向量。接着，另一个LSTM将这个向量作为输入，试图还原原始的声音信号。这两个LSTM共同训练，目标是使输入和输出尽可能接近。这与我们上周讲到的**Tokenization**（分词/Token化: 将连续的文本或语音数据分割成离散的单元（Token）的过程）和**Detokenization**（反分词/反Token化: 将离散的Token序列还原成连续的文本或语音数据的过程）概念几乎一致。这篇论文发表在**2016年**的**Interspeech**，即**寒武纪时代**的论文。据我所知，在语音领域，当时还没有人做过类似的事情。过去有一些尝试将声音信号转换为向量，但都是**有监督**（Supervised）的，需要标注数据。在这篇文章之前，应该没有人尝试过**无监督**（Unsupervised: 指在没有人工标注数据的情况下，模型通过学习数据自身的结构和模式进行训练）的方式。总之，我们训练Audio Word Vector，使其能将每段声音信号都转换为一个向量，从而加快搜索速度。

### 语音分割的挑战与Segmental Audio Word Vector

然而，这里遇到了一个问题：我们似乎假设声音已被切成一段一段的。但如何进行声音分割呢？尤其我们期望每段声音对应一个固定单位，最好是一个词，至少是一个音素。但声音信号中并没有像文字中标点符号那样明确的**边界**（Boundary）。词汇之间没有分隔，如何自动将声音信号切成一个个**语音片段**（Audio Segment: 语音信号中被分割出来的一小段，通常对应一个词汇或音素）呢？在2016年寒武纪时代，当研究者问我这个问题时，我总是说：“先别问我，我来想想，过几年我就解决了。”

后来，我们进行了一系列研究，试图在不使用语音识别的情况下，自动从声音信号中找出词汇间的边界。因为我们要做的是**无监督**的，所以机器必须在没有任何标注数据的情况下，通过听大量声音信号，自动识别哪些声音信号组合成常见单位（如音素或词汇）。**王育軒**（王育軒: 语音领域研究者）同学在**2017年**发表的一篇论文中，利用**无监督**方法训练的**LSTM**，通过其内部**门控机制**（Gate: 神经网络中控制信息流动的结构）来寻找边界。这个网络的训练有点像语言模型，它输入声音信号，预测未来的或被掩盖的声音信号。

后来，我想到一个新方法，可以将分割**语音片段**和将**语音片段**转换为向量这两个过程**联合训练**。这个方法被命名为**Segmental Audio Word Vector**（分段式语音词向量: 一种同时进行语音分割和词向量生成的技术），即在原有**Audio Word Vector**概念上增加了**分段**（Segmental）的概念。这篇论文于**2018年**发表，同样由**王育軒**同学完成。

**Segmental Audio Word Vector**的工作原理是：一个**LSTM编码器**（LSTM Encoder: LSTM网络中负责将输入序列编码为隐藏状态表示的部分）每次读取一小段声音信号（每个方格代表一个0.01秒的**帧**）。当它识别出这是一个频繁出现的模式时，就认为这是一个**语音片段**，并输出一个向量（**Embedding**或**Representation**）来代表这个**语音片段**的信息。然后，LSTM会清空记忆，重新读取下一段声音信号，重复此过程。通过这种方式，机器一方面自行决定**语音片段**的边界，另一方面也自行决定如何用向量描述每个**语音片段**。

这个模型如何训练呢？它有一个**LSTM解码器**（LSTM Decoder: LSTM网络中负责从隐藏状态生成输出序列的部分），每次读取一个向量，并试图还原该向量对应的原始**语音片段**。训练目标是使输入的整句话与输出的整句话尽可能接近。在训练过程中，如何将一段落转换为向量，以及如何确定段落边界，都由模型自己决定。这是一个**端到端**学习的网络架构。

然而，仅仅让输入和输出尽可能接近是不够的。因为如果这是唯一的训练目标，网络可能会采取“作弊”的方式：将每个**帧**都视为一个**语音片段**，这样还原最容易。因此，需要一个额外的限制：除了输入输出相似度高，机器选择的**语音片段**数量要尽可能少。模型必须学会用最少量的**语音片段**来表示一段声音信号，并将其还原。所以，LSTM在学习时有两个目标：一是还原，二是使用最少的向量表示声音信号。

### 无监督语音识别的突破

有了**Segmental Audio Word Vector**技术后，我们就可以一边分割**语音片段**，一边将其转换为向量，且整个训练过程是**无监督**的，不依赖标注数据。除了将其用于搜索，我有一天突然想到，能否直接利用这些向量进行**无监督语音识别**呢？假设这些向量与音素信息非常接近，因为我们发现**Audio Word Vector**能切割出音素层级的**语音片段**。如果每个**语音片段**的向量能直接转换为音素，再结合**词典**和**语言模型**，就能实现语音识别，而且我们希望这个转换过程也是**无监督**的。

当时的想法是：我们有一个非常简单的**线性模型**（Linear Model），它将这些向量读入并转换为**音素序列**（Phoneme Sequence）。如何训练这个小模型呢？我们采用了**GAN**（Generative Adversarial Network: 生成对抗网络，一种通过生成器和判别器对抗训练的深度学习模型）的思想。假设我们有一大堆文字，这些文字与语音无需成对。我们将这些文字通过查字典转换为**音素序列**。然后，训练一个**判别器**（Discriminator: GAN中负责判断数据真伪的部分），其任务是分辨一个**音素序列**是由语音识别系统生成的，还是真实的**音素序列**。而**生成器**（Generator: GAN中负责生成新数据的部分）则试图欺骗判别器。判别器和生成器**迭代交替训练**，这就是GAN的概念。整个训练过程是**无监督**的，不需要任何标注数据。我们完全不需要声音及其对应的文字，就能让模型学会将声音信号转换为**音素序列**，从而实现语音识别。

我们将这个结果发表在**2018年**的**Interspeech**。如果说2016年是寒武纪，那2018年我们可以说是进入了**奥陶纪**（Ordovician: 此处指AI领域技术发展的早期阶段）。在这个远古时代，我们实现了**无监督语音识别**，这应该是世界上第一个成功实现**无监督语音识别**的结果。在2018年，在**TIMIT**数据集上，我们的**音素错误率**（Phone Error Rate: 衡量音素识别系统性能的指标，错误率越低越好）达到了**60%多**。请注意，这是错误率，不是正确率，意味着错的比对的多。但我们第一次看到这个结果时，已经非常振奋，因为机器在完全没有任何标注数据的情况下，确实学到了东西。60%多的错误率并非随机猜测。

几个月后，**腾讯**也发表了**无监督语音识别**，错误率降至40%多。后来，我们实验室的**陳冠宇**（陳冠宇: 语音领域研究者）同学将错误率进一步压低到30%多。30%多的错误率是什么水平呢？回顾历史，这大概是人们刚开始使用**隐马尔可夫模型**时的水平。当时我们已经觉得非常了不起，因为这些有标注的模型训练的，而我们在完全没有标注数据的情况下，已经能与最差的有标注模型（即最差的**有监督模型**）达到相似的结果，与三四十年前的**有监督模型**不相上下。在2019年，我们甚至觉得这可能就是**无监督学习**的极限了，毕竟没有任何人教它，它是无师自通的。我当时以为我有生之年都不会再看到更多进步了。

### 自监督语音表示模型：Mockingjay、TERA、APC

然而，历史仍在不断推进。**2018年**，在**自然语言处理**（NLP）领域，**BERT**（Bidirectional Encoder Representations from Transformers: 一种基于Transformer的双向编码器，广泛应用于自然语言处理）等早期文本模型问世。我当时想，我们能否在语音领域也做一个语音版的**BERT**呢？于是，**劉廷偉**（劉廷偉: 语音领域研究者）同学便着手开发了语音版的**BERT**。

当时我们的计算资源非常有限，他用一张**NVIDIA GeForce GTX 1080**显卡，训练了一周，训练出了一个语音版的**BERT**。其概念与文本版**BERT**非常相似：取一段声音信号，将其中的一些地方**掩盖**（Mask），然后训练一个**Transformer**。这可能是第一个**自监督**（Self-Supervised: 指模型利用数据自身生成监督信号进行训练，无需人工标注）的**Transformer**语音模型。据我所知，在此之前，应该没有人用**Transformer**来训练这种语音**语言模型**。总之，这个**Transformer**以声音信号为输入，输出一排向量（**Representation**），其任务是与后面的一个**线性模型**（Linear Model）——一个非常小的**线性头**（Linear Head）配合，还原被掩盖的部分。

我们当时将这个模型命名为**Mockingjay**（Mockingjay: 一种语音版的BERT模型，通过掩码预测进行自监督训练），意为“学舌鸟”，因为它做的事情就是输入什么，就输出一模一样的东西。这个想法与文本版**BERT**几乎一致，也与我们上周介绍的**MaskGIT**（MaskGIT: 一种基于掩码生成图像的Transformer模型）非常相似，尽管**MaskGIT**是更晚的文章，其灵感也源于**BERT**。我们曾尝试用**Mockingjay**合成声音，但结果是完全无法理解的。

**Mockingjay**与文本模型仍有一些不同之处，它在某些方面是根据语音特性设计的。例如，在进行**掩码**时，文本**BERT**通常一次掩盖一个文本**Token**。但在语音上，如果只一次掩盖一个**帧**（Frame），效果会非常差。因为一个**帧**代表的声音信号非常小，模型只需对两边的**帧**进行**内插**（Interpolation），就能轻易猜出被掩盖的**帧**。因此，如果只掩盖一个**帧**，模型学不到什么。我们发现，在**掩码**时必须进行**连续掩码**（Consecutive Masking: 指在序列数据中连续掩盖多个Token或帧，以增加预测难度和模型学习效果），一次掩盖三到九个**帧**，才能得到较好的结果。

此外，在**掩码策略**（Masking Strategy）上也有突破。我们发现，**掩码**不一定只在时间方向上进行。如果在**频带方向**（Frequency Band Direction）上进行**掩码**，在某些任务上能获得更好的结果。我们发现，在频带方向上**掩码**对模型学习**语者相关任务**（Speaker-related Tasks）特别有帮助。因此，**Mockingjay**后来有了一个进阶版本，名为**TERA**（TERA: Mockingjay的进阶版本，考虑了频带方向的掩码），将频带**掩码**纳入考虑。**TERA**后来发表在语音领域顶级期刊**TASLP**（IEEE/ACM Transactions on Audio, Speech, and Language Processing: 语音和语言处理领域的顶级期刊）上，是当年**TASLP**下载次数前25名的文章。

### 语音版GPT与Meta的自监督模型

大约在同一时间，**鍾毓安**同学（他后来去**MIT**读博士）在**MIT**也训练了语音版的**GPT**。其概念与文本版**GPT**非常相似：取一段声音信号的前半段，模型训练目标是预测接下来的声音信号，即预测接下来的**帧**。但语音与文字仍有不同。语音版的**GPT**不能直接预测下一个**帧**，如果直接预测下一个**帧**，结果会很差。根据**鍾毓安**的论文，需要预测更久远以后的**帧**，例如至少三个**帧**以外的**帧**，才能得到较好的结果。原理与**Mockingjay**的**连续掩码**类似：如果只预测下一个**帧**，对模型来说太容易了。

当时那个模型名为**APC**（Auto-Regressive Predictive Coding: 自回归预测编码，一种语音自监督学习模型）。当年应该没有人真正尝试用这个模型合成声音。我预计，在那个年代的数据量和技术下，可能不会得到特别好的结果。其中一个原因可能是这类模型都是直接预测**连续表示**（Continuous Representation: 指用浮点数向量来表示数据，而非离散的符号），而没有添加**扩散模型**（Diffusion: 一种生成模型，通过逐步添加噪声和去噪来生成数据）或**流匹配**（Flow Matching: 一种生成模型，通过学习数据从简单分布到复杂分布的连续变换来生成数据）的**头**（Head）。在2019年**奥陶纪**时，人类还没有添加**扩散头**或**流匹配头**的概念。

在**Meta**（Meta: 一家全球领先的科技公司，拥有Facebook、Instagram等产品）那边，也开发了许多类似的语音语言模型。它们的目标都是输入一段声音信号，产生**表示**（Representation），而非真正输出声音信号。其中两个特别有代表性的研究成果是**wav2vec**（wav2vec: Meta开发的一系列自监督语音表示模型）和**HuBERT**（HuBERT: Meta开发的另一种自监督语音表示模型，通过掩码预测和聚类目标进行训练）。**wav2vec**的第一代于2019年初发表，比**Mockingjay**和**APC**还要早。第二代于2020年发表。后来，性能更好的**HuBERT**于2021年发表。当时，这些模型都被称为**表示模型**（Representation Model: 旨在将原始数据（如语音）转换为更抽象、更有意义的向量表示的模型），我们甚至不会称它们为生成模型，尽管有时也称之为语言模型，但我们并不会真的用它们来做生成。

### 表示模型在语音识别中的巨大作用

这些**表示模型**被训练出来的目的是为了获得更好的语音**表示**。它们的作用是：给定一段声音信号，将其转换为一个个向量。接下来，你可以根据需要执行任何任务，多数人希望做的是语音识别。因此，你会训练一个**下游模型**（Downstream Model: 指在预训练模型之上，针对特定任务进行微调的模型），其任务就是进行语音识别。如果训练时使用标注数据，它就是**有监督**的。这个**下游模型**以这些**表示**向量为输入，将其转换为语音识别结果。

有了这些**表示模型**后，语音识别的性能取得了显著突破。例如，在**LibriSpeech**（LibriSpeech: 一个大型的英文有声读物语料库，常用于语音识别基准测试）这个常用的语音**基准测试**（Benchmark）上，**词错误率**（Word Error Rate: 衡量语音识别系统性能的指标，错误率越低越好）显著降低。使用100小时标注数据训练**下游模型**的结果显示，如果没有**表示模型**，直接训练一个六层**LSTM**模型，错误率接近**5-6%**。但如果使用**表示模型**，**下游模型**只需两层**LSTM**，就能达到**3%**甚至更低的错误率。

更令人惊叹的是，即使将训练数据减少到仅10分钟（这非常少，相当于听三分之一的《大家说英语》），如果使用**wav2vec 2.0**或**HuBERT**作为**表示模型**，仍然可以获得**4-5%**的错误率，比使用六层**LSTM**和100小时数据的结果还要好。这表明这些**表示模型**非常有用，它们提取的**表示**非常适合用于语音识别，对语音识别有巨大帮助。

### SUPERB：通用语音表示模型的探索

在**wav2vec 2.0**问世时，其表现非常惊人。我当时直觉地想，如果我们将**Segmental Audio Word2Vec**替换为更强大的**wav2vec 2.0**，能否进一步提升**无监督语音识别**的水平？对于**无监督学习**的**GAN**部分，我们保持不变，只是将生成**表示**的**骨干模型**（Backbone Model: 深度学习模型中负责提取特征的主体部分）替换为更好的**表示模型**。当时我们的做法是，取出**wav2vec 2.0**的最后一层作为**表示**，然后训练**无监督ASR**。结果却非常糟糕，甚至不如**Segmental Audio Word2Vec**。当时我们觉得**wav2vec 2.0**没什么帮助，似乎被过誉了。但我们不知道，我们离**无监督ASR**的下一个突破仅一步之遥。

既然**wav2vec 2.0**对**无监督ASR**帮助不大，我们便将精力转向另一个方向。当时我思考的问题是：已经有许多优秀的语音**表示模型**，它们在语音识别上表现出色，例如**wav2vec 2.0**、**HuBERT**等。但语音相关的任务并非只有语音识别。语音中还包含语者信息、情绪信息，可以进行**语者识别**（Speaker Recognition）和**情绪识别**（Emotion Recognition）。这些**表示模型**在其他任务上也能取得好结果吗？我们能否拥有一个**通用模型**（Universal Model: 指一个模型能够处理多种不同任务或适应多种不同领域），一个**通用语音表示模型**（Universal Speech Representation Model: 能够为多种语音相关任务提供有效表示的通用模型）？

这个**通用模型**就像一个操作系统，你可以为其添加各种应用程序。真的存在这样的语音**表示模型**吗？回溯到2021年，当时大家都在开发语音**表示模型**，我会说我不认为它们能成为**通用模型**。但今天，大家都知道语音**表示模型**可以是**通用模型**。今天，大家已经期待一个**基础模型**（Foundation Model: 指在大量数据上预训练的、具有广泛通用能力的模型，可适应多种下游任务）是通用的、多功能的、全能的。但四五年前，人类并没有这样的期待，甚至我当时觉得不太可能轻易出现**通用语音表示模型**。因为不同的语音任务往往需要截然不同的信息。例如，语音识别需要提取文字信息，机器要学会**忽略语者信息**；而**语者识别**则需要机器学会**忽略内容信息**，只提取语者信息。这两个任务是互斥的，很难想象一个模型能同时做好。

### SUPERB项目的成果与影响

无论如何，我当时想知道是否存在**通用模型**，或者每个模型擅长什么，以便我们开发**通用模型**。因此，我组织了一个团队，将这个计划命名为**SUPERB**（Speech Processing Universal Performance Benchmark: 语音处理通用性能基准，一个评估通用语音表示模型的项目），即**Speech Processing Universal Performance Benchmark**的缩写。团队的第一作者和领导者是**楊書文**（楊書文: 语音领域研究者）同学。团队中还有许多知名资深成员，如**Meta**研究员**李尚文**（李尚文: Meta研究人员）和**Abdelrahman Mohamed**（即前面提到的**Hinton**的学生），以及**CMU**（Carnegie Mellon University: 卡内基梅隆大学，美国著名研究型大学）知名语音学者**Shinji Watanabe**（Shinji Watanabe: CMU知名语音学者）。我们组建团队共同完成了这项巨大任务。

**SUPERB**的目标是，将当时所有能找到的语音**表示模型**应用于我们认为重要的各种语音相关任务，评估它们在哪些任务上表现好，哪些任务上表现不佳。最初，我们都只使用每个模型的最后一层进行任务。我们发现，当我们将每个模型的最后一层交给**下游模型**时，在许多任务上，模型表现不佳，尤其是在**语者相关任务**上，例如**语者验证**（Speaker Verification: 识别说话人身份的任务），多数模型无法取得好表现，甚至不如不使用**表示模型**。

我当时得出的第一个结论是：这与我预料的一样，这些模型果然擅长语音识别，如**HuBERT**和**wav2vec**。擅长语音识别往往意味着可能不太能做好与语音识别互斥的任务，如**语者识别**。但当时我们也观察到一些比较奇怪的现象：例如，**wav2vec**和**HuBERT**的**大型版本**（Large Model）表现非常差，比小型版本差，甚至比随机结果还差。当时觉得有些奇怪。

后来我们进行了更深入的分析，发现这些**自监督模型**的每一层往往能提取非常不同的信息。在图中，横轴代表**表示模型**的层数（从第一层到第二十四层），纵轴代表如果使用该层进行某个任务所能达到的性能（数值越大越好）。红色线代表**音素识别**（即语音识别）的性能，蓝色线代表**语者识别**的性能。虚线是**wav2vec**模型，实线是**HuBERT**模型。从图中可以看出，如果看语音识别，这些模型的前几层不擅长语音识别，需要到后几层提取的**表示**才包含更多**内容信息**（Content Information），适合语音识别。而**语者信息**在前几层就能提取出来，到后几层反而减少。因此，如果要进行**语者相关任务**，前几层更合适。至于**wav2vec 2.0**，它是一个神奇的模型，其最后几层几乎没有任何信息，这与其训练机制有关。所以，之前进行**无监督ASR**时，只使用最后一层导致结果很差也就不足为奇了。

### 加权求和与SUPERB的深远影响

既然我们知道这些**表示模型**的不同层包含不同信息，那么如何应用这些信息呢？如何知道哪个任务应该使用哪一层呢？当时我们采用了**加权求和**（Weighted Sum: 将多个输入按不同权重相加的方法）的方法。我们将每一层的**表示**都提取出来，并为每一层乘以一个**权重**。这些**权重**是**下游模型**参数的一部分，在训练**下游模型**时也会一并训练。这样，**下游模型**会自行决定使用哪几层。如果是语音识别，模型可能会自动学习使用最后几层；如果是**语者识别**，模型会学习使用前几层。这种**加权求和**的方法非常有效。

**SUPERB**项目的结果显示，我们有一个巨大的表格，横轴代表不同的模型，颜色深浅代表模型表现（越深越好），每个纵列代表一个任务。表格显示，**WavLM**、**data2vec**、**HuBERT**、**wav2vec 2.0**等最强的模型，往往能在多数任务上表现出色。最下面一行**FBank**代表未使用**表示**的结果。从实验结果可以看出，使用**表示模型**后，所有任务的性能都得到了提升。这些**表示模型**加上**加权求和**后，确实可以是**十项全能**的。

我认为**SUPERB**的出现具有非常重要的意义，它告诉大家这些**自监督模型**非常强大，它们不仅能做语音识别，还有机会成为**通用模型**。因此，**SUPERB**这篇论文获得了很高的引用量，在**Interspeech**近五年所有发表论文中，引用次数排名第四。后来，**SUPERB**也成为了一个“宇宙”，一个品牌名称。如今，如果有人进行这种**通用模型**的评估，往往会冠以**SUPERB**之名，例如**AV-SUPERB**（Audio-visual SUPERB: 结合音频和视觉信息的SUPERB基准）、**ML-SUPERB**（Multilingual SUPERB: 多语言的SUPERB基准）、**Dynamic-SUPERB**（Dynamic SUPERB: 动态版本的SUPERB基准，可能涉及众包数据收集）和**Indic-SUPERB**（Indic-SUPERB: 针对印度语言的SUPERB基准）等。

### 无监督语音识别的再次飞跃

让我们回到**无监督语音识别**。我们之前提到，我们错过了**无监督语音识别**的一次重大突破。后来，**Meta**的研究人员也使用**wav2vec**进行**无监督语音识别**，但与我们最大的不同是，他们没有使用最后一层，而是使用了第15层。当然，除了选择第15层，他们还做了许多改进。使用第15层后，**无监督学习**的**音素错误率**突然暴跌，达到了大约**15%**。这意味着在**TIMIT**这个**基准测试**上，**无监督ASR**几乎可以做到与**有监督学习**一样好。我本以为**无监督学习**再怎么努力也无法与**有监督学习**相媲美，但在有了优秀的**表示模型**后，**无监督语音识别**居然能与**有监督学习**不相上下。

事实上，**无监督语音识别**最近仍在取得进展。例如，有一个模型叫做**wav2vec-U**（wav2vec-U: 一种基于wav2vec的无监督语音识别模型），这是我们实验室的同学与**孫帥豪**（孫帥豪: 语音领域研究者）老师合作的结果。我们不再报告**TIMIT**上的结果，因为**TIMIT**已被“攻克”。我们在**LibriSpeech**上进行测试，使用100小时的训练数据。如果使用**wav2vec**加上**无监督ASR**，错误率约为**19%**。几年后，我们实验室也参与了一个名为**EURO**（EURO: 一个无监督语音识别项目）的项目，它也能做**无监督ASR**，并且比**wav2vec-U**做得更好。后来，**wav2vec**加上**无监督ASR**，**Meta**又推出了2.0版，正确率更高，错误率可压低到**10%**左右。而**wav2vec-U 2.0**则能达到约**5.4%**的错误率，这在**LibriSpeech**上也是一个非常好的结果。因此，**无监督ASR**在这些年确实取得了突破，**wav2vec-U 2.0**最大的突破在于找到了更好的**分割方法**（Segmentation Method），这里使用了**强化学习**（Reinforcement Learning: 一种机器学习范式，通过与环境互动学习如何做出决策以最大化奖励）技术，但没有人工干预，而是利用**无监督信号**训练出更好的边界寻找方法。

### 语音问答系统发展

有了这些**表示模型**后，许多原本无法实现的事情突然变得可行。例如，我们实验室有一个长期的梦想：做**语音问答**（Speech QA: 直接从语音输入中提取答案的问答系统）。所谓**语音问答**，就是给一个**QA模型**一段声音信号（比如一段上课录音），然后问它一个问题，希望它能直接给出答案。我们实验室最早的**语音问答**项目是从**托福听力测验**（TOEFL Listening Comprehension: 托福考试中的听力理解部分）开始的。

**曾柏翔**（曾柏翔: 语音领域研究者）同学在2016年，即**寒武纪时代**，做了这个项目。当时之所以选择**托福听力测验**，是因为我觉得它有四个选项，那时还没有**生成**（Generative）的观念，认为机器直接输出答案是不可行的。但我觉得四个选项可以看作一个**分类问题**（Classification Problem），有机会直接训练一个模型，听声音、看问题，然后预测是A、B、C、D中的哪一个。当时做出来，正确率达到了**40%多**，比随机猜测好很多。但后来我发现，40%多的正确率申请不到好学校，需要70-80%的正确率才行。所以那时机器与人类的差距仍然巨大。

后来随着时间的演进，我们挑战了真正的**问答**，即让模型真正回答问题，而不仅仅是选择一个选项。这是**李佳軒**（李佳軒: 语音领域研究者）同学做的。当时的做法是，我们仍然非常依赖**语音识别系统**。我们需要先将一段录音通过语音识别模型转换为文字，问题也是声音的，也需要转换为文字。既然声音都变成了文字，问题也是文字，那么直接套用一个**文本模型**就可以回答问题了。因此，在有好的语音识别系统的情况下，能够进行**语音问答**，也就能进行**听力理解**（Listening Comprehension: 理解听到的语音内容的能力）。

但在有了像**HuBERT**这样的**语音表示模型**后，我们就有机会挑战更困难的问题。当时**李佳軒**在做的时候，我问她能否挑战做一个**端到端模型**，不用语音识别，直接**端到端**地训练一个模型。她确实尝试了，但训练不起来。然而，有了**语音表示模型**后，我们真的有机会做一个**端到端模型**，直接将声音信号输入给**语音表示模型**，它输出**表示**，然后一个**下游模型**读取这些**表示**，直接给出答案。

这个**语音问答**的结果，当时提出了一个名为**Dual**（Dual: 一个语音问答模型）的模型。**Dual**中有很多有趣的巧思，并非直接**端到端**地“硬训练”就能成功，需要多种方法才能实现**语音问答**。至于具体方法，我在2022年**ChatGPT**问世之前（即**史前时代**）曾做过一个演讲，讲解这类模型是如何训练出来的。

**Dual**模型的结果如何呢？横轴是语音识别系统的错误率，纵轴是**F1分数**（F1 Score: 衡量分类模型准确性的指标，是精确率和召回率的调和平均值），数值越大代表模型表现越好。绿色线代表**级联模型**（即语音识别系统加文本**QA模型**）。你可以想象，随着语音识别系统错误率越来越高，整体表现当然越来越差。但如果有一个**端到端模型**，它没有语音识别模块，直接输入声音输出答案，那么它与语音识别的错误率就没有什么关系。当语音识别错误率超过**25%**时，**端到端模型**相对于**级联模型**在当时也能占据一些优势。

如果你想了解更多关于**语音语言模型**开始之前发生的事情，可以阅读我与其他学者合写的一篇**综述论文**（Overview Paper），我们从**上古时代**、**寒武纪时代**一直讲到**史前时代**（ChatGPT出现之前），那时**语音语言模型**有哪些进展。当年，**楊書文**同学和**劉廷偉**同学还开发了一个非常好用的**工具包**（Toolkit）叫做**S3PRL**（S3PRL: 一个用于语音表示学习的开源工具包），它可以调用各种**语音表示模型**应用于各种任务。如果你今天有机会使用这些**语音表示模型**，我推荐你使用这个好用的**工具包**。

### 初代语音语言模型：GSLM与Token化

讲了这么久，其实都还没有真正讲到**语音语言模型**，我们甚至都还没有进入**石器时代**。接下来第二部分，我们终于要讲到初代**语音语言模型**的样子。最早可以称之为**语音语言模型**，并且与我们现在想象的、可以做**生成**（Generative）的模型比较类似的，我想应该是**Meta**的**Generative Spoken Language Model**（GSLM: Meta提出的生成式语音语言模型），简称**GSLM**。

它的做法与你今天可以想象的做法其实差不多：首先，你需要一个**分词器**（Tokenizer: 将连续数据（如语音）转换为离散Token序列的组件），可以将声音信号转换为**Token**。然后，你训练一个**自回归模型**（Autoregressive Model: 一种序列模型，通过预测序列中的下一个元素来生成序列），它与一般的文本**语言模型**没有太大差别，就是输入**Token**，预测下一个**Token**。最后，你需要一个**反分词器**（Detokenizer: 将Token序列转换回原始连续数据（如语音）的组件），可以将**Token**还原成声音信号。有了这三样东西，就完成了。

这个**分词器**是如何构建的呢？在上一个世代中，已经开发了许多**表示模型**。因此，在**GSLM**中，他们没有再花时间开发**分词器**，而是直接将**表示模型**改造为一个**分词器**。改造方式是：这些**表示模型**可以输入一段声音信号，输出一堆向量。他们直接对这些输出的向量进行**聚类**（Clustering: 将数据点分组，使得同一组内的数据点相似度高，不同组之间相似度低）。你可以进行**K-Means聚类**（K-Means Clustering: 一种常用的聚类算法，将数据划分为K个簇），或者使用一个**VQ层**（Vector Quantization Layer: 向量量化层，一种将连续向量映射到离散码本中的向量的层）。你将相似的向量集合起来，称它们为同一个**Token**。

后来也有许多不同的**语音语言模型**，对**Token**做了一些改进，例如去除重复的**Token**，或者将一些**Token模式**（Token Pattern），如“3后面总是接2”，合并为一个新的**Token**。这种方法其实是致敬了文本领域常用的**BPE**（Byte Pair Encoding: 字节对编码，一种常用的文本分词算法，通过合并频繁出现的字节对来创建新的Token）。

在早期的**语音语言模型**中，**分词器**通常不需要额外训练，因为已有的**表示模型**稍加改造即可用作**分词器**。但**反分词器**仍需训练。你需要训练一个模型，将**Token**作为输入，目标是生成这些**Token**对应的声音信号。我们希望输入一段声音信号，经过**Token化**后，能通过**反分词器**还原回原始声音信号。**反分词器**需要训练，但**分词器**通常不需要。

**GSLM**最初采用这种做法，但后来也有人采用了其他语音**Token**，例如另一组常用的**Token**叫做**Neural Speech Codec**（神经语音编解码器: 一种端到端训练的语音编解码器，用于将语音压缩成Token并还原）。在**Neural Speech Codec**中，**分词器**和**反分词器**是**联合训练**（Jointly Trained）的。它们都是巨大的网络，**分词器**将声音信号转换为**Token**，**Token**再通过**反分词器**转换回声音信号，目标是使输入输出尽可能相似。**Codec**一词中的“Co”指**Compression**（压缩），即**分词器**将语音**压缩**成**Token**；“Dec”指**Decompression**（解压缩），即**反分词器**将**Token****解压缩**成原始声音信号。

### Semantic Token与Acoustic Token

**分词器**主要有两种类型：一种是直接从**Speech Representation Model**改造而来，这种**Token**被称为**Semantic Token**（语义Token: 历史文献中指从语音表示模型中提取的、被认为包含语义信息的Token）。另一种是**分词器**和**反分词器****联合训练**，这种**Token**被称为**Acoustic Token**（声学Token: 历史文献中指由编解码器联合训练得到的、包含声学细节的Token）。不过，**Semantic Token**和**Acoustic Token**只是历史上某篇论文（即**AudioLM**）的称呼，后来大家一直沿用这种说法。事实上，**Speech Representation Model**无法提取真正的语义信息，所以称之为**Semantic Token**有些言过其实，但这已是约定俗成。

有了各种各样的**Token**，你可能会问，哪种**Token**更好呢？答案是“小孩子才做选择，大人是全都要”。你可以同时使用所有**Token**。例如，**AudioLM**（AudioLM: Google开发的一种基于自监督学习的语音生成模型）论文中提到，他们在生成时会同时生成**Semantic Token**和**Acoustic Token**，而且**Acoustic Token**还有不同的等级（粗粒度、细粒度）。

同时使用多组不同的**Token**后，下一个问题是采用何种**生成策略**（Generation Strategy）来生成不同的**Token**。如果只有一组**Token**，那就是**自回归**（Autoregressive: 指模型通过预测序列中的下一个元素来生成序列）地生成下一个**Token**即可。但如果有多组**Token**，谁先谁后，以何种方式、何种顺序生成，就成了一个研究问题。我已在另一个课程录影中讨论过这方面内容，如果大家对生成多种不同**Token**的策略感兴趣，可以参考上学期机器学习最后一堂课的录影（27分到39分）。

### 语音语言模型的训练挑战

然而，**语音语言模型**训练起来往往难以获得非常好的结果。我们实验室曾尝试**微调**（Fine-tune: 在预训练模型的基础上，使用特定任务的数据进行进一步训练）一个已在数万小时数据上训练的**语音语言模型**，但结果不尽如人意。我强调一下，这些**语音语言模型**做的事情都是**预训练**（Pre-train: 在大规模通用数据集上对模型进行初步训练，使其学习通用知识和能力），它们只是用大量未标注数据训练模型。因此，这些模型真正能做的只是**语音接龙**（Speech Completion），即帮你把一句话讲完，还不能进行对话。要进行对话，必须进行**微调**。

就像我们讲**语言模型**时提到的，**预训练模型**如果没有**聊天模板**（Chat Template: 指用于格式化输入给语言模型的对话或指令的特定结构），根本不会回答问题，只会把一句话不断讲下去。这些语音模型也是如此，它们只是把一句未完成的句子讲下去。我们给**语音语言模型**一段声音信号：“某个人刺杀了总统，接下来会发生什么事情呢？”模型觉得接下来会发生这样一些事情。你肯定觉得它在说什么啊，我也看不懂。我把这句话拿去问**ChatGPT**，它也说这句话根本不知道在说什么，只有一些词汇和短语，但组合起来毫无意义。所以，最初代的**语音语言模型**就是这个样子。

但这已经是**2023年**了，那时**ChatGPT**已问世，文字模型已进入**中世纪**。那时，人们不能接受这样的人工智能，一个不能好好说话的人工智能是不能接受的。当时初代的**语音语言模型**不太能好好说话，但我们还是尝试用它做一些事情，例如进行**上下文学习**（In-context Learning: 指大型语言模型在不更新参数的情况下，仅通过输入示例就能学习并执行新任务的能力）。我们告诉**语音语言模型**：“听到这句话，你就说是Happy；这句话就是Sad。”看它能否进行情绪识别。结果是无法做到。这些**语音语言模型**没有**上下文学习**的能力。我们后来发现，必须**微调**它，它才会有**上下文学习**的能力。也就是说，它并没有从那些未标注数据中自动启发**上下文学习**的能力。这与文本模型不同，**GPT-3**（GPT-3: OpenAI开发的大型语言模型，具有强大的文本生成和理解能力）作为**预训练模型**，即使没有经过**对齐**（Alignment: 指将不同模态或不同粒度的数据（如语音和文本）进行对应匹配的过程），也具有**上下文学习**的能力，但语音模型没有。

当时，我们实验室的博士生**張凱為**（張凱為: 语音领域研究者）同学进行了一系列研究，试图**提示**（Prompt: 提供给模型以引导其生成特定输出的文本或数据）这些**语音语言模型**。他使用的是**软提示**（Soft Prompt: 指通过可学习的向量而非离散文本来引导模型行为的提示），试图找到一些**提示**来激发这些**语音语言模型**的能力，让它们能做各种事情。他建立了一个网站记录这些研究。

### 算力与数据量的巨大鸿沟

为什么训练**语音语言模型**如此困难呢？我们来算一笔账。假设你有100万小时的声音信号，这听起来已经非常多了。100万小时的声音信号对应多少文字量呢？人类大约一分钟能讲100个**Token**。100万小时的声音信号对应**60亿个文字Token**。你可能觉得60亿个文字**Token**听起来很多，但对于一个**文本模型**来说，这根本非常少。不要忘了**Llama 3**（Llama 3: Meta开发的一系列大型语言模型）是用**15万亿个文字Token**进行训练的。

如果有人将**Llama 3**训练的这些文字数据全部转换为语音，例如用语音合成系统将这些文字全部朗读出来，会产生多少声音数据呢？**28万5千小时**。28万5千年前，地球上还没有现代智人。所以，你需要从那个时候就开始播放音频，一直听到现在，这才能相当于**Llama 3**学到的文字信息量。因此，如果你想让一个语音模型拥有等同于**Llama 3**的能力，它需要的数据量是**28万5千小时**，这是一个非常惊人的数据量。

也有人通过实验验证，**语音语言模型**确实更难学习。图中的黑点代表**文本模型**的能力，绿点代表**语音语言模型**的能力。纵轴是**评估指标**（Evaluation Metrics）。所有评估都只衡量了模型的**内容能力**（Content Capability），没有考虑模型能否理解情绪、环境音、语者信息等。它只问模型在进行**文字接龙**时，生成的句子是否合理。你会发现，**文本模型**当然是**吊打语音模型**，**语音模型**与**文本模型**不在同一个量级上。横轴是投入的运算资源，且是**对数尺度**（Log Scale）。你会发现，投入的运算资源越多，**文本模型**增长得越快，而**语音模型**的增长比**文本模型**慢得多。

如果我们要让**文本模型**和**语音模型**拥有同样的能力，考虑到横轴是**对数尺度**，所需的运算量大约是**文本模型**的**十倍到一百倍**。训练一个**文本模型**已经很痛苦了，再增加一百倍的运算资源，这很可能难以实现。而且，这里只讨论了内容部分，我们还没有讨论机器是否学到了语者信息、情绪信息或其他文字以外的信息。所以，看起来**语音语言模型**非常有挑战性。

我们实验室曾尝试用较大量的数据（约数万小时）来训练**语音语言模型**。当时**从零开始训练**（Train from Scratch），结果也不尽如人意。我后来与**Meta**最初做**GSLM**的团队交流，他们说即使数据扩展到十万小时，也做不出什么好结果。所以我当时觉得**语音语言模型**的挑战非常大，短时间内无法克服。这对于做研究的人来说，其实是好事，意味着我们还有很多事情可以做。

### GPT-4o的震撼与行业变革

然而，就在**2024年5月**，一切都变了。**2024年5月**，当**GPT-4o**（GPT-4o: OpenAI开发的多模态大型语言模型，支持语音、视觉和文本交互）即将发布时，几天前就有风声说他们做的是一个语音模型，只是不知道做得怎么样。所以在发布当天，我其实守在电脑前，想看看他们会做出什么。他们做出了一个我心目中**语音语言模型**应有的样子：不仅听得懂文字，还能听懂文字以外的信息，而且可以进行非常流畅的互动。

当时我看到后非常震惊，那时已是半夜两点，发现实验室的同学也都在线上，大家非常紧张，想看看**OpenAI**做了什么。我们终于感受到了**自然语言处理**（NLP）研究者的痛苦。**ChatGPT**问世后，**NLP**领域陷入了巨大的“灭顶之灾”，学术界能做的事情越来越少。我们语音界终于也要体会到**NLP**领域的“灭顶之灾”了，终于要体会到“没得写”的痛苦了（笑）。

**ChatGPT**发布后，我们立即制定了一个计划：也许接下来没什么模型可以训练了，我们要做的事情就是**衡量现有模型**，衡量**ChatGPT**能做到什么程度。大家先把音频文件和要做的**基准测试**准备好，它的API只要一上线，立刻就来评估它。系统一上线，我们就立刻评估它。但是第二天早上没有上线，下午没有上线，晚上也没有上线，过了好几天都还是没有上线。

有人会说，不是有吗？发布会后不是马上就有语音功能吗？那个语音功能**不是发布会上演示的语音功能**。发布会上演示的，或者他们试图让你相信的，是一个**端到端**的语音系统。我当时有点怀疑那是不是一个**端到端**的语音系统，但学生告诉我：“老师你傻啊，它博客上都跟你说是**端到端**的语音系统，它就是**端到端**的。”总之，我当时有点怀疑它是否真的是**端到端**的，它看起来非常**端到端**。但它发布的那个应用程序显然是**级联**的，它并没有发布他们所谓的高级语音模式，只发布了**级联**模式。那几天，一堆网红**UP主**（UP主: 指在视频分享网站上传原创视频的用户）录视频说“我有这个语音模式了”，但那些语音模式都是假的，都是**级联模型**，根本没有他们宣称的那些功能。

所以等了好几周，一直没有真正的**语音语言模型**应用出现。我记得**OpenAI**直到**9月底**才上线真正的**语音语言模型**，但上线后用起来确实非常惊艳。然而，几个月后情况又变了。在**OpenAI**刚发布他们的**Advanced Voice Mode Demo**时，我觉得非常震惊，难以想象它是如何实现的。但在几个月后，大家越来越清楚这类**语音语言模型**可能是如何构建的，所以觉得还有很多研究空间可以继续。

### 利用文本信息构建语音模型

虽然**OpenAI**没有立即发布这个**语音语言模型**，但在**2024年7月中旬**，一家名为**Kyutai**（Kyutai: 一家法国新创公司）的法国新创公司，开发了一个**语音语言模型**叫做**Moshi**（Moshi: Kyutai开发的一个开源语音语言模型）。**Moshi**已经发布，并在**10月**真正**开源**（Open Source: 指软件的源代码可以被公众自由使用、修改和分发）了它的模型权重。但在**7月**时，它的**演示版**（Demo）可以直接在线互动。所以，如果要说世界上第一个可以互动的**语音语言模型**，我反而觉得**Moshi**才是第一个。

**OpenAI**是如何在短时间内做出一个**语音语言模型**的呢？我当时的猜测是利用了一个强大的**文本模型**作为基础。在**GPT-4o**的语音模式**演示版**发布一周后，我录了一个视频，讲述我对**GPT-4o**语音技术背后的猜测，其中就提到了**利用文本信息**。如果你有空，可以再去看看我当年的猜测，看看与今天的技术是否非常相似。

什么叫**利用文本信息**呢？其实**利用文本信息**在语音领域并非新鲜事。我们前面讲过**语音问答系统**，提到在有了**Speech Representation Model**后，我们就能做一个**端到端**的**语音问答系统**。这个**语音问答系统**之所以能成功，还有一个诀窍：我们的**下游模型**其实是拿**BERT**做**初始化**（Initialization: 在模型训练开始前，为模型参数设置初始值）。所以可以想象，我们其实是将**Speech Representation Model**后面接上一个**BERT**作为整个**语音问答系统**。因此，我们过去就知道，可以拿**文本模型**来初始化一个**语音模型参数**，而且这往往是有用的。

有人可能会问，为什么没有在**语言模型**上试试看呢？我们试过。我们前面提到，我们收集了一万小时的数据，**从零开始训练**了一个**语音语言模型**，结果不佳。后来，我们尝试用当时的**Llama**来做**初始化**，结果仍然非常不好，无法达到**OpenAI**那样的效果。**Meta**那边也尝试拿一个**文本模型**来初始化**Speech Language Model**，他们的模型叫做**Twist**（Twist: Meta尝试利用文本模型初始化语音语言模型的研究项目）。**Twist**使用文本做**初始化**确实有帮助，但它仍然无法做出非常好的结果。像这样的模型，你仍然无法预期它说的每一句话都能被人听懂。

### 语音与文本混合训练：Interleaving与Hybrid生成

还有什么方法可以利用**文本信息**呢？另一种想法是能否打造一个**语音原生模型**。在训练**语音语言模型**时，我们将语音和文本信息**混合**（Mix）在一起，让模型不仅向文本学习，也向语音学习，两者同时学习。这种概念早在构建**Speech Representation Model**的**2021年**（即**中生代**）就已经存在。当时有许多研究试图将语音和文本**混合**在一起，共同训练一个**表示模型**。但如果只是简单**混合**训练，往往不一定能得到好结果。如何让两种看似非常不同的信息能够**对齐**（Align）在一起，需要非常多的技术。这里我们不细讲，我将一些相关论文放在这里供大家参考。

当然，在**语音语言模型**上，也有人尝试过将语音和文本**混合**在一起训练。怎么做呢？就是我有一堆文字，我有一堆语音。这里有一个比较强的假设，即假设文字和语音有对应的关系，比如这句话就是“How are you”。所以这与一般的**预训练**不同，一般的**预训练**我们假设完全没有标注数据，但这里我们假设每段声音都有其对应的文字。这需要某种人工干预才能获得，所以**预训练**并非完全**无监督**。

在训练时，会使用一个技巧叫做**交错**（Interleaving: 指在训练过程中将不同类型的数据（如语音Token和文本Token）交替或混合排列）。即将**文本序列**中的一些**Token**替换为语音，将**语音Token序列**中的一些**Token**替换为文字，然后继续训练。这种方法可以使机器更好地将语音和文本信息**对齐**在一起，更好地利用彼此的信息来强化语音和文本的能力。我这里引用了一些相关论文，其中一个比较知名的论文也是**Meta**做的，这个模型叫做**Spirit LM**（Spirit LM: Meta开发的一种将语音和文本信息交错训练的语言模型）。

### 避免遗忘：文本Token生成的重要性

然而，前面提到的两种方法——用**文本模型**做**初始化**，以及在训练时做**交错**——都无法真正打造出非常好的**语音语言模型**。在看了**OpenAI**的**演示版**后，我觉得他们有不同的方法。我当时的猜测是这样的（许多后来的研究也证实了这种猜测和方法非常有用）：他们同样从**文本模型**作为**初始化**，但在训练时，你需要教你的**语音模型**同时生成**文字Token**和**语音Token**。所以重点是，模型不仅要用**文本参数**做**初始化**，它还要能够学习同时生成**文字Token**和**语音Token**。

我们真正需要的是**语音Token**，因为这些**语音Token**会输入到**反分词器**中，还原出声音信号，这样你就能知道模型想说什么。但为什么生成**文字Token**仍然是必要的呢？因为我们在训练时，经常会遇到**遗忘问题**（Forgetting: 指模型在学习新任务时，忘记之前学到的知识或能力）。每次教模型一个新任务时，它很容易忘记旧任务。如果你教一个**文本模型**生成**语音Token**，但同时又不让它经常回顾如何生成**文字Token**，它很快就会忘记如何生成**文字Token**，只会生成**语音Token**。而那些**语音Token**又不足以让它学到**文本**那么丰富的信息，整个模型就会崩溃。

所以，今天在训练**语音模型**时，一个重点是**你仍然要教它生成文字Token**。这可以避免它忘记在文本时学到的信息，而这些文本信息非常重要。因为仅仅依靠语音数据，模型无法学到足够的信息。文字是语音的**压缩**，语音数据比较复杂，而文字数据比较简洁。今天机器通过大量文字数据，已经学到了非常丰富的信息。如果你想保留这些知识并用于语音互动，就必须避免它遗忘在文本时学到的信息。因此，在训练时**保持文字生成能力**，即**文字Token生成能力**，可能是非常关键的一步。

### 文本与语音Token长度不一致的挑战

然而，这里有一个小问题：**文字Token**的长度与**语音Token**的长度往往**天差地远**。一句话，例如“How are you?”，可能是三个**文字Token**，但它对应的**语音Token**可能比三个多很多，因为通常一个**语音Token**只对应0.02秒的声音信号，所以一秒钟就有50个**语音Token**。由于两者差异巨大，而且它们之间的**对齐关系**（Alignment Relationship）也非常复杂。例如，“How”可能对应前面的这些**Token**，“are”可能只对应这一小段**Token**，“you”可能对应这段**Token**。它们之间没有非常明确、一致、有规律的**对齐关系**。

因此，在生成时，如何将两者放在一起就成了一个需要考虑的问题。这里有不同的猜测。一个想法是**先生成文字再生成语音**，教机器先生成**文字Token**，再生成**语音Token**。对于生成**语音Token**，模型做的事情几乎等同于**语音合成**。它等于先生成文字，然后根据这些文字再进行**语音合成**，生成对应的**语音Token**。这种方法的缺点是**难以实现流式处理**（Streaming: 指数据在生成或接收的同时进行处理，无需等待全部数据完成）。当你与模型说话时，你期望模型立即回应。但像这种先生成文字再生成语音的模型，你必须先等它生成完文字（比如三秒），每次与它讲话时，它都会停顿三秒才开始回复，这会让人觉得非常不自然。

另一种方法是**词汇层级的交错**（Word-level Interleaving），即模型先生成一个**文字词汇**，然后生成这个**文字Token**对应的**语音Token**，再生成下一个**文字Token**，再生成对应的**语音Token**，以此类推。这看起来是一种有效的方式，但缺点是必须先进行**对齐**。你必须假设在训练时，你知道每个**文字Token**对应哪些**语音Token**。这种**对齐数据**并非无法获取，但需要额外精力。

还有一种方法是**分块交错**（Chunk-wise Interleaving），即我们完全不考虑语音和文字之间的对应关系，只规定每生成一个**文字Token**，接下来就生成两个**语音Token**，如此循环。你可能会问，如果**文字Token**和**语音Token**长度不一样，会不会**文字Token**都生成完了，**语音Token**还没生成完？有可能。如果**文字Token**生成完了，就继续生成**语音Token**。一些模型，例如**GLM-4-Voice**（GLM-4-Voice: 智谱AI开发的语音语言模型），就是用这种方法训练出来的。你可能会觉得，这样这些**Token**和这些文字不就没有对应关系了吗？也许今天已经输出到“Are”了，这边“How”还没念完。告诉你，没关系，这样训练也能成功。**GLM-4-Voice**就是用这种方法训练出来的。

还有其他想法。在刚才的想法中，我们每次只生成**语音Token**或**文字Token**。也可能在每个时间点，两种**Token**同时生成。也就是说，你有两个**语言模型头**（Language Model Head），一个专门生成**文字Token**，一个专门生成**语音Token**。所以每次都生成一个**文字Token**和一个**语音Token**。当然，这样你仍然需要处理文字和语音长度不一致的问题。历史上，有各种各样的方法，这些都是在去年年中到年底提出的一系列方法。**Moshi**在去年**10月**发布了它的技术报告，它也采用了这种包含文字生成的方式，但它有一套自己的方法来处理文字和语音**Token**长度不一致的问题。

### TASTE：新型语音Token化方法

后来我们实验室也朝这个方向做了一些尝试，我们做了**词汇层级的文本与语音混合**（Word-level Text and Speech Hybrid）。确实，你可以用这种方法训练出一个看起来还不错的**语音语言模型**。我在这里播放**蕭淇元**（蕭淇元: 语音领域研究者）同学提供的一个音频文件。首先你会听到输入给这个**语音语言模型**的声音信号。

接下来，模型进行**文字接龙**。大家要注意的是，它在进行**文字接龙**时，是**一边生成文字Token，一边生成语音Token**。它生成**文字Token**，生成**语音Token**，再生成**文字Token**，再生成**语音Token**。然后将**语音Token**通过**反分词器**，就能听到声音信号。我们也会将**文字Token**集合起来，让你知道如果看文字，模型想说什么。

（播放模型生成音频）

至少能听懂它在说什么。但你会发现，有些合成的地方，比如“Restriction”，它就念得一塌糊涂。我想这可能是因为训练数据非常少。在这个项目中，我们使用的训练数据不到1000小时。如果有些词汇模型从未听过，它可能就无法精确地念出来。只有1000小时的训练数据真的非常少，光是让模型好好回答问题可能都不太够，更不用说让它学到情绪、语者等相关信息。

如果要让机器学会这些文字以外的信息（我们常称之为**副语言信息**，Paralinguistic: 指语音中除了文字内容以外的、传达情感、语者身份等信息的特征，如语调、语速、音色），可能需要使用**Web Scale Data**，即非常大量的网络数据进行大规模**预训练**才能做到。所以，如果只用上千小时进行**微调**，你可能只能让模型回答问题，但很难让它学到语义以外的东西。

你可能会问，为什么不用更多数据呢？那是因为我们手上的算力不足以支撑更大规模的计算。训练这样的**语音模型**是非常痛苦的。痛苦在哪里呢？你想想看，文字通常是一秒钟的声音信号对应3到4个**Token**，可以说文字的频率大约是3到4赫兹（Hz）。而**语音Token**通常是50赫兹（Hz），即一秒有50个**Token**。所以可以想象，如果比较下面**语音模型**要生成的**序列长度**（Sequence Length）与上面**文本模型**的**序列长度**，前者至少是后者的十倍以上。十倍以上的**序列**意味着你需要**百倍以上**的算力。你训练一个普通的**文本模型**已经很痛苦了，再加一百倍的算力，谁有算力能做这么大规模的运算呢？

然而，许多大公司就是用这样的方法训练出了他们的**语音语言模型**。如果你有足够的算力，当然可以自己做。但在学校，我们不可能获得这样的算力，所以我们需要更好的方法。

### TASTE：更合适的语音表示方式

有什么更好的方法呢？我想跟大家分享的一个方向是寻找**更合适的语音表示方式**。为什么有机会寻找更合适的语音表示方式呢？这项研究是与**MTK Research**（MTK Research: 联发科研究部门）团队合作的，第一作者是我们实验室的博士生**曾亮軒**（曾亮軒: 语音领域研究者），**MTK**的**陳宜昌**（陳宜昌: MTK研究人员）和**NTU**（National Taiwan University: 台湾大学）的**李冠儀**（李冠儀: 语音领域研究者）同学也参与了这项计划。这是今年年初发表在**arXiv**（arXiv: 一个预印本服务器，用于发布科学论文）上的文章。

为什么可能存在更合适的语音表示方式呢？现在我们已经知道，在生成这些**语音Token**时，模型会同时生成**文字Token**。如果我们相信这种**语音文字混合生成**（Hybrid Generation: 指模型同时生成两种或多种不同类型的数据，如语音Token和文本Token）的方式是未来的主流，那么在构建**语音Token**时，就应该考虑到**文字Token**。

如何做到在构建**语音Token**时考虑到**文字Token**呢？我有两个想法。第一个是，我们希望**语音Token**的数量能够配合**文字Token**，两者之间不要有非常复杂的关联性。过去的**语音Token**通常是**固定时长**（Fixed Duration）的，每个**语音Token**对应一小段时间（比如0.02秒）。我们不想要这种**Token**，我们期望有**动态时长**（Dynamic Duration）的**语音Token**，每个**Token**对应不同长度的时间，并且与文字有简单直接的对应关系，甚至是一对一的关系。所以，我们期望有一个**分词器**，它能做到：一句话如果是三个**文字Token**（“How are you”），它就只输出三个**语音Token**。这三个**语音Token**分别告诉我们这三个**文字Token**（How、Are、You）应该如何发音，才是合适的回复。

这是第一个想法。如果**语音Token**和**文字Token**之间的关系非常简单，例如它们之间有一对一的关系，每输出一个**文字Token**，就输出一个**语音Token**，那么我们在训练**语言模型**时就会变得非常简单。我们不需要考虑各种策略，只需记住训练模型每次生成一个**文字Token**，就生成一个**语音Token**。这可能使训练更加简便，让模型更容易学习这样的**序列**（Sequence）。

另一个考量是，既然生成结果中已经有文字，那么**语音Token**就不需要包含文字信息。过去在构建这些**语音Token**时，人们的想法往往是这些**语音Token**应该**包罗万象**，包含语音中所有的信息。但现在，如果我们在生成时，文字一定会伴随**语音Token**产生，那么**语音Token**其实只需要专注于存储那些文字无法表达的信息即可。文字信息交由**语言模型**已经产生的**文字Token**来表达。

所以，我们的**分词器**在接收输入时，不能只接收这些**语音Token**，它还需要将**文字Token**作为输入。**分词器**会根据这些**文字Token**和**语音Token**来生成声音信号。而这些**文字Token**已经告诉**反分词器**这句话要说什么。除了要说什么，这句话如何被说出来，用什么样的声音说出来，则由**语音Token**来操控**反分词器**完成这些事情。

### TASTE模型架构与实验

有了这些想法后，**曾亮軒**同学构思了一个神奇的模型，叫做**TASTE**（TASTE: TASTE Align Speech Tokenization And Embedding 的缩写，一种新的语音Token化方法）。**TASTE**提取**Token**的方式是这样的：首先，我们有一个传统的**Speech Representation Model**（这里你可以使用任何**Speech Representation Model**，在我们的论文中直接使用了**Whisper**的**编码器**），它会产生一排**表示**，每个**表示**对应固定的时间。接下来，我们会从不同的层提取两排**表示**（具体提取哪两排是尚待研究的问题，论文中根据经验选择了最合适的两层）。

然后，我们假设这段声音已经进行了语音识别，所以你知道它对应的文字是什么。接下来，你会将这里的文字作为**注意力机制**（Attention Mechanism）中的**查询**（Query），将那两个**表示**一个作为**键**（Key），一个作为**值**（Value）。至于哪一层适合作为**键**，哪一层适合作为**值**，需要通过实验来确定。然后进行**注意力计算**，将**查询**与**键**计算**注意力权重**（Attention Weight），并对**值**进行**加权求和**。每个**查询**都会给我们一个**Token**，一个**表示**。接下来会进行**量化**（Quantization），所以最终输出的是一个**离散Token**。如果语音识别结果有几个**文字Token**，我们就提取几个**语音Token**。

那么，这些**语音Token**如何还原回声音信号呢？从**预训练的语音编码器**到**聚合器**（Aggregator），从声音信号到这些**语音Token**，合起来就是我们的**分词器**。接下来我们看**反分词器**（Detokenizer）的样子。**反分词器**除了接收这些**Audio Token**之外，还会接收**文字信息**。它会将语音识别出来的结果也作为输入。所以，你可以将这个**反分词器**看作是一个**语音合成模型**。事实上，我们的**反分词器**就是直接拿一个**语音合成模型**（**CosyVoice**）进行**初始化**的。所以我们的架构与**CosyVoice**这个**语音合成模型**一模一样。

与一般**语音合成**不同的是，一般**语音合成**是根据文字生成声音信号。但我们现在除了**文字Token**之外，还有一些**语音Token**。这些**语音Token**告诉我们这些**文字Token**应该如何发音才是对的。所以，这个**反分词器**就根据文字和**语音Token**去还原声音信号。训练时，与一般的**语音Token**和**图像Token**训练方式一样，就是要**最小化重建误差**（Minimize Reconstruction Error）。输入一段声音信号，变成**Token**，**Token**通过**反分词器**还原回来后，输入与还原的结果要尽可能接近。然后，你就可以**端到端**地训练**聚合器**和**反分词器**，你就拥有了一个可以提取与文字数量相同的**语音Token**的**分词器**了。这个方法就叫做**TASTE**。

我们在这里做一个小实验来展示一下这些**语音Token**中可能包含了什么信息。这个实验展示了这些**语音Token**包含了**时长信息**，它告诉你一个**文字Token**要念多长。我们先给**分词器**一段音频文件，这段音频是一个人以很慢的语速念一个句子。这句话输入进去后，就产生一排**Token**。我们另外一个句子是一个人念得很快。我们将这两个句子的部分**Token**对调。我们将第一个句子的“Came around to”对应的**语音Token**与第二个句子的“News on the”对应的**语音Token**对调。对调后，有了不同的**语音Token**，再输入到**反分词器**中还原回声音。

（播放音频示例）

你听出来了吗？“Came around to”这三个字突然被加速了。这表明这三个**Token**告诉**反分词器**，念这三个字时要加速。再看另一个例子，当“News on the”换成蓝色**Token**后，你会发现这三个字念的时候被减速了。这说明这些**语音Token**确实告诉我们一个词汇，一个**文字Token**，要如何被念出来。

### TASTE模型效果展示与局限

接下来，有了这个新的**分词器**后，就是训练了。你需要做的是找一些语音数据，这些语音数据不需要任何标注，我们并不真的需要文字标注，所以这种数据可以轻易获取。然后，你对这些语音数据进行**语音识别**，得到它对应的文字。有了对应的文字后，你就看有几个**文字Token**，就提取几个**语音Token**。然后，你就可以训练一个**语音语言模型**，它每次就是输入一个**文字Token**，就输出一个**语音Token**。这个**语音语言模型**也可以从**文字语言模型**进行**初始化**，确保它能继承**文字模型**已有的能力。

大家要注意，这里的模型只是**预训练模型**，我们没有进行**监督式微调**（Supervised Fine-Tuning: 监督式微调，使用有标注数据对预训练模型进行进一步训练）。我们只用大量没有标注的数据来训练它。所谓大量，大约是一万小时左右。你可能觉得一万小时很多，但我的学生在大公司工作时常告诉我：“老师你不要再说一万小时很多了，正确的模型训练的语音单位是**亿**。”他们都是用上亿小时的数据来训练模型的，真的非常惊人。

总之，在我们有限的算力下，用不到一万小时的数据来**预训练**一个模型。这里没有进行**SFT**，所以模型唯一能做的事情就是**语音接龙**：你给它前半句话，它帮你把后半句话讲完。这样的模型如何评估呢？我们也需要进行一些**评估**来展示模型的表现。评估方式是：你给模型一段声音信号，让它进行**声音信号接龙**。接完后，如何判断它说得好不好呢？我们从三个方面评估：首先，我们会将接出来的声音信号进行**语音识别**，然后将识别结果交给**GPT-4**，问它：“你觉得这句话合不合理？”这衡量的是它的**语义连贯性**（Semantic Coherence）。其次，我们会测量一个叫做**UTMOS**（UTMOS: 语音质量评估指标）的东西，它代表语音的**质量**。当然，这些都是**自动化评估方法**。有时一个模型可能骗过了这些**自动化评估方法**，但实际表现不好。所以最后，我们还需要人工评估，看哪一个模型生成的结果是最好的。

结果如何呢？我们主要比较的对象是**TWIST**和**Spirit LM**这两个模型。**TWIST**就是用**文本模型**做**初始化**，但它没有保持生成文字的能力。**Spirit LM**则是在训练时进行**交错**，将语音和文字以**交错**的方式训练。我们的模型是**Ours**。你会发现，我们的模型在所有三个评估指标上都比**基线模型**（Baseline Model: 用于比较新模型性能的参考模型）要好。我们甚至尝试了一下，这些**预训练模型**能否回答问题。虽然是**预训练模型**，但它在进行**接龙**时，有时也能接出正确的答案。所以它仍然具有一定程度的回答问题能力，虽然不如那些**监督式微调**的模型做得好，但它能回答问题。

这个模型实际表现怎么样呢？上个学期我已做过**演示**，展示这些模型的英文能力。你可能会问为什么这里会有**Ruby**（Ruby: 一种面向对象的编程语言，此处可能指代某个特定人物或项目），你可以看那个视频就知道了。这里我们来**演示**一下模型的中文能力。我给它前半句中文，让它把这句话说完。我先用我自己的声音尝试一下：“大家好。”模型就继续接下去：“大家好，我是，我是奎老师。”为什么是“奎老师”我也不知道，反正它讲出来就是这个样子。要注意，它是一个**预训练模型**，所以就像我们之前的**文本预训练模型**一样，**文本预训练模型**在做**文字接龙**时会“乱讲”，它也一样，它是生成**语音Token**，它想讲什么就讲什么。

（播放中文演示音频）

至少是知道它在讲什么。但你会发现，有一些合成的地方，比如“Restriction”，它就是乱念一通。我想之所以，它有一些字会乱念的原因，是因为训练资料其实非常的少。在这个Project里面，我们用的训练资料是不到1000个小时的。如果有一些词汇，模型从来没有听过，它怎么念，它可能就没有办法精确的把那个词汇念出来。那只有一千小时训练资料，真的非常的少。光要让模型好好回答问题，可能都不太够。更遑论是要让它学到一些什么Emotion啊、Speaker啊相关的东西。如果要让机器能够学到这些文字以外的资讯，那我們常常把文字以外的資訊统称为**Paralinguistic**（副语言信息: 指语音中除了文字内容以外的、传达情感、语者身份等信息的特征，如语调、语速、音色）的资讯。让模型学到这些**Paralinguistic**的资讯，可能需要用**Web Scale Data**，用非常大量的资料，网路上爬到的大量资料，做一个大规模的**Pre-train**才有办法办到。所以如果只用上千个小时做**Fine-tune**，你可能只能让模型回答问题，但不容易让它学到语意以外的东西。

### 语音语言模型的后期训练：SFT与RLHF

到目前为止，我们提到的**语音语言模型**都只是**预训练模型**。但你知道**预训练**并非构建一个模型的全部。**预训练**之后，你还需要进行**SFT**（Supervised Fine-Tuning: 监督式微调，使用有标注数据对预训练模型进行进一步训练）和**RLHF**（Reinforcement Learning from Human Feedback: 基于人类反馈的强化学习，通过人类偏好来优化模型行为）。

对于**语音模型**的**SFT**和**RLHF**，目前没有非常特别的想法。当然，在进行**SFT**时，你需要教模型进行**一问一答**，所以此时你显然需要**对话数据**。如果你都使用非对话数据（例如大量演讲）来训练它，它就只会听一个人的声音，然后用同一个人的声音继续讲下去。所以，要教它进行对话，你需要收集一些**对话数据**来训练**语音语言模型**。

但有时**对话数据**可能无法大量收集。因此，一个常见的想法是**自己合成对话数据**。你可以让**文本模型**生成对话脚本，再用**语音合成模型**将对话脚本朗读出来，这样你就有了语音对话数据，可以用来训练模型。

接下来需要进行**RLHF**，让模型的输出更贴近人类需求。这里的**RLHF**与一般的**文本RLHF**可能没有本质上的不同。人类可以提供**反馈**（Feedback），告诉**语音语言模型**哪个答案好，哪个答案不好。你甚至可以训练一个**奖励模型**（Reward Model: 评估模型输出质量并提供奖励信号的模型，用于RLHF）。这个**奖励模型**接收**语音语言模型**的输入和输出，判断它们组合起来是否构成一个好的对话。人类会教**奖励模型**生成合适的**奖励**，然后你再用**奖励模型**为**语音语言模型**提供**反馈**，使其输出更接近人类需求。这与文本模型的做法可能没有非常核心的差异，所以这里我们讲得比较简短。

我们来看几个**语音语言模型RLHF**的例子。第一个例子是**林冠廷**（林冠廷: 语音领域研究者）同学在**Amazon**（Amazon: 一家全球领先的电子商务和云计算公司）实习时做的。这篇论文发表于去年**11月**，大约一年前。在AI领域，一年前就是很久很久以前了，所以这是一篇早期的论文。当时关注的还不是如何让**语音语言模型**学会**副语言信息**。**林冠廷**同学在更早的论文中，曾尝试让**语音语言模型**学习**副语言信息**，但那是**SFT**阶段的工作。在**RLHF**阶段，他当时思考的是能否让这些模型好好说话，至少说出人能听懂的话。因为他当时使用的模型还没有进行文本和语音的**混合**，只生成**文字Token**，所以模型说出来的话往往是人听不懂的。他试图通过**RLHF**来纠正这些模型，让它们能好好说话。

论文中直接复制的图显示，他们将模型的输出进行**语音识别**，先转换为文字，再将这些文字交给一个**文本模型**，让这个**文本模型**评分，判断答案是好是坏。**语音语言模型**获得这样的评分后，就得到了**反馈**，从而可以**微调语音语言模型**。最终，即使不使用文本和语音**Token**的**混合解码**（Hybrid Decoding），通过**强化学习**的力量，也能让模型好好说话。

我想跟大家分享的是，**楊書文**同学在**ByteDance**（字节跳动: 一家中国科技公司，拥有TikTok等产品）实习时也发表了一篇论文，这是上个月才放到**arXiv**上的文章。他们的论文也做了**RLHF**，但他们更关注如何让模型学习**副语言数据**。他们的模型可以做到：给定内容相同的句子，但情绪或其他**副语言信息**不同，模型能否做出适当的回应。例如，如果是不同的性别、不同的年龄（老人的声音、小孩的声音），模型能否做出适当的回应。他们有一个**演示**，我可以展示一下。

（播放演示音频）

这个**演示**展示了，当一个人用兴高采烈的声音说“我现在知道是谁在我们团队里面得到了升迁的机会”时，模型会用高兴的声音给出正面回应。而当一个人用失落的声音说“好像没有选他”时，模型会安慰他。还有许多其他例子，例如模型能识别讽刺与真诚的语气，并给出相应的回应。在性别方面，当女性询问预防癌症的检查时，模型会建议乳房检测；当男性询问时，模型会给出不同的建议。在年龄方面，当大人询问设置银行账户时，模型会给出正常回答；当小孩询问时，模型会知道他是小孩，而不帮他办理。所以，这就是**语音语言模型**能做的事情。

### 语音语言模型的推理能力：STITCH

接下来，我要讲下一步的研究：让**语音语言模型**一边说一边思考。我们知道，现在很多**文本模型**都具有**推理能力**（Reasoning: 模型在给出答案前进行逻辑分析和思考的过程），通常翻译为**深度思考**。例如**ChatGPT**在回答问题时，常常会显示“正在思考”；**DeepSeek**会告诉你它“已思考”；**Gemini**会告诉你它有某种“思路”。通常这些**思考过程**默认是隐藏的，但你可以点击展开，看看模型在想什么。**Claude**也具有这种**推理功能**。

现在**文本模型**拥有这些**推理功能**，并且这些**推理功能**确实可以强化**文本模型**的能力。所谓**推理功能**，实际上是模型在产生答案之前，额外生成一些**文字Token**。这些**文字Token**不需要给用户看，模型生成出来给自己看，可以帮助模型得到更正确的答案。对于语音来说，我们能否做同样的事情呢？

到目前为止，我们讲的**语音模型**都是输入语音输出语音。但实际上，输出的不仅是语音，而是**语音与文字的混合**（Hybrid）。今天能运行的**语音语言模型**通常都是这样做的。不过，为了简便起见，我接下来不再画出**文字Token**。但在模型得到答案之前，我们能否让它像**文本模型**一样，也进行**推理**呢？这里的**推理**输出的可能也是**文字Token**，也不给用户看，通过**推理**让模型得到更好的结果。

确实有人尝试过这个方向，例如**Audio Reasoner**和**Audio Thinker**都将**语音语言模型**与**推理能力**结合。然而，如果我们将**推理能力**加入这种**对话模型**，你会发现在实际应用中会带来巨大问题。如果每次你对模型说完一句话，它需要花费很长时间（可能是10秒、15秒）进行**推理**，然后才输出回应，这会非常影响用户体验。如果你今天使用**ChatGPT**的文本版在线平台，你可能可以接受在电脑前等待一下。但如果我们要让这些**语音语言模型**模仿人类真实的对话，对话时每次都要等10秒才能回应，这实在难以接受。

那么怎么办呢？我们能否让模型**一边说话一边思考**？其实人类也是这样。我们在与另一个人讲话时，同时也在思考接下来要说什么。例如，作为一名老师，在很多场合常有人问问题，演讲结束后大家会举手提问。通常人们问完问题，都期望你立即给出答案。如果你说我隔30秒再给答案，大家是不能接受的。但有时如果你没有立即的答案怎么办呢？这时你通常会说：“嗯，这真是个好问题。”以此争取几秒钟的时间思考答案。我们能否让模型做同样的事情？它能否**边听边讲**？它能否**边讲边想**？

### STITCH：边说边思考的语音语言模型

如何让模型做到**边说话边思考**呢？我们本来以为这可能需要一个新的模型架构，例如需要两个平行的**解码器**（Decoder），一个负责生成**语音Token**，一个负责生成**推理Token**，它们之间互相进行**注意力机制**。这样可以使模型**一边说话一边思考**。但是，接下来要讲的**姜成翰**（姜成翰: 语音领域研究者）同学的研究发现，**不需要改变语言模型的架构**，使用原有的语言模型架构就有机会让模型做到**一边说话一边思考**。他的论文已于今年**7月**发布在**arXiv**上，他将这个模型命名为**STITCH**（STITCH: 一种允许语音语言模型在对话中边说边思考的模型）。这是他与**Microsoft**（微软: 一家全球领先的科技公司）的研究人员合作的成果。

**STITCH**是如何运作的呢？在讲解**STITCH**之前，我先举一个例子，看看现在的**语音语言模型**是如何运作的。这里以**GLM-4-Voice-9B**模型为例。这类模型在对话时，会先生成一些**Token**。它生成的39个**Token**中，通常是先生成13个**文字Token**，再生成26个**语音Token**，总共39个**Token**为一组。生成完39个**Token**后，它会将这些**Token**交给**反分词器**还原出声音信号。在播放声音信号的同时，它会再生另外39个**Token**。它必须在声音信号播放完之前，生成完这39个**Token**，并通过**反分词器**合成出声音信号。这样，在前一段音频播放完后，新的声音信号可以立即衔接，人听起来就会是连贯的，不会有不自然的地方。

在播放前一段音频的时间内，**GLM-4-Voice**有足够的时间生成39个**Token**并合成声音吗？非常足够。如果你有一张**NVIDIA A100**（NVIDIA A100: 英伟达推出的一款高性能GPU，常用于深度学习训练）显卡，生成39个**Token**加上**反分词器**只需要大约0.5秒。而用0.5秒生成39个**Token**后，合成出来的声音大约对应2秒的时间。所以，只要模型能在2秒内生成下一段音频，你的声音听起来就不会有不自然的地方。而生成下一段音频只需要0.5秒，这意味着你的模型其实有1.5秒的时间是空闲的，这1.5秒可以作为**缓冲区**（Buffer）。

我们可以利用这1.5秒的时间做什么呢？能否让它在这1.5秒内进行**推理**？能否让模型在这1.5秒内生成一些**推理Token**？1.5秒的时间足以让模型生成至少100个**推理Token**，通过这些**推理Token**来强化模型的能力。而要让模型生成**语音Token**，再生成**推理Token**，再生成**语音Token**，其实不需要修改内容，它就是一个正常的**语言模型**，你只需要修改你的训练数据即可。

### STITCH的训练与效果

**STITCH**提出的想法是这样的：通常我们说要增加**推理**能力，是你对模型说一句话，它产生**文字推理**，然后给出答案。它产生的是**语音Token**，需要**反分词器**还原声音信号。但**STITCH**的做法是：你对模型说一句话，它先生成**语音Token**，立即将**语音Token**还原出声音信号。它可能一开始会重复你的问题，然后说：“这真是个好问题。”然后才开始回答。接下来它会生成**推理Token**。只要在声音播放完之前，它能生成**推理Token**，并生成足够的**语音Token**，再合成出下一段声音，那么在前一段声音播放完之前，下一段声音就已经生成出来，声音听起来就不会有不自然的地方。同时，在空闲时，它还可以生成一些**文字Token**，这相当于它**一边讲话一边思考**，而没有在讲话时进行深入思考，从而得到更好的答案。

接下来问题是如何训练这样的模型呢？现在世界上已经有许多训练**语音到语音模型**的对话数据。这些对话数据就是你有一句输入，有一句输出，它们都是语音的。我们这里做的事情是，先对它们进行**文字转录**（Transcription），即**ASR**（Automatic Speech Recognition: 自动语音识别），将其转换为文字。然后将这些文字交给**GPT-4o**，告诉它：“现在如果要产生这个答案，你觉得要做什么样的**推理**？”让它写出**推理过程**。

接下来在训练时，我们将这个**推理过程**切成一段一段的，每100个**Token**一段。语音输出也要切成一段一段的，比如39个**Token**一段。然后，你教你的**语音语言模型**：输出语音的第一段**Token**后，接下来就输出**推理**的第一段**Token**；再输出语音的第二段**Token**，再输出**推理**的第二段**Token**，就这样持续下去。这样，模型就会学会在两段语音之间，利用一些空档进行思考。

这一招有用吗？**姜成翰**同学的实验是进一步**微调GLM-4 Voice**。**GLM-4 Voice**本来没有**推理能力**，他通过**微调**使其具备**推理能力**。结果如何呢？他在一系列数学问题上进行测试。这里展示的是不同数学语料库的平均结果。原版**GLM-4 Voice**的正确率约为**53%**。经过**微调**后，如果**没有增加推理**，它其实可以从53%提高到大约**63%**，仍然有10%的进步。这可能是因为我们的一些训练数据是**GLM**没有的，所以**微调**后即使不增加**推理能力**，仍然有一些提升。

如果你为它增加**推理能力**，但不是使用**STITCH**，你可以得到**79%**的正确率。也就是说，模型听完一段声音后，它先在脑中生成一大堆**文字Token**，思考完之后（无论多久，人都必须等待），然后它再生成答案。这样可以得到**79%**的正确率。而我们这里，如果使用**STITCH**方法，我们有两个实验结果（具体细节请查阅论文）。即使是较差的结果，也能达到约**78%**的正确率。而且要注意，当我们进行**推理**时，使用传统**推理**方法，人必须等待模型进行**推理**，这是一种很不自然的互动。但当我们使用**STITCH**时，这个模型的**延迟**（Delay），即人的等待时间，与没有**推理**的情况是相同的。我们通过利用两次生成**语音Token**之间的间隔时间进行**推理**，可以将正确率从62%提高到78%，而相对于让人等待，只损失了1%的正确率。

此外，虽然我没有展示实验结果（论文中有），模型的**音频质量**不会因为使用**STITCH**而受到影响。**姜成翰**同学还做了另一个实验：他设想，如果拥有**A100**显卡，生成39个**Token**只需0.5秒。但如果你没有**A100**呢？不是每个人都有**A100**。如果你使用性能较差的GPU，生成**语音Token**的时间就会更长，**缓冲区**就会更短，能生成的**文字Token**就会更少。在**推理**时，我们能否在训练时都训练生成100个**推理Token**，但在**推理**时，根据当前使用的GPU等级，**动态地调整**生成多少**文字Token**呢？他发现这是可行的。训练时都是固定100个**Token**，但在**推理**时，即使减少**Token**数量，**推理**仍然能带来帮助。

以下是实验结果。横轴是**推理**使用的**Token**数量（从60个到100个），不同的线代表不同**语料库**上的结果，纵轴是正确率（数值越大越好）。你可以很明显地看到，**Token**越多，性能当然越好。虽然**Token**越多性能越好，但图中那些没有连接起来的点代表没有**推理**的结果。你会发现，虽然**推理**通常更好，但没有**推理**在很多情况下仍然有帮助。所以，即使你在**推理**时没有好的GPU，你仍然可以让模型进行**推理**，只是**推理Token**的数量少一点，你仍然可以通过**推理**来提升模型的能力。

### 外部推理与未来展望

我们再进一步思考：**推理过程**能否不由**语音模型**自己生成？毕竟**语音模型**往往更专注于生成好听的声音，其**推理能力**可能不如那些**文本模型**强。我们能否使用另一个**文本模型**在背后进行**推理**，将其**推理结果**交给**语音模型**，从而强化**语音模型**的答案呢？这确实是可行的。

这里的做法是：当你给**语音模型**一段声音信号时，你同时进行**语音识别**，将那段语音识别的文字交给一个**文本模型**。接下来，**语音模型**会生成**语音Token**。本来我们会让**语音模型**自己生成**推理Token**，但我们现在不用**语音模型**自己的**推理**，而是让**文本模型**生成**推理结果**。让**文本模型**根据文字输入（通过**语音识别**得到的结果）和**语音模型**的输出（**语音模型**的输出其实包含文字）来生成**推理结果**。**文本模型**生成**推理结果**后，**语音模型**再根据**文本模型**的**推理结果**生成**语音Token**。这个过程不断反复，看看能否通过**文本模型推理**来强化**语音模型**的能力。

结果确实是可行的。这里的纵轴是模型在各个不同**语料库**上的正确率平均值。每条线代表我们使用了不同的模型来生成**推理**。红色这条线是模型的**推理**（**文字推理**）与生成语音使用的是同一个模型。最好的这条线是换了一个**文本模型**，这里使用的是**GLM-4 9B Chat**（GLM-4 9B Chat: 智谱AI开发的GLM-4系列中的一个强大文本模型），让它来生成**推理**，再由**语音模型**来回答答案，可以得到更好的结果。如果你故意使用一个较差的**文本模型**，例如**Llama 3.2 1B**（Llama 3.2 1B: Meta Llama系列中的一个较小文本模型），你会发现结果会差很多。这个实验结果告诉我们一件事：**推理**确实对模型有影响。它并不是说“我就**推理**乱讲，我就不要管**推理**，我就光产生答案就好”。其实**推理**会影响模型的结果。如果给它差的**推理**，它最终就会得到差的结果。

最后，这是一个**演示**，让你了解这个模型运作起来是什么样子。你会听到有人念一个数学问题，然后模型用语音来回答这个问题。

（播放演示音频）

你刚才听到模型讲话时没有任何停顿，它就是顺畅地把话说出来。但你可以看到它在背后其实是做了一个**推理过程**。模型实际运作过程是这样的：红色的东西代表**语音Token**，那是人看不懂的，就是一堆**语音Token**。黄色的东西是**文字Token**，但那些**文字Token**是用来支持那些**语音Token**的。而绿色的东西是模型在做**推理**时说出来的文字。你看这里它会条列一点、两点、三点、四点，但这些是不会被念出来的。你就想成这些东西是模型在心里思考的，人类不需要看这么复杂的东西。你会发现，模型一开始先复述问题，然后用一些语音生成一些**语音Token**。在复述问题的同时，它在心里就开始条列，想说这个问题要怎么解。然后它再讲一些话，再想如何解。所以刚才听到模型并没有列出很完整的计算过程，但它可以给你一个正确的答案。这就是**STITCH**做的事情。

### 语音语言模型的未来研究方向

其实在有了**STITCH**之后，接下来也有很多模型有类似的想法，例如让模型**一边讲话一边使用工具**，**一边听声音一边使用工具**等等。我今天就把一些相关的论文列在这里给大家参考。

这就是今天想跟大家分享的内容。我今天讲述的是我以第一人称视角，我们团队曾经参与的研究。但是这里还有很多很多没有讲到的与**语音语言模型**有关的技术。有一个很关键的技术还没有讲到，那就是对于**语音语言模型**来说，有一个非常重要且需要具备的技能，就是它要能够**超越回合制问答**。

什么叫**超越回合制问答**呢？通常我们使用**文本模型**时，例如**ChatGPT**，就是**一问一答**。你给它一个输入，它给你一个输出，它输出完才轮到你讲话。但在**语音对话**中，这个对话是**全双工**（Full Duplex: 指通信系统能够同时进行双向数据传输）的。**全双工**是什么意思呢？**全双工**意味着沟通是双向的，两个人可能同时发出声音信号。例如，其中一个人在讲什么事情，你就会发出一些声音，比如你说“你说的都对”，代表你有在听他讲话。在对方讲话时，你得一边发出声音，这样才能变成一个自然的对话。或者你在讲话时，对方说“这不是我想听的”，你停下来，你会同时听他在讲什么，然后他叫你停下来，你可能就会停下来。或者你觉得他说得根本是错的，你可能会主动插话打断，说“不不不，你说的错了，不要再讲了”，他在讲话，你还是继续讲，想要声音压过他。所以，两个人对话时，其实可能是同时在说话的。

如何处理这个问题呢？今天当然有不同的解法，但还没有标准答案。但在有标准答案之前，也许我们需要知道什么样的答案才是标准的。我们期待一个**全双工模型**能做到什么样的事情呢？所以我们实验室的**林冠廷**同学和**上官思雲**（上官思雲: 语音领域研究者）同学就做了一系列与**全双工**有关的**评估**，看看今天号称可以做**全双工**的模型，它们都能做到什么样的事情。由于今天时间有限，我们就不再细讲这个部分，我把论文链接列在这里给大家参考。

还有很多能力，也许我们希望有一天**语音语言模型**可以具备。举例来说，**语音语言模型**既然要与人类互动，那它要与存在于现实中的人类互动，它就得知道人类**物理世界的时间流逝**。今天**语音语言模型**能知道**物理世界的时间流逝**吗？它对**物理世界的时间**有概念吗？看起来是没有的。**張凱為**同学在这方面做了一系列的测试，你可以看看他的这篇论文。例如，如果我们今天问这些**语音语言模型**，叫它十秒钟不要说话，基本上没有**语音语言模型**能真正做到这件事情，因为它根本没有“十秒”这样的概念，它根本不知道多长是十秒。那么，如何让**语音语言模型**能够意识到现实生活中真实的**物理世界时间流逝**，这也是另一个研究问题。总之，还有太多未解的问题尚待研究。

最后，如果你想了解更多与**语音语言模型**有关的事情，可以参考一下我与其他学者写的一篇**综述论文**，我们从**语音语言模型**的起源开始讲起，一直讲到今日**语音语言模型**的发展。这就是我今天想跟大家分享的内容。