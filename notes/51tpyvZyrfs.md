---
area: tech-insights
category: technology
companies_orgs: []
date: '2025-08-16'
draft: true
guest: ''
insight: ''
layout: post.njk
products_models: []
project:
- ai-impact-analysis
series: ''
source: https://www.youtube.com/watch?v=51tpyvZyrfs
speaker: Best Partners TV
status: evergreen
summary: 本文探讨DeepMind Genie 3与世界模型，对比其与大语言模型的异同，强调AI理解世界对实现通用人工智能（AGI）的关键作用，并展望全能模型的未来。
tags:
- agi
- llm
- world-model
title: DeepMind Genie 3与世界模型：AI理解世界是通往AGI的必经之路
companies:
- deepmind-genie-3
---

### 引言：Genie 3与世界模型的核心议题

2025年8月12日，Google开发负责人洛根·基尔帕特里克（Logan Kilpatrick）与DeepMind的首席执行官德米斯·哈萨比斯（Demis Hassabis）进行了一场信息量丰富的对谈。此次访谈不仅涉及**通用人工智能**（AGI: Artificial General Intelligence, 具备与人类同等或超越人类智能水平的人工智能）的进展和AI未来所需具备的能力，更重点提及了DeepMind刚刚发布的**Genie 3**（DeepMind Genie 3: DeepMind开发的100%可控、实时的AI世界引擎）。Genie 3是一个100%可控、实时的AI世界引擎，其视频演示已广为人知。或许有人会认为这不过是一个高端的视频生成器，但实际上，Genie 3背后代表的是一个最值得深入讨论的概念——**世界模型**（World Model: AI在内部构建的环境模拟器，用于预测结果、想象未来和评估策略）。今天，我们将借此访谈内容，探讨一个可能之前很少思考的问题：如果一个AI不理解这个世界，它还能算是智能的吗？

### Genie系列模型的演进与突破

在回答上述问题之前，有必要先真正认识一下Genie系列模型。早在2024年，DeepMind就发布了第一代Genie模型，其主打口号是“用视频训练AI来理解世界”。当时，它的能力还非常有限，只能根据用户输入的图像或语义生成十几秒的视频片段，质量也比较粗糙。不仅帧率低、画面模糊，有时人物动作还会扭曲，与我们早期使用**Midjourney**（Midjourney: 一款流行的AI图像生成工具）制作图片、用**Runway**（Runway: 一款流行的AI视频生成工具）制作视频的效果相差无几。然而，DeepMind的野心深藏不露，他们实际上是想用这些视频作为教材，让AI从中学习物理规律、空间动态和因果关系，就像小孩通过看动画片就能学会“水是会流的”、“人摔倒了要站起身”等物理概念和常识。

到了Genie 2，它已经能够生成更为连贯的3D环境，例如一个人在屋子里走路、滑雪、翻滚等。但当时的互动性依然有限，大多数时候用户只能观看AI表演一小段短片，既不能干预，也不能下达命令去控制，相当于一个自动播放的窗口。此外，Genie 2的记忆是断裂的，如果前一帧看到了一个红色的滑雪板，下一帧它可能就直接变成了绿色；或者一个物体刚刚出现在左边，两秒后可能就凭空消失了。因此，从实用性角度看，Genie 1和Genie 2更像是一种概念验证，类似于我们能够用视频来教会AI“梦见”一个世界，但它还无法维持梦的连贯性。

然而，到了这次的Genie 3，情况被彻底改变了。它不仅将画面的清晰度提升到了720p，还能够稳定地以24**FPS**（帧率: Frames Per Second, 衡量视频或动画每秒显示的帧数）的速率实时生成画面。这意味着在AI生成的世界里，每走一步，它都能立刻刷新眼前的场景，没有任何卡顿，就像在玩一款开放世界游戏一样，而这一切完全是由AI实时创造的。

不仅如此，Genie 3还首次引入了“**提示式世界事件**”（Promptable World Events: 通过文本提示词控制AI生成世界中事件的机制）的机制。用户不光可以走、可以看，还可以实时地给它下达剧情指令。从控制方式来看，Genie 3不再只是观看视频那么简单，而是真正支持了第一人称视角的导航和实时互动，让用户可以体验在虚拟世界中的生活。例如，如果想把一条山林小径变成AI的世界，只需给出提示词，如“在冰川湖畔奔跑”、“穿行于林中分岔小径”、“跨越流淌的山间溪流”、“坐落在美丽的白雪皑皑的群山和松林之间”、“丰富的野生动物使旅程充满乐趣”，Genie 3会立刻生成这样的环境。用户可以进入这个场景，观察水流如何绕过石头、鸟儿如何飞翔、阳光如何洒下。再比如，如果想生成一个飓风现场，Genie 3会创建一个可交互的三维环境，用户可以置身其中，以第一人称视角看着海浪一波波地拍打公路、棕榈树在风中剧烈摇摆，所有细节都保持了逻辑和物理上的一致性。这也解释了为何哈萨比斯在访谈中会多次强调Genie是DeepMind在模拟世界理解这件事情上迈出的最重要一步，也是DeepMind从AlphaGo一路走来最想实现的梦想之一。换句话说，能够生成一个世界，这本身就是对AI是否理解世界的最好测试。

### 世界模型为何难以普及？挑战与壁垒

也许看到这里，读者会有一个疑问：为什么像**GPT**（Generative Pre-trained Transformer: 一种基于Transformer架构的预训练语言模型）这样的大语言模型更新迭代迅速，各家公司产品层出不穷，而世界模型似乎从未走进大众视野，始终只有少数几家公司在孤独地推进呢？要理解这个问题，不妨先问问自己：训练一个AI学会说话，和让它理解世界，哪个更难？

应该说，大语言模型这些年突飞猛进，很大一部分原因是它自带“外挂”：一是数据量庞大，二是训练成本低。互联网上几十**TB**（太字节: Terabyte, 衡量数据存储容量的单位，1TB = 1024GB）的文本语料可以随意抓取，如公众号文章、小说、维基百科、知乎、Reddit等，人类已经用语言把世界描绘得密密麻麻。更核心的一点在于，语言本质上其实就是一维序列，一句话接着一句话，所以训练方式也很直接，只需预测下一个**词元**（Token: 文本或数据被分割成的最小有意义单位）即可，可以说是成本低、效率高。

然而，对于世界模型来说，它要做的是预测下一个“世界”，首先面对的就是数据问题。模型需要训练的是视频、物理和因果关系的数据，但这类现成数据并不多。想要训练AI理解世界，单纯依靠文本基本无法实现，它需要的是图像、视频、动作轨迹、物理动态、空间结构、因果链条等等信息。这些信息不仅数据量大，而且复杂度高。例如，视频的一帧高清图像就相当于几万个词元，一段视频可能就是上百万个词元。此外，它还涉及到时序、空间一致性，甚至角色之间的交互与反馈，因此必须将世界一帧一帧地清晰生成。关键是，这些数据从何而来？它不像大语言模型那样能抓取网页数据，基本上都得靠自己来制造数据。

例如，DeepMind选择用游戏Minecraft来合成环境；Meta则用机器人来采集第一人称视频；而英伟达的Cosmos模型背后，是千万小时的车载视频、**LiDAR**（LiDAR: Light Detection and Ranging，激光雷达）、深度图、边缘图、多模态标签等数据。即使我们真能获取到数据，还需经历拆分、去噪、标注、去重、分词、空间结构、跨模态对齐、词元压缩等一系列过程。英伟达曾提到，哪怕只训练一个能生成5秒钟720p分辨率视频的模型，也需要**PB**（拍字节: Petabyte, 衡量数据存储容量的单位，1PB = 1024TB）级的视频数据和百万美元级别的**GPU**（图形处理器: Graphics Processing Unit, 一种专门用于处理图像渲染和并行计算的微处理器）资源。这个门槛对于初创公司来说，基本上都很难达到。

其次，世界模型还面临算法方面的挑战。大语言模型的任务是生成所谓“合理”的句子，即使它“胡说八道”一点，只要读起来通顺，用户可能都很难发现。但世界模型则不同，它必须做到因果成立、物理合理、空间连续。例如，一个杯子从桌上掉下去，不能下一秒就消失了；一辆车拐了弯，必须保持同一个方向，不能突然就漂移上天了；一个角色说要出门，不能下一秒就直接出现在山顶了。也就是说，世界模型不仅要生成内容，还得维持这个世界的逻辑闭环。要做到这点，模型内部必须构建一个完整的“世界模拟器”，能够预测结果、想象未来、评估路径，并对未知场景做出合理回应。这背后的计算复杂度相比大语言模型来说几乎是指数级上升的。以**Dreamer V3**（Dreamer V3: DeepMind开发的一种基于世界模型的强化学习算法）算法为例，为了让AI在脑海中模拟Minecraft的场景，它不得不在每一帧中预测图像、奖励、是否终止、行为反馈，每一项都与下一步紧密相连，一步出错就会满盘皆错。

此外，大语言模型的进展很大程度上得益于**Transformer**（Transformer: 一种基于自注意力机制的深度学习模型架构，广泛用于自然语言处理）架构和算力的加持，上下文窗口越来越大，模型参数规模也越来越大。但世界模型很难纯粹依靠堆砌算力来解决问题，因为它面临的是一些更复杂的问题，例如它既要看图像，又要预测运动；既要记住过去，还要能够推演未来；既要生成细节，又要逻辑连贯；甚至还得考虑从动作到反馈再到后果的因果链条。因此，不同公司都在尝试自己的混合架构。例如，DeepMind的DreamerV3使用的是**循环状态空间模型**（RSSM: Recurrent State Space Model, Dreamer V3中用于预测和学习世界动态的核心组件）；英伟达Cosmos-Reason1使用的是**Mamba**（Mamba: 一种新型选择性状态空间模型，旨在提高序列建模效率）、**MLP**（多层感知器: Multilayer Perceptron, 一种前馈神经网络）和Transformer的混合体；Meta的NWM使用的是**CDiT**（条件扩散网络: Conditional Diffusion Transformer, 一种结合Transformer和扩散模型的生成式网络）。如果说GPT的成功来自于把全人类写下来的语言压缩成一个预测器，那么世界模型想要成功，就得依靠构建一个从视觉、动作、因果等要素中，构建出一个有逻辑可遵循的“小宇宙”。哪个更难，应该一目了然。

### 通往AGI的必经之路：世界模型的深层意义

那么，既然世界模型如此难以构建，为什么还要去做呢？因为它是我们离**AGI**最近的一条路，甚至可以说，如果没有世界模型，就不可能诞生真正意义上的通用人工智能。我们人类认知的根基从来不是语言，而是经验。语言只是我们记录世界的方式，而不是感知世界的方式。德国计算机科学家**于尔根·施密德胡伯**（Jürgen Schmidhuber: 德国计算机科学家，人工智能领域的先驱）很早就指出，一个**具身智能体**（Embodied Agent: 拥有物理身体或在虚拟环境中与世界交互的智能体）如果想进行有效的学习，就必须在脑海中构建出环境的“内部模型”，也就是所谓的**World Model**。借助它，智能体才可以在没有真实交互成本的情况下，在想象中进行“行动、反馈、更新”的闭环，从而像人类一样，在梦中学习、在梦中试错、在梦中总结出通用的策略。

这个观点在2018年的**《World Models》论文**（World Models论文: 2018年由Ha and Schmidhuber发表的论文，系统性验证了世界模型在强化学习中的应用）中首次被系统性验证。研究人员训练了一个生成式**RNN网络**（循环神经网络: Recurrent Neural Network, 一种处理序列数据的神经网络）去模拟游戏场景，然后在这个模拟出来的世界中训练控制策略。最终，这个只在“梦中”练习过的策略，居然可以在真实的游戏环境中直接上场完成任务。

**图灵奖**（Turing Award: 计算机科学领域的最高荣誉）得主、Meta首席科学家**杨立昆**（Yann LeCun: Meta首席人工智能科学家，图灵奖得主）也同样把世界模型放在核心地位。他曾公开强调，没有对世界的建模，AI就无法进行真正的推理。他提出的**JEPA**（联合嵌入预测架构: Joint Embedding Predictive Architecture, Yann LeCun提出的一种预测性学习模型）模型尝试跳出像素层面的建模，转向预测隐藏状态的抽象表示，强调的是模型预测未来潜在表征的能力，而非逐个像素的生成。这种思路与人类的认知极为相似，因为我们并不是通过逐帧还原画面的，而是基于抽象模型来推测世界会如何演化的。如今，Genie 3也继承了这种观念，它可以预测某个动作将如何影响场景、可以回忆之前帧的状态来确保逻辑上的一致性，而这些能力正是世界模型的核心。

大语言模型虽然能够生成条理清晰的文字，但终究只能在“语言的世界”里活动。它们对重力、摩擦、遮挡、空间关系等方面的知识，都是靠语言语料“猜”出来的。而世界模型的目标，则是希望让AI在脑海中建立一个物理上可信的现实模型。例如，GPT可以告诉你“骑车要掌握平衡”、“拐弯要减速”，但它自己从来没有骑过。而在沙盒世界里骑了上千小时虚拟自行车的Genie 3，即使它写不出一句“优雅”的文本，也能够精准地避开障碍、掌握重心，并实时调整策略。显然，我们期待的AGI更应该是后者。如果说大语言模型是大脑的“逻辑中枢”，那么世界模型就是AI的“运动皮层”与“感觉神经”。没有这些组成部分，AGI只能是停留在我们的嘴皮子上。

而且，世界模型也不只是为了建立起一个“看起来像是世界”的模拟器，而是为了给智能体提供行为试错的空间，甚至于，它是智能体意识的投影空间，是规划与预演的底座，更是让智能体能够脱离人类提示、自主做出策略选择的重要前提。即使大语言模型能够制定计划，但它无法验证计划是否可行。而世界模型可以在脑海中尝试运行每个计划的分支，从中挑出最优的路径。这也就是为什么所有追求AGI的研究团队最终都会走到世界模型这条路上的原因。

### 结论：真正智能的根基与全能模型的未来

所以，如果我们回到那个最开始的问题：一个AI如果不理解这个世界，它还能算是智能的吗？Genie 3给出的答案是：不能，至少不能算是真正的智能。当然，我们可以让大语言模型去模仿一个“聪明人”的样子，让它能考满分、能讲道理、能把文章写得头头是道，但它的“聪明”，就像是一个从来没有出过门、靠听别人描述来理解世界的孩子一样。你和它说“地震来了”，它想象的是文字，而不是晃动的地面；你说“风吹过树叶”，它浮现的是一组词语，而不是沙沙的响声。回想人类自己，我们并不是因为能够说话才变得足够聪明，而是因为从小就摔过跤、踩过坑、吹过风、淋过雨，在这些一次次的身体记忆中，我们才逐渐理解了这个世界，也才有后来的思考与表达。所以，真正的智能，必须先从感知这个世界开始。

如果再往更长远的未来看，也许就像哈萨比斯在访谈中说的，Genie、**Veo**（Veo: Google DeepMind的视频生成模型）、**Gemini**（Gemini: Google DeepMind开发的多模态大模型）这些目前相对独立的模型，注定将逐渐走向融合，形成所谓的“**全能模型**”（Omni Model: 能够处理语言、多媒体，进行物理推理和内容生成的综合性AI模型）。它既能处理语言、多媒体，又能进行物理推理和内容生成。这才是通往AGI的终极之路。