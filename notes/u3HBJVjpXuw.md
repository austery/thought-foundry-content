---
area: tech-work
category: ai-ml
companies_orgs: []
date: '2025-10-04'
draft: true
guest: ''
insight: ''
layout: post.njk
media_books:
- 《苦涩的教训》
people:
- Richard Sutton
- Ilya Sutskever
products_models:
- AlphaGo
- AlphaZero
project:
- ai-impact-analysis
series: ''
source: https://www.youtube.com/watch?v=u3HBJVjpXuw
speaker: Dwarkesh Patel
status: evergreen
summary: 本文深入探讨了**理查德·萨顿**（Richard Sutton）关于AI学习的观点，特别是其“**苦涩的教训**”（The Bitter Lesson）理论。演讲者反思了当前大型语言模型（LLM）的训练模式，认为其依赖人类数据且效率低下，并提出了持续学习和模仿学习作为未来AI发展的关键方向，以期实现更接近人类和动物的学习能力。
tags:
- learning
- reinforcement-learning
- world-model
title: 对萨顿访谈的思考：AI学习范式与未来
---

### 对萨顿观点的理解

大家对**萨顿**（Sutton）的访谈有很多想法。我自己也一直在思考，现在对萨顿的视角有了比采访时更清晰的理解。我想分享一下我如何理解他的世界观。理查德（Richard），如果还有任何错误或误解，请多包涵。从你的想法中学到东西非常有益。这是我对理查德（Richard）立场的“钢人”（Steelman）式理解。

### 苦涩的教训

他写了著名的文章《**苦涩的教训**》（The Bitter Lesson）。这篇文章是关于什么的？它并不是说你只想尽可能多地浪费计算资源。苦涩的教训指出，你应该开发那些最有效、最可扩展地利用计算资源的技术。

### LLM训练的低效性

在大型语言模型（LLM）上花费的大部分计算资源都用于部署运行阶段。然而，在整个部署阶段，模型并没有学习任何东西，它只在训练这个特殊阶段学习。这显然不是计算资源的有效利用方式。更糟糕的是，这个训练阶段本身就非常低效。这些模型通常需要相当于数千年人类经验的数据进行训练。

### 依赖人类数据的局限性

而且，在训练阶段，它们所有的学习都直接来自人类数据。这一点在预训练数据上很明显。但在我们对这些LLM进行的**RLHF**（Reinforcement Learning from Human Feedback: 通过人类反馈进行强化学习）中，情况也是如此：这些强化学习环境是人类提供的“游乐场”，用来教LLM我们预设的特定技能。智能体并没有通过与世界的有机、自主的互动来学习。

只能从人类数据中学习，而人类数据是一种缺乏弹性且难以扩展的资源，这并非利用计算资源的有效方式。此外，这些LLM在训练中学到的不是一个真正的“**世界模型**”（World Model: 能够预测环境如何响应不同行动的模型），而是它们正在构建一个关于人类接下来会说什么的模型。这导致它们依赖于人类衍生的概念。

可以这样想：假设你用1900年之前的所有数据训练了一个LLM，它可能无法从零开始推导出相对论。

### 持续学习的必要性

这里有一个更根本的原因，说明这种范式最终会被取代：LLM无法进行“**在职学习**”（On-the-job Learning: 在实际工作或应用中进行学习）。因此，我们需要新的架构来实现这种持续学习。一旦我们有了这样的架构，就不再需要特殊的训练阶段——智能体将能够像所有人类一样，甚至像所有动物一样，即时学习。这种新范式将使我们当前的方法，以及LLM超乎寻常的低样本效率的特殊训练阶段，变得完全过时。

这就是我对理查德（Richard）立场的理解。我与理查德（Rich）的主要分歧在于，我认为他用来区分LLM与真正智能的概念，实际上并没有那么互斥或截然对立。

### 模仿学习与RL的连续性

例如，我认为模仿学习与强化学习（RL）是连续且互补的。同样，对人类的建模可以为你提供一个先验知识，从而促进学习“真正”的世界模型。我也不惊讶于未来某种形式的“**测试时微调**”（Test-time Fine-tuning: 在模型进行预测时进行微调）能够复制持续学习，因为我们已经通过“**上下文学习**”（In-context Learning: 在输入上下文中提供示例来指导模型行为）在一定程度上实现了这一点。

让我们从我的观点开始：模仿学习与RL是连续且互补的。我曾几次问理查德（Richard），预训练的LLM是否可以作为一个良好的先验，让我们在其上积累经验学习（即进行RL），从而导向**AGI**（Artificial General Intelligence: 通用人工智能）。

**伊利亚·萨茨基弗**（Ilya Sutskever）几个月前做了一个非常有趣的演讲，他把预训练数据比作化石燃料。我认为这个类比非常有启发性。化石燃料不是可再生资源，但这并不意味着我们的文明在使用它们时走上了死胡同。事实上，它们至关重要。你无法直接从1800年代的水车直接过渡到太阳能电池板和聚变反应堆。我们必须使用这种廉价、方便且充足的中间媒介来达到下一步。

### AlphaGo与AlphaZero的启示

**AlphaGo**（它以人类游戏为条件进行训练）和**AlphaZero**（它从零开始进行引导训练）都是超人类的围棋选手。当然，AlphaZero更好。所以你可以问：我们，或者第一批AGI，最终会开发出一种不需要知识初始化、能够从一开始就自我引导的通用学习技术吗？它会超越当时训练出的最优秀的AI吗？我认为这两个问题的答案都可能是肯定的。

但这是否意味着模仿学习在开发第一个AGI，甚至第一个**ASI**（Artificial Superintelligence: 超级人工智能）中不能扮演任何角色呢？不。AlphaGo仍然是超人类的，尽管它最初是由人类玩家数据引导的。人类数据不一定是有害的。只是在足够大的规模下，它不再有显著的帮助。AlphaZero也比AlphaGo使用了更多的计算资源。

### 人类文化的学习类比

数万年来知识的积累显然对人类的成功至关重要。在任何知识领域，成千上万（甚至可能数百万）的前人参与了我们理解的构建，并将其传给下一代。我们显然不是发明了我们所说的语言，也不是我们使用的法律体系。同样，我们手机中的大多数技术也不是由今天活着的人直接发明的。这个过程更类似于模仿学习，而不是从零开始的RL。

当然，我们是在像LLM那样预测下一个token来完成这种文化学习吗？不，当然不是。即使是人类进行的模仿学习，也不像我们为LLM预训练所做的监督学习。但我们也不是到处寻找某个明确定义的标量奖励。没有哪种机器学习范式能完美描述人类的学习。我们所做的事情既类似于RL，也类似于监督学习。

就像飞机之于鸟类，监督学习可能最终会成为人类文化学习的“飞机”。

### 学习技术的连续性

我也不认为这些学习技术在本质上是不同的。模仿学习只是短视界的RL。一个“回合”（Episode）就是一次预测一个token。LLM基于其对世界的理解以及序列中不同信息片段之间的关系，对下一个token做出猜测。它会根据预测下一个token的准确程度获得奖励。

现在，我听到有人说：“不，不，那不是真实情况！它只是在学习人类可能说什么。”我同意。但有一个不同的问题，我认为这对于理解这些模型的**可扩展性**（Scalability: 指系统处理不断增长的工作负载的能力）更为重要：我们能否利用这种模仿学习来帮助模型从“**真实情况**”（Ground Truth: 指客观事实或数据本身）中学习得更好？

我认为答案是，显而易见的是，是的。在对预训练的基础模型进行RL后，我们已经让它们赢得了IMO（International Mathematical Olympiad: 国际数学奥林匹克）竞赛的金牌，并从零开始编写了完整的应用程序。这些是“真实情况”的检验。你能解决这个未见过的数学奥林匹克问题吗？你能构建这个应用程序来匹配特定的功能请求吗？但你无法从零开始通过RL让模型完成这些任务。至少我们目前还不知道如何做到。你需要一个合理的人类数据先验来启动这个RL过程。

你是否想把这个先验称为“世界模型”，还是仅仅是人类模型，我认为并不重要，老实说，这似乎是一个语义上的争论。因为你真正关心的是，这个人类模型是否能帮助你开始从真实情况中学习——也就是成为一个“真正的”世界模型。

这有点像对一个正在给牛奶巴氏消毒的人说：“嘿，别再煮牛奶了，因为我们最终想把它冷着喝！”当然。但这只是一个促进最终产出的中间步骤。

### LLM对世界的表征

顺便说一句，LLM显然正在发展对世界的深刻表征，因为它们的训练过程激励它们这样做。我使用LLM来学习从生物学到AI再到历史的各种知识，它们能够以惊人的灵活性和连贯性做到这一点。

那么，LLM是否专门被训练来模拟其行为如何影响世界？不，它们没有。但如果我们不允许称它们的表征为“世界模型”，那么我们就是根据构建模型所需的**过程**来定义“世界模型”这个术语，而不是根据这个概念所暗示的明显能力来定义。

### 持续学习的挑战

持续学习。抱歉又提起我的老调重弹。我像个只写了一个好段子的喜剧演员，但我会把它榨干。一个通过基于结果的奖励进行RL的LLM，每个回合大约学习1比特的信息，而一个回合可能包含数万个token。显然，动物和人类从与环境的互动中提取的信息量，远不止每个回合的奖励信号。

概念上，我们应该如何看待动物身上发生的事情？我认为我们是通过观察来学习建模世界。这种外层RL激励着其他学习系统从环境中提取最大信号。在理查德（Richard）的OaK架构中，他称之为“**转移模型**”（Transition Model: 描述状态如何从一个转移到另一个的模型）。

如果我们试图将这个功能规格归入现代LLM，你会做的是对所有观察到的token进行微调。据我所知，我的研究员朋友们说，实际上最简单的方法效果并不好。

以高吞吐量持续从环境中学习，对于真正的AGI显然是必要的。而这显然不存在于