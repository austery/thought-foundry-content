---
title: 深度解析：为何语言模型会产生幻觉？
summary: 深入剖析OpenAI与佐治亚理工的联合研究，揭示语言模型产生幻觉的两大根源：预训练阶段的统计误差传导，以及后训练阶段的评估机制激励错位，并探讨相应的解决方案。
area: "tech-engineering"
category: technology
project: []
tags:
- design
- large-language-model
- llm
- model-evaluation
companies_orgs: []
products_models: []
date: 2025-09-11
author: Lei
speaker: Best Partners TV
draft: true
guest: null
insight: null
layout: post.njk
series: null
source: null
status: evergreen
---
### 引言：揭开“幻觉之谜”

大家好，这里是最佳拍档，我是大飞。今天要和大家深度拆解一篇有希望能彻底解开“幻觉之谜”的重磅研究，即来自OpenAI和佐治亚理工联合发表的论文《为什么语言模型会有幻觉（Why Language Models Hallucinate）》。

这篇论文的价值在于，它没有把幻觉归咎于“模型不够大”、“训练数据不够多”这类表面的原因，而是用严谨的统计理论和实证案例证明，幻觉本质上是两个核心问题的产物：分别是预训练阶段的“统计误差传导”，以及后训练阶段的“评估机制激励错位”。换句话说，幻觉不是技术上的“意外”，而是现有训练和评估逻辑下的“必然结果”。

今天我们就从这两个核心问题入手，一步步搞懂幻觉的来龙去脉，以及到底该怎么解决它。

### 第一部分：预训练阶段的“统计误差传导”

我们首先要明确一个前提：大语言模型的预训练目标是为了学习人类语言的概率分布，简单来说，就是让模型知道“哪些句子更常见、更合理”。比如“我吃饭”比“饭吃我”更合理，“今天天气很好”比“今天天气很绿”更合理。

很多人会觉得，只要给模型喂足够多的“无错误数据”，它就能只输出正确的内容。但是这篇论文用数学证明了，即使训练数据100%无误差，预训练后的模型依然会产生错误，因为生成正确内容比判断内容是否正确更难。

#### 核心理论：IIV二分类问题

这里就要引入论文的核心理论工具：**IIV二分类问题**（Is-It-Valid: 即有效性判断）。简单来说，就是给模型一个句子，让它判断这个句子是“有效”的（正确，记为+）还是“无效”的（错误，记为-）。这是一个典型的有监督分类任务，训练数据里正确和错误的句子各占50%。

论文通过数学推导证明了一个关键结论：模型的生成误差率（输出错误内容的概率），至少是它在IIV分类任务中错误率的2倍。

为什么会这样呢？因为生成任务本质上包含了无数个“隐性的IIV判断”。比如模型要输出“亚当的生日是多少”，它首先得在脑子里判断一系列分类问题：这个日期是亚当的生日吗？这个日期的格式对吗？我有没有见过这个信息？等等。只要其中一个分类判断错了，最终的生成结果就会错。

#### 误差来源分析

论文通过几个例子直观地解释了这一点：

- **拼写判断:** 对于“Greetings”和“Greatings”这类问题，模型IIV分类准确率很高，因为拼写规律明显，所以生成时也很少犯拼写错误。这就是为什么现在的大模型基本不会写错别字。

- **字母计数:** 当被问及“DEEPSEEK里有几个D？”时，正确答案是1，但DeepSeek-V3模型在10次测试里会回答2或3，甚至有其他模型回答6、7。这是因为“数字母”这个任务的IIV分类难度更高，模型需要先将单词拆成单个字母再逐个计数，而很多模型的架构不擅长处理这种细粒度任务，导致IIV分类错误率高，最终生成了错误的答案。有趣的是，论文提到DeepSeek R1能正确回答，因为它采用了链式推理，这说明模型对任务的“拟合能力”越强，生成的误差就越低。

- **生日事实判断:** 这类问题的IIV分类难度最高，因为生日是“没有规律的任意事实”，既不像拼写有固定规则，也不像字母计数有明确步骤，全靠训练数据里的记忆。如果训练数据里亚当的生日只出现过一次甚至没出现过，模型就无法准确判断“某个日期是否正确”，IIV的分类错误率就会飙升，生成时自然就会“编一个看似合理的日期”。

#### 任意事实与单例率

论文里把“生日、论文标题”这类无规律、全靠记忆的信息称为“**任意事实**（Arbitrary Facts）”。这类事实的最大特点是没有可以学习的模式，完全依赖训练数据中的出现频率。比如“爱因斯坦的生日是03-14”这句话在训练数据里出现了成千上万次，模型的IIV分类准确率接近100%，生成时也不会错。但是像某个小众学者的生日，可能只在一篇讣告里出现过一次，模型的IIV分类准确率就会很低，生成时大概率会编一个日期。

论文中用“**单例率**（Singleton Rate, sr）”来量化这种误差。所谓“单例”，是指训练数据中“仅出现过一次且非‘我不知道’（IDK）”的提示-响应对。论文证明，对于任意事实，模型的幻觉率下限等于单例率。简单说，如果训练数据里20%的生日事实是“单例”，那么模型在回答这些生日问题的时候，至少有20%的概率会产生幻觉。

这个结论背后的逻辑来自艾伦·图灵提出的“Good-Turing缺失质量估计”。简单理解就是，训练数据中“只出现过一次的事件”，可以用来估计“没出现过的事件”的概率。因此，对于模型来说，“单例”越多，说明它没见过的“正确事实”越多，生成时就越容易用编出来的错误事实来填补空白。

#### 模型自身缺陷 (Poor Models)

除了“任意事实”，模型自身的缺陷也会导致预训练误差，比如模型架构无法拟合目标概念，或者虽然架构足够强但没训练好。最经典的例子是“三元语言模型”的缺陷，它只能根据前两个词来预测下一个词。面对需要长上下文理解的任务，如果模型架构的“拟合能力”不够，其IIV分类误差就会很高，进而导致生成误差率至少达到50%。

之前提到的“字母计数”问题，DeepSeek-V3之所以会数错，本质上也是因为模型缺陷。它的分词器会把“DEEPSEEK”拆成“D/EEP/SEE/K”这样的token，而不是单个字母，导致模型无法直接“数字母”。而DeepSeek-R1通过“链式推理”，用推理能力弥补了模型架构的缺陷，降低了IIV分类误差，最终生成了正确答案。

#### 其他预训练误差因素

论文还提到了三个导致预训练误差的重要因素：

1. **计算复杂度:** 有些问题本身就是“计算上不可解”的，比如“解密一段随机的加密文本”。模型无法在有限时间内破解，只能输出一个错误的解密结果。
2. **分布偏移 (Distribution Shift):** 测试时的提示词和训练数据里的提示词“长得不一样”。比如训练数据里很少出现“一磅羽毛和一磅铅哪个更重”这种“陷阱题”，模型就容易出错。这不是模型没学好，而是训练数据的分布没覆盖到这类情况。
3. **垃圾进垃圾出 (GIGO):** 现实中，训练数据里必然会包含错误、谣言甚至阴谋论。模型会学习这些错误信息，并在生成时复制出来，因为它无法判断这些内容是错误的。GIGO会让预训练的误差进一步升高。

### 第二部分：后训练阶段的“评估机制激励错位”

预训练有误差很正常，但为什么经过**RLHF**（Reinforcement Learning from Human Feedback: 基于人类反馈的强化学习）、DPO等后训练优化方法，幻觉还是没消失，甚至更严重了呢？

论文的答案很尖锐：不是后训练技术不行，而是现有的评估机制在“鼓励幻觉”。就像老师判卷时，认为瞎写一个答案比空着不写得分更高，学生自然会选择瞎写。大语言模型的评估基准也是如此，“编一个看似合理的错误答案”比“说我不知道”得分更高，所以模型自然会选择幻过。

#### 二元评分体系的问题

现在主流的模型评估基准几乎都采用“二元评分（Binary Grading）”，即正确得1分，错误或弃权得0分。在10个最有影响力的评估基准中，9个是纯二元评分。

论文用一个简单的数学推导证明了，在二元评分体系下，无论模型对答案的置信度有多低，“猜测”都是最优选择。假设模型对一个问题的答案只有10%的把握，那么“猜测”的期望得分是 0.1 分（10%×1 + 90%×0），而“弃权”的得分是0分。显然猜测更划算。

因此，在这种激励下，模型会优先选择“输出一个答案”，而不是“承认不知道”，哪怕这个答案是编的。一个诚实但会弃权的模型（Model A），在现有基准上的得分会低于一个经常幻觉但总能蒙对一些题目的模型（Model B）。而厂商在优化模型时，只会看“基准得分”，不会看“幻觉率”，这就导致后训练时，模型会逐渐放弃“诚实的表达不确定性”，转而学习“如何更好地猜测”，最终幻觉率居高不下。

更严重的是，很多评估基准还用“大模型作为裁判”来打分。论文指出，模型裁判经常会把“错误但冗长的解题过程”判为正确，因为“看起来很专业”，这进一步鼓励模型“编造详细的错误内容”，而不是“简洁地承认不会”。

#### 评估机制的“流行病”

研究者试图通过“增加专门的幻觉评估基准”来解决问题，但论文认为这没用。因为现有主流评估的影响力远大于这些新基准，厂商还是会优先优化主流基准的得分。这就像学校新增了一门“诚实度考试”，但高考还是只看分数，学生依然会在高考中蒙题。

论文把这种现象称为“惩罚不确定性的流行病”。要解决它，不能只靠增加新的评估，而是要修改现有主流评估的评分逻辑。

### 第三部分：解决方案

解决方案也应从预训练和后训练两点入手。论文给出了两个简单却根本的建议：

1. **在评估中“明确置信度目标”**
2. **“修改主流评估的评分逻辑”**

#### 明确置信度目标

论文建议，在评估的提示词中，明确告诉模型“什么时候该回答，什么时候该弃权”。比如在问题后加上：“只有当你对答案的置信度超过90%时才回答。正确得1分，错误扣9分，回答‘我不知道’得0分”。

这样一来，模型会自动在置信度高于90%时回答，否则弃权，不会再盲目猜测。我们可以算一笔账：

- 如果置信度是90%，回答的期望得分是 0 分（90%×1 + 10%×(-9)），与弃权得分相同。
- 如果置信度是91%，期望得分高于弃权。
- 如果置信度是89%，期望得分低于弃权。

不同的置信度阈值对应不同的应用场景，比如医疗领域需要高阈值，而日常聊天可以用低阈值。“明确置信度目标”需要写在提示词里，让模型清楚地知道评分规则。实验证明，在prompt里加入“错误扣2分”的说明后，模型的幻觉率下降了30%，同时正确回答率没有明显下降。

#### 修改主流评估基准

对于后训练，解决幻觉的关键不是新增“幻觉评估”，而是把“置信度目标”融入现有的主流评估基准。比如修改SWE-bench的评分规则，让提交错误补丁被扣分；MMLU-Pro可以增加“我不知道”的选项，并给予微小的正分。

为什么要修改主流评估基准？因为它们是厂商优化模型的“指挥棒”。如果这些评估不修改，厂商就不会优先去优化幻觉问题，因为主流评估的得分直接影响模型的市场竞争力。人类考试也经历过类似的改革，比如SAT考试从“答错不扣分”改为“答错扣分”，从而引导学生在没把握时放弃猜题。大模型的评估也应该借鉴这种思路。

论文还提出了“**行为校准**（Behavioral Calibration）”的概念，不是要求模型输出概率值，而是要求模型“在置信度>t时输出答案，否则弃权”。这会成为未来大语言模型评估的重要指标。

### 局限与未来展望

虽然这篇论文的分析非常深刻，但它也承认现有框架还有一些局限：

1. **内容类型:** 只考虑了“合理的错误内容”，忽略了“无意义的内容”（如乱码），但这对结论影响不大。
2. **问题类型:** 主要分析了“事实性问题”，对“开放式生成”（如写传记）的幻觉讨论较少，未来可能需要建立“幻觉程度评分体系”。
3. **RAG并非万能:** **检索增强生成**（RAG: Retrieval-Augmented Generation）只能解决“训练数据里没有的事实”，无法解决“检索不到信息”的情况。模型在检索不到时依然会猜测。只有结合“检索+置信度目标”才能彻底解决问题。
4. **上下文歧义:** 没有考虑“潜在上下文歧义”问题。比如用户问“我的电话坏了”，模型可能误解“电话”的指代。未来需要让模型学会“追问”，而不是盲目回答。

### 结论

这篇论文给我的最大启发是，AI的问题很多时候不是技术问题，而是“人的问题”。我们设计了什么样的训练目标，就会得到什么样的模型；我们设计了什么样的评估规则，就会引导模型走向什么样的方向。要解决幻觉问题，不仅需要优化模型，更需要优化我们对AI的“期望和评估方式”。毕竟，我们需要的是一个“真正可信的助手”，而不是一个“只会考满分的考生”。